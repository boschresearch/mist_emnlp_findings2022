#24
In	_	_
open-ended	_	_
text	_	_
generation	_	_
,	_	_
these	_	_
models	_	_
show	_	_
remarkable	_	_
robustness	_	_
under	_	_
sampling	_	_
(	_	_
Radford	_	_
et	_	_
al.	_	_
,	_	_
2019	_	_
;	_	_
Holtzman	_	_
et	_	_
al.	_	_
,	_	_
2020	_	_
)	_	_
.	_	_

#25
This	_	_
observation	_	_
,	_	_
coupled	_	_
with	_	_
the	_	_
examples	_	_
presented	_	_
in	_	_
Figure	_	_
1	_	_
,	_	_
suggests	_	_
that	_	_
treating	_	_
QG	_	_
for	_	_
QA	_	_
as	_	_
a	_	_
more	_	_
open-ended	_	_
generation	_	_
problem	_	_
and	_	_
relying	_	_
on	_	_
the	_	_
power	_	_
of	_	_
modern	_	_
text	_	_
generators	_	_
to	_	_
produce	_	_
diverse	_	_
yet	_	_
accurate	_	_
samples	_	_
might	speculation	_
yield	_	_
better	_	_
QA	_	_
results	_	_
than	_	_
the	_	_
current	_	_
approach	_	_
of	_	_
optimizing	_	_
for	_	_
the	_	_
“most	_	_
likely”	_	_
question	_	_
.	_	_

#26
We	_	_
test	_	_
this	_	_
hypothesis	_	_
by	_	_
fine-tuning	_	_
a	_	_
pre-trained	_	_
transformer-based	_	_
masked	_	_
LM	_	_
(	_	_
Liu	_	_
et	_	_
al.	_	_
,	_	_
http	_	_
:	_	_
//aqleaderboard.tomhosking.co.uk/squad	_	_
2019	_	_
)	_	_
for	_	_
QG	_	_
,	_	_
and	_	_
sampling	_	_
questions	_	_
from	_	_
it	_	_
using	_	_
top-p	_	_
nucleus	_	_
sampling	_	_
(	_	_
Holtzman	_	_
et	_	_
al.	_	_
,	_	_
2020	_	_
)	_	_
.	_	_