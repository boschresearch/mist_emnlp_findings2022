#22
This	_	_
attempt	_	_
is	_	_
in	_	_
contrast	_	_
with	_	_
the	_	_
common	_	_
approach	_	_
of	_	_
using	_	_
the	_	_
output	_	_
of	_	_
the	_	_
last	_	_
layers	_	_
of	_	_
a	_	_
transformer-based	_	_
encoder	_	_
as	_	_
the	_	_
representation	_	_
that	_	_
is	_	_
fed	_	_
into	_	_
a	_	_
downstream	_	_
task	_	_
.	_	_

#23
Our	_	_
hypothesis	_	_
is	_	_
those	_	_
final	_	_
representations	_	_
from	_	_
the	_	_
top	_	_
of	_	_
the	_	_
transformer-based	_	_
encoders	_	_
might	speculation	_
not	_	_
be	_	_
the	_	_
best	_	_
not	_	_
only	_	_
in	_	_
carrying	_	_
primitive	_	_
linguistic	_	_
properties	_	_
of	_	_
the	_	_
language	_	_
but	_	_
also	_	_
for	_	_
downstream	_	_
tasks	_	_
.	_	_

#24
All	_	_
the	_	_
source	_	_
code	_	_
is	_	_
publicly	_	_
available1	_	_
.	_	_

#138
Note	_	_
that	_	_
the	_	_
newly	_	_
constructed	_	_
sentence	_	_
embeddings	_	_
consist	_	_
of	_	_
attention	_	_
head	_	_
outputs	_	_
only	_	_
.	_	_

#139
Our	_	_
results	_	_
imply	_	_
that	_	_
these	_	_
embeddings	_	_
might	speculation	_
possess	_	_
substantial	_	_
information	_	_
as	_	_
much	_	_
as	_	_
the	_	_
hidden	_	_
states	_	_
of	_	_
the	_	_
layers	_	_
,	_	_
which	_	_
are	_	_
produced	_	_
by	_	_
passing	_	_
through	_	_
the	_	_
multi-head	_	_
attention	_	_
layers	_	_
and	_	_
the	_	_
feed-forward	_	_
network	_	_
.	_	_

#140
5	_	_
Boosting	_	_
Downstream	_	_
Task	_	_

#177
These	_	_
results	_	_
imply	_	_
that	_	_
the	_	_
fine-tuning	_	_
process	_	_
leverages	_	_
the	_	_
initial	_	_
superior	_	_
attention	_	_
heads	_	_
regardless	_	_
of	_	_
their	_	_
corresponding	_	_
locations	_	_
inside	_	_
the	_	_
model	_	_
rather	_	_
than	_	_
trains	_	_
arbitrary	_	_
attention	_	_
heads	_	_
.	_	_

#178
This	_	_
behavior	_	_
might	speculation	_
be	_	_
the	_	_
reason	_	_
for	_	_
explaining	_	_
the	_	_
additional	_	_
performance	_	_
increment	_	_
on	_	_
the	_	_
downstream	_	_
tasks	_	_
.	_	_

#179
We	_	_
conjecture	_	_
that	_	_
our	_	_
reconstruction	_	_
method	_	_
could	_	_
act	_	_
as	_	_
a	_	_
partial	_	_
residual	_	_
connection	_	_
as	_	_
in	_	_
DenseNet	_	_
(	_	_
Huang	_	_
et	_	_
al.	_	_
,	_	_
2017	_	_
)	_	_
during	_	_
the	_	_
fine-tuning	_	_
process	_	_
by	_	_
feeding	_	_
the	_	_
reconstructed	_	_
embedding	_	_
to	_	_
the	_	_
input	_	_
of	_	_
the	_	_
classifier	_	_
which	_	_
creates	_	_
the	_	_
direct	_	_
gradient	_	_
flow	_	_
from	_	_
the	_	_
final	_	_
objective	_	_
loss	_	_
of	_	_
downstream	_	_
tasks	_	_
toward	_	_
the	_	_
internal	_	_
superior	_	_
attention	_	_
heads	_	_
.	_	_