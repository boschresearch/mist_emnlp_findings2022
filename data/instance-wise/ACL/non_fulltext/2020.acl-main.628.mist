#43
Supervised	_	_
“siamese”	_	_
models	_	_
(	_	_
Reimers	_	_
and	_	_
Gurevych	_	_
,	_	_
2019	_	_
)	_	_
on	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
while	_	_
not	_	_
competitive	_	_
with	_	_
cross-sentence	_	_
attention	_	_
,	_	_
can	_	_
cache	_	_
sentence	_	_
embeddings	_	_
independently	_	_
of	_	_
one	_	_
another	_	_
.	_	_

#44
For	_	_
instance	_	_
,	_	_
to	_	_
calculate	_	_
the	_	_
pairwise	_	_
similarities	_	_
of	_	_
N	_	_
sentences	_	_
,	_	_
a	_	_
cross-sentence	_	_
attention	_	_
system	_	_
must	deontic	_
calculate	_	_
O	_	_
(	_	_
N2	_	_
)	_	_
slow	_	_
sentence	_	_
pair	_	_
embeddings	_	_
,	_	_
while	_	_
the	_	_
siamese	_	_
model	_	_
calculates	_	_
O	_	_
(	_	_
N	_	_
)	_	_
slow	_	_
sentence	_	_
embeddings	_	_
and	_	_
O	_	_
(	_	_
N2	_	_
)	_	_
fast	_	_
vector	_	_
similarities	_	_
.	_	_

#45
Our	_	_
meta-embeddings	_	_
are	_	_
also	_	_
cacheable	_	_
(	_	_
and	_	_
hence	_	_
scalable	_	_
)	_	_
,	_	_
but	_	_
they	_	_
do	_	_
not	_	_
require	_	_
supervision	_	_
.	_	_

#126
Our	_	_
main	_	_
baselines	_	_
are	_	_
our	_	_
single-source	_	_
embeddings	_	_
.	_	_

#127
Wieting	_	_
and	_	_
Kiela	_	_
(	_	_
2019	_	_
)	_	_
warn	_	_
that	_	_
high-dimensional	_	_
sentence	_	_
representations	_	_
can	_	_
have	_	_
an	_	_
advantage	_	_
over	_	_
low-dimensional	_	_
ones	_	_
,	_	_
i.e.	_	_
,	_	_
our	_	_
meta-embeddings	_	_
might	speculation	_
be	_	_
better	_	_
than	_	_
lowerdimensional	_	_
single-source	_	_
embeddings	_	_
due	_	_
to	_	_
size	_	_
alone	_	_
.	_	_

#128
To	_	_
exclude	_	_
this	_	_
possibility	_	_
,	_	_
we	_	_
also	_	_
up-project	_	_
smaller	_	_
embeddings	_	_
by	_	_
a	_	_
random	_	_
d	_	_
×	_	_
dj	_	_
matrix	_	_
sampled	_	_
from	_	_
:	_	_
U	_	_
(	_	_
−	_	_
1√	_	_
dj	_	_
,	_	_
1√	_	_
dj	_	_
)	_	_