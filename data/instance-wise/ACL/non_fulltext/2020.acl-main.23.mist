#51
Our	_	_
work	_	_
falls	_	_
in	_	_
the	_	_
category	_	_
of	_	_
semi-supervised	_	_
learning	_	_
yet	_	_
differs	_	_
from	_	_
the	_	_
existing	_	_
works	_	_
in	_	_
the	_	_
following	_	_
ways	_	_
:	_	_
(	_	_
1	_	_
)	_	_
Our	_	_
model	_	_
decouples	_	_
the	_	_
text	_	_
generation	_	_
module	_	_
from	_	_
the	_	_
condition	_	_
representation	_	_
module	_	_
which	_	_
two	_	_
are	_	_
tightly	_	_
fused	_	_
as	_	_
a	_	_
single	_	_
one	_	_
in	_	_
previous	_	_
studies	_	_
,	_	_
enabling	_	_
possible	_	_
exploitation	_	_
for	_	_
pre-trained	_	_
Language	_	_
Models	_	_
(	_	_
e.g.	_	_
,	_	_
GPT-2	_	_
(	_	_
Radford	_	_
et	_	_
al.	_	_
,	_	_
2019	_	_
)	_	_
)	_	_
.	_	_

#52
(	_	_
2	_	_
)	_	_
Our	_	_
model	_	_
allows	_	_
single-condition	_	_
generation	_	_
,	_	_
which	_	_
could	capability-speculation	_
inspire	_	_
new	_	_
applications	_	_
like	_	_
polite	_	_
speech	_	_
generator	_	_
(	_	_
Niu	_	_
and	_	_
Bansal	_	_
,	_	_
2018	_	_
)	_	_
and	_	_
data	_	_
augmentation	_	_
(	_	_
Guo	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
)	_	_
.	_	_

#53
(	_	_
3	_	_
)	_	_
Our	_	_
model	_	_
can	_	_
handle	_	_
emerging	_	_
conditions	_	_
while	_	_
achieving	_	_
state-of-the-art	_	_
performance	_	_
with	_	_
fewer	_	_
parameters	_	_
and	_	_
less	_	_
training	_	_
time	_	_
.	_	_

#69
The	_	_
goal	_	_
of	_	_
a	_	_
VAE	_	_
model	_	_
is	_	_
to	_	_
learn	_	_
a	_	_
decoder	_	_
pθ	_	_
(	_	_
ŷ|z	_	_
,	_	_
ci	_	_
)	_	_
that	_	_
takes	_	_
the	_	_
latent	_	_
variable	_	_
z	_	_
and	_	_
the	_	_
condition	_	_
ci	_	_
to	_	_
calculate	_	_
the	_	_
distribution	_	_
over	_	_
the	_	_
text	_	_
samples	_	_
Yi	_	_
.	_	_

#70
Thus	_	_
,	_	_
when	_	_
the	_	_
condition	_	_
ci	_	_
and	_	_
a	_	_
randomly	_	_
sampled	_	_
latent	_	_
variable	_	_
z	_	_
∼	_	_
p	_	_
(	_	_
z	_	_
)	_	_
specified	_	_
,	_	_
the	_	_
model	_	_
could	capability	_
generate	_	_
realistic	_	_
text	_	_
samples	_	_
matching	_	_
the	_	_
given	_	_
condition	_	_
.	_	_

#71
4	_	_
Pre-train	_	_
and	_	_
Plug-in	_	_
Variational	_	_

#72
Auto-Encoder	_	_
As	_	_
a	_	_
basis	_	_
for	_	_
semi-supervised	_	_
learning	_	_
,	_	_
a	_	_
large	_	_
unlabeled	_	_
corpus	_	_
should	deontic	_
include	_	_
diverse	_	_
text	_	_
which	_	_
covers	_	_
a	_	_
vast	_	_
spectrum	_	_
of	_	_
conditions	_	_
.	_	_

#73
Thus	_	_
,	_	_
text	_	_
under	_	_
each	_	_
condition	_	_
forms	_	_
a	_	_
conditional	_	_
latent	_	_
space	_	_
,	_	_
which	_	_
could	options	_
be	_	_
mapped	_	_
from	_	_
a	_	_
larger	_	_
global	_	_
latent	_	_
space	_	_
.	_	_

#74
Based	_	_
on	_	_
this	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
PRETRAINVAE	_	_
and	_	_
a	_	_
PLUGINVAE	_	_
to	_	_
derive	_	_
the	_	_
global	_	_
and	_	_
conditional	_	_
latent	_	_
space	_	_
,	_	_
respectively	_	_
.	_	_

#96
To	_	_
enhance	_	_
the	_	_
diversity	_	_
of	_	_
generated	_	_
text	_	_
,	_	_
we	_	_
introduce	_	_
an	_	_
extra	_	_
constant	_	_
term	_	_
β	_	_
to	_	_
control	_	_
the	_	_
amount	_	_
of	_	_
encoded	_	_
information	_	_
in	_	_
VAE	_	_
(	_	_
Dupont	_	_
,	_	_
2018	_	_
;	_	_
Chen	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Kim	_	_
and	_	_
Mnih	_	_
,	_	_
2018	_	_
)	_	_
.	_	_

#97
By	_	_
setting	_	_
β	_	_
to	_	_
an	_	_
appropriate	_	_
value	_	_
,	_	_
PLUGINVAE	_	_
could	capability	_
extract	_	_
compact	_	_
conditional	_	_
information	_	_
without	_	_
sacrificing	_	_
the	_	_
fluency	_	_
or	_	_
accuracy	_	_
.	_	_

#98
Although	_	_
we	_	_
can	_	_
already	_	_
generate	_	_
conditional	_	_
text	_	_
under	_	_
a	_	_
single	_	_
condition	_	_
by	_	_
Equation	_	_
3	_	_
,	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
even	_	_
further	_	_
improve	_	_
the	_	_
conditionality	_	_
by	_	_
introducing	_	_
negative	_	_
samples	_	_
.	_	_

#100
Thus	_	_
,	_	_
the	_	_
loss	_	_
function	_	_
of	_	_
PLUGINVAE	_	_
with	_	_
negative	_	_
samples	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LPLUGINVAE	_	_
(	_	_
vyi	_	_
,	_	_
v	_	_
′	_	_
yi	_	_
)	_	_
=	_	_
Lsingle	_	_
(	_	_
vyi	_	_
)	_	_
−	_	_
γ	_	_
Lsingle	_	_
(	_	_
v	_	_
′	_	_
yi	_	_
)	_	_
(	_	_
5	_	_
)	_	_
where	_	_
vyi	_	_
is	_	_
a	_	_
batch	_	_
of	_	_
encoded	_	_
samples	_	_
under	_	_
condition	_	_
ci	_	_
,	_	_
and	_	_
v	_	_
′	_	_
yi	_	_
is	_	_
a	_	_
batch	_	_
of	_	_
encoded	_	_
negative	_	_
samples	_	_
;	_	_
γ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
balancing	_	_
the	_	_
positive	_	_
and	_	_
negative	_	_
samples	_	_
.	_	_

#101
For	_	_
different	_	_
tasks	_	_
,	_	_
the	_	_
best	_	_
setting	_	_
for	_	_
γ	_	_
may	options	_
vary	_	_
.	_	_

#102
Intuitively	_	_
,	_	_
the	_	_
larger	_	_
the	_	_
difference	_	_
between	_	_
the	_	_
conditions	_	_
is	_	_
,	_	_
the	_	_
smaller	_	_
γ	_	_
should	deontic	_
be	_	_
.	_	_

#103
4.2	_	_
Workflow	_	_

#191
We	_	_
evaluate	_	_
the	_	_
results	_	_
with	_	_
two	_	_
metrics	_	_
,	_	_
accuracy	_	_
and	_	_
diversity	_	_
.	_	_

#192
For	_	_
accuracy	_	_
,	_	_
we	_	_
train	_	_
a	_	_
sentiment	_	_
classifier	_	_
and	_	_
categorical	_	_
classifier	_	_
(	_	_
Kim	_	_
,	_	_
2014	_	_
)	_	_
,	_	_
which	_	_
could	capability	_
achieve	_	_
accuracy	_	_
of	_	_
90	_	_
%	_	_
and	_	_
97	_	_
%	_	_
on	_	_
the	_	_
validation	_	_
set	_	_
,	_	_
respectively	_	_
.	_	_

#193
The	_	_
accuracy	_	_
of	_	_
length	_	_
task	_	_
can	_	_
be	_	_
directly	_	_
calculated	_	_
with	_	_
the	_	_
word	_	_
count	_	_
of	_	_
generated	_	_
text	_	_
.	_	_

#259
From	_	_
the	_	_
results	_	_
in	_	_
Table	_	_
6	_	_
,	_	_
we	_	_
find	_	_
that	_	_
β	_	_
controls	_	_
the	_	_
balance	_	_
between	_	_
diversity	_	_
and	_	_
accuracy	_	_
.	_	_

#260
Specifically	_	_
,	_	_
when	_	_
β	_	_
is	_	_
too	_	_
large	_	_
,	_	_
more	_	_
diverse	_	_
samples	_	_
could	capability-options	_
be	_	_
generated	_	_
,	_	_
but	_	_
the	_	_
accuracy	_	_
may	speculation	_
be	_	_
sacrificed	_	_
slightly	_	_
.	_	_

#261
On	_	_
the	_	_
contrary	_	_
,	_	_
when	_	_
β	_	_
is	_	_
too	_	_
small	_	_
,	_	_
the	_	_
accuracy	_	_
could	capability-options	_
climb	_	_
to	_	_
a	_	_
higher	_	_
value	_	_
,	_	_
but	_	_
meanwhile	_	_
,	_	_
the	_	_
diversity	_	_
drops	_	_
drastically	_	_
.	_	_

#262
Empirically	_	_
,	_	_
we	_	_
find	_	_
that	_	_
β	_	_
=	_	_
5	_	_
is	_	_
an	_	_
appropriate	_	_
value	_	_
for	_	_
all	_	_
tasks	_	_
.	_	_

#276
We	_	_
choose	_	_
three	_	_
typical	_	_
errors	_	_
and	_	_
list	_	_
them	_	_
in	_	_
Table	_	_
9	_	_
.	_	_

#277
In	_	_
the	_	_
first	_	_
sentence	_	_
,	_	_
“shocked”	_	_
is	_	_
a	_	_
subtle	_	_
word	_	_
which	_	_
may	capability-options	_
indicate	_	_
either	_	_
positive	_	_
or	_	_
negative	_	_
sentiment	_	_
depending	_	_
on	_	_
the	_	_
context	_	_
.	_	_

#278
Thus	_	_
,	_	_
with	_	_
a	_	_
greedy	_	_
decoding	_	_
strategy	_	_
,	_	_
it	_	_
may	options	_
be	_	_
incorrectly	_	_
decoded	_	_
into	_	_
the	_	_
other	_	_
polarity	_	_
.	_	_

#279
We	_	_
believe	_	_
this	_	_
kind	_	_
of	_	_
errors	_	_
could	feasibility	_
be	_	_
fixed	_	_
with	_	_
more	_	_
elaborate	_	_
decoding	_	_
strategies	_	_
(	_	_
e.g.	_	_
,	_	_
Weighted	_	_
Decoding	_	_
(	_	_
See	_	_
et	_	_
al.	_	_
,	_	_
2019	_	_
)	_	_
)	_	_
.	_	_

#280
In	_	_
the	_	_
second	_	_
sentence	_	_
,	_	_
the	_	_
length	_	_
is	_	_
limited	_	_
by	_	_
the	_	_
nature	_	_
of	_	_
an	_	_
interrogative	_	_
sentence	_	_
.	_	_

#282
In	_	_
the	_	_
third	_	_
sentence	_	_
,	_	_
we	_	_
remark	_	_
an	_	_
overlapping	_	_
problem	_	_
between	_	_
classes	_	_
.	_	_

#283
Some	_	_
topics	_	_
(	_	_
e.g.	_	_
,	_	_
music	_	_
album	_	_
)	_	_
may	options	_
appear	_	_
in	_	_
both	_	_
business	_	_
and	_	_
entertainment	_	_
news	_	_
.	_	_

#284
In	_	_
some	_	_
way	_	_
,	_	_
these	_	_
samples	_	_
can	_	_
also	_	_
be	_	_
considered	_	_
as	_	_
correctly	_	_
conditioned	_	_
ones	_	_
,	_	_
which	_	_
highlights	_	_
the	_	_
importance	_	_
of	_	_
a	_	_
fine-grained	_	_
human	_	_
evaluation	_	_
on	_	_
this	_	_
task	_	_
.	_	_