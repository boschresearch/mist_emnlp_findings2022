#16
Using	_	_
phrases	_	_
instead	_	_
of	_	_
words	_	_
enables	_	_
conventional	_	_
SMT	_	_
to	_	_
condition	_	_
on	_	_
a	_	_
wider	_	_
range	_	_
of	_	_
context	_	_
,	_	_
and	_	_
results	_	_
in	_	_
better	_	_
performance	_	_
in	_	_
re-ordering	_	_
and	_	_
modeling	_	_
long-distance	_	_
dependencies	_	_
.	_	_

#17
It	_	_
is	_	_
intuitive	_	_
to	_	_
let	_	_
the	_	_
NMT	_	_
model	_	_
additionally	_	_
condition	_	_
on	_	_
phrase	_	_
level	_	_
representations	_	_
to	_	_
capture	_	_
long-distance	_	_
dependencies	_	_
better	_	_
,	_	_
but	_	_
there	_	_
are	_	_
two	_	_
main	_	_
issues	_	_
which	_	_
prevent	_	_
NMT	_	_
from	_	_
directly	_	_
using	_	_
phrases	_	_
:	_	_
•	_	_
There	_	_
are	_	_
more	_	_
phrases	_	_
than	_	_
tokens	_	_
,	_	_
and	_	_
the	_	_
phrase	_	_
table	_	_
is	_	_
much	_	_
larger	_	_
than	_	_
the	_	_
word	_	_
vocabulary	_	_
,	_	_
which	_	_
is	_	_
not	_	_
affordable	_	_
for	_	_
NMT	_	_
;	_	_
•	_	_
Distribution	_	_
over	_	_
phrases	_	_
is	_	_
much	_	_
sparser	_	_
than	_	_
that	_	_
over	_	_
words	_	_
,	_	_
which	_	_
may	speculation	_
lead	_	_
to	_	_
data	_	_
sparsity	_	_
and	_	_
hurt	_	_
the	_	_
performance	_	_
of	_	_
NMT	_	_
.	_	_

#18
Instead	_	_
of	_	_
using	_	_
phrases	_	_
directly	_	_
in	_	_
NMT	_	_
,	_	_
in	_	_
this	_	_
work	_	_
,	_	_
we	_	_
address	_	_
the	_	_
issues	_	_
above	_	_
with	_	_
the	_	_
following	_	_
contributions	_	_
:	_	_
•	_	_
To	_	_
address	_	_
the	_	_
large	_	_
phrase	_	_
table	_	_
issue	_	_
,	_	_
we	_	_
propose	_	_
an	_	_
attentive	_	_
feature	_	_
extraction	_	_
model	_	_
and	_	_
generate	_	_
phrase	_	_
representation	_	_
based	_	_
on	_	_
token	_	_
representations	_	_
.	_	_

#48
The	_	_
self-attention	_	_
network	_	_
uses	_	_
the	_	_
query	_	_
sequence	_	_
also	_	_
as	_	_
the	_	_
key	_	_
sequence	_	_
and	_	_
the	_	_
value	_	_
sequence	_	_
in	_	_
computation	_	_
,	_	_
while	_	_
the	_	_
cross-attention	_	_
feeds	_	_
another	_	_
vector	_	_
sequence	_	_
to	_	_
attend	_	_
as	_	_
queries	_	_
and	_	_
values	_	_
.	_	_

#49
Comparing	_	_
the	_	_
computation	_	_
of	_	_
the	_	_
attentional	_	_
network	_	_
with	_	_
RNNs	_	_
,	_	_
it	_	_
is	_	_
obvious	_	_
that	_	_
the	_	_
attention	_	_
computation	_	_
connects	_	_
distant	_	_
words	_	_
with	_	_
a	_	_
shorter	_	_
network	_	_
path	_	_
,	_	_
and	_	_
intuitively	_	_
it	_	_
should	inference	_
perform	_	_
better	_	_
in	_	_
capturing	_	_
long-distance	_	_
dependencies	_	_
.	_	_

#50
However	_	_
,	_	_
empirical	_	_
results	_	_
show	_	_
that	_	_
its	_	_
ability	_	_
in	_	_
modeling	_	_
long-range	_	_
dependencies	_	_
does	_	_
not	_	_
significantly	_	_
outperform	_	_
RNNs	_	_
.	_	_

#77
Each	_	_
encoder	_	_
layer	_	_
will	_	_
produce	_	_
a	_	_
vector	_	_
sequence	_	_
as	_	_
the	_	_
phrase	_	_
representation	_	_
.	_	_

#78
We	_	_
do	_	_
not	_	_
use	_	_
the	_	_
multi-head	_	_
attention	_	_
in	_	_
the	_	_
computation	_	_
of	_	_
the	_	_
phrase-representation	_	_
attention	_	_
because	_	_
of	_	_
two	_	_
reasons	_	_
:	_	_
•	_	_
The	_	_
multi-head	_	_
attention	_	_
calculates	_	_
weights	_	_
through	_	_
dot-product	_	_
,	_	_
we	_	_
suggest	_	_
that	_	_
a	_	_
2-layer	_	_
neural	_	_
network	_	_
might	speculation	_
be	_	_
more	_	_
powerful	_	_
at	_	_
semantic	_	_
level	_	_
feature	_	_
extraction	_	_
,	_	_
and	_	_
it	_	_
is	_	_
less	_	_
likely	_	_
to	_	_
be	_	_
affected	_	_
by	_	_
positional	_	_
embeddings	_	_
which	_	_
are	_	_
likely	_	_
to	_	_
vote	_	_
up	_	_
adjacent	_	_
vectors	_	_
;	_	_
•	_	_
Though	_	_
we	_	_
employ	_	_
a	_	_
2-layer	_	_
neural	_	_
network	_	_
,	_	_
it	_	_
only	_	_
has	_	_
one	_	_
linear	_	_
transformation	_	_
and	_	_
a	_	_
vector	_	_
to	_	_
calculate	_	_
attention	_	_
weights	_	_
,	_	_
which	_	_
contains	_	_
fewer	_	_
parameters	_	_
than	_	_
the	_	_
multi-head	_	_
attention	_	_
model	_	_
that	_	_
has	_	_
4	_	_
linear	_	_
transformations	_	_
.	_	_

#79
Recent	_	_
studies	_	_
show	_	_
that	_	_
different	_	_
encoder	_	_
layers	_	_
capture	_	_
linguistic	_	_
properties	_	_
of	_	_
different	_	_
levels	_	_
(	_	_
Peters	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
)	_	_
,	_	_
and	_	_
aggregating	_	_
layers	_	_
is	_	_
of	_	_
profound	_	_
value	_	_
to	_	_
better	_	_
fuse	_	_
semantic	_	_
information	_	_
(	_	_
Shen	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Dou	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Wang	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Dou	_	_
et	_	_
al.	_	_
,	_	_
2019	_	_
)	_	_
.	_	_

#80
We	_	_
assume	_	_
that	_	_
different	_	_
decoder	_	_
layers	_	_
may	speculation	_
value	_	_
different	_	_
levels	_	_
of	_	_
information	_	_
i.e.	_	_
the	_	_
representation	_	_
of	_	_
different	_	_
encoder	_	_
layers	_	_
differently	_	_
,	_	_
thus	_	_
we	_	_
weighted	_	_
combined	_	_
phrase	_	_
representations	_	_
from	_	_
every	_	_
encoder	_	_
layer	_	_
for	_	_
each	_	_
decoder	_	_
layer	_	_
with	_	_
the	_	_
Transparent	_	_
Attention	_	_
(	_	_
TA	_	_
)	_	_
mechanism	_	_
(	_	_
Bapna	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
)	_	_
.	_	_

#81
For	_	_
the	_	_
decoder	_	_
layer	_	_
j	_	_
,	_	_
the	_	_
phrase	_	_
representation	_	_
Rj	_	_
dphrase	_	_
fed	_	_
into	_	_
that	_	_
layer	_	_
is	_	_
calculated	_	_
by	_	_
:	_	_
Rj	_	_
dphrase	_	_
=	_	_
d∑	_	_
i=0	_	_
wj	_	_
iR	_	_
i	_	_
ephrase	_	_
(	_	_
6	_	_
)	_	_
where	_	_
wj	_	_
i	_	_
are	_	_
softmax	_	_
normalized	_	_
parameters	_	_
trained	_	_
jointly	_	_
with	_	_
the	_	_
full	_	_
model	_	_
to	_	_
learn	_	_
the	_	_
importance	_	_
of	_	_
encoder	_	_
layers	_	_
for	_	_
the	_	_
jth	_	_
decoder	_	_
layer	_	_
.	_	_

#88
Compared	_	_
to	_	_
the	_	_
computation	_	_
order	_	_
of	_	_
the	_	_
standard	_	_
Transformer	_	_
,	_	_
the	_	_
new	_	_
computation	_	_
order	_	_
performs	_	_
additional	_	_
attending	_	_
at	_	_
phrase	_	_
level	_	_
before	_	_
attending	_	_
source	_	_
token	_	_
representations	_	_
at	_	_
token	_	_
level	_	_
.	_	_

#89
We	_	_
conjecture	_	_
that	_	_
attending	_	_
at	_	_
phrase	_	_
level	_	_
should	inference	_
be	_	_
easier	_	_
than	_	_
at	_	_
token	_	_
level	_	_
,	_	_
and	_	_
attention	_	_
results	_	_
at	_	_
phrase	_	_
level	_	_
may	_	_
aid	_	_
the	_	_
attention	_	_
computation	_	_
at	_	_
the	_	_
token-level	_	_
.	_	_

#90
For	_	_
a	_	_
given	_	_
input	_	_
sequence	_	_
x	_	_
and	_	_
a	_	_
phrase	_	_
vector	_	_
sequence	_	_
Rphrase	_	_
,	_	_
the	_	_
attentive	_	_
combination	_	_
network	_	_
first	_	_
attends	_	_
the	_	_
phrase	_	_
representation	_	_
sequence	_	_
and	_	_
computes	_	_
the	_	_
attention	_	_
output	_	_
outphrase	_	_
as	_	_
follows	_	_
:	_	_
outphrase	_	_
=	_	_
AttnMH	_	_
(	_	_
x	_	_
,	_	_
Rphrase	_	_
)	_	_
(	_	_
7	_	_
)	_	_
where	_	_
AttnMH	_	_
is	_	_
a	_	_
multi-head	_	_
cross-attention	_	_
network	_	_
with	_	_
x	_	_
as	_	_
keys	_	_
and	_	_
Rphrase	_	_
as	_	_
corresponding	_	_
queries	_	_
and	_	_
values	_	_
.	_	_

#144
According	_	_
to	_	_
Tang	_	_
et	_	_
al.	_	_
(	_	_
2018	_	_
)	_	_
,	_	_
the	_	_
number	_	_
of	_	_
attention	_	_
heads	_	_
in	_	_
Transformers	_	_
impacts	_	_
their	_	_
ability	_	_
to	_	_
capture	_	_
long-distance	_	_
dependencies	_	_
,	_	_
and	_	_
specifically	_	_
,	_	_
many-headed	_	_
multi-head	_	_
attention	_	_
is	_	_
essential	_	_
for	_	_
modeling	_	_
long-distance	_	_
phenomena	_	_
with	_	_
only	_	_
self-attention	_	_
.	_	_

#145
The	_	_
Transformer	_	_
Big	_	_
model	_	_
with	_	_
twice	_	_
the	_	_
number	_	_
of	_	_
heads	_	_
in	_	_
the	_	_
multi-head	_	_
attention	_	_
network	_	_
compared	_	_
to	_	_
those	_	_
in	_	_
the	_	_
Transformer	_	_
Base	_	_
model	_	_
,	_	_
should	inference	_
be	_	_
better	_	_
at	_	_
capturing	_	_
long-distance	_	_
dependencies	_	_
.	_	_

#146
However	_	_
,	_	_
comparing	_	_
with	_	_
the	_	_
Transformer	_	_
Base	_	_
,	_	_
the	_	_
improvement	_	_
of	_	_
the	_	_
Transformer	_	_
Big	_	_
on	_	_
long	_	_
sentences	_	_
(	_	_
+1.20	_	_
BLEU	_	_
for	_	_
sentences	_	_
with	_	_
more	_	_
than	_	_
45	_	_
tokens	_	_
)	_	_
was	_	_
similar	_	_
to	_	_
that	_	_
on	_	_
short	_	_
sentences	_	_
(	_	_
+1.14	_	_
BLEU	_	_
for	_	_
sentences	_	_
with	_	_
no	_	_
more	_	_
than	_	_
15	_	_
tokens	_	_
)	_	_
,	_	_
while	_	_
our	_	_
approach	_	_
to	_	_
model	_	_
phrases	_	_
in	_	_
the	_	_
Transformer	_	_
model	_	_
even	_	_
brings	_	_
significantly	_	_
(	_	_
p	_	_
<	_	_
0.01	_	_
)	_	_
more	_	_
improvements	_	_
(	_	_
+1.72	_	_
BLEU	_	_
)	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
longer	_	_
sentences	_	_
with	_	_
the	_	_
Transformer	_	_
Base	_	_
setting	_	_
(	_	_
8	_	_
heads	_	_
)	_	_
than	_	_
the	_	_
Transformer	_	_
Big	_	_
with	_	_
16	_	_
heads	_	_
(	_	_
+1.20	_	_
BLEU	_	_
)	_	_
.	_	_

#151
To	_	_
verify	_	_
whether	_	_
our	_	_
method	_	_
can	_	_
improve	_	_
the	_	_
capability	_	_
of	_	_
the	_	_
NMT	_	_
model	_	_
to	_	_
capture	_	_
long-distance	_	_
dependencies	_	_
,	_	_
we	_	_
also	_	_
conducted	_	_
a	_	_
linguistically-informed	_	_
verb-subject	_	_
agreement	_	_
analysis	_	_
on	_	_
the	_	_
Lingeval97	_	_
dataset	_	_
(	_	_
Sennrich	_	_
,	_	_
2017	_	_
)	_	_
following	_	_
Tang	_	_
et	_	_
al.	_	_
(	_	_
2018	_	_
)	_	_
.	_	_

#152
In	_	_
German	_	_
,	_	_
subjects	_	_
and	_	_
verbs	_	_
must	deontic	_
agree	_	_
with	_	_
one	_	_
another	_	_
in	_	_
grammatical	_	_
number	_	_
and	_	_
person	_	_
.	_	_

#153
In	_	_
Lingeval97	_	_
,	_	_
each	_	_
contrastive	_	_
translation	_	_
pair	_	_
consists	_	_
of	_	_
a	_	_
correct	_	_
reference	_	_
translation	_	_
,	_	_
and	_	_
a	_	_
contrastive	_	_
example	_	_
that	_	_
has	_	_
been	_	_
minimally	_	_
modified	_	_
to	_	_
introduce	_	_
one	_	_
translation	_	_
error	_	_
.	_	_