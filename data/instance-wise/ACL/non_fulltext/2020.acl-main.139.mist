#5
When	_	_
in-domain	_	_
labelled	_	_
data	_	_
is	_	_
available	_	_
,	_	_
transfer	_	_
learning	_	_
techniques	_	_
can	_	_
be	_	_
used	_	_
to	_	_
adapt	_	_
existing	_	_
NER	_	_
models	_	_
to	_	_
the	_	_
target	_	_
domain	_	_
.	_	_

#6
But	_	_
what	_	_
should	deontic	_
one	_	_
do	_	_
when	_	_
there	_	_
is	_	_
no	_	_
hand-labelled	_	_
data	_	_
for	_	_
the	_	_
target	_	_
domain	_	_
?	_	_

#7
This	_	_
paper	_	_
presents	_	_
a	_	_
simple	_	_
but	_	_
powerful	_	_
approach	_	_
to	_	_
learn	_	_
NER	_	_
models	_	_
in	_	_
the	_	_
absence	_	_
of	_	_
labelled	_	_
data	_	_
through	_	_
weak	_	_
supervision	_	_
.	_	_

#25
New	_	_
labelling	_	_
functions	_	_
can	_	_
be	_	_
easily	_	_
inserted	_	_
to	_	_
leverage	_	_
the	_	_
knowledge	_	_
sources	_	_
at	_	_
our	_	_
disposal	_	_
for	_	_
a	_	_
given	_	_
textual	_	_
domain	_	_
.	_	_

#26
Furthermore	_	_
,	_	_
labelling	_	_
functions	_	_
can	_	_
often	_	_
be	_	_
ported	_	_
across	_	_
domains	_	_
,	_	_
which	_	_
is	_	_
not	_	_
the	_	_
case	_	_
for	_	_
manual	_	_
annotations	_	_
that	_	_
must	deontic	_
be	_	_
reiterated	_	_
for	_	_
every	_	_
target	_	_
domain	_	_
.	_	_

#27
The	_	_
contributions	_	_
of	_	_
this	_	_
paper	_	_
are	_	_
as	_	_
follows	_	_
:	_	_
1.	_	_
A	_	_
broad	_	_
collection	_	_
of	_	_
labelling	_	_
functions	_	_
for	_	_
NER	_	_
,	_	_
including	_	_
neural	_	_
models	_	_
trained	_	_
on	_	_
various	_	_
textual	_	_
domains	_	_
,	_	_
gazetteers	_	_
,	_	_
heuristic	_	_
functions	_	_
,	_	_
and	_	_
document-level	_	_
constraints	_	_
.	_	_

#53
The	_	_
approach	_	_
most	_	_
closely	_	_
related	_	_
to	_	_
this	_	_
paper	_	_
is	_	_
Safranchik	_	_
et	_	_
al.	_	_
(	_	_
2020	_	_
)	_	_
,	_	_
which	_	_
describe	_	_
a	_	_
similar	_	_
weak	_	_
supervision	_	_
framework	_	_
for	_	_
sequence	_	_
labelling	_	_
based	_	_
on	_	_
an	_	_
extension	_	_
of	_	_
HMMs	_	_
called	_	_
linked	_	_
hidden	_	_
Markov	_	_
models	_	_
.	_	_

#54
The	_	_
authors	_	_
introduce	_	_
a	_	_
new	_	_
type	_	_
of	_	_
noisy	_	_
rules	_	_
,	_	_
called	_	_
linking	_	_
rules	_	_
,	_	_
to	_	_
determine	_	_
how	_	_
sequence	_	_
elements	_	_
should	deontic	_
be	_	_
grouped	_	_
into	_	_
spans	_	_
of	_	_
same	_	_
tag	_	_
.	_	_

#55
The	_	_
main	_	_
differences	_	_
between	_	_
their	_	_
approach	_	_
and	_	_
this	_	_
paper	_	_
are	_	_
the	_	_
linking	_	_
rules	_	_
,	_	_
which	_	_
are	_	_
not	_	_
employed	_	_
here	_	_
,	_	_
and	_	_
the	_	_
choice	_	_
of	_	_
labelling	_	_
functions	_	_
,	_	_
in	_	_
particular	_	_
the	_	_
document-level	_	_
relations	_	_
detailed	_	_
in	_	_
Section	_	_
3.1	_	_
.	_	_

#94
As	_	_
noted	_	_
in	_	_
(	_	_
Krishnan	_	_
and	_	_
Manning	_	_
,	_	_
2006	_	_
;	_	_
Wang	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
)	_	_
,	_	_
named	_	_
entities	_	_
occurring	_	_
multiple	_	_
times	_	_
through	_	_
a	_	_
document	_	_
have	_	_
a	_	_
high	_	_
probability	_	_
of	_	_
belonging	_	_
to	_	_
the	_	_
same	_	_
category	_	_
.	_	_

#95
For	_	_
instance	_	_
,	_	_
while	_	_
Komatsu	_	_
may	capability-options	_
both	_	_
refer	_	_
to	_	_
a	_	_
Japanese	_	_
town	_	_
or	_	_
a	_	_
multinational	_	_
corporation	_	_
,	_	_
a	_	_
text	_	_
including	_	_
this	_	_
mention	_	_
will	_	_
either	_	_
be	_	_
about	_	_
the	_	_
town	_	_
or	_	_
the	_	_
company	_	_
,	_	_
but	_	_
rarely	_	_
both	_	_
at	_	_
the	_	_
same	_	_
time	_	_
.	_	_

#96
To	_	_
capture	_	_
these	_	_
non-local	_	_
dependencies	_	_
,	_	_
we	_	_
define	_	_
the	_	_
following	_	_
label	_	_
consistency	_	_
model	_	_
:	_	_
given	_	_
a	_	_
text	_	_
span	_	_
e	_	_
occurring	_	_
in	_	_
a	_	_
given	_	_
document	_	_
,	_	_
we	_	_
look	_	_
for	_	_
all	_	_
spans	_	_
Ze	_	_
in	_	_
the	_	_
document	_	_
that	_	_
contain	_	_
the	_	_
same	_	_
string	_	_
as	_	_
e	_	_
.	_	_

#98
The	_	_
above	_	_
formula	_	_
depends	_	_
on	_	_
a	_	_
distribution	_	_
Plabel	_	_
(	_	_
z	_	_
)	_	_
,	_	_
which	_	_
can	_	_
be	_	_
defined	_	_
on	_	_
the	_	_
basis	_	_
of	_	_
other	_	_
labelling	_	_
functions	_	_
.	_	_

#99
Alternatively	_	_
,	_	_
a	_	_
two-stage	_	_
model	_	_
similar	_	_
to	_	_
(	_	_
Krishnan	_	_
and	_	_
Manning	_	_
,	_	_
2006	_	_
)	_	_
could	feasibility-options	_
be	_	_
employed	_	_
to	_	_
first	_	_
aggregate	_	_
local	_	_
labelling	_	_
functions	_	_
and	_	_
subsequently	_	_
apply	_	_
document-level	_	_
functions	_	_
on	_	_
aggregated	_	_
predictions	_	_
.	_	_

#100
Another	_	_
insight	_	_
from	_	_
Grosz	_	_
and	_	_
Sidner	_	_
(	_	_
1986	_	_
)	_	_
is	_	_
the	_	_
importance	_	_
of	_	_
the	_	_
attentional	_	_
structure	_	_
.	_	_

#119
The	_	_
parameters	_	_
are	_	_
estimated	_	_
with	_	_
the	_	_
Baum-Welch	_	_
algorithm	_	_
,	_	_
which	_	_
is	_	_
a	_	_
variant	_	_
of	_	_
EM	_	_
algorithm	_	_
that	_	_
relies	_	_
on	_	_
the	_	_
forward-backward	_	_
algorithm	_	_
to	_	_
compute	_	_
the	_	_
statistics	_	_
for	_	_
the	_	_
expectation	_	_
step	_	_
.	_	_

#120
To	_	_
ensure	_	_
faster	_	_
convergence	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
new	_	_
constraint	_	_
to	_	_
the	_	_
likelihood	_	_
function	_	_
:	_	_
for	_	_
each	_	_
token	_	_
position	_	_
i	_	_
,	_	_
the	_	_
corresponding	_	_
latent	_	_
label	_	_
si	_	_
must	deontic	_
have	_	_
a	_	_
non-zero	_	_
probability	_	_
in	_	_
at	_	_
least	_	_
one	_	_
labelling	_	_
function	_	_
(	_	_
the	_	_
likelihood	_	_
of	_	_
this	_	_
label	_	_
is	_	_
otherwise	_	_
set	_	_
to	_	_
zero	_	_
for	_	_
that	_	_
position	_	_
)	_	_
.	_	_

#121
In	_	_
other	_	_
words	_	_
,	_	_
the	_	_
aggregation	_	_
model	_	_
will	_	_
only	_	_
predict	_	_
a	_	_
particular	_	_
label	_	_
if	_	_
this	_	_
label	_	_
is	_	_
produced	_	_
by	_	_
least	_	_
one	_	_
labelling	_	_
function	_	_
.	_	_

#130
Once	_	_
the	_	_
labelling	_	_
functions	_	_
are	_	_
aggregated	_	_
on	_	_
documents	_	_
from	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
we	_	_
can	_	_
train	_	_
a	_	_
sequence	_	_
labelling	_	_
model	_	_
on	_	_
the	_	_
unified	_	_
annotations	_	_
,	_	_
without	_	_
imposing	_	_
any	_	_
constraints	_	_
on	_	_
the	_	_
type	_	_
of	_	_
model	_	_
to	_	_
use	_	_
.	_	_

#131
To	_	_
take	_	_
advantage	_	_
of	_	_
the	_	_
posterior	_	_
marginal	_	_
distribution	_	_
p̃s	_	_
over	_	_
the	_	_
latent	_	_
labels	_	_
,	_	_
the	_	_
optimisation	_	_
should	deontic	_
seek	_	_
to	_	_
minimise	_	_
the	_	_
expected	_	_
loss	_	_
with	_	_
respect	_	_
to	_	_
p̃s	_	_
:	_	_
θ̂	_	_
=	_	_
argmin	_	_
θ	_	_
n∑	_	_
i	_	_
Ey∼p̃s	_	_
[	_	_
loss	_	_
(	_	_
hθ	_	_
(	_	_
xi	_	_
)	_	_
,	_	_
y	_	_
)	_	_
]	_	_
(	_	_
7	_	_
)	_	_
where	_	_
hθ	_	_
(	_	_
·	_	_
)	_	_
is	_	_
the	_	_
output	_	_
of	_	_
the	_	_
sequence	_	_
labelling	_	_
model	_	_
.	_	_

#132
This	_	_
is	_	_
equivalent	_	_
to	_	_
minimising	_	_
the	_	_
cross-entropy	_	_
error	_	_
between	_	_
the	_	_
outputs	_	_
of	_	_
the	_	_
neural	_	_
model	_	_
and	_	_
the	_	_
probabilistic	_	_
labels	_	_
produced	_	_
by	_	_
the	_	_
aggregation	_	_
model	_	_
.	_	_

#194
If	_	_
both	_	_
labelling	_	_
functions	_	_
make	_	_
the	_	_
same	_	_
prediction	_	_
on	_	_
a	_	_
given	_	_
token	_	_
,	_	_
we	_	_
count	_	_
this	_	_
as	_	_
an	_	_
agreement	_	_
,	_	_
whereas	_	_
conflicting	_	_
predictions	_	_
(	_	_
ignoring	_	_
O	_	_
labels	_	_
)	_	_
,	_	_
are	_	_
seen	_	_
as	_	_
disagreement	_	_
.	_	_

#195
Large	_	_
differences	_	_
may	speculation	_
exist	_	_
between	_	_
these	_	_
functions	_	_
for	_	_
specific	_	_
labels	_	_
,	_	_
especially	_	_
MISC	_	_
.	_	_

#196
The	_	_
functions	_	_
with	_	_
the	_	_
highest	_	_
overlap	_	_
are	_	_
those	_	_
making	_	_
predictions	_	_
on	_	_
all	_	_
labels	_	_
,	_	_
while	_	_
labelling	_	_
functions	_	_
specialised	_	_
to	_	_
few	_	_
labels	_	_
(	_	_
such	_	_
as	_	_
legal	_	_
detector	_	_
)	_	_
often	_	_
have	_	_
less	_	_
overlap	_	_
.	_	_

#216
To	_	_
leverage	_	_
all	_	_
possible	_	_
knowledge	_	_
sources	_	_
available	_	_
for	_	_
the	_	_
task	_	_
,	_	_
the	_	_
approach	_	_
uses	_	_
a	_	_
broad	_	_
spectrum	_	_
of	_	_
labelling	_	_
functions	_	_
,	_	_
including	_	_
data-driven	_	_
NER	_	_
models	_	_
,	_	_
gazetteers	_	_
,	_	_
heuristic	_	_
functions	_	_
,	_	_
and	_	_
document-level	_	_
relations	_	_
between	_	_
entities	_	_
.	_	_

#217
Labelling	_	_
functions	_	_
may	options	_
be	_	_
specialised	_	_
to	_	_
recognise	_	_
specific	_	_
labels	_	_
while	_	_
ignoring	_	_
others	_	_
.	_	_

#218
Furthermore	_	_
,	_	_
unlike	_	_
previous	_	_
weak	_	_
supervision	_	_
approaches	_	_
,	_	_
labelling	_	_
functions	_	_
may	options	_
produce	_	_
probabilistic	_	_
predictions	_	_
.	_	_

#219
The	_	_
outputs	_	_
of	_	_
these	_	_
labelling	_	_
functions	_	_
are	_	_
then	_	_
merged	_	_
together	_	_
using	_	_
a	_	_
hidden	_	_
Markov	_	_
model	_	_
whose	_	_
parameters	_	_
are	_	_
estimated	_	_
with	_	_
the	_	_
Baum-Welch	_	_
algorithm	_	_
.	_	_

#224
Future	_	_
work	_	_
will	_	_
investigate	_	_
how	_	_
to	_	_
take	_	_
into	_	_
account	_	_
potential	_	_
correlations	_	_
between	_	_
labelling	_	_
functions	_	_
in	_	_
the	_	_
aggregation	_	_
model	_	_
,	_	_
as	_	_
done	_	_
in	_	_
e.g.	_	_
(	_	_
Bach	_	_
et	_	_
al.	_	_
,	_	_
2017	_	_
)	_	_
.	_	_

#225
Furthermore	_	_
,	_	_
some	_	_
of	_	_
the	_	_
labelling	_	_
functions	_	_
can	_	_
be	_	_
rather	_	_
noisy	_	_
and	_	_
model	_	_
selection	_	_
of	_	_
the	_	_
optimal	_	_
subset	_	_
of	_	_
the	_	_
labelling	_	_
functions	_	_
might	capability-speculation	_
well	_	_
improve	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
model	_	_
.	_	_

#226
Model	_	_
selection	_	_
approaches	_	_
that	_	_
can	_	_
be	_	_
adapted	_	_
are	_	_
discussed	_	_
in	_	_
Adams	_	_
and	_	_
Beling	_	_
(	_	_
2019	_	_
)	_	_
;	_	_
Hubin	_	_
(	_	_
2019	_	_
)	_	_
.	_	_