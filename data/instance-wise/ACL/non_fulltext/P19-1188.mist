#9
Indeed	_	_
,	_	_
compared	_	_
to	_	_
earlier	_	_
logical	_	_
form-based	_	_
methods	_	_
,	_	_
neural	_	_
networks	_	_
can	_	_
sometimes	_	_
require	_	_
orders	_	_
of	_	_
magnitude	_	_
more	_	_
data	_	_
.	_	_

#10
The	_	_
data-hungriness	_	_
of	_	_
neural	_	_
approaches	_	_
is	_	_
not	_	_
surprising	_	_
–	_	_
starting	_	_
with	_	_
classic	_	_
logical	_	_
forms	_	_
improves	_	_
data	_	_
efficiency	_	_
by	_	_
presenting	_	_
a	_	_
system	_	_
with	_	_
pre-made	_	_
abstractions	_	_
,	_	_
where	_	_
end-to-end	_	_
neural	_	_
approaches	_	_
must	deontic	_
do	_	_
the	_	_
hard	_	_
work	_	_
of	_	_
inducing	_	_
abstractions	_	_
on	_	_
their	_	_
own	_	_
.	_	_

#11
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
aim	_	_
to	_	_
combine	_	_
the	_	_
power	_	_
of	_	_
neural	_	_
networks	_	_
with	_	_
the	_	_
data-efficiency	_	_
of	_	_
logical	_	_
forms	_	_
by	_	_
pre-learning	_	_
abstractions	_	_
in	_	_
a	_	_
semi-supervised	_	_
way	_	_
,	_	_
satiating	_	_
part	_	_
of	_	_
the	_	_
network’s	_	_
data	_	_
hunger	_	_
on	_	_
cheaper	_	_
unlabeled	_	_
data	_	_
from	_	_
the	_	_
environment	_	_
.	_	_

#44
During	_	_
this	_	_
environment	_	_
learning	_	_
phase	_	_
,	_	_
we	_	_
train	_	_
a	_	_
conditional	_	_
autoencoder	_	_
of	_	_
s′	_	_
given	_	_
s	_	_
by	_	_
introducing	_	_
an	_	_
additional	_	_
encoder	_	_
E	_	_
(	_	_
s	_	_
,	_	_
s′	_	_
)	_	_
7→	_	_
a	_	_
to	_	_
go	_	_
along	_	_
with	_	_
decoder	_	_
D	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
7→	_	_
s′	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
(	_	_
i	_	_
)	_	_
.	_	_

#45
Both	_	_
E	_	_
and	_	_
D	_	_
are	_	_
given	_	_
the	_	_
initial	_	_
state	_	_
s	_	_
,	_	_
and	_	_
E	_	_
must	deontic	_
create	_	_
a	_	_
representation	_	_
of	_	_
the	_	_
final	_	_
state	_	_
s′	_	_
so	_	_
that	_	_
D	_	_
can	_	_
reproduce	_	_
it	_	_
from	_	_
s	_	_
.	_	_

#46
The	_	_
parameters	_	_
of	_	_
E	_	_
and	_	_
D	_	_
are	_	_
trained	_	_
to	_	_
maximize	_	_
log	_	_
likelihood	_	_
of	_	_
s′	_	_
under	_	_
the	_	_
output	_	_
distribution	_	_
of	_	_
D.	_	_
arg	_	_
max	_	_
θE	_	_
,	_	_
θD	_	_
[	_	_
logPD	_	_
(	_	_
s′|s	_	_
,	_	_
E	_	_
(	_	_
s	_	_
,	_	_
s′	_	_
)	_	_
]	_	_
(	_	_
1	_	_
)	_	_

#47
If	_	_
given	_	_
enough	_	_
capacity	_	_
,	_	_
the	_	_
representation	_	_
a	_	_
might	_	_
encode	_	_
all	_	_
the	_	_
information	_	_
necessary	_	_
to	_	_
produce	_	_
s′	_	_
,	_	_
allowing	_	_
the	_	_
decoder	_	_
to	_	_
ignore	_	_
s	_	_
.	_	_

#48
However	_	_
,	_	_
with	_	_
a	_	_
limited	_	_
representation	_	_
space	_	_
,	_	_
the	_	_
decoder	_	_
must	deontic	_
learn	_	_
to	_	_
integrate	_	_
information	_	_
from	_	_
a	_	_
and	_	_
s	_	_
,	_	_
leading	_	_
a	_	_
to	_	_
capture	_	_
an	_	_
abstract	_	_
representation	_	_
of	_	_
the	_	_
transformation	_	_
between	_	_
s	_	_
and	_	_
s′	_	_
.	_	_

#49
To	_	_
be	_	_
effective	_	_
,	_	_
the	_	_
representation	_	_
a	_	_
needs	_	_
to	_	_
be	_	_
widely	_	_
applicable	_	_
in	_	_
the	_	_
environment	_	_
and	_	_
align	_	_
well	_	_
with	_	_
the	_	_
types	_	_
of	_	_
state	_	_
transitions	_	_
that	_	_
typically	_	_
occur	_	_
.	_	_

#96
The	_	_
output	_	_
of	_	_
the	_	_
decoder	_	_
module	_	_
is	_	_
over	_	_
the	_	_
same	_	_
grid	_	_
,	_	_
and	_	_
a	_	_
softmax	_	_
over	_	_
colors	_	_
(	_	_
or	_	_
empty	_	_
)	_	_
is	_	_
used	_	_
to	_	_
select	_	_
the	_	_
block	_	_
at	_	_
each	_	_
position	_	_
.	_	_

#97
Note	_	_
that	_	_
the	_	_
original	_	_
work	_	_
in	_	_
this	_	_
environment	_	_
restricted	_	_
outputs	_	_
to	_	_
states	_	_
reachable	_	_
from	_	_
the	_	_
initial	_	_
state	_	_
by	_	_
a	_	_
logical	_	_
form	_	_
,	_	_
but	_	_
here	_	_
we	_	_
allow	_	_
any	_	_
arbitrary	_	_
state	_	_
to	_	_
be	_	_
output	_	_
and	_	_
the	_	_
model	_	_
must	deontic	_
learn	_	_
to	_	_
select	_	_
from	_	_
a	_	_
much	_	_
larger	_	_
hypothesis	_	_
space	_	_
.	_	_

#98
The	_	_
encoder	_	_
module	_	_
E	_	_
consists	_	_
of	_	_
convolutions	_	_
over	_	_
the	_	_
states	_	_
s	_	_
and	_	_
s′	_	_
,	_	_
subtraction	_	_
of	_	_
the	_	_
two	_	_
representations	_	_
,	_	_
pooling	_	_
over	_	_
locations	_	_
,	_	_
and	_	_
finally	_	_
a	_	_
fully	_	_
connected	_	_
network	_	_
which	_	_
outputs	_	_
the	_	_
representation	_	_
a	_	_
.	_	_

#126
State	_	_
transitions	_	_
for	_	_
environment	_	_
learning	_	_
are	_	_
generated	_	_
synthetically	_	_
by	_	_
selecting	_	_
words	_	_
from	_	_
a	_	_
dictionary	_	_
and	_	_
applying	_	_
regular	_	_
expressions	_	_
,	_	_
where	_	_
the	_	_
regular	_	_
expressions	_	_
to	_	_
apply	_	_
were	_	_
sampled	_	_
from	_	_
a	_	_
regular-expression	_	_
generation	_	_
procedure	_	_
written	_	_
by	_	_
the	_	_
creators	_	_
of	_	_
the	_	_
original	_	_
dataset	_	_
.	_	_

#127
The	_	_
environment	_	_
learning	_	_
procedure	_	_
is	_	_
exposed	_	_
to	_	_
transitions	_	_
from	_	_
thousands	_	_
of	_	_
unique	_	_
regular	_	_
expressions	_	_
that	_	_
it	_	_
must	deontic	_
make	_	_
sense	_	_
of	_	_
and	_	_
learn	_	_
to	_	_
represent	_	_
.	_	_

#128
4.2.1	_	_
Network	_	_
Architecture	_	_

#196
Our	_	_
environment	_	_
learning	_	_
procedure	_	_
could	_	_
be	_	_
viewed	_	_
as	_	_
a	_	_
language	_	_
learning	_	_
game	_	_
where	_	_
the	_	_
encoder	_	_
is	_	_
a	_	_
speaker	_	_
and	_	_
the	_	_
decoder	_	_
is	_	_
a	_	_
listener	_	_
.	_	_

#197
The	_	_
speaker	_	_
must	deontic	_
create	_	_
a	_	_
“language”	_	_
a	_	_
that	_	_
allows	_	_
the	_	_
decoder	_	_
to	_	_
complete	_	_
a	_	_
task	_	_
.	_	_

#198
Many	_	_
of	_	_
these	_	_
papers	_	_
have	_	_
found	_	_
that	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
induce	_	_
representations	_	_
that	_	_
align	_	_
semantically	_	_
with	_	_
language	_	_
humans	_	_
use	_	_
,	_	_
as	_	_
explored	_	_
in	_	_
detail	_	_
in	_	_
Andreas	_	_
and	_	_
Klein	_	_
(	_	_
2017	_	_
)	_	_
.	_	_