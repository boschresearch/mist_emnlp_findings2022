#17
In	_	_
response	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
explicitly	_	_
enhance	_	_
the	_	_
its	_	_
self-attention	_	_
mechanism	_	_
(	_	_
a	_	_
core	_	_
component	_	_
of	_	_
this	_	_
architecture	_	_
)	_	_
to	_	_
include	_	_
syntactic	_	_
information	_	_
without	_	_
compromising	_	_
its	_	_
flexibility	_	_
.	_	_

#18
Recent	_	_
studies	_	_
have	_	_
,	_	_
in	_	_
fact	_	_
,	_	_
shown	_	_
that	_	_
self-attention	_	_
networks	_	_
benefit	_	_
from	_	_
modeling	_	_
local	_	_
contexts	_	_
by	_	_
reducing	_	_
the	_	_
dispersion	_	_
of	_	_
the	_	_
attention	_	_
distribution	_	_
(	_	_
Shaw	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Yang	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
,	_	_
2019	_	_
)	_	_
,	_	_
and	_	_
that	_	_
they	_	_
might	capability-speculation	_
not	_	_
capture	_	_
the	_	_
inherent	_	_
syntactic	_	_
structure	_	_
of	_	_
languages	_	_
as	_	_
well	_	_
as	_	_
recurrent	_	_
models	_	_
,	_	_
especially	_	_
in	_	_
low-resource	_	_
settings	_	_
(	_	_
Tran	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
;	_	_
Tang	_	_
et	_	_
al.	_	_
,	_	_
2018	_	_
)	_	_
.	_	_

#19
Here	_	_
,	_	_
we	_	_
present	_	_
parent-scaled	_	_
self-attention	_	_
(	_	_
PASCAL	_	_
)	_	_
:	_	_
a	_	_
novel	_	_
,	_	_
parameter-free	_	_
local	_	_
attention	_	_
mechanism	_	_
that	_	_
lets	_	_
the	_	_
model	_	_
focus	_	_
on	_	_
the	_	_
dependency	_	_
parent	_	_
of	_	_
each	_	_
token	_	_
when	_	_
encoding	_	_
the	_	_
source	_	_
sentence	_	_
.	_	_

#50
In	_	_
addition	_	_
,	_	_
non-negligible	_	_
weights	_	_
are	_	_
placed	_	_
on	_	_
the	_	_
neighbors	_	_
of	_	_
the	_	_
parent	_	_
token	_	_
,	_	_
allowing	_	_
the	_	_
attention	_	_
mechanism	_	_
to	_	_
also	_	_
attend	_	_
to	_	_
them	_	_
.	_	_

#51
This	_	_
could	speculation	_
be	_	_
useful	_	_
,	_	_
for	_	_
instance	_	_
,	_	_
to	_	_
learn	_	_
idiomatic	_	_
expressions	_	_
such	_	_
as	_	_
prepositional	_	_
verbs	_	_
in	_	_
English	_	_
.	_	_

#52
The	_	_
second	_	_
property	_	_
of	_	_
Gaussian-like	_	_
distributions	_	_
that	_	_
we	_	_
exploit	_	_
is	_	_
their	_	_
support	_	_
:	_	_
While	_	_
most	_	_
of	_	_
the	_	_
weight	_	_
is	_	_
placed	_	_
in	_	_
a	_	_
small	_	_
window	_	_
of	_	_
tokens	_	_
around	_	_
the	_	_
mean	_	_
of	_	_
the	_	_
distribution	_	_
,	_	_
all	_	_
the	_	_
values	_	_
in	_	_
the	_	_
sequence	_	_
are	_	_
actually	_	_
multiplied	_	_
by	_	_
non-zero	_	_
factors	_	_
;	_	_
allowing	_	_
a	_	_
token	_	_
j	_	_
farther	_	_
away	_	_
from	_	_
the	_	_
parent	_	_
of	_	_
token	_	_
t	_	_
,	_	_
pt	_	_
,	_	_
to	_	_
still	_	_
play	_	_
a	_	_
role	_	_
in	_	_
the	_	_
representation	_	_
of	_	_
t	_	_
if	_	_
its	_	_
score	_	_
shtj	_	_
is	_	_
high	_	_
.	_	_