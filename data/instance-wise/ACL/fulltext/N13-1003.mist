#2
We	_	_
use	_	_
sparse	_	_
features	_	_
to	_	_
address	_	_
reordering	_	_
,	_	_
which	_	_
is	_	_
often	_	_
considered	_	_
a	_	_
weak	_	_
point	_	_
of	_	_
phrase-based	_	_
translation	_	_
.	_	_

#3
Using	_	_
a	_	_
hierarchical	_	_
reordering	_	_
model	_	_
as	_	_
our	_	_
baseline	_	_
,	_	_
we	_	_
show	_	_
that	_	_
simple	_	_
features	_	_
coupling	_	_
phrase	_	_
orientation	_	_
to	_	_
frequent	_	_
words	_	_
or	_	_
wordclusters	_	_
can	capability-options	_
improve	_	_
translation	_	_
quality	_	_
,	_	_
with	_	_
boosts	_	_
of	_	_
up	_	_
to	_	_
1.2	_	_
BLEU	_	_
points	_	_
in	_	_
Chinese-	_	_
English	_	_
and	_	_
1.8	_	_
in	_	_
Arabic-English	_	_
.	_	_

#4
We	_	_
compare	_	_
this	_	_
solution	_	_
to	_	_
a	_	_
more	_	_
traditional	_	_
maximum	_	_
entropy	_	_
approach	_	_
,	_	_
where	_	_
a	_	_
probability	_	_
model	_	_
with	_	_
similar	_	_
features	_	_
is	_	_
trained	_	_
on	_	_
word-	_	_
aligned	_	_
bitext	_	_
.	_	_

#5
We	_	_
show	_	_
that	_	_
sparse	_	_
decoder	_	_
features	_	_
outperform	_	_
maximum	_	_
entropy	_	_
handily	_	_
,	_	_
indicating	_	_
that	_	_
there	_	_
are	_	_
major	_	_
advantages	_	_
to	_	_
optimizing	_	_
reordering	_	_
features	_	_
directly	_	_
for	_	_
BLEU	_	_
with	_	_
the	_	_
decoder	_	_
in	_	_
the	_	_
loop	_	_
.	_	_

#6
1	_	_
Introduction	_	_
With	_	_
the	_	_
growing	_	_
adoption	_	_
of	_	_
tuning	_	_
algorithms	_	_
that	_	_
can	capability	_
handle	_	_
thousands	_	_
of	_	_
features	_	_
(	_	_
Chiang	_	_
et	_	_
al.	_	_
,	_	_
2008	_	_
;	_	_
Hopkins	_	_
and	_	_
May	_	_
,	_	_
2011	_	_
)	_	_
,	_	_
SMT	_	_
system	_	_
designers	_	_
now	_	_
face	_	_
a	_	_
choice	_	_
when	_	_
incorporating	_	_
new	_	_
ideas	_	_
into	_	_
their	_	_
translation	_	_
models	_	_
.	_	_

#7
Maximum	_	_
likelihood	_	_
models	_	_
can	feasibility	_
be	_	_
estimated	_	_
from	_	_
large	_	_
word-	_	_
aligned	_	_
bitexts	_	_
,	_	_
creating	_	_
a	_	_
small	_	_
number	_	_
of	_	_
highly	_	_
informative	_	_
decoder	_	_
features	_	_
;	_	_
or	_	_
the	_	_
same	_	_
ideas	_	_
can	feasibility	_
be	_	_
incorporated	_	_
into	_	_
the	_	_
decoder’s	_	_
linear	_	_
model	_	_
directly	_	_
.	_	_

#8
There	_	_
are	_	_
trade-offs	_	_
to	_	_
each	_	_
approach	_	_
.	_	_

#9
Maximum	_	_
likelihood	_	_
models	_	_
can	feasibility	_
be	_	_
estimated	_	_
from	_	_
millions	_	_
of	_	_
sentences	_	_
of	_	_
bitext	_	_
,	_	_
but	_	_
optimize	_	_
a	_	_
mismatched	_	_
objective	_	_
,	_	_
predicting	_	_
events	_	_
observed	_	_
in	_	_
word	_	_
aligned	_	_
bitext	_	_
instead	_	_
of	_	_
optimizing	_	_
translation	_	_
quality	_	_
.	_	_

#10
Sparse	_	_
decoder	_	_
features	_	_
have	_	_
the	_	_
opposite	_	_
problem	_	_
;	_	_
with	_	_
the	_	_
decoder	_	_
in	_	_
the	_	_
loop	_	_
,	_	_
we	_	_
can	feasibility	_
only	_	_
tune	_	_
on	_	_
small	_	_
development	_	_
sets,1	_	_
but	_	_
a	_	_
translation	_	_
error	_	_
metric	_	_
directly	_	_
informs	_	_
training	_	_
.	_	_

#11
We	_	_
investigate	_	_
this	_	_
trade-off	_	_
in	_	_
the	_	_
context	_	_
of	_	_
re-	_	_
ordering	_	_
models	_	_
for	_	_
phrase-based	_	_
decoding	_	_
.	_	_

#15
Features	_	_
from	_	_
hierarchical	_	_
and	_	_
syntax-based	_	_
translation	_	_
(	_	_
Chiang	_	_
et	_	_
al.	_	_
,	_	_
2009	_	_
)	_	_
do	_	_
not	_	_
easily	_	_
transfer	_	_
to	_	_
the	_	_
phrase-based	_	_
paradigm	_	_
,	_	_
and	_	_
most	_	_
work	_	_
that	_	_
has	_	_
looked	_	_
at	_	_
large	_	_
feature	_	_
counts	_	_
in	_	_
the	_	_
context	_	_
of	_	_
phrase-based	_	_
translation	_	_
has	_	_
focused	_	_
on	_	_
the	_	_
learning	_	_
method	_	_
,	_	_
and	_	_
not	_	_
the	_	_
features	_	_
themselves	_	_
(	_	_
Hopkins	_	_
and	_	_
May	_	_
,	_	_
2011	_	_
;	_	_
Cherry	_	_
and	_	_
Foster	_	_
,	_	_
2012	_	_
;	_	_
Gimpel	_	_
and	_	_
Smith	_	_
,	_	_
2012	_	_
)	_	_
.	_	_

#16
We	_	_
show	_	_
that	_	_
by	_	_
targeting	_	_
reordering	_	_
,	_	_
large	_	_
gains	_	_
can	feasibility	_
be	_	_
made	_	_
with	_	_
relatively	_	_
simple	_	_
features	_	_
.	_	_

#17
2	_	_
Background	_	_
Phrase-based	_	_
machine	_	_
translation	_	_
constructs	_	_
its	_	_
target	_	_
sentence	_	_
from	_	_
left-to-right	_	_
,	_	_
with	_	_
each	_	_
translation	_	_
operation	_	_
selecting	_	_
a	_	_
source	_	_
phrase	_	_
and	_	_
appending	_	_
its	_	_
translation	_	_
to	_	_
the	_	_
growing	_	_
target	_	_
sentence	_	_
,	_	_
until	_	_
1Some	_	_
systems	_	_
tune	_	_
for	_	_
BLEU	_	_
on	_	_
much	_	_
larger	_	_
sets	_	_
(	_	_
Simianer	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
;	_	_
He	_	_
and	_	_
Deng	_	_
,	_	_
2012	_	_
)	_	_
,	_	_
but	_	_
these	_	_
require	_	_
exceptional	_	_
commitments	_	_
of	_	_
resources	_	_
and	_	_
time	_	_
.	_	_

#21
2.1	_	_
Lexicalized	_	_
Reordering	_	_
Implemented	_	_
in	_	_
a	_	_
number	_	_
of	_	_
phrase-based	_	_
decoders	_	_
,	_	_
the	_	_
lexicalized	_	_
reordering	_	_
model	_	_
(	_	_
RM	_	_
)	_	_
uses	_	_
word-	_	_
aligned	_	_
data	_	_
to	_	_
determine	_	_
how	_	_
each	_	_
phrase-pair	_	_
tends	_	_
to	_	_
be	_	_
reordered	_	_
during	_	_
translation	_	_
(	_	_
Tillmann	_	_
,	_	_
2004	_	_
;	_	_
Koehn	_	_
et	_	_
al.	_	_
,	_	_
2005	_	_
;	_	_
Koehn	_	_
et	_	_
al.	_	_
,	_	_
2007	_	_
)	_	_
.	_	_

#22
The	_	_
core	_	_
idea	_	_
in	_	_
this	_	_
RM	_	_
is	_	_
to	_	_
divide	_	_
reordering	_	_
events	_	_
into	_	_
three	_	_
orientations	_	_
that	_	_
can	feasibility	_
be	_	_
easily	_	_
determined	_	_
both	_	_
during	_	_
decoding	_	_
and	_	_
from	_	_
word-aligned	_	_
data	_	_
.	_	_

#23
The	_	_
orientations	_	_
can	capability-feasibility	_
be	_	_
described	_	_
in	_	_
terms	_	_
of	_	_
the	_	_
previously	_	_
translated	_	_
source	_	_
phrase	_	_
(	_	_
prev	_	_
)	_	_
and	_	_
the	_	_
next	_	_
source	_	_
phrase	_	_
to	_	_
be	_	_
translated	_	_
(	_	_
next	_	_
)	_	_
:	_	_
•	_	_
Monotone	_	_
(	_	_
M	_	_
)	_	_
:	_	_
next	_	_
immediately	_	_
follows	_	_
prev	_	_
.	_	_

#24
•	_	_
Swap	_	_
(	_	_
S	_	_
)	_	_
:	_	_
prev	_	_
immediately	_	_
follows	_	_
next	_	_
.	_	_

#25
•	_	_
Discontinuous	_	_
(	_	_
D	_	_
)	_	_
:	_	_
next	_	_
and	_	_
prev	_	_
are	_	_
not	_	_
adjacent	_	_
in	_	_
the	_	_
source	_	_
.	_	_

#26
Note	_	_
that	_	_
prev	_	_
and	_	_
next	_	_
can	feasibility-options	_
be	_	_
defined	_	_
for	_	_
constructing	_	_
a	_	_
translation	_	_
from	_	_
left-to-right	_	_
or	_	_
from	_	_
righttoleft	_	_
.	_	_

#27
Most	_	_
decoders	_	_
incorporate	_	_
RMs	_	_
for	_	_
both	_	_
directions	_	_
;	_	_
our	_	_
discussion	_	_
will	_	_
generally	_	_
only	_	_
cover	_	_
lefttoright	_	_
,	_	_
with	_	_
the	_	_
right-to-left	_	_
case	_	_
being	_	_
implicit	_	_
and	_	_
symmetrical	_	_
.	_	_

#28
As	_	_
the	_	_
decoder	_	_
extends	_	_
its	_	_
hypothesis	_	_
by	_	_
translating	_	_
a	_	_
source	_	_
phrase	_	_
,	_	_
we	_	_
can	feasibility	_
assess	_	_
the	_	_
implied	_	_
orientations	_	_
to	_	_
determine	_	_
if	_	_
the	_	_
resulting	_	_
reordering	_	_
makes	_	_
sense	_	_
.	_	_

#29
This	_	_
is	_	_
done	_	_
using	_	_
the	_	_
probability	_	_
of	_	_
an	_	_
orientation	_	_
given	_	_
the	_	_
phrase	_	_
pair	_	_
pp	_	_
=	_	_
[	_	_
src	_	_
,	_	_
tgt	_	_
]	_	_
extending	_	_
the	_	_
hypothesis:2	_	_
P	_	_
(	_	_
o|pp	_	_
)	_	_
≈	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
∑	_	_
o	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
(	_	_
1	_	_
)	_	_
where	_	_
o	_	_
∈	_	_
{	_	_
M	_	_
,	_	_
S	_	_
,	_	_
D	_	_
}	_	_
,	_	_
cnt	_	_
uses	_	_
simple	_	_
heuristics	_	_
on	_	_
word-alignments	_	_
to	_	_
count	_	_
phrase	_	_
pairs	_	_
and	_	_
their	_	_
orientations	_	_
,	_	_
and	_	_
the	_	_
≈	_	_
symbol	_	_
allows	_	_
for	_	_
smoothing	_	_
.	_	_

#39
We	_	_
assume	_	_
the	_	_
HRM	_	_
as	_	_
our	_	_
baseline	_	_
reordering	_	_
model	_	_
.	_	_

#40
It	_	_
is	_	_
important	_	_
to	_	_
note	_	_
that	_	_
although	_	_
our	_	_
maximum	_	_
entropy	_	_
and	_	_
sparse	_	_
reordering	_	_
solutions	_	_
build	_	_
on	_	_
the	_	_
HRM	_	_
,	_	_
the	_	_
features	_	_
in	_	_
this	_	_
paper	_	_
can	feasibility	_
still	_	_
be	_	_
applied	_	_
without	_	_
a	_	_
shift-reduce	_	_
stack	_	_
,	_	_
by	_	_
using	_	_
the	_	_
previously	_	_
translated	_	_
phrase	_	_
where	_	_
we	_	_
use	_	_
the	_	_
top	_	_
of	_	_
the	_	_
stack	_	_
.	_	_

#41
2.3	_	_
Maximum	_	_
Entropy	_	_
Reordering	_	_
One	_	_
frequent	_	_
observation	_	_
regarding	_	_
both	_	_
the	_	_
RM	_	_
and	_	_
the	_	_
HRM	_	_
is	_	_
that	_	_
the	_	_
statistics	_	_
used	_	_
to	_	_
grade	_	_
orientations	_	_
are	_	_
very	_	_
sparse	_	_
.	_	_

#61
Orientation	_	_
counts	_	_
for	_	_
phrase	_	_
pairs	_	_
are	_	_
collected	_	_
from	_	_
bitext	_	_
,	_	_
using	_	_
the	_	_
method	_	_
described	_	_
by	_	_
Galley	_	_
and	_	_
Manning	_	_
(	_	_
2008	_	_
)	_	_
.	_	_

#62
The	_	_
probability	_	_
model	_	_
P	_	_
(	_	_
o|pp	_	_
=	_	_
[	_	_
src	_	_
,	_	_
tgt	_	_
]	_	_
)	_	_
is	_	_
estimated	_	_
using	_	_
recursive	_	_
MAP	_	_
smoothing	_	_
:	_	_
P	_	_
(	_	_
o|pp	_	_
)	_	_
=	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
+	_	_
αsPs	_	_
(	_	_
o|src	_	_
)	_	_
+	_	_
αtPt	_	_
(	_	_
o|tgt	_	_
)	_	_
∑	_	_
o	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
+	_	_
αs	_	_
+	_	_
αt	_	_
Ps	_	_
(	_	_
o|src	_	_
)	_	_
=	_	_
∑	_	_
tgt	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
src	_	_
,	_	_
tgt	_	_
)	_	_
+	_	_
αgPg	_	_
(	_	_
o	_	_
)	_	_
∑	_	_
o	_	_
,	_	_
tgt	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
src	_	_
,	_	_
tgt	_	_
)	_	_
+	_	_
αg	_	_
Pt	_	_
(	_	_
o|tgt	_	_
)	_	_
=	_	_
∑	_	_
src	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
src	_	_
,	_	_
tgt	_	_
)	_	_
+	_	_
αgPg	_	_
(	_	_
o	_	_
)	_	_
∑	_	_
o	_	_
,	_	_
src	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
src	_	_
,	_	_
tgt	_	_
)	_	_
+	_	_
αg	_	_
Pg	_	_
(	_	_
o	_	_
)	_	_
=	_	_
∑	_	_
pp	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
+	_	_
αu/3∑	_	_
o	_	_
,	_	_
pp	_	_
cnt	_	_
(	_	_
o	_	_
,	_	_
pp	_	_
)	_	_
+	_	_
αu	_	_
(	_	_
2	_	_
)	_	_
where	_	_
the	_	_
various	_	_
α	_	_
parameters	_	_
can	feasibility	_
be	_	_
tuned	_	_
empirically	_	_
.	_	_

#63
In	_	_
practice	_	_
,	_	_
the	_	_
model	_	_
is	_	_
not	_	_
particularly	_	_
sensitive	_	_
to	_	_
these	_	_
parameters.4	_	_
3.2	_	_
Maximum	_	_
Entropy	_	_
Model	_	_
Next	_	_
,	_	_
we	_	_
describe	_	_
our	_	_
implementation	_	_
of	_	_
a	_	_
maximum	_	_
entropy	_	_
HRM	_	_
.	_	_

#87
3.3	_	_
Sparse	_	_
Reordering	_	_
Features	_	_
The	_	_
maximum	_	_
entropy	_	_
approach	_	_
uses	_	_
features	_	_
to	_	_
model	_	_
the	_	_
distribution	_	_
of	_	_
orientations	_	_
found	_	_
in	_	_
word	_	_
alignments	_	_
.	_	_

#88
Alternatively	_	_
,	_	_
a	_	_
number	_	_
of	_	_
recent	_	_
tuning	_	_
methods	_	_
,	_	_
such	_	_
as	_	_
MIRA	_	_
(	_	_
Chiang	_	_
et	_	_
al.	_	_
,	_	_
2008	_	_
)	_	_
or	_	_
PRO	_	_
(	_	_
Hopkins	_	_
and	_	_
May	_	_
,	_	_
2011	_	_
)	_	_
,	_	_
can	capability	_
handle	_	_
thousands	_	_
of	_	_
features	_	_
.	_	_

#89
These	_	_
could	feasibility-options	_
be	_	_
used	_	_
to	_	_
tune	_	_
similar	_	_
features	_	_
to	_	_
maximize	_	_
BLEU	_	_
directly	_	_
.	_	_

#90
Given	_	_
the	_	_
appropriate	_	_
tuning	_	_
architecture	_	_
,	_	_
the	_	_
sparse	_	_
feature	_	_
approach	_	_
is	_	_
actually	_	_
simpler	_	_
in	_	_
many	_	_
ways	_	_
than	_	_
the	_	_
maximum	_	_
entropy	_	_
approach	_	_
.	_	_

#92
Instead	_	_
,	_	_
one	_	_
simply	_	_
implements	_	_
the	_	_
desired	_	_
features	_	_
in	_	_
the	_	_
decoder’s	_	_
feature	_	_
API	_	_
and	_	_
then	_	_
tunes	_	_
as	_	_
normal	_	_
.	_	_

#93
The	_	_
challenge	_	_
is	_	_
to	_	_
design	_	_
features	_	_
so	_	_
that	_	_
the	_	_
model	_	_
can	capability-feasibility	_
be	_	_
learned	_	_
from	_	_
small	_	_
tuning	_	_
sets	_	_
.	_	_

#94
The	_	_
standard	_	_
approach	_	_
for	_	_
sparse	_	_
feature	_	_
design	_	_
in	_	_
SMT	_	_
is	_	_
to	_	_
lexicalize	_	_
only	_	_
on	_	_
extremely	_	_
frequent	_	_
words	_	_
,	_	_
such	_	_
as	_	_
the	_	_
top-80	_	_
words	_	_
from	_	_
each	_	_
language	_	_
(	_	_
Chiang	_	_
et	_	_
al.	_	_
,	_	_
2009	_	_
;	_	_
Hopkins	_	_
and	_	_
May	_	_
,	_	_
2011	_	_
)	_	_
.	_	_

#96
These	_	_
clusters	_	_
mirror	_	_
parts-of-speech	_	_
quite	_	_
effectively	_	_
(	_	_
Blunsom	_	_
and	_	_
Cohn	_	_
,	_	_
2011	_	_
)	_	_
,	_	_
without	_	_
requiring	_	_
linguistic	_	_
resources	_	_
.	_	_

#97
They	_	_
should	deontic	_
provide	_	_
useful	_	_
generalization	_	_
for	_	_
reordering	_	_
decisions	_	_
.	_	_

#98
Inspired	_	_
by	_	_
recent	_	_
successes	_	_
in	_	_
semi-supervised	_	_
learning	_	_
(	_	_
Koo	_	_
et	_	_
al.	_	_
,	_	_
2008	_	_
;	_	_
corpus	_	_
sentences	_	_
words	_	_
(	_	_
ar	_	_
)	_	_
words	_	_
(	_	_
en	_	_
)	_	_
train	_	_
1,490,514	_	_
46,403,734	_	_
47,109,486	_	_
dev	_	_
1,663	_	_
45,243	_	_
50,550	_	_
mt08	_	_
1,360	_	_
45,002	_	_
51,341	_	_
mt09	_	_
1,313	_	_
40,684	_	_
46,813	_	_
Table	_	_
3	_	_
:	_	_
Arabic-English	_	_
Corpus	_	_
.	_	_

#107
We	_	_
represent	_	_
the	_	_
current	_	_
top	_	_
of	_	_
the	_	_
stack	_	_
(	_	_
top	_	_
)	_	_
using	_	_
its	_	_
first	_	_
and	_	_
last	_	_
source	_	_
words	_	_
(	_	_
accessible	_	_
from	_	_
the	_	_
HRM	_	_
stack	_	_
)	_	_
,	_	_
and	_	_
its	_	_
last	_	_
target	_	_
word	_	_
(	_	_
accessible	_	_
using	_	_
language	_	_
model	_	_
context	_	_
)	_	_
.	_	_

#108
Furthermore	_	_
,	_	_
for	_	_
discontinuous	_	_
(	_	_
D	_	_
)	_	_
orientations	_	_
,	_	_
we	_	_
can	feasibility	_
include	_	_
an	_	_
indicator	_	_
for	_	_
each	_	_
source	_	_
word	_	_
between	_	_
the	_	_
current	_	_
top	_	_
of	_	_
the	_	_
stack	_	_
and	_	_
the	_	_
phrase	_	_
being	_	_
added	_	_
.	_	_

#109
Because	_	_
the	_	_
sparse	_	_
feature	_	_
HRM	_	_
has	_	_
no	_	_
access	_	_
to	_	_
phrase-pair	_	_
or	_	_
monolingual	_	_
phrase	_	_
features	_	_
,	_	_
and	_	_
because	_	_
it	_	_
completely	_	_
ignores	_	_
our	_	_
large	_	_
supply	_	_
of	_	_
word-aligned	_	_
training	_	_
data	_	_
,	_	_
we	_	_
view	_	_
it	_	_
as	_	_
complimentary	_	_
to	_	_
the	_	_
relative	_	_
frequency	_	_
HRM	_	_
.	_	_

#149
shown	_	_
)	_	_
,	_	_
we	_	_
confirmed	_	_
that	_	_
if	_	_
a	_	_
replication	_	_
produced	_	_
low	_	_
scores	_	_
in	_	_
one	_	_
test	_	_
,	_	_
it	_	_
also	_	_
produced	_	_
low	_	_
scores	_	_
in	_	_
the	_	_
other	_	_
.	_	_

#150
This	_	_
means	_	_
that	_	_
one	_	_
should	inference	_
be	_	_
able	_	_
to	_	_
outperform	_	_
the	_	_
average	_	_
case	_	_
by	_	_
using	_	_
a	_	_
dev-test	_	_
set	_	_
to	_	_
select	_	_
among	_	_
replications	_	_
.	_	_

#151
5.1	_	_
Maximum	_	_
Entropy	_	_
Analysis	_	_
The	_	_
next	_	_
two	_	_
sections	_	_
examine	_	_
our	_	_
two	_	_
solutions	_	_
in	_	_
detail	_	_
,	_	_
starting	_	_
with	_	_
the	_	_
Maxent	_	_
HRM	_	_
.	_	_

#212
All	_	_
representations	_	_
perform	_	_
well	_	_
on	_	_
their	_	_
own	_	_
,	_	_
with	_	_
the	_	_
finer-grained	_	_
ones	_	_
performing	_	_
better	_	_
.	_	_

#213
Including	_	_
multiple	_	_
representations	_	_
provides	_	_
a	_	_
slight	_	_
boost	_	_
,	_	_
but	_	_
these	_	_
experiments	_	_
suggest	_	_
that	_	_
a	_	_
leaner	_	_
model	_	_
could	capability-speculation	_
certainly	_	_
drop	_	_
one	_	_
or	_	_
two	_	_
representations	_	_
with	_	_
little	_	_
impact	_	_
.	_	_

#214
In	_	_
its	_	_
current	_	_
implementation	_	_
,	_	_
the	_	_
Sparse	_	_
HRM	_	_
is	_	_
roughly	_	_
4	_	_
times	_	_
slower	_	_
than	_	_
the	_	_
baseline	_	_
decoder	_	_
.	_	_

#216
To	_	_
affect	_	_
reordering	_	_
,	_	_
each	_	_
sparse	_	_
feature	_	_
template	_	_
is	_	_
re-applied	_	_
with	_	_
each	_	_
hypothesis	_	_
extension	_	_
.	_	_

#217
However	_	_
,	_	_
the	_	_
intersected	_	_
feature	_	_
set	_	_
from	_	_
§5.1	_	_
is	_	_
only	_	_
2	_	_
times	_	_
slower	_	_
,	_	_
and	_	_
could	feasibility-options	_
be	_	_
made	_	_
faster	_	_
still	_	_
.	_	_

#218
That	_	_
feature	_	_
set	_	_
uses	_	_
only	_	_
within-phrase	_	_
features	_	_
to	_	_
asses	_	_
orientations	_	_
;	_	_
therefore	_	_
,	_	_
the	_	_
total	_	_
weight	_	_
for	_	_
each	_	_
orientation	_	_
for	_	_
each	_	_
phrase-pair	_	_
could	feasibility-options	_
be	_	_
pre-calculated	_	_
,	_	_
making	_	_
its	_	_
cost	_	_
comparable	_	_
to	_	_
the	_	_
baseline	_	_
.	_	_

#219
Chinese-English	_	_
tune	_	_
mt06	_	_
mt08	_	_
Base	_	_
27.7	_	_
39.9	_	_
33.7	_	_
+Sparse	_	_
HRM	_	_
29.2	_	_
41.0	_	_
34.1	_	_
Arabic-English	_	_
tune	_	_
mt08	_	_
mt09	_	_
Base	_	_
49.6	_	_
49.1	_	_
51.6	_	_
+Sparse	_	_
HRM	_	_
51.7	_	_
49.9	_	_
52.2	_	_
Table	_	_
9	_	_
:	_	_
The	_	_
effect	_	_
of	_	_
Sparse	_	_
HRMs	_	_
on	_	_
complex	_	_
systems	_	_
.	_	_

#231
The	_	_
impact	_	_
of	_	_
these	_	_
reordering	_	_
features	_	_
is	_	_
reduced	_	_
slightly	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
more	_	_
carefully	_	_
tuned	_	_
translation	_	_
and	_	_
language	_	_
models	_	_
,	_	_
but	_	_
they	_	_
remain	_	_
a	_	_
strong	_	_
contributor	_	_
to	_	_
translation	_	_
quality	_	_
.	_	_

#232
6	_	_
Conclusion	_	_
We	_	_
have	_	_
shown	_	_
that	_	_
sparse	_	_
reordering	_	_
features	_	_
can	capability-options	_
improve	_	_
the	_	_
quality	_	_
of	_	_
phrase-based	_	_
translations	_	_
,	_	_
even	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
lexicalized	_	_
reordering	_	_
models	_	_
that	_	_
track	_	_
the	_	_
same	_	_
orientations	_	_
.	_	_

#233
We	_	_
have	_	_
compared	_	_
this	_	_
solution	_	_
to	_	_
a	_	_
maximum	_	_
entropy	_	_
model	_	_
,	_	_
which	_	_
does	_	_
not	_	_
improve	_	_
our	_	_
HRM	_	_
baseline	_	_
.	_	_

#234
Our	_	_
analysis	_	_
of	_	_
the	_	_
maximum	_	_
entropy	_	_
solution	_	_
indicates	_	_
that	_	_
smoothing	_	_
the	_	_
orientation	_	_
estimates	_	_
is	_	_
not	_	_
a	_	_
major	_	_
concern	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
millions	_	_
of	_	_
sentences	_	_
of	_	_
bitext	_	_
.	_	_

#235
This	_	_
implies	_	_
that	_	_
our	_	_
sparse	_	_
features	_	_
are	_	_
achieving	_	_
their	_	_
improvement	_	_
because	_	_
they	_	_
optimize	_	_
BLEU	_	_
with	_	_
the	_	_
decoder	_	_
in	_	_
the	_	_
loop	_	_
,	_	_
side-stepping	_	_
the	_	_
objective	_	_
mismatch	_	_
that	_	_
can	options	_
occur	_	_
when	_	_
training	_	_
on	_	_
word-aligned	_	_
data	_	_
.	_	_

#236
The	_	_
fact	_	_
that	_	_
this	_	_
is	_	_
possible	_	_
with	_	_
such	_	_
small	_	_
tuning	_	_
corpora	_	_
is	_	_
both	_	_
surprising	_	_
and	_	_
encouraging	_	_
.	_	_

#239
We	_	_
would	_	_
also	_	_
like	_	_
to	_	_
investigate	_	_
features	_	_
inspired	_	_
by	_	_
transition-based	_	_
parsing	_	_
,	_	_
such	_	_
as	_	_
features	_	_
that	_	_
look	_	_
further	_	_
down	_	_
the	_	_
reordering	_	_
stack	_	_
.	_	_

#240
Finally	_	_
,	_	_
as	_	_
there	_	_
is	_	_
evidence	_	_
that	_	_
ideas	_	_
from	_	_
lexicalized	_	_
reordering	_	_
can	capability-feasibility	_
help	_	_
hierarchical	_	_
phrase-based	_	_
SMT	_	_
(	_	_
Huck	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
,	_	_
it	_	_
would	_	_
be	_	_
interesting	_	_
to	_	_
explore	_	_
the	_	_
use	_	_
of	_	_
sparse	_	_
RMs	_	_
in	_	_
that	_	_
setting	_	_
.	_	_

#241
Acknowledgments	_	_
Thanks	_	_
to	_	_
George	_	_
Foster	_	_
,	_	_
Roland	_	_
Kuhn	_	_
and	_	_
the	_	_
anonymous	_	_
reviewers	_	_
for	_	_
their	_	_
valuable	_	_
comments	_	_
on	_	_
an	_	_
earlier	_	_
draft	_	_
.	_	_