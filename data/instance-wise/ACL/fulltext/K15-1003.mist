#0
Proceedings	_	_
of	_	_
the	_	_
19th	_	_
Conference	_	_
on	_	_
Computational	_	_
Language	_	_
Learning	_	_
,	_	_
pages	_	_
22–31	_	_
,	_	_
Beijing	_	_
,	_	_
China	_	_
,	_	_
July	_	_
30-31	_	_
,	_	_
2015.	_	_
c©2015	_	_
Association	_	_
for	_	_
Computational	_	_
Linguistics	_	_
A	_	_
Supertag-Context	_	_
Model	_	_
for	_	_
Weakly-Supervised	_	_
CCG	_	_
Parser	_	_
Learning	_	_
Dan	_	_
Garrette∗	_	_
Chris	_	_
Dyer†	_	_
Jason	_	_
Baldridge‡	_	_
Noah	_	_
A.	_	_
Smith†	_	_
∗Computer	_	_
Science	_	_
&	_	_
Engineering	_	_
,	_	_
University	_	_
of	_	_
Washington	_	_
,	_	_
dhg	_	_
@	_	_
cs.washington.edu	_	_
†School	_	_
of	_	_
Computer	_	_
Science	_	_
,	_	_
Carnegie	_	_
Mellon	_	_
University	_	_
,	_	_
{	_	_
cdyer	_	_
,	_	_
nasmith	_	_
}	_	_
@	_	_
cs.cmu.edu	_	_
‡Department	_	_
of	_	_
Linguistics	_	_
,	_	_
University	_	_
of	_	_
Texas	_	_
at	_	_
Austin	_	_
,	_	_
jbaldrid	_	_
@	_	_
utexas.edu	_	_
Abstract	_	_
Combinatory	_	_
Categorial	_	_
Grammar	_	_
(	_	_
CCG	_	_
)	_	_
is	_	_
a	_	_
lexicalized	_	_
grammar	_	_
formalism	_	_
in	_	_
which	_	_
words	_	_
are	_	_
associated	_	_
with	_	_
categories	_	_
that	_	_
specify	_	_
the	_	_
syntactic	_	_
configurations	_	_
in	_	_
which	_	_
they	_	_
may	deontic	_
occur	_	_
.	_	_

#1
We	_	_
present	_	_
a	_	_
novel	_	_
parsing	_	_
model	_	_
with	_	_
the	_	_
capacity	_	_
to	_	_
capture	_	_
the	_	_
associative	_	_
adjacent-category	_	_
relationships	_	_
intrinsic	_	_
to	_	_
CCG	_	_
by	_	_
parameterizing	_	_
the	_	_
relationships	_	_
between	_	_
each	_	_
constituent	_	_
label	_	_
and	_	_
the	_	_
preterminal	_	_
categories	_	_
directly	_	_
to	_	_
its	_	_
left	_	_
and	_	_
right	_	_
,	_	_
biasing	_	_
the	_	_
model	_	_
toward	_	_
constituent	_	_
categories	_	_
that	_	_
can	capability	_
combine	_	_
with	_	_
their	_	_
contexts	_	_
.	_	_

#2
This	_	_
builds	_	_
on	_	_
the	_	_
intuitions	_	_
of	_	_
Klein	_	_
and	_	_
Manning’s	_	_
(	_	_
2002	_	_
)	_	_
“constituentcontext”	_	_
model	_	_
,	_	_
which	_	_
demonstrated	_	_
the	_	_
value	_	_
of	_	_
modeling	_	_
context	_	_
,	_	_
but	_	_
has	_	_
the	_	_
advantage	_	_
of	_	_
being	_	_
able	_	_
to	_	_
exploit	_	_
the	_	_
properties	_	_
of	_	_
CCG	_	_
.	_	_

#10
This	_	_
DET—VERB	_	_
context	_	_
also	_	_
frequently	_	_
applies	_	_
to	_	_
the	_	_
single-word	_	_
sequence	_	_
NOUN	_	_
and	_	_
to	_	_
ADJ	_	_
ADJ	_	_
NOUN	_	_
.	_	_

#11
From	_	_
this	_	_
,	_	_
we	_	_
might	feasibility-speculation	_
deduce	_	_
that	_	_
DET—	_	_
VERB	_	_
is	_	_
a	_	_
likely	_	_
context	_	_
for	_	_
a	_	_
noun	_	_
phrase	_	_
.	_	_

#12
CCM	_	_
is	_	_
able	_	_
to	_	_
learn	_	_
which	_	_
POS	_	_
contexts	_	_
are	_	_
likely	_	_
,	_	_
and	_	_
does	_	_
so	_	_
via	_	_
a	_	_
probabilistic	_	_
generative	_	_
model	_	_
,	_	_
providing	_	_
a	_	_
statistical	_	_
,	_	_
data-driven	_	_
take	_	_
on	_	_
substitutability	_	_
.	_	_

#13
However	_	_
,	_	_
since	_	_
there	_	_
is	_	_
nothing	_	_
intrinsic	_	_
about	_	_
the	_	_
POS	_	_
pair	_	_
DET—VERB	_	_
that	_	_
indicates	_	_
a	_	_
priori	_	_
that	_	_
it	_	_
is	_	_
a	_	_
likely	_	_
constituent	_	_
context	_	_
,	_	_
this	_	_
fact	_	_
must	deontic-inference	_
be	_	_
inferred	_	_
entirely	_	_
from	_	_
the	_	_
data	_	_
.	_	_

#14
Baldridge	_	_
(	_	_
2008	_	_
)	_	_
observed	_	_
that	_	_
unlike	_	_
opaque	_	_
,	_	_
atomic	_	_
POS	_	_
labels	_	_
,	_	_
the	_	_
rich	_	_
structures	_	_
of	_	_
Combinatory	_	_
Categorial	_	_
Grammar	_	_
(	_	_
CCG	_	_
)	_	_
(	_	_
Steedman	_	_
,	_	_
2000	_	_
;	_	_
Steedman	_	_
and	_	_
Baldridge	_	_
,	_	_
2011	_	_
)	_	_
categories	_	_
reflect	_	_
universal	_	_
grammatical	_	_
properties	_	_
.	_	_

#15
CCG	_	_
is	_	_
a	_	_
lexicalized	_	_
grammar	_	_
formalism	_	_
in	_	_
which	_	_
every	_	_
constituent	_	_
in	_	_
a	_	_
sentence	_	_
is	_	_
associated	_	_
with	_	_
a	_	_
structured	_	_
category	_	_
that	_	_
specifies	_	_
its	_	_
syntactic	_	_
relationship	_	_
to	_	_
other	_	_
constituents	_	_
.	_	_

#16
For	_	_
example	_	_
,	_	_
a	_	_
category	_	_
might	options	_
encode	_	_
that	_	_
“this	_	_
constituent	_	_
can	capability	_
combine	_	_
with	_	_
a	_	_
noun	_	_
phrase	_	_
to	_	_
the	_	_
right	_	_
(	_	_
an	_	_
object	_	_
)	_	_
and	_	_
then	_	_
a	_	_
noun	_	_
phrase	_	_
to	_	_
the	_	_
left	_	_
(	_	_
a	_	_
subject	_	_
)	_	_
to	_	_
produce	_	_
a	_	_
sentence”	_	_
instead	_	_
of	_	_
simply	_	_
VERB	_	_
.	_	_

#17
CCG	_	_
has	_	_
proven	_	_
useful	_	_
as	_	_
a	_	_
framework	_	_
for	_	_
grammar	_	_
induction	_	_
due	_	_
to	_	_
its	_	_
ability	_	_
to	_	_
incorporate	_	_
linguistic	_	_
knowledge	_	_
to	_	_
guide	_	_
parser	_	_
learning	_	_
by	_	_
,	_	_
for	_	_
example	_	_
,	_	_
specifying	_	_
rules	_	_
in	_	_
lexical-expansion	_	_
algorithms	_	_
(	_	_
Bisk	_	_
and	_	_
Hockenmaier	_	_
,	_	_
2012	_	_
;	_	_
2013	_	_
)	_	_
or	_	_
encoding	_	_
that	_	_
information	_	_
as	_	_
priors	_	_
within	_	_
a	_	_
Bayesian	_	_
framework	_	_
(	_	_
Garrette	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
)	_	_
.	_	_

#20
Baldridge	_	_
further	_	_
notes	_	_
that	_	_
due	_	_
to	_	_
the	_	_
natural	_	_
associativity	_	_
of	_	_
CCG	_	_
,	_	_
adjacent	_	_
categories	_	_
tend	_	_
to	_	_
be	_	_
combinable	_	_
.	_	_

#21
We	_	_
previously	_	_
showed	_	_
that	_	_
incorporating	_	_
this	_	_
intuition	_	_
into	_	_
a	_	_
Bayesian	_	_
prior	_	_
can	capability	_
help	_	_
train	_	_
a	_	_
CCG	_	_
supertagger	_	_
(	_	_
Garrette	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
.	_	_

#22
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
present	_	_
a	_	_
novel	_	_
parsing	_	_
model	_	_
that	_	_
is	_	_
designed	_	_
specifically	_	_
for	_	_
the	_	_
capacity	_	_
to	_	_
capture	_	_
both	_	_
of	_	_
these	_	_
universal	_	_
,	_	_
intrinsic	_	_
properties	_	_
of	_	_
CCG	_	_
.	_	_

#23
We	_	_
do	_	_
so	_	_
by	_	_
extending	_	_
our	_	_
previous	_	_
,	_	_
PCFG-based	_	_
parsing	_	_
model	_	_
to	_	_
include	_	_
parameters	_	_
that	_	_
govern	_	_
the	_	_
relationship	_	_
between	_	_
constituent	_	_
categories	_	_
and	_	_
the	_	_
preterminal	_	_
categories	_	_
(	_	_
also	_	_
known	_	_
as	_	_
supertags	_	_
)	_	_
to	_	_
the	_	_
left	_	_
and	_	_
right	_	_
.	_	_

#24
The	_	_
advantage	_	_
of	_	_
modeling	_	_
context	_	_
within	_	_
a	_	_
CCG	_	_
framework	_	_
is	_	_
that	_	_
while	_	_
CCM	_	_
must	deontic	_
learn	_	_
which	_	_
contexts	_	_
are	_	_
likely	_	_
purely	_	_
from	_	_
the	_	_
data	_	_
,	_	_
the	_	_
CCG	_	_
categories	_	_
give	_	_
us	_	_
obvious	_	_
a	_	_
priori	_	_
information	_	_
about	_	_
whether	_	_
a	_	_
context	_	_
is	_	_
likely	_	_
for	_	_
a	_	_
given	_	_
constituent	_	_
based	_	_
on	_	_
whether	_	_
the	_	_
categories	_	_
are	_	_
combinable	_	_
.	_	_

#25
Biasing	_	_
our	_	_
model	_	_
towards	_	_
both	_	_
simple	_	_
categories	_	_
and	_	_
connecting	_	_
contexts	_	_
encourages	_	_
learning	_	_
structures	_	_
with	_	_
simpler	_	_
syntax	_	_
and	_	_
that	_	_
have	_	_
a	_	_
better	_	_
global	_	_
“fit”	_	_
.	_	_

#26
The	_	_
Bayesian	_	_
framework	_	_
is	_	_
well-matched	_	_
to	_	_
our	_	_
problem	_	_
since	_	_
our	_	_
inductive	_	_
biases	_	_
—	_	_
those	_	_
derived	_	_
from	_	_
universal	_	_
grammar	_	_
principles	_	_
,	_	_
weak	_	_
supervision	_	_
,	_	_
and	_	_
estimations	_	_
based	_	_
on	_	_
unannotated	_	_
data	_	_
—	_	_
can	feasibility	_
be	_	_
encoded	_	_
as	_	_
priors	_	_
,	_	_
and	_	_
we	_	_
can	feasibility	_
use	_	_
Markov	_	_
chain	_	_
Monte	_	_
Carlo	_	_
(	_	_
MCMC	_	_
)	_	_
inference	_	_
procedures	_	_
to	_	_
automatically	_	_
blend	_	_
these	_	_
biases	_	_
with	_	_
unannotated	_	_
text	_	_
that	_	_
reflects	_	_
the	_	_
way	_	_
language	_	_
is	_	_
actually	_	_
used	_	_
“in	_	_
the	_	_
wild”	_	_
.	_	_

#27
Thus	_	_
,	_	_
we	_	_
learn	_	_
context	_	_
information	_	_
based	_	_
on	_	_
statistics	_	_
in	_	_
the	_	_
data	_	_
like	_	_
CCM	_	_
,	_	_
but	_	_
have	_	_
the	_	_
advantage	_	_
of	_	_
additional	_	_
,	_	_
a	_	_
priori	_	_
biases	_	_
.	_	_

#28
It	_	_
is	_	_
important	_	_
to	_	_
note	_	_
that	_	_
the	_	_
Bayesian	_	_
setup	_	_
allows	_	_
us	_	_
to	_	_
use	_	_
these	_	_
universal	_	_
biases	_	_
as	_	_
soft	_	_
constraints	_	_
:	_	_
they	_	_
guide	_	_
the	_	_
learner	_	_
toward	_	_
more	_	_
appropriate	_	_
grammars	_	_
,	_	_
but	_	_
may	deontic	_
be	_	_
overridden	_	_
when	_	_
there	_	_
is	_	_
compelling	_	_
contradictory	_	_
evidence	_	_
in	_	_
the	_	_
data	_	_
.	_	_

#29
Methodologically	_	_
,	_	_
this	_	_
work	_	_
serves	_	_
as	_	_
an	_	_
example	_	_
of	_	_
how	_	_
linguistic-theoretical	_	_
commitments	_	_
can	feasibility	_
be	_	_
used	_	_
to	_	_
benefit	_	_
data-driven	_	_
methods	_	_
,	_	_
not	_	_
only	_	_
through	_	_
the	_	_
construction	_	_
of	_	_
a	_	_
model	_	_
family	_	_
from	_	_
a	_	_
grammar	_	_
,	_	_
as	_	_
done	_	_
in	_	_
our	_	_
previous	_	_
work	_	_
,	_	_
but	_	_
also	_	_
when	_	_
exploiting	_	_
statistical	_	_
associations	_	_
about	_	_
which	_	_
the	_	_
theory	_	_
is	_	_
silent	_	_
.	_	_

#30
While	_	_
there	_	_
has	_	_
been	_	_
much	_	_
work	_	_
in	_	_
computational	_	_
modeling	_	_
of	_	_
the	_	_
interaction	_	_
between	_	_
universal	_	_
grammar	_	_
and	_	_
observable	_	_
data	_	_
in	_	_
the	_	_
context	_	_
of	_	_
studying	_	_
child	_	_
language	_	_
acquisition	_	_
(	_	_
e.g.	_	_
,	_	_
Villavicencio	_	_
,	_	_
2002	_	_
;	_	_
Goldwater	_	_
,	_	_
2007	_	_
)	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
applying	_	_
these	_	_
principles	_	_
to	_	_
the	_	_
design	_	_
of	_	_
models	_	_
and	_	_
learning	_	_
procedures	_	_
that	_	_
result	_	_
in	_	_
better	_	_
parsing	_	_
tools	_	_
.	_	_

#31
Given	_	_
our	_	_
desire	_	_
to	_	_
train	_	_
NLP	_	_
models	_	_
in	_	_
low-supervision	_	_
scenarios	_	_
,	_	_
the	_	_
possibility	_	_
of	_	_
constructing	_	_
inductive	_	_
biases	_	_
out	_	_
of	_	_
universal	_	_
properties	_	_
of	_	_
language	_	_
is	_	_
enticing	_	_
:	_	_
if	_	_
we	_	_
can	feasibility	_
do	_	_
this	_	_
well	_	_
,	_	_
then	_	_
it	_	_
only	_	_
needs	_	_
to	_	_
be	_	_
done	_	_
once	_	_
,	_	_
and	_	_
can	feasibility	_
be	_	_
applied	_	_
to	_	_
any	_	_
language	_	_
or	_	_
domain	_	_
without	_	_
adaptation	_	_
.	_	_

#32
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
seek	_	_
to	_	_
learn	_	_
from	_	_
only	_	_
raw	_	_
data	_	_
and	_	_
an	_	_
incomplete	_	_
dictionary	_	_
mapping	_	_
some	_	_
words	_	_
to	_	_
sets	_	_
of	_	_
potential	_	_
supertags	_	_
.	_	_

#36
2	_	_
Combinatory	_	_
Categorial	_	_
Grammar	_	_
In	_	_
the	_	_
CCG	_	_
formalism	_	_
,	_	_
every	_	_
constituent	_	_
,	_	_
including	_	_
those	_	_
at	_	_
the	_	_
lexical	_	_
level	_	_
,	_	_
is	_	_
associated	_	_
with	_	_
a	_	_
structured	_	_
CCG	_	_
category	_	_
that	_	_
defines	_	_
that	_	_
constituent’s	_	_
relationships	_	_
to	_	_
the	_	_
other	_	_
constituents	_	_
in	_	_
the	_	_
sentence	_	_
.	_	_

#37
Categories	_	_
are	_	_
defined	_	_
by	_	_
a	_	_
recursive	_	_
structure	_	_
,	_	_
where	_	_
a	_	_
category	_	_
is	_	_
either	_	_
atomic	_	_
(	_	_
possibly	_	_
with	_	_
features	_	_
)	_	_
,	_	_
or	_	_
a	_	_
function	_	_
from	_	_
one	_	_
category	_	_
to	_	_
another	_	_
,	_	_
as	_	_
indicated	_	_
by	_	_
a	_	_
slash	_	_
operator	_	_
:	_	_
C	_	_
→	_	_
{	_	_
s	_	_
,	_	_
sdcl	_	_
,	_	_
sadj	_	_
,	_	_
sb	_	_
,	_	_
np	_	_
,	_	_
n	_	_
,	_	_
nnum	_	_
,	_	_
pp	_	_
,	_	_
...	_	_
}	_	_
C	_	_
→	_	_
{	_	_
(	_	_
C/C	_	_
)	_	_
,	_	_
(	_	_
C	_	_
\C	_	_
)	_	_
}	_	_
Categories	_	_
of	_	_
adjacent	_	_
constituents	_	_
can	feasibility	_
be	_	_
combined	_	_
using	_	_
one	_	_
of	_	_
a	_	_
set	_	_
of	_	_
combination	_	_
rules	_	_
to	_	_
form	_	_
categories	_	_
of	_	_
higher-level	_	_
constituents	_	_
,	_	_
as	_	_
seen	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#38
The	_	_
direction	_	_
of	_	_
the	_	_
slash	_	_
operator	_	_
gives	_	_
the	_	_
behavior	_	_
of	_	_
the	_	_
function	_	_
.	_	_

#39
A	_	_
category	_	_
(	_	_
s\np	_	_
)	_	_
/pp	_	_
might	options	_
describe	_	_
an	_	_
intransitive	_	_
verb	_	_
with	_	_
a	_	_
prepositional	_	_
phrase	_	_
complement	_	_
;	_	_
it	_	_
combines	_	_
on	_	_
the	_	_
right	_	_
(	_	_
/	_	_
)	_	_
with	_	_
a	_	_
constituent	_	_
with	_	_
category	_	_
pp	_	_
,	_	_
and	_	_
then	_	_
on	_	_
the	_	_
left	_	_
(	_	_
\	_	_
)	_	_
with	_	_
a	_	_
noun	_	_
phrase	_	_
(	_	_
np	_	_
)	_	_
that	_	_
serves	_	_
as	_	_
its	_	_
subject	_	_
.	_	_

#40
23	_	_
s	_	_
np	_	_
np/n	_	_
n	_	_
s\np	_	_
(	_	_
s\np	_	_
)	_	_
/pp	_	_
pp	_	_
pp/np	_	_
np	_	_
The	_	_
man	_	_
walks	_	_
to	_	_
work	_	_
Figure	_	_
1	_	_
:	_	_
CCG	_	_
parse	_	_
for	_	_
“The	_	_
man	_	_
walks	_	_
to	_	_
work.”	_	_
We	_	_
follow	_	_
Lewis	_	_
and	_	_
Steedman	_	_
(	_	_
2014	_	_
)	_	_
in	_	_
allowing	_	_
a	_	_
small	_	_
set	_	_
of	_	_
generic	_	_
,	_	_
linguistically-plausible	_	_
unary	_	_
and	_	_
binary	_	_
grammar	_	_
rules	_	_
.	_	_

#44
As	_	_
a	_	_
motivating	_	_
example	_	_
,	_	_
consider	_	_
the	_	_
sentence	_	_
“The	_	_
lazy	_	_
dog	_	_
sleeps”	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
.	_	_

#45
The	_	_
word	_	_
lazy	_	_
,	_	_
with	_	_
category	_	_
n/n	_	_
,	_	_
can	capability-deontic-options	_
either	_	_
combine	_	_
with	_	_
dog	_	_
(	_	_
n	_	_
)	_	_
via	_	_
the	_	_
Forward	_	_
Application	_	_
rule	_	_
(	_	_
>	_	_
)	_	_
,	_	_
or	_	_
with	_	_
The	_	_
(	_	_
np/n	_	_
)	_	_
via	_	_
the	_	_
Forward	_	_
Composition	_	_
(	_	_
>	_	_
B	_	_
)	_	_
rule	_	_
.	_	_

#46
Baldridge	_	_
(	_	_
2008	_	_
)	_	_
showed	_	_
that	_	_
this	_	_
tendency	_	_
for	_	_
adjacent	_	_
supertags	_	_
to	_	_
be	_	_
combinable	_	_
can	feasibility	_
be	_	_
used	_	_
to	_	_
bias	_	_
a	_	_
sequence	_	_
model	_	_
in	_	_
order	_	_
to	_	_
learn	_	_
better	_	_
CCG	_	_
supertaggers	_	_
.	_	_

#47
However	_	_
,	_	_
we	_	_
can	rhetorical	_
see	_	_
that	_	_
if	_	_
the	_	_
supertags	_	_
of	_	_
adjacent	_	_
words	_	_
lazy	_	_
(	_	_
n/n	_	_
)	_	_
and	_	_
dog	_	_
(	_	_
n	_	_
)	_	_
combine	_	_
,	_	_
then	_	_
they	_	_
will	_	_
produce	_	_
the	_	_
category	_	_
n	_	_
,	_	_
which	_	_
describes	_	_
the	_	_
entire	_	_
constituent	_	_
span	_	_
“lazy	_	_
dog”	_	_
.	_	_

#48
Since	_	_
we	_	_
have	_	_
produced	_	_
a	_	_
new	_	_
category	_	_
that	_	_
subsumes	_	_
that	_	_
entire	_	_
span	_	_
,	_	_
a	_	_
valid	_	_
parse	_	_
must	deontic	_
next	_	_
combine	_	_
that	_	_
n	_	_
with	_	_
one	_	_
of	_	_
the	_	_
remaining	_	_
supertags	_	_
to	_	_
the	_	_
left	_	_
or	_	_
right	_	_
,	_	_
producing	_	_
either	_	_
(	_	_
The·	_	_
(	_	_
lazy·dog	_	_
)	_	_
)	_	_
·sleeps	_	_
or	_	_
The·	_	_
(	_	_
(	_	_
lazy·dog	_	_
)	_	_
·sleeps	_	_
)	_	_
.	_	_

#49
Because	_	_
we	_	_
know	_	_
that	_	_
one	_	_
(	_	_
or	_	_
both	_	_
)	_	_
of	_	_
these	_	_
combinations	_	_
must	inference	_
be	_	_
valid	_	_
,	_	_
we	_	_
will	_	_
similarly	_	_
want	_	_
a	_	_
strong	_	_
prior	_	_
on	_	_
the	_	_
connectivity	_	_
between	_	_
lazy·dog	_	_
and	_	_
its	_	_
supertag	_	_
context	_	_
:	_	_
The↔	_	_
(	_	_
lazy·dog	_	_
)	_	_
↔sleeps	_	_
.	_	_

#50
Assuming	_	_
T	_	_
is	_	_
the	_	_
full	_	_
set	_	_
of	_	_
known	_	_
categories	_	_
,	_	_
the	_	_
generative	_	_
process	_	_
for	_	_
our	_	_
model	_	_
is	_	_
:	_	_
np/n	_	_
n/n	_	_
n	_	_
s\np	_	_
The	_	_
lazy	_	_
dog	_	_
sleeps	_	_
n	_	_
Figure	_	_
2	_	_
:	_	_
Higher-level	_	_
category	_	_
n	_	_
subsumes	_	_
the	_	_
categories	_	_
of	_	_
its	_	_
constituents	_	_
.	_	_

#51
Thus	_	_
,	_	_
n	_	_
should	deontic	_
have	_	_
a	_	_
strong	_	_
prior	_	_
on	_	_
combinability	_	_
with	_	_
its	_	_
adjacent	_	_
supertags	_	_
np/n	_	_
and	_	_
s\np	_	_
.	_	_

#52
Parameters	_	_
:	_	_
θROOT	_	_
∼	_	_
Dir	_	_
(	_	_
αROOT	_	_
,	_	_
θROOT-0	_	_
)	_	_
θBINt	_	_
∼	_	_
Dir	_	_
(	_	_
αBIN	_	_
,	_	_
θBIN-0	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
θUNt	_	_
∼	_	_
Dir	_	_
(	_	_
αUN	_	_
,	_	_
θUN-0	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
θTERMt	_	_
∼	_	_
Dir	_	_
(	_	_
αTERM	_	_
,	_	_
θTERM-0t	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
λt	_	_
∼	_	_
Dir	_	_
(	_	_
αλ	_	_
,	_	_
λ0	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
θLCTXt	_	_
∼	_	_
Dir	_	_
(	_	_
αLCTX	_	_
,	_	_
θLCTX-0t	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
θRCTXt	_	_
∼	_	_
Dir	_	_
(	_	_
αRCTX	_	_
,	_	_
θRCTX-0t	_	_
)	_	_
∀t	_	_
∈	_	_
T	_	_
Sentence	_	_
:	_	_
do	_	_
s	_	_
∼	_	_
Cat	_	_
(	_	_
θROOT	_	_
)	_	_
y	_	_
|	_	_
s	_	_
∼	_	_
SCM	_	_
(	_	_
s	_	_
)	_	_
until	_	_
the	_	_
tree	_	_
y	_	_
is	_	_
valid	_	_
where	_	_
〈`	_	_
,	_	_
y	_	_
,	_	_
r〉	_	_
|	_	_
t	_	_
∼	_	_
SCM	_	_
(	_	_
t	_	_
)	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
z	_	_
∼	_	_
Cat	_	_
(	_	_
λt	_	_
)	_	_
if	_	_
z	_	_
=	_	_
B	_	_
:	_	_
〈u	_	_
,	_	_
v〉	_	_
|	_	_
t	_	_
∼	_	_
Cat	_	_
(	_	_
θBINt	_	_
)	_	_
yL	_	_
|	_	_
u	_	_
∼	_	_
SCM	_	_
(	_	_
u	_	_
)	_	_
,	_	_
yR	_	_
|	_	_
v	_	_
∼	_	_
SCM	_	_
(	_	_
v	_	_
)	_	_
y	_	_
=	_	_
〈yL	_	_
,	_	_
yR〉	_	_
if	_	_
z	_	_
=	_	_
U	_	_
:	_	_
〈u〉	_	_
|	_	_
t	_	_
∼	_	_
Cat	_	_
(	_	_
θUNt	_	_
)	_	_
y	_	_
|	_	_
u	_	_
∼	_	_
SCM	_	_
(	_	_
u	_	_
)	_	_
if	_	_
z	_	_
=	_	_
T	_	_
:	_	_
w	_	_
|	_	_
t	_	_
∼	_	_
Cat	_	_
(	_	_
θTERMt	_	_
)	_	_
y	_	_
=	_	_
w	_	_
`	_	_
|	_	_
t	_	_
∼	_	_
Cat	_	_
(	_	_
θLCTXt	_	_
)	_	_
,	_	_
r	_	_
|	_	_
t	_	_
∼	_	_
Cat	_	_
(	_	_
θRCTXt	_	_
)	_	_
The	_	_
process	_	_
begins	_	_
by	_	_
sampling	_	_
the	_	_
parameters	_	_
from	_	_
Dirichlet	_	_
distributions	_	_
:	_	_
a	_	_
distribution	_	_
θROOT	_	_
over	_	_
root	_	_
categories	_	_
,	_	_
a	_	_
conditional	_	_
distribution	_	_
θBINt	_	_
over	_	_
binary	_	_
branching	_	_
productions	_	_
given	_	_
category	_	_
t	_	_
,	_	_
θUNt	_	_
for	_	_
unary	_	_
rewrite	_	_
productions	_	_
,	_	_
θTERMt	_	_
for	_	_
terminal	_	_
(	_	_
word	_	_
)	_	_
productions	_	_
,	_	_
and	_	_
θLCTXt	_	_
and	_	_
θ	_	_
RCTX	_	_
t	_	_
for	_	_
left	_	_
and	_	_
right	_	_
contexts	_	_
.	_	_

#56
For	_	_
each	_	_
subtree	_	_
rooted	_	_
by	_	_
a	_	_
category	_	_
t	_	_
,	_	_
we	_	_
generate	_	_
a	_	_
left	_	_
context	_	_
supertag	_	_
`	_	_
and	_	_
a	_	_
right	_	_
context	_	_
supertag	_	_
r.	_	_
Then	_	_
,	_	_
we	_	_
sam24	_	_
samAij	_	_
samBik	_	_
Ckj	_	_
ti-1	_	_
tjti	_	_
tj-1tk-1	_	_
tk	_	_
Figure	_	_
3	_	_
:	_	_
The	_	_
generative	_	_
process	_	_
starting	_	_
with	_	_
non-terminal	_	_
Aij	_	_
,	_	_
where	_	_
tx	_	_
is	_	_
the	_	_
supertag	_	_
for	_	_
wx	_	_
,	_	_
the	_	_
word	_	_
at	_	_
position	_	_
x	_	_
,	_	_
and	_	_
“A→	_	_
B	_	_
C”	_	_
is	_	_
a	_	_
valid	_	_
production	_	_
in	_	_
the	_	_
grammar	_	_
.	_	_

#57
We	_	_
can	rhetorical	_
see	_	_
that	_	_
nonterminal	_	_
Aij	_	_
generates	_	_
nonterminals	_	_
Bik	_	_
and	_	_
Ckj	_	_
(	_	_
solid	_	_
arrows	_	_
)	_	_
as	_	_
well	_	_
as	_	_
generating	_	_
left	_	_
context	_	_
ti-1	_	_
and	_	_
right	_	_
context	_	_
tj	_	_
(	_	_
dashed	_	_
arrows	_	_
)	_	_
;	_	_
likewise	_	_
for	_	_
Bik	_	_
and	_	_
Ckj	_	_
.	_	_

#58
The	_	_
triangle	_	_
under	_	_
a	_	_
non-terminal	_	_
indicates	_	_
the	_	_
complete	_	_
subtree	_	_
rooted	_	_
by	_	_
the	_	_
node	_	_
.	_	_

#62
See	_	_
Figure	_	_
3	_	_
for	_	_
a	_	_
graphical	_	_
depiction	_	_
of	_	_
the	_	_
generative	_	_
behavior	_	_
of	_	_
the	_	_
process	_	_
.	_	_

#63
Finally	_	_
,	_	_
since	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
generate	_	_
a	_	_
supertag	_	_
context	_	_
category	_	_
that	_	_
does	_	_
not	_	_
match	_	_
the	_	_
actual	_	_
category	_	_
generated	_	_
by	_	_
the	_	_
neighboring	_	_
constituent	_	_
,	_	_
we	_	_
must	deontic	_
allow	_	_
our	_	_
process	_	_
to	_	_
reject	_	_
such	_	_
invalid	_	_
trees	_	_
and	_	_
re-attempt	_	_
to	_	_
sample	_	_
.	_	_

#64
Like	_	_
CCM	_	_
,	_	_
this	_	_
model	_	_
is	_	_
deficient	_	_
since	_	_
the	_	_
same	_	_
supertags	_	_
are	_	_
generated	_	_
multiple	_	_
times	_	_
,	_	_
and	_	_
parses	_	_
with	_	_
conflicting	_	_
supertags	_	_
are	_	_
not	_	_
valid	_	_
.	_	_

#65
Since	_	_
we	_	_
are	_	_
not	_	_
generating	_	_
from	_	_
the	_	_
model	_	_
,	_	_
this	_	_
does	_	_
not	_	_
introduce	_	_
difficulties	_	_
(	_	_
Klein	_	_
and	_	_
Manning	_	_
,	_	_
2002	_	_
)	_	_
.	_	_

#66
One	_	_
additional	_	_
complication	_	_
that	_	_
must	deontic	_
be	_	_
addressed	_	_
is	_	_
that	_	_
left-frontier	_	_
non-terminal	_	_
categories	_	_
—	_	_
those	_	_
whose	_	_
subtree	_	_
span	_	_
includes	_	_
the	_	_
first	_	_
word	_	_
of	_	_
the	_	_
sentence	_	_
—	_	_
do	_	_
not	_	_
have	_	_
a	_	_
left-side	_	_
supertag	_	_
to	_	_
use	_	_
as	_	_
context	_	_
.	_	_

#67
For	_	_
these	_	_
cases	_	_
,	_	_
we	_	_
use	_	_
the	_	_
special	_	_
sentence-start	_	_
symbol	_	_
〈S〉	_	_
to	_	_
serve	_	_
as	_	_
context	_	_
.	_	_

#71
To	_	_
formalize	_	_
the	_	_
notion	_	_
of	_	_
what	_	_
it	_	_
means	_	_
for	_	_
a	_	_
category	_	_
to	_	_
be	_	_
more	_	_
“plausible”	_	_
,	_	_
we	_	_
extend	_	_
the	_	_
category	_	_
generator	_	_
of	_	_
our	_	_
previous	_	_
work	_	_
,	_	_
which	_	_
we	_	_
will	_	_
call	_	_
PCAT	_	_
.	_	_

#72
We	_	_
can	feasibility	_
define	_	_
PCAT	_	_
using	_	_
a	_	_
probabilistic	_	_
grammar	_	_
(	_	_
Garrette	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
.	_	_

#73
The	_	_
grammar	_	_
may	deontic	_
first	_	_
generate	_	_
a	_	_
start	_	_
or	_	_
end	_	_
category	_	_
(	_	_
〈S〉	_	_
,	_	_
〈E〉	_	_
)	_	_
with	_	_
probability	_	_
pse	_	_
or	_	_
a	_	_
special	_	_
tokendeletion	_	_
category	_	_
(	_	_
〈D〉	_	_
;	_	_
explained	_	_
in	_	_
§5	_	_
)	_	_
with	_	_
probability	_	_
pdel	_	_
,	_	_
or	_	_
a	_	_
standard	_	_
CCG	_	_
category	_	_
C	_	_
:	_	_
X→〈S〉	_	_
|	_	_
〈E〉	_	_
pse	_	_
X→〈D〉	_	_
pdel	_	_
X→C	_	_
(	_	_
1−	_	_
(	_	_
2pse	_	_
+	_	_
pdel	_	_
)	_	_
)	_	_
·PC	_	_
(	_	_
C	_	_
)	_	_
For	_	_
each	_	_
sentence	_	_
s	_	_
,	_	_
there	_	_
will	_	_
be	_	_
one	_	_
〈S〉	_	_
and	_	_
one	_	_
〈E〉	_	_
,	_	_
so	_	_
we	_	_
set	_	_
pse	_	_
=	_	_
1/	_	_
(	_	_
25	_	_
+	_	_
2	_	_
)	_	_
,	_	_
since	_	_
the	_	_
average	_	_
sentence	_	_
length	_	_
in	_	_
the	_	_
corpora	_	_
is	_	_
roughly	_	_
25	_	_
.	_	_

#74
To	_	_
discourage	_	_
the	_	_
model	_	_
from	_	_
deleting	_	_
tokens	_	_
(	_	_
only	_	_
applies	_	_
during	_	_
testing	_	_
)	_	_
,	_	_
we	_	_
set	_	_
pdel	_	_
=	_	_
10−100	_	_
.	_	_

#75
For	_	_
PC	_	_
,	_	_
the	_	_
distribution	_	_
over	_	_
standard	_	_
categories	_	_
,	_	_
we	_	_
use	_	_
a	_	_
recursive	_	_
definition	_	_
based	_	_
on	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
CCG	_	_
category	_	_
.	_	_

#76
If	_	_
p	_	_
=	_	_
1−	_	_
p	_	_
,	_	_
then:1	_	_
C→a	_	_
pterm	_	_
·patom	_	_
(	_	_
a	_	_
)	_	_
C→A/A	_	_
pterm	_	_
·pfwd	_	_
·	_	_
(	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
+	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
2	_	_
)	_	_
C→A/B	_	_
pterm	_	_
·pfwd	_	_
·	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
·PC	_	_
(	_	_
B	_	_
)	_	_
C→A\A	_	_
pterm	_	_
·pfwd	_	_
·	_	_
(	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
+	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
2	_	_
)	_	_
C→A\B	_	_
pterm	_	_
·pfwd	_	_
·	_	_
pmod	_	_
·PC	_	_
(	_	_
A	_	_
)	_	_
·PC	_	_
(	_	_
B	_	_
)	_	_
The	_	_
category	_	_
grammar	_	_
captures	_	_
important	_	_
aspects	_	_
of	_	_
what	_	_
makes	_	_
a	_	_
category	_	_
more	_	_
or	_	_
less	_	_
likely	_	_
:	_	_
(	_	_
1	_	_
)	_	_
simplicity	_	_
is	_	_
preferred	_	_
,	_	_
with	_	_
a	_	_
higher	_	_
pterm	_	_
meaning	_	_
a	_	_
stronger	_	_
emphasis	_	_
on	_	_
simplicity	_	_
;	_	_
2	_	_
(	_	_
2	_	_
)	_	_
atomic	_	_
types	_	_
may	options	_
occur	_	_
at	_	_
different	_	_
rates	_	_
,	_	_
as	_	_
given	_	_
by	_	_
patom	_	_
;	_	_
(	_	_
3	_	_
)	_	_
modifier	_	_
categories	_	_
(	_	_
A/A	_	_
or	_	_
A\A	_	_
)	_	_
are	_	_
more	_	_
likely	_	_
than	_	_
similar-complexity	_	_
non-modifiers	_	_
(	_	_
such	_	_
as	_	_
an	_	_
adverb	_	_
that	_	_
modifies	_	_
a	_	_
verb	_	_
)	_	_
;	_	_
and	_	_
(	_	_
4	_	_
)	_	_
operators	_	_
may	options	_
occur	_	_
at	_	_
different	_	_
rates	_	_
,	_	_
as	_	_
given	_	_
by	_	_
pfwd	_	_
.	_	_

#77
We	_	_
can	feasibility	_
use	_	_
PCAT	_	_
to	_	_
define	_	_
priors	_	_
on	_	_
our	_	_
production	_	_
parameters	_	_
that	_	_
bias	_	_
our	_	_
model	_	_
toward	_	_
rules	_	_
1Note	_	_
that	_	_
this	_	_
version	_	_
has	_	_
also	_	_
updated	_	_
the	_	_
probability	_	_
definitions	_	_
for	_	_
modifiers	_	_
to	_	_
be	_	_
sums	_	_
,	_	_
incorporating	_	_
the	_	_
fact	_	_
that	_	_
anyA/A	_	_
is	_	_
also	_	_
aA/B	_	_
(	_	_
likewise	_	_
forA\A	_	_
)	_	_
.	_	_

#78
This	_	_
ensures	_	_
that	_	_
our	_	_
grammar	_	_
defines	_	_
a	_	_
valid	_	_
probability	_	_
distribution	_	_
.	_	_

#84
The	_	_
right-side	_	_
context	_	_
of	_	_
a	_	_
non-terminal	_	_
category	_	_
—	_	_
the	_	_
probability	_	_
of	_	_
generating	_	_
a	_	_
category	_	_
to	_	_
the	_	_
right	_	_
of	_	_
the	_	_
current	_	_
constituent’s	_	_
category	_	_
—	_	_
corresponds	_	_
directly	_	_
to	_	_
the	_	_
category	_	_
transitions	_	_
used	_	_
for	_	_
the	_	_
HMM	_	_
supertagger	_	_
of	_	_
Garrette	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2014	_	_
)	_	_
.	_	_

#85
Thus	_	_
,	_	_
the	_	_
right-side	_	_
context	_	_
prior	_	_
mean	_	_
θRCTX-0t	_	_
can	options	_
be	_	_
biased	_	_
in	_	_
exactly	_	_
the	_	_
same	_	_
way	_	_
as	_	_
the	_	_
HMM	_	_
supertagger’s	_	_
transitions	_	_
:	_	_
toward	_	_
context	_	_
supertags	_	_
that	_	_
connect	_	_
to	_	_
the	_	_
constituent	_	_
label	_	_
.	_	_

#86
To	_	_
encode	_	_
a	_	_
notion	_	_
of	_	_
combinability	_	_
,	_	_
we	_	_
follow	_	_
Baldridge’s	_	_
(	_	_
2008	_	_
)	_	_
definition	_	_
.	_	_

#87
Briefly	_	_
,	_	_
let	_	_
κ	_	_
(	_	_
t	_	_
,	_	_
u	_	_
)	_	_
∈	_	_
{	_	_
0	_	_
,	_	_
1	_	_
}	_	_
be	_	_
an	_	_
indicator	_	_
of	_	_
whether	_	_
t	_	_
combines	_	_
with	_	_
u	_	_
(	_	_
in	_	_
that	_	_
order	_	_
)	_	_
.	_	_

#88
For	_	_
any	_	_
binary	_	_
rule	_	_
that	_	_
can	capability	_
combine	_	_
t	_	_
to	_	_
u	_	_
,	_	_
κ	_	_
(	_	_
t	_	_
,	_	_
u	_	_
)	_	_
=1	_	_
.	_	_

#89
To	_	_
ensure	_	_
that	_	_
our	_	_
prior	_	_
captures	_	_
the	_	_
natural	_	_
associativity	_	_
of	_	_
CCG	_	_
,	_	_
we	_	_
define	_	_
combinability	_	_
in	_	_
this	_	_
context	_	_
to	_	_
include	_	_
composition	_	_
rules	_	_
as	_	_
well	_	_
as	_	_
application	_	_
rules	_	_
.	_	_

#93
atoms	_	_
have	_	_
features	_	_
associated	_	_
,	_	_
then	_	_
the	_	_
atoms	_	_
are	_	_
allowed	_	_
to	_	_
unify	_	_
if	_	_
the	_	_
features	_	_
match	_	_
,	_	_
or	_	_
if	_	_
at	_	_
least	_	_
one	_	_
of	_	_
them	_	_
does	_	_
not	_	_
have	_	_
a	_	_
feature	_	_
.	_	_

#94
In	_	_
defining	_	_
κ	_	_
,	_	_
it	_	_
is	_	_
also	_	_
important	_	_
to	_	_
ignore	_	_
possible	_	_
arguments	_	_
on	_	_
the	_	_
wrong	_	_
side	_	_
of	_	_
the	_	_
combination	_	_
since	_	_
they	_	_
can	feasibility	_
be	_	_
consumed	_	_
without	_	_
affecting	_	_
the	_	_
connection	_	_
between	_	_
the	_	_
two	_	_
.	_	_

#95
To	_	_
achieve	_	_
this	_	_
for	_	_
κ	_	_
(	_	_
t	_	_
,	_	_
u	_	_
)	_	_
,	_	_
it	_	_
is	_	_
assumed	_	_
that	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
consume	_	_
all	_	_
preceding	_	_
arguments	_	_
of	_	_
t	_	_
and	_	_
all	_	_
following	_	_
arguments	_	_
of	_	_
u.	_	_
So	_	_
κ	_	_
(	_	_
np	_	_
,	_	_
(	_	_
s\np	_	_
)	_	_
/np	_	_
)	_	_
=	_	_
1	_	_
.	_	_

#111
During	_	_
sampling	_	_
,	_	_
we	_	_
restrict	_	_
the	_	_
tag	_	_
choices	_	_
for	_	_
a	_	_
word	_	_
w	_	_
to	_	_
categories	_	_
allowed	_	_
by	_	_
the	_	_
tag	_	_
dictionary	_	_
.	_	_

#112
Since	_	_
real-world	_	_
learning	_	_
scenarios	_	_
will	_	_
always	_	_
lack	_	_
complete	_	_
knowledge	_	_
of	_	_
the	_	_
lexicon	_	_
,	_	_
we	_	_
,	_	_
too	_	_
,	_	_
want	_	_
to	_	_
allow	_	_
for	_	_
unknown	_	_
words	_	_
;	_	_
for	_	_
these	_	_
,	_	_
we	_	_
assume	_	_
the	_	_
word	_	_
may	deontic	_
take	_	_
any	_	_
known	_	_
supertag	_	_
.	_	_

#113
We	_	_
refer	_	_
to	_	_
the	_	_
sequence	_	_
of	_	_
word	_	_
tokens	_	_
as	_	_
w	_	_
and	_	_
a	_	_
non-terminal	_	_
category	_	_
covering	_	_
the	_	_
span	_	_
i	_	_
through	_	_
j	_	_
−	_	_
1	_	_
as	_	_
yij	_	_
.	_	_

#124
y0n	_	_
∼	_	_
θROOTt	_	_
·	_	_
p	_	_
(	_	_
w0	_	_
:	_	_
n−1	_	_
|	_	_
y0n	_	_
=	_	_
t	_	_
)	_	_
x	_	_
|	_	_
yij	_	_
∼	_	_
〈	_	_
θBINyij	_	_
(	_	_
〈u	_	_
,	_	_
v〉	_	_
)	_	_
·	_	_
p	_	_
(	_	_
wi	_	_
:	_	_
k−1	_	_
|	_	_
yik	_	_
=	_	_
u	_	_
)	_	_
·	_	_
p	_	_
(	_	_
wk	_	_
:	_	_
j−1	_	_
|	_	_
ykj	_	_
=	_	_
v	_	_
)	_	_
∀	_	_
yik	_	_
,	_	_
ykj	_	_
when	_	_
j	_	_
>	_	_
i+	_	_
1	_	_
,	_	_
θUNyij	_	_
(	_	_
〈u〉	_	_
)	_	_
·	_	_
p	_	_
(	_	_
wi	_	_
:	_	_
j−1	_	_
|	_	_
y′ij	_	_
=	_	_
u	_	_
)	_	_
∀	_	_
y′ij	_	_
,	_	_
θTERMyij	_	_
(	_	_
wi	_	_
)	_	_
when	_	_
j	_	_
=	_	_
i+	_	_
1	_	_
〉	_	_
where	_	_
x	_	_
is	_	_
either	_	_
a	_	_
split	_	_
point	_	_
k	_	_
and	_	_
pair	_	_
of	_	_
categories	_	_
yik	_	_
,	_	_
ykj	_	_
resulting	_	_
from	_	_
a	_	_
binary	_	_
rewrite	_	_
rule	_	_
,	_	_
a	_	_
single	_	_
category	_	_
y′ij	_	_
resulting	_	_
from	_	_
a	_	_
unary	_	_
rule	_	_
,	_	_
or	_	_
a	_	_
word	_	_
w	_	_
resulting	_	_
from	_	_
a	_	_
terminal	_	_
rule	_	_
.	_	_

#125
The	_	_
MH	_	_
procedure	_	_
requires	_	_
an	_	_
acceptance	_	_
distribution	_	_
A	_	_
that	_	_
is	_	_
used	_	_
to	_	_
accept	_	_
or	_	_
reject	_	_
a	_	_
tree	_	_
sampled	_	_
from	_	_
the	_	_
proposal	_	_
Q.	_	_
The	_	_
probability	_	_
of	_	_
accepting	_	_
new	_	_
tree	_	_
y′	_	_
given	_	_
the	_	_
previous	_	_
tree	_	_
y	_	_
is	_	_
:	_	_
A	_	_
(	_	_
y′	_	_
|	_	_
y	_	_
)	_	_
=	_	_
min	_	_
(	_	_
1	_	_
,	_	_
P	_	_
(	_	_
y′	_	_
)	_	_
P	_	_
(	_	_
y	_	_
)	_	_
Q	_	_
(	_	_
y	_	_
)	_	_
Q	_	_
(	_	_
y′	_	_
)	_	_
)	_	_
Since	_	_
Q	_	_
is	_	_
defined	_	_
as	_	_
a	_	_
subset	_	_
of	_	_
P	_	_
’s	_	_
parameters	_	_
,	_	_
it	_	_
is	_	_
the	_	_
case	_	_
that	_	_
:	_	_
P	_	_
(	_	_
y	_	_
)	_	_
=	_	_
Q	_	_
(	_	_
y	_	_
)	_	_
·	_	_
p	_	_
(	_	_
y	_	_
|	_	_
θLCTX	_	_
,	_	_
θRCTX	_	_
)	_	_
After	_	_
substituting	_	_
this	_	_
for	_	_
each	_	_
P	_	_
inA	_	_
,	_	_
all	_	_
of	_	_
theQ	_	_
factors	_	_
cancel	_	_
,	_	_
yielding	_	_
the	_	_
acceptance	_	_
distribution	_	_
defined	_	_
purely	_	_
in	_	_
terms	_	_
of	_	_
context	_	_
parameters	_	_
:	_	_
A	_	_
(	_	_
y′	_	_
|	_	_
y	_	_
)	_	_
=	_	_
min	_	_
(	_	_
1	_	_
,	_	_
p	_	_
(	_	_
y′	_	_
|	_	_
θLCTX	_	_
,	_	_
θRCTX	_	_
)	_	_
p	_	_
(	_	_
y	_	_
|	_	_
θLCTX	_	_
,	_	_
θRCTX	_	_
)	_	_
)	_	_
For	_	_
completeness	_	_
,	_	_
we	_	_
note	_	_
that	_	_
the	_	_
probability	_	_
of	_	_
a	_	_
tree	_	_
y	_	_
given	_	_
only	_	_
the	_	_
context	_	_
parameters	_	_
is:5	_	_
p	_	_
(	_	_
y	_	_
|	_	_
θLCTX	_	_
,	_	_
θRCTX	_	_
)	_	_
=∏	_	_
0≤i	_	_
<	_	_
j≤n	_	_
θLCTX	_	_
(	_	_
yi−1	_	_
,	_	_
i	_	_
|	_	_
yij	_	_
)	_	_
·	_	_
θRCTX	_	_
(	_	_
yj	_	_
,	_	_
j+1	_	_
|	_	_
yij	_	_
)	_	_
5Note	_	_
that	_	_
there	_	_
may	options	_
actually	_	_
be	_	_
multiple	_	_
yij	_	_
due	_	_
to	_	_
unary	_	_
rules	_	_
that	_	_
“loop	_	_
back”	_	_
to	_	_
the	_	_
same	_	_
position	_	_
(	_	_
i	_	_
,	_	_
j	_	_
)	_	_
;	_	_
all	_	_
of	_	_
these	_	_
much	_	_
be	_	_
included	_	_
in	_	_
the	_	_
product	_	_
.	_	_

#126
27	_	_
Before	_	_
we	_	_
begin	_	_
sampling	_	_
,	_	_
we	_	_
initialize	_	_
each	_	_
distribution	_	_
to	_	_
its	_	_
prior	_	_
mean	_	_
(	_	_
θROOT=θROOT-0	_	_
,	_	_
θBINt	_	_
=θ	_	_
BIN-0	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#133
We	_	_
use	_	_
this	_	_
pool	_	_
of	_	_
trees	_	_
to	_	_
compute	_	_
model	_	_
parameters	_	_
using	_	_
the	_	_
same	_	_
procedure	_	_
as	_	_
we	_	_
used	_	_
directly	_	_
above	_	_
to	_	_
sample	_	_
parameters	_	_
,	_	_
except	_	_
that	_	_
instead	_	_
of	_	_
drawing	_	_
a	_	_
Dirichlet	_	_
sample	_	_
based	_	_
on	_	_
the	_	_
vector	_	_
of	_	_
counts	_	_
,	_	_
we	_	_
simply	_	_
normalize	_	_
those	_	_
counts	_	_
.	_	_

#134
However	_	_
,	_	_
since	_	_
we	_	_
require	_	_
a	_	_
final	_	_
model	_	_
that	_	_
can	capability	_
parse	_	_
sentences	_	_
efficiently	_	_
,	_	_
we	_	_
drop	_	_
the	_	_
context	_	_
parameters	_	_
,	_	_
making	_	_
the	_	_
model	_	_
a	_	_
standard	_	_
PCFG	_	_
,	_	_
which	_	_
allows	_	_
us	_	_
to	_	_
use	_	_
the	_	_
probabilistic	_	_
CKY	_	_
algorithm	_	_
.	_	_

#135
5	_	_
Experiments	_	_
In	_	_
our	_	_
evaluation	_	_
we	_	_
compared	_	_
our	_	_
supertagcontext	_	_
approach	_	_
to	_	_
(	_	_
our	_	_
reimplementation	_	_
of	_	_
)	_	_
the	_	_
best-performing	_	_
model	_	_
of	_	_
our	_	_
previous	_	_
work	_	_
(	_	_
Garrette	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
)	_	_
,	_	_
which	_	_
SCM	_	_
extends	_	_
.	_	_

#146
We	_	_
ran	_	_
our	_	_
sampler	_	_
for	_	_
50	_	_
burn-in	_	_
and	_	_
50	_	_
sampling	_	_
iterations	_	_
.	_	_

#147
CCG	_	_
parsers	_	_
are	_	_
typically	_	_
evaluated	_	_
on	_	_
the	_	_
dependencies	_	_
they	_	_
produce	_	_
instead	_	_
of	_	_
their	_	_
CCG	_	_
derivations	_	_
directly	_	_
since	_	_
there	_	_
can	options	_
be	_	_
many	_	_
different	_	_
CCG	_	_
parse	_	_
trees	_	_
that	_	_
all	_	_
represent	_	_
the	_	_
same	_	_
dependency	_	_
relationships	_	_
(	_	_
spurious	_	_
ambiguity	_	_
)	_	_
,	_	_
and	_	_
CCG-to-dependency	_	_
conversion	_	_
can	capability	_
collapse	_	_
those	_	_
differences	_	_
.	_	_

#148
To	_	_
convert	_	_
a	_	_
CCG	_	_
tree	_	_
into	_	_
a	_	_
dependency	_	_
tree	_	_
,	_	_
we	_	_
follow	_	_
Lewis	_	_
and	_	_
Steedman	_	_
6In	_	_
order	_	_
to	_	_
ensure	_	_
that	_	_
these	_	_
concentration	_	_
parameters	_	_
,	_	_
while	_	_
high	_	_
,	_	_
were	_	_
not	_	_
dominating	_	_
the	_	_
posterior	_	_
distributions	_	_
,	_	_
we	_	_
ran	_	_
experiments	_	_
in	_	_
which	_	_
they	_	_
were	_	_
set	_	_
much	_	_
higher	_	_
(	_	_
including	_	_
using	_	_
the	_	_
prior	_	_
alone	_	_
)	_	_
,	_	_
and	_	_
found	_	_
that	_	_
accuracies	_	_
plummeted	_	_
in	_	_
those	_	_
cases	_	_
,	_	_
demonstrating	_	_
that	_	_
there	_	_
is	_	_
a	_	_
good	_	_
balance	_	_
with	_	_
the	_	_
prior	_	_
.	_	_

#175
Because	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
understanding	_	_
how	_	_
our	_	_
models	_	_
perform	_	_
under	_	_
varying	_	_
amounts	_	_
of	_	_
su29	_	_
supervision	_	_
,	_	_
we	_	_
executed	_	_
sequences	_	_
of	_	_
experiments	_	_
in	_	_
which	_	_
we	_	_
reduced	_	_
the	_	_
size	_	_
of	_	_
the	_	_
corpus	_	_
from	_	_
which	_	_
the	_	_
tag	_	_
dictionary	_	_
is	_	_
drawn	_	_
,	_	_
thus	_	_
reducing	_	_
the	_	_
amount	_	_
of	_	_
information	_	_
provided	_	_
to	_	_
the	_	_
model	_	_
.	_	_

#176
As	_	_
this	_	_
information	_	_
is	_	_
reduced	_	_
,	_	_
so	_	_
is	_	_
the	_	_
size	_	_
of	_	_
the	_	_
full	_	_
inventory	_	_
of	_	_
known	_	_
CCG	_	_
categories	_	_
that	_	_
can	feasibility	_
be	_	_
used	_	_
as	_	_
supertags	_	_
.	_	_

#177
Additionally	_	_
,	_	_
a	_	_
smaller	_	_
tag	_	_
dictionary	_	_
means	_	_
that	_	_
there	_	_
will	_	_
be	_	_
vastly	_	_
more	_	_
unknown	_	_
words	_	_
;	_	_
since	_	_
our	_	_
model	_	_
must	deontic	_
assume	_	_
that	_	_
these	_	_
words	_	_
may	options	_
take	_	_
any	_	_
supertag	_	_
from	_	_
the	_	_
full	_	_
set	_	_
of	_	_
known	_	_
labels	_	_
,	_	_
the	_	_
model	_	_
must	deontic	_
contend	_	_
with	_	_
a	_	_
greatly	_	_
increased	_	_
level	_	_
of	_	_
ambiguity	_	_
.	_	_

#178
The	_	_
results	_	_
of	_	_
our	_	_
experiments	_	_
are	_	_
given	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#194
This	_	_
allowed	_	_
them	_	_
to	_	_
induce	_	_
the	_	_
inventory	_	_
of	_	_
languagespecific	_	_
types	_	_
from	_	_
the	_	_
training	_	_
data	_	_
,	_	_
without	_	_
prior	_	_
language-specific	_	_
knowledge	_	_
.	_	_

#195
7	_	_
Conclusion	_	_
Because	_	_
of	_	_
the	_	_
structured	_	_
nature	_	_
of	_	_
CCG	_	_
categories	_	_
and	_	_
the	_	_
logical	_	_
framework	_	_
in	_	_
which	_	_
they	_	_
must	deontic	_
assemble	_	_
to	_	_
form	_	_
valid	_	_
parse	_	_
trees	_	_
,	_	_
the	_	_
CCG	_	_
formalism	_	_
offers	_	_
multiple	_	_
opportunities	_	_
to	_	_
bias	_	_
model	_	_
learning	_	_
based	_	_
on	_	_
universal	_	_
,	_	_
intrinsic	_	_
properties	_	_
of	_	_
the	_	_
grammar	_	_
.	_	_

#196
In	_	_
this	_	_
paper	_	_
we	_	_
presented	_	_
a	_	_
novel	_	_
parsing	_	_
model	_	_
with	_	_
the	_	_
capacity	_	_
to	_	_
capture	_	_
the	_	_
associative	_	_
adjacent-category	_	_
relationships	_	_
intrinsic	_	_
to	_	_
CCG	_	_
by	_	_
parameterizing	_	_
supertag	_	_
contexts	_	_
,	_	_
the	_	_
supertags	_	_
appearing	_	_
on	_	_
either	_	_
side	_	_
of	_	_
each	_	_
constituent	_	_
.	_	_