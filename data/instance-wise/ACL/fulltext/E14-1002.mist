#6
Experiments	_	_
show	_	_
that	_	_
the	_	_
decoding	_	_
time	_	_
is	_	_
halved	_	_
and	_	_
forestrescoring	_	_
is	_	_
6	_	_
times	_	_
faster	_	_
,	_	_
while	_	_
reaching	_	_
accuracy	_	_
not	_	_
significantly	_	_
different	_	_
from	_	_
state	_	_
of	_	_
the	_	_
art	_	_
.	_	_

#7
1	_	_
Introduction	_	_
Machine	_	_
Translation	_	_
(	_	_
MT	_	_
)	_	_
can	capability-feasibility	_
be	_	_
addressed	_	_
as	_	_
a	_	_
structured	_	_
prediction	_	_
task	_	_
(	_	_
Brown	_	_
et	_	_
al.	_	_
,	_	_
1993	_	_
;	_	_
Yamada	_	_
and	_	_
Knight	_	_
,	_	_
2001	_	_
;	_	_
Koehn	_	_
et	_	_
al.	_	_
,	_	_
2003	_	_
)	_	_
.	_	_

#8
MT’s	_	_
goal	_	_
is	_	_
to	_	_
learn	_	_
a	_	_
mapping	_	_
function	_	_
,	_	_
f	_	_
,	_	_
from	_	_
an	_	_
input	_	_
sentence	_	_
,	_	_
x	_	_
,	_	_
into	_	_
y	_	_
=	_	_
(	_	_
t	_	_
,	_	_
h	_	_
)	_	_
,	_	_
where	_	_
t	_	_
is	_	_
the	_	_
sentence	_	_
translated	_	_
into	_	_
the	_	_
target	_	_
language	_	_
,	_	_
and	_	_
h	_	_
is	_	_
the	_	_
hidden	_	_
correspondence	_	_
structure	_	_
(	_	_
Liang	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
)	_	_
.	_	_

#9
In	_	_
Hierarchical	_	_
MT	_	_
(	_	_
HMT	_	_
)	_	_
(	_	_
Chiang	_	_
,	_	_
2005	_	_
)	_	_
the	_	_
hidden	_	_
correspondence	_	_
structure	_	_
is	_	_
the	_	_
synchronous-tree	_	_
composed	_	_
by	_	_
instantiations	_	_
of	_	_
synchronous	_	_
rules	_	_
from	_	_
the	_	_
input	_	_
grammar	_	_
,	_	_
G.	_	_
Statistical	_	_
models	_	_
usually	_	_
define	_	_
f	_	_
as	_	_
:	_	_
f	_	_
(	_	_
x	_	_
)	_	_
=	_	_
arg	_	_
maxy∈Y	_	_
Score	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
,	_	_
where	_	_
Score	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
is	_	_
a	_	_
function	_	_
whose	_	_
parameters	_	_
can	feasibility	_
be	_	_
learned	_	_
with	_	_
a	_	_
specialized	_	_
learning	_	_
algorithm	_	_
.	_	_

#10
In	_	_
MT	_	_
applications	_	_
,	_	_
it	_	_
is	_	_
not	_	_
possible	_	_
to	_	_
enumerate	_	_
all	_	_
y	_	_
∈	_	_
Y	_	_
.	_	_

#15
If	_	_
this	_	_
extreme	_	_
level	_	_
of	_	_
pruning	_	_
was	_	_
applied	_	_
to	_	_
the	_	_
CKY-like	_	_
beam-decoding	_	_
used	_	_
in	_	_
standard	_	_
HMT	_	_
,	_	_
translation	_	_
quality	_	_
would	_	_
be	_	_
severely	_	_
degraded	_	_
.	_	_

#16
This	_	_
is	_	_
because	_	_
the	_	_
bottom-up	_	_
inference	_	_
order	_	_
imposed	_	_
by	_	_
CKY-like	_	_
beam-decoding	_	_
means	_	_
that	_	_
all	_	_
pruning	_	_
decisions	_	_
must	deontic-inference	_
be	_	_
based	_	_
on	_	_
a	_	_
bottom-up	_	_
approximation	_	_
of	_	_
contextual	_	_
features	_	_
,	_	_
which	_	_
leads	_	_
to	_	_
search	_	_
errors	_	_
that	_	_
affect	_	_
the	_	_
quality	_	_
of	_	_
reordering	_	_
and	_	_
lexical-choice	_	_
(	_	_
Gesmundo	_	_
and	_	_
Henderson	_	_
,	_	_
2011	_	_
)	_	_
.	_	_

#17
UMT	_	_
solves	_	_
this	_	_
problem	_	_
by	_	_
removing	_	_
the	_	_
bottom-up	_	_
inference	_	_
order	_	_
constraint	_	_
,	_	_
allowing	_	_
many	_	_
different	_	_
inference	_	_
orders	_	_
for	_	_
the	_	_
same	_	_
tree	_	_
structure	_	_
,	_	_
and	_	_
learning	_	_
the	_	_
inference	_	_
order	_	_
where	_	_
the	_	_
decoder	_	_
can	capability	negation
be	_	_
the	_	_
most	_	_
confident	_	_
in	_	_
its	_	_
pruning	_	_
decisions	_	_
.	_	_

#18
Removing	_	_
the	_	_
bottom-up	_	_
inference	_	_
order	_	_
constraint	_	_
makes	_	_
it	_	_
possible	_	_
to	_	_
condition	_	_
on	_	_
top-down	_	_
structure	_	_
and	_	_
surrounding	_	_
context	_	_
.	_	_

#32
For	_	_
ease	_	_
of	_	_
presentation	_	_
,	_	_
and	_	_
following	_	_
synchronous-grammar	_	_
based	_	_
MT	_	_
practice	_	_
,	_	_
we	_	_
will	_	_
henceforth	_	_
restrict	_	_
our	_	_
focus	_	_
to	_	_
binary	_	_
grammars	_	_
(	_	_
Zhang	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
;	_	_
Wang	_	_
et	_	_
al.	_	_
,	_	_
2007	_	_
)	_	_
.	_	_

#33
A	_	_
UMT	_	_
decoder	_	_
can	capability-feasibility	_
be	_	_
formulated	_	_
as	_	_
a	_	_
function	_	_
,	_	_
f	_	_
,	_	_
that	_	_
maps	_	_
a	_	_
source	_	_
sentence	_	_
,	_	_
x	_	_
∈	_	_
X	_	_
,	_	_
into	_	_
a	_	_
structure	_	_
defined	_	_
by	_	_
y	_	_
=	_	_
(	_	_
t	_	_
,	_	_
h	_	_
)	_	_
∈	_	_
Y	_	_
,	_	_
where	_	_
t	_	_
is	_	_
the	_	_
translation	_	_
in	_	_
the	_	_
target	_	_
language	_	_
,	_	_
and	_	_
h	_	_
is	_	_
the	_	_
synchronous	_	_
tree	_	_
structure	_	_
generating	_	_
the	_	_
input	_	_
sentence	_	_
on	_	_
the	_	_
source	_	_
side	_	_
and	_	_
its	_	_
translation	_	_
on	_	_
the	_	_
target	_	_
side	_	_
.	_	_

#34
Synchronous-trees	_	_
are	_	_
composed	_	_
of	_	_
instantiations	_	_
of	_	_
synchronous-rules	_	_
,	_	_
r	_	_
,	_	_
from	_	_
a	_	_
grammar	_	_
,	_	_
G.	_	_
A	_	_
UMT	_	_
decoder	_	_
builds	_	_
synchronous-trees	_	_
,	_	_
h	_	_
,	_	_
by	_	_
recursively	_	_
expanding	_	_
partial	_	_
synchronous-trees	_	_
,	_	_
τ	_	_
.	_	_

#38
Let	_	_
ci	_	_
=	_	_
(	_	_
ri	_	_
,	_	_
rji	_	_
)	_	_
be	_	_
the	_	_
notation	_	_
to	_	_
represent	_	_
the	_	_
connection	_	_
between	_	_
the	_	_
i-th	_	_
rule	_	_
and	_	_
the	_	_
rule	_	_
rji	_	_
.	_	_

#39
The	_	_
set	_	_
of	_	_
connections	_	_
can	feasibility-rhetorical	_
be	_	_
expressed	_	_
as	_	_
:	_	_
C	_	_
≡	_	_
{	_	_
(	_	_
r1	_	_
,	_	_
rj1	_	_
)	_	_
,	_	_
(	_	_
r2	_	_
,	_	_
rj2	_	_
)	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
(	_	_
rk−1	_	_
,	_	_
rjk−1	_	_
)	_	_
}	_	_
3	_	_
)	_	_
The	_	_
postcondition	_	_
set	_	_
,	_	_
P	_	_
,	_	_
which	_	_
specifies	_	_
the	_	_
non-terminals	_	_
in	_	_
τ	_	_
that	_	_
are	_	_
available	_	_
for	_	_
creating	_	_
new	_	_
connections	_	_
.	_	_

#40
Each	_	_
postcondition	_	_
,	_	_
pi	_	_
=	_	_
(	_	_
rx	_	_
,	_	_
X	_	_
y	_	_
)	_	_
i	_	_
,	_	_
indicates	_	_
that	_	_
the	_	_
rule	_	_
rx	_	_
has	_	_
the	_	_
non-terminal	_	_
X	_	_
y	_	_
available	_	_
for	_	_
connections	_	_
.	_	_

#41
The	_	_
index	_	_
y	_	_
identifies	_	_
the	_	_
non-terminal	_	_
in	_	_
the	_	_
rule	_	_
.	_	_

#42
In	_	_
a	_	_
binary	_	_
grammar	_	_
y	_	_
can	capability	_
take	_	_
only	_	_
3	_	_
values	_	_
:	_	_
1	_	_
for	_	_
the	_	_
first	_	_
non-terminal	_	_
(	_	_
the	_	_
left	_	_
child	_	_
of	_	_
the	_	_
source	_	_
side	_	_
)	_	_
,	_	_
2	_	_
for	_	_
the	_	_
second	_	_
non-terminal	_	_
,	_	_
and	_	_
h	_	_
for	_	_
the	_	_
head	_	_
.	_	_

#43
The	_	_
postcondition	_	_
set	_	_
can	feasibility-rhetorical	_
be	_	_
expressed	_	_
as	_	_
:	_	_
P≡	_	_
{	_	_
(	_	_
rx1	_	_
,	_	_
Xy1	_	_
)	_	_
1	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
(	_	_
rxm	_	_
,	_	_
Xym	_	_
)	_	_
m	_	_
}	_	_
4	_	_
)	_	_
The	_	_
set	_	_
of	_	_
carries	_	_
,	_	_
K	_	_
.	_	_

#44
We	_	_
define	_	_
a	_	_
different	_	_
carry	_	_
,	_	_
κi	_	_
,	_	_
for	_	_
each	_	_
non-terminal	_	_
available	_	_
for	_	_
connections	_	_
.	_	_

#47
Let	_	_
κi	_	_
be	_	_
the	_	_
carry	_	_
associated	_	_
with	_	_
the	_	_
postcondition	_	_
pi	_	_
.	_	_

#48
The	_	_
set	_	_
of	_	_
carries	_	_
can	feasibility-rhetorical	_
be	_	_
expressed	_	_
as	_	_
:	_	_
K	_	_
≡	_	_
{	_	_
κ1	_	_
,	_	_
κ2	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
κm	_	_
}	_	_
Partial	_	_
synchronous-trees	_	_
,	_	_
τ	_	_
,	_	_
are	_	_
expanded	_	_
by	_	_
performing	_	_
connection-actions	_	_
.	_	_

#49
Given	_	_
a	_	_
τ	_	_
we	_	_
can	feasibility	_
connect	_	_
to	_	_
it	_	_
a	_	_
new	_	_
rule	_	_
,	_	_
r̂	_	_
,	_	_
using	_	_
one	_	_
available	_	_
non-	_	_
terminal	_	_
represented	_	_
by	_	_
postcondition	_	_
,	_	_
pi	_	_
∈	_	_
P	_	_
,	_	_
and	_	_
obtain	_	_
a	_	_
new	_	_
partial	_	_
synchronous-tree	_	_
τ̂	_	_
.	_	_

#50
Formally	_	_
:	_	_
τ̂	_	_
≡	_	_
〈	_	_
τ	_	_
⋖	_	_
â	_	_
〉	_	_
,	_	_
where	_	_
,	_	_
â	_	_
=	_	_
[	_	_
r̂	_	_
,	_	_
pi	_	_
]	_	_
,	_	_
represents	_	_
the	_	_
connection-action	_	_
.	_	_

#82
At	_	_
line	_	_
21	_	_
new	_	_
connection-actions	_	_
are	_	_
added	_	_
to	_	_
Q.	_	_
These	_	_
are	_	_
the	_	_
candidate	_	_
actions	_	_
proposing	_	_
a	_	_
connection	_	_
to	_	_
the	_	_
available	_	_
non-terminals	_	_
of	_	_
the	_	_
selected	_	_
action’s	_	_
new	_	_
rule	_	_
r̂	_	_
.	_	_

#83
The	_	_
rules	_	_
used	_	_
for	_	_
these	_	_
new	_	_
candidate-actions	_	_
must	deontic	negation
not	_	_
be	_	_
in	_	_
conflict	_	_
with	_	_
the	_	_
current	_	_
structure	_	_
of	_	_
τ	_	_
(	_	_
e.g	_	_
.	_	_
the	_	_
rule	_	_
can	capability	negation
not	_	_
generate	_	_
a	_	_
source	_	_
side	_	_
terminal	_	_
that	_	_
is	_	_
already	_	_
covered	_	_
by	_	_
τ	_	_
)	_	_
.	_	_

#84
3	_	_
Discriminative	_	_
Reinforcement	_	_
Learning	_	_
Training	_	_
a	_	_
UMT	_	_
model	_	_
simply	_	_
means	_	_
training	_	_
the	_	_
parameter	_	_
vector	_	_
w	_	_
that	_	_
is	_	_
used	_	_
to	_	_
choose	_	_
the	_	_
best	_	_
scoring	_	_
action	_	_
during	_	_
decoding	_	_
.	_	_

#85
We	_	_
propose	_	_
a	_	_
novel	_	_
method	_	_
to	_	_
apply	_	_
a	_	_
kind	_	_
of	_	_
minimum	_	_
error	_	_
rate	_	_
training	_	_
(	_	_
MERT	_	_
)	_	_
to	_	_
w.	_	_
Because	_	_
each	_	_
action	_	_
choice	_	_
must	deontic	_
be	_	_
evaluated	_	_
in	_	_
the	_	_
context	_	_
of	_	_
the	_	_
complete	_	_
translation-derivation	_	_
,	_	_
we	_	_
formalize	_	_
this	_	_
method	_	_
in	_	_
terms	_	_
of	_	_
Reinforcement	_	_
Learning	_	_
.	_	_

#86
We	_	_
propose	_	_
Discriminative	_	_
Reinforcement	_	_
Learning	_	_
as	_	_
an	_	_
appropriate	_	_
way	_	_
to	_	_
train	_	_
a	_	_
UMT	_	_
model	_	_
to	_	_
maximize	_	_
the	_	_
BLEU	_	_
score	_	_
of	_	_
the	_	_
complete	_	_
derivation	_	_
.	_	_

#87
First	_	_
we	_	_
define	_	_
DRL	_	_
as	_	_
a	_	_
novel	_	_
generic	_	_
training	_	_
framework	_	_
.	_	_

#88
3.1	_	_
Generic	_	_
Framework	_	_
of	_	_
DRL	_	_
RL	_	_
can	feasibility	_
be	_	_
applied	_	_
to	_	_
any	_	_
task	_	_
,	_	_
T	_	_
,	_	_
that	_	_
can	capability-feasibility	_
be	_	_
formalized	_	_
in	_	_
terms	_	_
of	_	_
:	_	_
1	_	_
)	_	_
The	_	_
set	_	_
of	_	_
states	_	_
S1	_	_
;	_	_
2	_	_
)	_	_
A	_	_
set	_	_
of	_	_
actions	_	_
As	_	_
for	_	_
each	_	_
state	_	_
s	_	_
∈	_	_
S	_	_
;	_	_
3	_	_
)	_	_
The	_	_
transition	_	_
function	_	_
T	_	_
:	_	_
S	_	_
×	_	_
As	_	_
→	_	_
S	_	_
,	_	_
that	_	_
specifies	_	_
the	_	_
next	_	_
state	_	_
given	_	_
a	_	_
source	_	_
state	_	_
and	_	_
performed	_	_
action2	_	_
;	_	_
4	_	_
)	_	_
The	_	_
reward	_	_
function	_	_
,	_	_
R	_	_
:	_	_
S	_	_
×As	_	_
→	_	_
R	_	_
;	_	_
5	_	_
)	_	_
The	_	_
discount	_	_
factor	_	_
,	_	_
γ	_	_
∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
.	_	_

#89
A	_	_
policy	_	_
is	_	_
defined	_	_
as	_	_
any	_	_
map	_	_
π	_	_
:	_	_
S	_	_
→	_	_
A.	_	_
Its	_	_
value	_	_
function	_	_
is	_	_
given	_	_
by	_	_
:	_	_
V	_	_
π	_	_
(	_	_
s0	_	_
)	_	_
=	_	_
σ∑	_	_
i=0	_	_
γiR	_	_
(	_	_
si	_	_
,	_	_
π	_	_
(	_	_
si	_	_
)	_	_
)	_	_
(	_	_
1	_	_
)	_	_
where	_	_
path	_	_
(	_	_
s0|π	_	_
)	_	_
≡	_	_
〈s0	_	_
,	_	_
s1	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
sσ|π〉	_	_
is	_	_
the	_	_
sequence	_	_
of	_	_
states	_	_
determined	_	_
by	_	_
following	_	_
policy	_	_
π	_	_
starting	_	_
at	_	_
state	_	_
s0	_	_
.	_	_

#91
Because	_	_
we	_	_
are	_	_
taking	_	_
a	_	_
discriminative	_	_
approach	_	_
to	_	_
learn	_	_
w	_	_
,	_	_
we	_	_
formalize	_	_
our	_	_
optimization	_	_
task	_	_
similarly	_	_
to	_	_
an	_	_
inverse	_	_
reinforcement	_	_
learning	_	_
problem	_	_
(	_	_
Ng	_	_
and	_	_
Russell	_	_
,	_	_
2000	_	_
)	_	_
:	_	_
we	_	_
are	_	_
given	_	_
information	_	_
about	_	_
the	_	_
optimal	_	_
action	_	_
sequence	_	_
and	_	_
we	_	_
want	_	_
to	_	_
learn	_	_
a	_	_
discriminative	_	_
reward	_	_
function	_	_
.	_	_

#92
As	_	_
in	_	_
other	_	_
discriminative	_	_
approaches	_	_
,	_	_
this	_	_
1S	_	_
can	options	_
be	_	_
either	_	_
finite	_	_
or	_	_
infinite	_	_
.	_	_

#93
2For	_	_
simplicity	_	_
we	_	_
describe	_	_
a	_	_
deterministic	_	_
process	_	_
.	_	_

#96
In	_	_
RL	_	_
,	_	_
an	_	_
optimal	_	_
policy	_	_
π∗	_	_
is	_	_
one	_	_
which	_	_
,	_	_
at	_	_
each	_	_
state	_	_
s	_	_
,	_	_
chooses	_	_
the	_	_
action	_	_
which	_	_
maximizes	_	_
the	_	_
future	_	_
reward	_	_
Qπ	_	_
∗	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
.	_	_

#97
We	_	_
assume	_	_
that	_	_
the	_	_
future	_	_
discriminative	_	_
reward	_	_
can	feasibility	_
be	_	_
approximated	_	_
with	_	_
a	_	_
linear	_	_
function	_	_
Q̃π	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
in	_	_
some	_	_
feature-	_	_
vector	_	_
representation	_	_
φ	_	_
:	_	_
S	_	_
×As	_	_
→	_	_
Rd	_	_
that	_	_
maps	_	_
a	_	_
state-action	_	_
pair	_	_
to	_	_
a	_	_
d-dimensional	_	_
features	_	_
vector	_	_
:	_	_
vecQ̃π	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
=	_	_
w	_	_
φ	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
(	_	_
3	_	_
)	_	_
where	_	_
w	_	_
∈	_	_
Rd	_	_
.	_	_

#98
This	_	_
gives	_	_
us	_	_
the	_	_
following	_	_
policy	_	_
:	_	_
πw	_	_
(	_	_
s	_	_
)	_	_
=	_	_
arg	_	_
max	_	_
a∈As	_	_
w	_	_
φ	_	_
(	_	_
s	_	_
,	_	_
a	_	_
)	_	_
(	_	_
4	_	_
)	_	_
The	_	_
set	_	_
of	_	_
parameters	_	_
of	_	_
this	_	_
policy	_	_
is	_	_
the	_	_
vector	_	_
w.	_	_
With	_	_
this	_	_
formalization	_	_
,	_	_
all	_	_
we	_	_
need	_	_
to	_	_
learn	_	_
is	_	_
a	_	_
vector	_	_
w	_	_
such	_	_
that	_	_
the	_	_
resulting	_	_
decisions	_	_
are	_	_
compatible	_	_
with	_	_
the	_	_
given	_	_
information	_	_
about	_	_
the	_	_
optimal	_	_
action	_	_
sequence	_	_
.	_	_

#108
We	_	_
have	_	_
described	_	_
the	_	_
DRL	_	_
meta-algorithm	_	_
to	_	_
be	_	_
as	_	_
general	_	_
as	_	_
possible	_	_
.	_	_

#109
When	_	_
applied	_	_
to	_	_
a	_	_
specific	_	_
problem	_	_
,	_	_
more	_	_
details	_	_
can	feasibility	_
be	_	_
specified	_	_
:	_	_
1	_	_
)	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
choose	_	_
specific	_	_
sampling	_	_
techniques	_	_
to	_	_
implement	_	_
lines	_	_
3	_	_
and	_	_
5	_	_
;	_	_
2	_	_
)	_	_
the	_	_
test	_	_
at	_	_
line	_	_
6	_	_
needs	_	_
to	_	_
be	_	_
detailed	_	_
according	_	_
to	_	_
the	_	_
nature	_	_
of	_	_
T	_	_
and	_	_
D	_	_
;	_	_
3	_	_
)	_	_
the	_	_
update	_	_
statement	_	_
at	_	_
line	_	_
7	_	_
can	feasibility	_
be	_	_
replaced	_	_
with	_	_
a	_	_
more	_	_
sophisticated	_	_
update	_	_
approach	_	_
.	_	_

#110
We	_	_
address	_	_
these	_	_
issues	_	_
and	_	_
describe	_	_
a	_	_
range	_	_
of	_	_
alternatives	_	_
as	_	_
we	_	_
apply	_	_
DRL	_	_
to	_	_
UMT	_	_
in	_	_
Section	_	_
3.2	_	_
.	_	_

#111
3.2	_	_
Application	_	_
of	_	_
DRL	_	_
to	_	_
UMT	_	_
To	_	_
apply	_	_
DRL	_	_
we	_	_
formalize	_	_
the	_	_
task	_	_
of	_	_
translating	_	_
x	_	_
with	_	_
UMT	_	_
as	_	_
T	_	_
≡	_	_
{	_	_
S	_	_
,	_	_
{	_	_
As	_	_
}	_	_
,	_	_
T	_	_
,	_	_
R	_	_
,	_	_
γ	_	_
}	_	_
:	_	_
1	_	_
)	_	_
The	_	_
set	_	_
of	_	_
states	_	_
S	_	_
is	_	_
the	_	_
space	_	_
of	_	_
all	_	_
possible	_	_
UMT	_	_
partial	_	_
synchronous-trees	_	_
,	_	_
τ	_	_
;	_	_
2	_	_
)	_	_
The	_	_
set	_	_
Aτ	_	_
,	_	_
x	_	_
is	_	_
the	_	_
set	_	_
of	_	_
connection-actions	_	_
that	_	_
can	capability	_
expand	_	_
τ	_	_
connecting	_	_
new	_	_
synchronous-	_	_
rule	_	_
instantiations	_	_
matching	_	_
the	_	_
input	_	_
sentence	_	_
x	_	_
on	_	_
the	_	_
source	_	_
side	_	_
;	_	_
3	_	_
)	_	_
The	_	_
transition	_	_
function	_	_
T	_	_
is	_	_
the	_	_
connection	_	_
function	_	_
τ̂	_	_
≡	_	_
〈	_	_
τ	_	_
⋖	_	_
a	_	_
〉	_	_
formalized	_	_
in	_	_
Section	_	_
2	_	_
and	_	_
detailed	_	_
by	_	_
the	_	_
procedure	_	_
CreateConnection	_	_
(	_	_
·	_	_
)	_	_
in	_	_
Algorithm	_	_
1	_	_
;	_	_
4	_	_
)	_	_
The	_	_
true	_	_
reward	_	_
function	_	_
R	_	_
is	_	_
the	_	_
BLEU	_	_
score	_	_
.	_	_

#112
BLEU	_	_
is	_	_
a	_	_
loss	_	_
function	_	_
that	_	_
quantifies	_	_
the	_	_
difference	_	_
between	_	_
the	_	_
reference	_	_
translation	_	_
and	_	_
the	_	_
output	_	_
translation	_	_
t.	_	_
The	_	_
BLEU	_	_
score	_	_
can	feasibility	_
be	_	_
computed	_	_
only	_	_
when	_	_
a	_	_
terminal	_	_
state	_	_
is	_	_
reached	_	_
and	_	_
a	_	_
full	_	_
translation	_	_
is	_	_
available	_	_
.	_	_

#113
Thus	_	_
,	_	_
the	_	_
rewards	_	_
are	_	_
all	_	_
zero	_	_
except	_	_
at	_	_
terminal	_	_
states	_	_
,	_	_
called	_	_
a	_	_
Pure	_	_
De3Preliminary	_	_
experiments	_	_
with	_	_
updating	_	_
only	_	_
the	_	_
features	_	_
for	_	_
â	_	_
and	_	_
a′	_	_
produced	_	_
substantially	_	_
worse	_	_
results	_	_
.	_	_

#123
For	_	_
the	_	_
update	_	_
statement	_	_
at	_	_
line	_	_
7	_	_
we	_	_
use	_	_
the	_	_
Averaged	_	_
Perceptron	_	_
technique	_	_
(	_	_
Freund	_	_
and	_	_
Schapire	_	_
,	_	_
1999	_	_
)	_	_
.	_	_

#124
Algorithm	_	_
2	_	_
can	feasibility	_
be	_	_
easily	_	_
adapted	_	_
to	_	_
implement	_	_
the	_	_
efficient	_	_
Averaged	_	_
Perceptron	_	_
updates	_	_
(	_	_
e.g	_	_
.	_	_
see	_	_
Section	_	_
2.1.1	_	_
of	_	_
(	_	_
Daumé	_	_
III	_	_
,	_	_
2006	_	_
)	_	_
)	_	_
.	_	_

#125
In	_	_
preliminary	_	_
experiments	_	_
,	_	_
we	_	_
found	_	_
that	_	_
other	_	_
more	_	_
aggressive	_	_
update	_	_
technique	_	_
,	_	_
such	_	_
as	_	_
Passive-Aggressive	_	_
(	_	_
Crammer	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
)	_	_
,	_	_
Aggressive	_	_
(	_	_
Shen	_	_
et	_	_
al.	_	_
,	_	_
2007	_	_
)	_	_
,	_	_
or	_	_
MIRA	_	_
(	_	_
Crammer	_	_
and	_	_
Singer	_	_
,	_	_
2003	_	_
)	_	_
,	_	_
lead	_	_
to	_	_
worst	_	_
accuracy	_	_
.	_	_

#126
To	_	_
see	_	_
why	_	_
this	_	_
might	speculation	_
be	_	_
,	_	_
consider	_	_
that	_	_
a	_	_
MT	_	_
decoder	_	_
needs	_	_
to	_	_
learn	_	_
to	_	_
construct	_	_
structures	_	_
(	_	_
t	_	_
,	_	_
h	_	_
)	_	_
,	_	_
while	_	_
the	_	_
training	_	_
data	_	_
specifies	_	_
the	_	_
gold	_	_
translation	_	_
t∗	_	_
but	_	_
gives	_	_
no	_	_
information	_	_
on	_	_
the	_	_
hidden-correspondence	_	_
structure	_	_
h.	_	_
As	_	_
discussed	_	_
in	_	_
(	_	_
Liang	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
)	_	_
,	_	_
there	_	_
are	_	_
output	_	_
structures	_	_
that	_	_
match	_	_
the	_	_
reference	_	_
translation	_	_
using	_	_
a	_	_
wrong	_	_
internal	_	_
structure	_	_
(	_	_
e.g	_	_
.	_	_
assuming	_	_
wrong	_	_
internal	_	_
alignment	_	_
)	_	_
.	_	_

#127
While	_	_
in	_	_
other	_	_
cases	_	_
the	_	_
output	_	_
translation	_	_
can	options	_
be	_	_
a	_	_
valid	_	_
alternative	_	_
translation	_	_
but	_	_
gets	_	_
a	_	_
low	_	_
BLEU	_	_
score	_	_
because	_	_
it	_	_
differs	_	_
from	_	_
t∗	_	_
.	_	_

#128
Aggressively	_	_
promoting/penalizing	_	_
structures	_	_
whose	_	_
correctness	_	_
can	feasibility	_
be	_	_
only	_	_
partially	_	_
verified	_	_
can	feasibility	negation
be	_	_
expected	_	_
to	_	_
harm	_	_
generalization	_	_
ability	_	_
.	_	_

#129
4	_	_
Undirected	_	_
Features	_	_
In	_	_
this	_	_
section	_	_
we	_	_
show	_	_
how	_	_
the	_	_
features	_	_
designed	_	_
for	_	_
bottom-up	_	_
HMT	_	_
can	feasibility	_
be	_	_
adapted	_	_
to	_	_
the	_	_
undirected	_	_
approach	_	_
,	_	_
and	_	_
we	_	_
introduce	_	_
a	_	_
new	_	_
feature	_	_
from	_	_
the	_	_
class	_	_
of	_	_
undirected	_	_
features	_	_
that	_	_
are	_	_
made	_	_
possible	_	_
by	_	_
the	_	_
undirected	_	_
approach	_	_
.	_	_

#130
Local	_	_
features	_	_
depend	_	_
only	_	_
on	_	_
the	_	_
action	_	_
rule	_	_
r.	_	_
These	_	_
features	_	_
can	feasibility	_
be	_	_
used	_	_
in	_	_
the	_	_
undirected	_	_
approach	_	_
without	_	_
adaptation	_	_
,	_	_
since	_	_
they	_	_
are	_	_
independent	_	_
of	_	_
the	_	_
surrounding	_	_
structure	_	_
.	_	_

#131
For	_	_
our	_	_
experiments	_	_
we	_	_
use	_	_
a	_	_
standard	_	_
set	_	_
of	_	_
local	_	_
features	_	_
:	_	_
the	_	_
probability	_	_
of	_	_
the	_	_
source	_	_
phrase	_	_
given	_	_
the	_	_
target	_	_
phrase	_	_
;	_	_
the	_	_
lexical	_	_
translation	_	_
probabilities	_	_
of	_	_
the	_	_
source	_	_
words	_	_
given	_	_
the	_	_
target	_	_
words	_	_
;	_	_
the	_	_
lexical	_	_
translation	_	_
probabilities	_	_
of	_	_
the	_	_
target	_	_
words	_	_
given	_	_
the	_	_
source	_	_
words	_	_
;	_	_
and	_	_
the	_	_
Word	_	_
Penalty	_	_
feature	_	_
.	_	_

#143
An	_	_
action	_	_
rule	_	_
r	_	_
is	_	_
connected	_	_
to	_	_
τ	_	_
via	_	_
one	_	_
of	_	_
r’s	_	_
non-terminals	_	_
,	_	_
Xr	_	_
,	_	_
τ	_	_
.	_	_

#144
Thus	_	_
,	_	_
the	_	_
score	_	_
of	_	_
the	_	_
interaction	_	_
between	_	_
r	_	_
and	_	_
the	_	_
context	_	_
structure	_	_
attached	_	_
to	_	_
Xr	_	_
,	_	_
τ	_	_
can	feasibility	_
be	_	_
computed	_	_
exactly	_	_
,	_	_
while	_	_
the	_	_
score	_	_
of	_	_
the	_	_
structures	_	_
attached	_	_
to	_	_
other	_	_
r	_	_
nonterminals	_	_
(	_	_
i.e	_	_
.	_	_
those	_	_
in	_	_
postconditions	_	_
)	_	_
can	feasibility	negation
not	_	_
be	_	_
computed	_	_
since	_	_
these	_	_
branches	_	_
are	_	_
missing	_	_
.	_	_

#145
Each	_	_
of	_	_
these	_	_
postcondition	_	_
nonterminals	_	_
has	_	_
an	_	_
associated	_	_
CFF	_	_
feature	_	_
,	_	_
which	_	_
is	_	_
an	_	_
upper	_	_
bound	_	_
on	_	_
the	_	_
score	_	_
of	_	_
its	_	_
missing	_	_
branch	_	_
.	_	_

#146
More	_	_
precisely	_	_
,	_	_
it	_	_
is	_	_
an	_	_
upper	_	_
bound	_	_
on	_	_
the	_	_
context-free	_	_
component	_	_
of	_	_
this	_	_
score	_	_
.	_	_

#147
This	_	_
upper	_	_
bound	_	_
can	feasibility	_
be	_	_
exactly	_	_
and	_	_
efficiently	_	_
computed	_	_
using	_	_
the	_	_
Forest	_	_
Rescoring	_	_
Framework	_	_
(	_	_
Huang	_	_
and	_	_
Chiang	_	_
,	_	_
2007	_	_
;	_	_
Huang	_	_
,	_	_
2008	_	_
)	_	_
.	_	_

#148
This	_	_
framework	_	_
separates	_	_
the	_	_
MT	_	_
decoding	_	_
in	_	_
two	_	_
steps	_	_
.	_	_

#193
To	_	_
measure	_	_
the	_	_
significance	_	_
of	_	_
the	_	_
variation	_	_
,	_	_
we	_	_
compute	_	_
the	_	_
sign	_	_
test	_	_
and	_	_
measure	_	_
the	_	_
one-tail	_	_
p-value	_	_
for	_	_
the	_	_
presented	_	_
models	_	_
in	_	_
comparison	_	_
to	_	_
HMT	_	_
b30	_	_
.	_	_

#194
From	_	_
the	_	_
values	_	_
reported	_	_
in	_	_
the	_	_
fourth	_	_
column	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
observe	_	_
that	_	_
the	_	_
BLEU	_	_
score	_	_
variations	_	_
would	_	_
not	_	_
normally	_	_
be	_	_
considered	_	_
significant	_	_
.	_	_

#195
For	_	_
example	_	_
,	_	_
at	_	_
WMT-11	_	_
two	_	_
systems	_	_
were	_	_
considered	_	_
equivalent	_	_
if	_	_
p	_	_
>	_	_
0.1	_	_
,	_	_
as	_	_
in	_	_
these	_	_
cases	_	_
.	_	_

#196
The	_	_
accuracy	_	_
can	feasibility	negation
not	_	_
be	_	_
compared	_	_
in	_	_
terms	_	_
of	_	_
search	_	_
score	_	_
since	_	_
the	_	_
models	_	_
we	_	_
are	_	_
comparing	_	_
are	_	_
trained	_	_
with	_	_
distinct	_	_
algorithms	_	_
and	_	_
thus	_	_
the	_	_
search	_	_
scores	_	_
are	_	_
not	_	_
comparable	_	_
.	_	_

#197
comTo	_	_
test	_	_
the	_	_
impact	_	_
of	_	_
the	_	_
CFF	_	_
features	_	_
,	_	_
we	_	_
trained	_	_
and	_	_
tested	_	_
UMT	_	_
with	_	_
DRL	_	_
with	_	_
and	_	_
without	_	_
these	_	_
features	_	_
.	_	_

#219
This	_	_
model	_	_
has	_	_
a	_	_
O	_	_
(	_	_
n	_	_
log	_	_
n	_	_
)	_	_
decoding	_	_
complexity	_	_
and	_	_
accuracy	_	_
performance	_	_
close	_	_
to	_	_
the	_	_
O	_	_
(	_	_
n2	_	_
)	_	_
graph-based	_	_
parsers	_	_
(	_	_
Mcdonald	_	_
et	_	_
al.	_	_
,	_	_
2005	_	_
)	_	_
.	_	_

#220
Similarities	_	_
can	feasibility-rhetorical	_
be	_	_
found	_	_
between	_	_
DRL	_	_
and	_	_
previous	_	_
work	_	_
that	_	_
applies	_	_
discriminative	_	_
training	_	_
to	_	_
structured	_	_
prediction	_	_
:	_	_
Collins	_	_
and	_	_
Roark	_	_
(	_	_
2004	_	_
)	_	_
present	_	_
an	_	_
Incremental	_	_
Parser	_	_
trained	_	_
with	_	_
the	_	_
Perceptron	_	_
algorithm	_	_
.	_	_

#221
Their	_	_
approach	_	_
is	_	_
specific	_	_
to	_	_
dependency	_	_
parsing	_	_
and	_	_
requires	_	_
a	_	_
function	_	_
to	_	_
test	_	_
exact	_	_
match	_	_
of	_	_
tree	_	_
structures	_	_
to	_	_
trigger	_	_
parameter	_	_
updates	_	_
.	_	_

#222
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
DRL	_	_
can	feasibility	_
be	_	_
applied	_	_
to	_	_
any	_	_
structured	_	_
prediction	_	_
task	_	_
and	_	_
can	capability	_
handle	_	_
any	_	_
kind	_	_
of	_	_
reward	_	_
function	_	_
.	_	_

#223
LASO	_	_
(	_	_
Daumé	_	_
III	_	_
and	_	_
Marcu	_	_
,	_	_
2005	_	_
;	_	_
Daumé	_	_
III	_	_
et	_	_
al.	_	_
,	_	_
2005	_	_
)	_	_
and	_	_
SEARN	_	_
(	_	_
Daumé	_	_
III	_	_
et	_	_
al.	_	_
,	_	_
2009	_	_
;	_	_
Daumé	_	_
III	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
)	_	_
are	_	_
generic	_	_
frameworks	_	_
for	_	_
discriminative	_	_
training	_	_
for	_	_
structured	_	_
prediction	_	_
:	_	_
LASO	_	_
requires	_	_
a	_	_
function	_	_
that	_	_
tests	_	_
correctness	_	_
of	_	_
partial	_	_
structures	_	_
to	_	_
trigger	_	_
early	_	_
updates	_	_
,	_	_
while	_	_
SEARN	_	_
requires	_	_
an	_	_
optimal	_	_
policy	_	_
to	_	_
initialize	_	_
the	_	_
learning	_	_
algorithm	_	_
.	_	_

#224
Such	_	_
a	_	_
test	_	_
function	_	_
or	_	_
optimal	_	_
policy	_	_
can	feasibility	negation
not	_	_
be	_	_
computed	_	_
for	_	_
tasks	_	_
such	_	_
as	_	_
MT	_	_
where	_	_
the	_	_
hidden	_	_
correspondence	_	_
structure	_	_
h	_	_
is	_	_
not	_	_
provided	_	_
in	_	_
the	_	_
training	_	_
data	_	_
.	_	_

#225
7	_	_
Discussion	_	_
and	_	_
Future	_	_
Work	_	_
In	_	_
general	_	_
,	_	_
we	_	_
believe	_	_
that	_	_
greedy-discriminative	_	_
solutions	_	_
are	_	_
promising	_	_
for	_	_
tasks	_	_
like	_	_
MT	_	_
,	_	_
where	_	_
there	_	_
is	_	_
not	_	_
a	_	_
single	_	_
correct	_	_
solution	_	_
:	_	_
normally	_	_
there	_	_
are	_	_
many	_	_
correct	_	_
ways	_	_
to	_	_
translate	_	_
the	_	_
same	_	_
sentence	_	_
,	_	_
and	_	_
for	_	_
each	_	_
correct	_	_
translation	_	_
there	_	_
are	_	_
many	_	_
different	_	_
derivation-trees	_	_
generating	_	_
that	_	_
translation	_	_
,	_	_
and	_	_
each	_	_
correct	_	_
derivation	_	_
tree	_	_
can	feasibility	_
be	_	_
built	_	_
greedily	_	_
following	_	_
different	_	_
inference	_	_
orders	_	_
.	_	_

#226
Therefore	_	_
,	_	_
the	_	_
set	_	_
of	_	_
correct	_	_
decoding	_	_
paths	_	_
is	_	_
a	_	_
reasonable	_	_
portion	_	_
of	_	_
UMT’s	_	_
search	_	_
space	_	_
,	_	_
giving	_	_
a	_	_
well-designed	_	_
greedy	_	_
algorithm	_	_
a	_	_
chance	_	_
to	_	_
find	_	_
a	_	_
good	_	_
translation	_	_
even	_	_
without	_	_
beam	_	_
search	_	_
.	_	_

#228
But	_	_
to	_	_
take	_	_
full	_	_
advantage	_	_
of	_	_
the	_	_
power	_	_
of	_	_
discriminative	_	_
training	_	_
and	_	_
the	_	_
lower	_	_
decoding	_	_
complexity	_	_
,	_	_
it	_	_
would	_	_
be	_	_
possible	_	_
to	_	_
vastly	_	_
increase	_	_
the	_	_
number	_	_
of	_	_
features	_	_
.	_	_

#229
The	_	_
UMT’s	_	_
undirected	_	_
nature	_	_
allows	_	_
the	_	_
integration	_	_
of	_	_
non-bottom-up	_	_
contextual	_	_
features	_	_
,	_	_
which	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
used	_	_
by	_	_
standard	_	_
HMT	_	_
and	_	_
PbMT	_	_
.	_	_

#230
And	_	_
the	_	_
use	_	_
of	_	_
a	_	_
historybased	_	_
model	_	_
allows	_	_
features	_	_
from	_	_
an	_	_
arbitrarily	_	_
wide	_	_
context	_	_
,	_	_
since	_	_
the	_	_
model	_	_
does	_	_
not	_	_
need	_	_
to	_	_
be	_	_
factorized	_	_
.	_	_

#234
This	_	_
model	_	_
combines	_	_
advantages	_	_
given	_	_
by	_	_
the	_	_
use	_	_
of	_	_
hierarchical	_	_
synchronousgrammars	_	_
with	_	_
a	_	_
more	_	_
efficient	_	_
decoding	_	_
algorithm	_	_
.	_	_

#235
UMT’s	_	_
nature	_	_
allows	_	_
us	_	_
to	_	_
design	_	_
novel	_	_
undirected	_	_
features	_	_
that	_	_
better	_	_
approximate	_	_
contextual	_	_
features	_	_
(	_	_
such	_	_
as	_	_
the	_	_
LM	_	_
)	_	_
,	_	_
and	_	_
to	_	_
introduce	_	_
a	_	_
new	_	_
class	_	_
of	_	_
undirected	_	_
features	_	_
that	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
used	_	_
by	_	_
standard	_	_
bottom-up	_	_
decoders	_	_
.	_	_

#236
Furthermore	_	_
,	_	_
we	_	_
generalize	_	_
the	_	_
training	_	_
algorithm	_	_
into	_	_
a	_	_
generic	_	_
Discriminative	_	_
Reinforcement	_	_
Learning	_	_
meta-algorithm	_	_
that	_	_
can	capability-feasibility	_
be	_	_
applied	_	_
to	_	_
any	_	_
structured	_	_
prediction	_	_
task	_	_
.	_	_