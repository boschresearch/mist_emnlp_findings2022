#0
Proceedings	_	_
of	_	_
the	_	_
19th	_	_
Conference	_	_
on	_	_
Computational	_	_
Language	_	_
Learning	_	_
,	_	_
pages	_	_
62–72	_	_
,	_	_
Beijing	_	_
,	_	_
China	_	_
,	_	_
July	_	_
30-31	_	_
,	_	_
2015.	_	_
c©2015	_	_
Association	_	_
for	_	_
Computational	_	_
Linguistics	_	_
Analyzing	_	_
Optimization	_	_
for	_	_
Statistical	_	_
Machine	_	_
Translation	_	_
:	_	_
MERT	_	_
Learns	_	_
Verbosity	_	_
,	_	_
PRO	_	_
Learns	_	_
Length	_	_
Francisco	_	_
Guzmán	_	_
Preslav	_	_
Nakov	_	_
and	_	_
Stephan	_	_
Vogel	_	_
ALT	_	_
Research	_	_
Group	_	_
Qatar	_	_
Computing	_	_
Research	_	_
Institute	_	_
,	_	_
HBKU	_	_
{	_	_
fguzman	_	_
,	_	_
pnakov	_	_
,	_	_
svogel	_	_
}	_	_
@	_	_
qf.org.qa	_	_
Abstract	_	_
We	_	_
study	_	_
the	_	_
impact	_	_
of	_	_
source	_	_
length	_	_
and	_	_
verbosity	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
parameter	_	_
optimizers	_	_
such	_	_
as	_	_
MERT	_	_
and	_	_
PRO	_	_
for	_	_
statistical	_	_
machine	_	_
translation	_	_
.	_	_

#1
In	_	_
particular	_	_
,	_	_
we	_	_
test	_	_
whether	_	_
the	_	_
verbosity	_	_
of	_	_
the	_	_
resulting	_	_
translations	_	_
can	feasibility	_
be	_	_
modified	_	_
by	_	_
varying	_	_
the	_	_
length	_	_
or	_	_
the	_	_
verbosity	_	_
of	_	_
the	_	_
tuning	_	_
sentences	_	_
.	_	_

#2
We	_	_
find	_	_
that	_	_
MERT	_	_
learns	_	_
the	_	_
tuning	_	_
set	_	_
verbosity	_	_
very	_	_
well	_	_
,	_	_
while	_	_
PRO	_	_
is	_	_
sensitive	_	_
to	_	_
both	_	_
the	_	_
verbosity	_	_
and	_	_
the	_	_
length	_	_
of	_	_
the	_	_
source	_	_
sentences	_	_
in	_	_
the	_	_
tuning	_	_
set	_	_
;	_	_
yet	_	_
,	_	_
overall	_	_
PRO	_	_
learns	_	_
best	_	_
from	_	_
highverbosity	_	_
tuning	_	_
datasets	_	_
.	_	_

#4
In	_	_
the	_	_
mean	_	_
time	_	_
,	_	_
until	_	_
we	_	_
develop	_	_
such	_	_
controlled	_	_
scenarios	_	_
,	_	_
we	_	_
recommend	_	_
using	_	_
PRO	_	_
with	_	_
a	_	_
large	_	_
verbosity	_	_
tuning	_	_
set	_	_
,	_	_
which	_	_
,	_	_
in	_	_
our	_	_
experiments	_	_
,	_	_
yields	_	_
highest	_	_
BLEU	_	_
across	_	_
datasets	_	_
and	_	_
language	_	_
pairs	_	_
.	_	_

#5
1	_	_
Introduction	_	_
Statistical	_	_
machine	_	_
translation	_	_
(	_	_
SMT	_	_
)	_	_
systems	_	_
nowadays	_	_
are	_	_
complex	_	_
and	_	_
consist	_	_
of	_	_
many	_	_
components	_	_
such	_	_
as	_	_
a	_	_
translation	_	_
model	_	_
,	_	_
a	_	_
reordering	_	_
model	_	_
,	_	_
a	_	_
language	_	_
model	_	_
,	_	_
etc.	_	_
,	_	_
each	_	_
of	_	_
which	_	_
could	options	_
have	_	_
several	_	_
sub-components	_	_
.	_	_

#6
All	_	_
components	_	_
and	_	_
their	_	_
elements	_	_
work	_	_
together	_	_
to	_	_
score	_	_
full	_	_
and	_	_
partial	_	_
hypotheses	_	_
proposed	_	_
by	_	_
the	_	_
SMT	_	_
system’s	_	_
search	_	_
algorithms	_	_
.	_	_

#7
Thus	_	_
,	_	_
putting	_	_
them	_	_
together	_	_
requires	_	_
assigning	_	_
them	_	_
relative	_	_
weights	_	_
,	_	_
e.g.	_	_
,	_	_
how	_	_
much	_	_
weight	_	_
we	_	_
should	deontic	_
give	_	_
to	_	_
the	_	_
translation	_	_
model	_	_
vs	_	_
.	_	_

#8
the	_	_
language	_	_
model	_	_
vs	_	_
.	_	_

#10
These	_	_
relative	_	_
weights	_	_
are	_	_
typically	_	_
learned	_	_
discriminatively	_	_
in	_	_
a	_	_
log-linear	_	_
framework	_	_
,	_	_
and	_	_
their	_	_
values	_	_
are	_	_
optimized	_	_
to	_	_
maximize	_	_
some	_	_
automatic	_	_
metric	_	_
,	_	_
typically	_	_
BLEU	_	_
,	_	_
on	_	_
a	_	_
tuning	_	_
dataset	_	_
.	_	_

#11
Given	_	_
this	_	_
setup	_	_
,	_	_
it	_	_
is	_	_
clear	_	_
that	_	_
the	_	_
choice	_	_
of	_	_
a	_	_
tuning	_	_
set	_	_
and	_	_
its	_	_
characteristics	_	_
,	_	_
can	capability	_
have	_	_
significant	_	_
impact	_	_
on	_	_
the	_	_
SMT	_	_
system’s	_	_
performance	_	_
:	_	_
if	_	_
the	_	_
experimental	_	_
framework	_	_
(	_	_
training	_	_
data	_	_
,	_	_
tuning	_	_
set	_	_
,	_	_
and	_	_
test	_	_
set	_	_
)	_	_
is	_	_
highly	_	_
consistent	_	_
,	_	_
i.e.	_	_
,	_	_
there	_	_
is	_	_
close	_	_
similarity	_	_
in	_	_
terms	_	_
of	_	_
genre	_	_
,	_	_
domain	_	_
and	_	_
verbosity,1	_	_
then	_	_
translation	_	_
quality	_	_
can	feasibility	_
be	_	_
improved	_	_
by	_	_
careful	_	_
selection	_	_
of	_	_
tuning	_	_
sentences	_	_
that	_	_
exhibit	_	_
high	_	_
degree	_	_
of	_	_
similarity	_	_
to	_	_
the	_	_
test	_	_
set	_	_
(	_	_
Zheng	_	_
et	_	_
al.	_	_
,	_	_
2010	_	_
;	_	_
Li	_	_
et	_	_
al.	_	_
,	_	_
2010	_	_
)	_	_
.	_	_

#12
In	_	_
our	_	_
recent	_	_
work	_	_
(	_	_
Nakov	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
,	_	_
we	_	_
have	_	_
studied	_	_
the	_	_
relationship	_	_
between	_	_
optimizers	_	_
such	_	_
as	_	_
MERT	_	_
,	_	_
PRO	_	_
and	_	_
MIRA	_	_
,	_	_
and	_	_
we	_	_
have	_	_
pointed	_	_
out	_	_
that	_	_
PRO	_	_
tends	_	_
to	_	_
generate	_	_
relatively	_	_
shorter	_	_
translations	_	_
,	_	_
which	_	_
could	speculation	_
lead	_	_
to	_	_
lower	_	_
BLEU	_	_
scores	_	_
on	_	_
testing	_	_
.	_	_

#13
Our	_	_
solution	_	_
there	_	_
was	_	_
to	_	_
fix	_	_
the	_	_
objective	_	_
function	_	_
being	_	_
optimized	_	_
:	_	_
PRO	_	_
uses	_	_
sentence-level	_	_
smoothed	_	_
BLEU+1	_	_
,	_	_
as	_	_
opposed	_	_
to	_	_
the	_	_
standard	_	_
dataset-level	_	_
BLEU	_	_
.	_	_

#15
More	_	_
specifically	_	_
,	_	_
we	_	_
study	_	_
how	_	_
the	_	_
verbosity	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
average	_	_
target/source	_	_
sentence	_	_
length	_	_
ratio	_	_
,	_	_
learned	_	_
by	_	_
optimizers	_	_
such	_	_
as	_	_
MERT	_	_
and	_	_
PRO	_	_
depends	_	_
on	_	_
the	_	_
nature	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
.	_	_

#16
This	_	_
could	speculation	_
potentially	_	_
allow	_	_
us	_	_
to	_	_
manipulate	_	_
the	_	_
verbosity	_	_
of	_	_
the	_	_
translation	_	_
hypotheses	_	_
generated	_	_
at	_	_
test	_	_
time	_	_
by	_	_
changing	_	_
some	_	_
characteristics	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
.	_	_

#17
1Verbosity	_	_
also	_	_
depends	_	_
on	_	_
the	_	_
translator	_	_
;	_	_
it	_	_
is	_	_
often	_	_
a	_	_
stylistic	_	_
choice	_	_
.	_	_

#33
Naturally	_	_
,	_	_
the	_	_
verbosity	_	_
varies	_	_
across	_	_
different	_	_
tuning/testing	_	_
datasets	_	_
,	_	_
e.g.	_	_
,	_	_
because	_	_
of	_	_
style	_	_
,	_	_
translator	_	_
choice	_	_
,	_	_
etc	_	_
.	_	_

#34
Interestingly	_	_
,	_	_
verbosity	_	_
can	options	_
also	_	_
differ	_	_
across	_	_
sentences	_	_
with	_	_
different	_	_
source	_	_
lengths	_	_
drawn	_	_
from	_	_
the	_	_
same	_	_
dataset	_	_
.	_	_

#35
This	_	_
is	_	_
illustrated	_	_
in	_	_
Figure	_	_
1	_	_
,	_	_
which	_	_
plots	_	_
the	_	_
average	_	_
sample	_	_
source	_	_
length	_	_
vs	_	_
.	_	_

#36
the	_	_
average	_	_
verbosity	_	_
for	_	_
100	_	_
samples	_	_
,	_	_
each	_	_
containing	_	_
500	_	_
randomly	_	_
selected	_	_
sentence	_	_
pairs	_	_
,	_	_
drawn	_	_
from	_	_
the	_	_
concatenation	_	_
of	_	_
the	_	_
MT04	_	_
,	_	_
MT05	_	_
,	_	_
MT06	_	_
,	_	_
MT09	_	_
datasets	_	_
for	_	_
Arabic-English	_	_
and	_	_
of	_	_
newstest2008-2011	_	_
for	_	_
Spanish-English.4	_	_
We	_	_
can	feasibility-rhetorical	_
see	_	_
that	_	_
for	_	_
Arabic-English	_	_
,	_	_
the	_	_
English	_	_
translations	_	_
are	_	_
longer	_	_
than	_	_
the	_	_
Arabic	_	_
source	_	_
sentences	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
verbosity	_	_
is	_	_
greater	_	_
than	_	_
one	_	_
.	_	_

#37
This	_	_
relationship	_	_
is	_	_
accentuated	_	_
by	_	_
length	_	_
:	_	_
verbosity	_	_
increases	_	_
with	_	_
sentence	_	_
length	_	_
:	_	_
see	_	_
the	_	_
slightly	_	_
positive	_	_
slope	_	_
of	_	_
the	_	_
regression	_	_
line	_	_
.	_	_

#38
Note	_	_
that	_	_
the	_	_
increasing	_	_
verbosity	_	_
can	feasibility	_
be	_	_
observed	_	_
in	_	_
singlereference	_	_
sets	_	_
(	_	_
we	_	_
used	_	_
the	_	_
first	_	_
reference	_	_
)	_	_
,	_	_
and	_	_
to	_	_
a	_	_
lesser	_	_
extent	_	_
in	_	_
multiple-reference	_	_
sets	_	_
(	_	_
five	_	_
references	_	_
for	_	_
MT04	_	_
and	_	_
MT05	_	_
,	_	_
and	_	_
four	_	_
for	_	_
MT06	_	_
and	_	_
MT09	_	_
)	_	_
.	_	_

#39
For	_	_
Spanish-English	_	_
,	_	_
the	_	_
story	_	_
is	_	_
different	_	_
:	_	_
here	_	_
the	_	_
English	_	_
sentences	_	_
tend	_	_
to	_	_
be	_	_
shorter	_	_
than	_	_
the	_	_
Spanish	_	_
ones	_	_
,	_	_
and	_	_
the	_	_
verbosity	_	_
decreases	_	_
as	_	_
the	_	_
sentence	_	_
length	_	_
increases	_	_
.	_	_

#50
We	_	_
are	_	_
also	_	_
interested	_	_
in	_	_
the	_	_
question	_	_
of	_	_
how	_	_
the	_	_
hypothesis	_	_
verbosity	_	_
learned	_	_
by	_	_
optimizers	_	_
such	_	_
as	_	_
MERT	_	_
and	_	_
PRO	_	_
depends	_	_
on	_	_
the	_	_
nature	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
,	_	_
i.e.	_	_
,	_	_
its	_	_
verbosity	_	_
.	_	_

#51
Understanding	_	_
this	_	_
could	speculation	_
potentially	_	_
allow	_	_
us	_	_
to	_	_
manipulate	_	_
the	_	_
hypothesis	_	_
verbosity	_	_
of	_	_
the	_	_
translations	_	_
generated	_	_
at	_	_
test	_	_
time	_	_
simply	_	_
by	_	_
changing	_	_
the	_	_
characteristics	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
in	_	_
a	_	_
systematic	_	_
and	_	_
controlled	_	_
way	_	_
.	_	_

#52
While	_	_
controlling	_	_
the	_	_
verbosity	_	_
of	_	_
a	_	_
tuning	_	_
set	_	_
might	options	concessive
be	_	_
an	_	_
appealing	_	_
idea	_	_
,	_	_
this	_	_
is	_	_
unrealistic	_	_
in	_	_
practice	_	_
,	_	_
given	_	_
that	_	_
the	_	_
verbosity	_	_
of	_	_
a	_	_
test	_	_
set	_	_
is	_	_
always	_	_
unknown	_	_
.	_	_

#53
However	_	_
,	_	_
the	_	_
results	_	_
in	_	_
Figure	_	_
1	_	_
suggest	_	_
that	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
manipulate	_	_
verbosity	_	_
by	_	_
controlling	_	_
the	_	_
average	_	_
source	_	_
sentence	_	_
length	_	_
of	_	_
the	_	_
dataset	_	_
(	_	_
and	_	_
the	_	_
source-side	_	_
length	_	_
is	_	_
always	_	_
known	_	_
for	_	_
any	_	_
test	_	_
set	_	_
)	_	_
.	_	_

#55
In	_	_
order	_	_
to	_	_
shed	_	_
some	_	_
light	_	_
on	_	_
our	_	_
initial	_	_
question	_	_
(	_	_
whether	_	_
the	_	_
SMT	_	_
parameter	_	_
optimizers	_	_
are	_	_
able	_	_
to	_	_
learn	_	_
the	_	_
verbosity	_	_
from	_	_
the	_	_
tuning	_	_
dataset	_	_
)	_	_
,	_	_
we	_	_
contrast	_	_
the	_	_
verbosity	_	_
that	_	_
two	_	_
different	_	_
optimizers	_	_
,	_	_
MERT	_	_
and	_	_
PRO	_	_
,	_	_
learn	_	_
as	_	_
a	_	_
function	_	_
of	_	_
the	_	_
average	_	_
length	_	_
of	_	_
the	_	_
sentences	_	_
in	_	_
the	_	_
tuning	_	_
dataset.5	_	_
5In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
consider	_	_
both	_	_
optimizers	_	_
,	_	_
MERT	_	_
and	_	_
PRO	_	_
,	_	_
as	_	_
black-boxes	_	_
.	_	_

#56
For	_	_
a	_	_
detailed	_	_
analysis	_	_
of	_	_
how	_	_
their	_	_
inner	_	_
workings	_	_
can	capability-options	_
affect	_	_
optimization	_	_
,	_	_
see	_	_
our	_	_
earlier	_	_
work	_	_
(	_	_
Nakov	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
.	_	_

#57
4	_	_
Experiments	_	_
and	_	_
Evaluation	_	_
We	_	_
experimented	_	_
with	_	_
single-reference	_	_
and	_	_
multireference	_	_
tuning	_	_
and	_	_
testing	_	_
datasets	_	_
for	_	_
two	_	_
language	_	_
pairs	_	_
:	_	_
Spanish-English	_	_
and	_	_
ArabicEnglish	_	_
.	_	_

#80
In	_	_
each	_	_
graph	_	_
,	_	_
there	_	_
are	_	_
36	_	_
points	_	_
(	_	_
many	_	_
of	_	_
them	_	_
very	_	_
close	_	_
and	_	_
overlapping	_	_
)	_	_
since	_	_
we	_	_
performed	_	_
three	_	_
reruns	_	_
with	_	_
our	_	_
twelve	_	_
tuning	_	_
datasets	_	_
(	_	_
three	_	_
length-based	_	_
subsets	_	_
for	_	_
each	_	_
of	_	_
the	_	_
four	_	_
original	_	_
tuning	_	_
datasets	_	_
)	_	_
.	_	_

#81
There	_	_
are	_	_
several	_	_
observations	_	_
that	_	_
we	_	_
can	feasibility-rhetorical	_
make	_	_
:	_	_
(	_	_
1	_	_
)	_	_
MERT	_	_
is	_	_
fairly	_	_
stable	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
length	_	_
of	_	_
the	_	_
input	_	_
tuning	_	_
sentences	_	_
.	_	_

#82
Note	_	_
how	_	_
the	_	_
MERT	_	_
regression	_	_
lines	_	_
imitate	_	_
those	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#85
Interestingly	_	_
,	_	_
its	_	_
length	_	_
ratio	_	_
is	_	_
more	_	_
sensitive	_	_
to	_	_
the	_	_
input	_	_
length	_	_
(	_	_
r=0.67	_	_
)	_	_
:	_	_
on	_	_
short	_	_
sentences	_	_
,	_	_
it	_	_
learns	_	_
to	_	_
output	_	_
translations	_	_
that	_	_
are	_	_
slightly	_	_
shorter	_	_
than	_	_
the	_	_
reference	_	_
,	_	_
while	_	_
on	_	_
long	_	_
sentences	_	_
,	_	_
it	_	_
yields	_	_
increasingly	_	_
longer	_	_
translations	_	_
.	_	_

#86
The	_	_
dependence	_	_
of	_	_
PRO	_	_
on	_	_
source	_	_
length	_	_
can	feasibility	_
be	_	_
explained	_	_
by	_	_
the	_	_
sentence-level	_	_
smoothing	_	_
in	_	_
BLEU+1	_	_
and	_	_
the	_	_
broken	_	_
balance	_	_
between	_	_
BLEU’s	_	_
precision	_	_
component	_	_
and	_	_
BP	_	_
(	_	_
Nakov	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
.	_	_

#87
The	_	_
problem	_	_
is	_	_
bigger	_	_
for	_	_
short	_	_
sentences	_	_
since	_	_
there	_	_
+1	_	_
is	_	_
added	_	_
to	_	_
smaller	_	_
counts	_	_
;	_	_
this	_	_
results	_	_
in	_	_
preference	_	_
for	_	_
shorter	_	_
translations	_	_
.	_	_

#88
(	_	_
2	_	_
)	_	_
Looking	_	_
at	_	_
the	_	_
results	_	_
for	_	_
Arabic-English	_	_
,	_	_
we	_	_
observe	_	_
that	_	_
having	_	_
multiple	_	_
references	_	_
makes	_	_
both	_	_
MERT	_	_
and	_	_
PRO	_	_
appear	_	_
more	_	_
stable	_	_
,	_	_
allowing	_	_
them	_	_
to	_	_
generate	_	_
hypotheses	_	_
that	_	_
are	_	_
less	_	_
spread	_	_
,	_	_
and	_	_
closer	_	_
to	_	_
1	_	_
.	_	_

#89
This	_	_
can	feasibility	_
be	_	_
attributed	_	_
to	_	_
the	_	_
best	_	_
match	_	_
reference	_	_
length	_	_
,	_	_
which	_	_
naturally	_	_
dampens	_	_
the	_	_
effect	_	_
of	_	_
verbosity	_	_
during	_	_
optimization	_	_
by	_	_
selecting	_	_
the	_	_
reference	_	_
that	_	_
is	_	_
closest	_	_
to	_	_
the	_	_
respective	_	_
hypothesis	_	_
.	_	_

#90
Overall	_	_
,	_	_
we	_	_
can	feasibility	_
conclude	_	_
that	_	_
MERT	_	_
learns	_	_
the	_	_
tuning	_	_
set’s	_	_
verbosity	_	_
more	_	_
accurately	_	_
than	_	_
PRO	_	_
.	_	_

#91
PRO	_	_
learns	_	_
verbosity	_	_
that	_	_
is	_	_
more	_	_
dependent	_	_
on	_	_
the	_	_
source	_	_
side	_	_
length	_	_
of	_	_
the	_	_
sentences	_	_
in	_	_
the	_	_
tuning	_	_
dataset	_	_
.	_	_

#104
For	_	_
MERT	_	_
,	_	_
tuning	_	_
on	_	_
a	_	_
specific	_	_
length	_	_
condition	_	_
yields	_	_
the	_	_
best	_	_
results	_	_
when	_	_
testing	_	_
on	_	_
a	_	_
similar	_	_
condition	_	_
,	_	_
i.e.	_	_
,	_	_
zero-loss	_	_
.	_	_

#105
This	_	_
is	_	_
a	_	_
satisfactory	_	_
result	_	_
since	_	_
it	_	_
confirms	_	_
the	_	_
common	_	_
wisdom	_	_
that	_	_
tuning	_	_
datasets	_	_
should	deontic	_
be	_	_
as	_	_
similar	_	_
as	_	_
possible	_	_
to	_	_
test-time	_	_
input	_	_
in	_	_
terms	_	_
of	_	_
source	_	_
side	_	_
length	_	_
.	_	_

#106
In	_	_
contrast	_	_
,	_	_
PRO	_	_
behaves	_	_
better	_	_
when	_	_
tuning	_	_
on	_	_
mid-length	_	_
tuning	_	_
sets	_	_
.	_	_

#116
Length	_	_
and	_	_
Verbosity	_	_
The	_	_
above	_	_
results	_	_
give	_	_
rise	_	_
to	_	_
some	_	_
interesting	_	_
questions	_	_
:	_	_
What	_	_
if	_	_
we	_	_
do	_	_
not	_	_
know	_	_
the	_	_
sourceside	_	_
length	_	_
of	_	_
the	_	_
test	_	_
set	_	_
?	_	_

#117
What	_	_
if	_	_
we	_	_
can	feasibility	_
choose	_	_
a	_	_
tuning	_	_
set	_	_
based	_	_
on	_	_
its	_	_
verbosity	_	_
?	_	_

#118
Would	_	_
it	_	_
then	_	_
be	_	_
better	_	_
to	_	_
choose	_	_
based	_	_
on	_	_
length	_	_
or	_	_
based	_	_
on	_	_
verbosity	_	_
?	_	_

#120
This	_	_
time	_	_
,	_	_
we	_	_
translated	_	_
the	_	_
full	_	_
test	_	_
datasets	_	_
(	_	_
e.g.	_	_
,	_	_
MT06	_	_
,	_	_
MT09	_	_
)	_	_
;	_	_
the	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
2	_	_
.	_	_

#121
We	_	_
can	feasibility-rhetorical	_
make	_	_
the	_	_
following	_	_
observations	_	_
:	_	_
(	_	_
1	_	_
)	_	_
The	_	_
best	_	_
results	_	_
for	_	_
PRO	_	_
are	_	_
better	_	_
than	_	_
the	_	_
best	_	_
results	_	_
for	_	_
MERT	_	_
,	_	_
in	_	_
all	_	_
conditions	_	_
.	_	_

#122
(	_	_
2	_	_
)	_	_
Length-based	_	_
tuning	_	_
subsets	_	_
:	_	_
With	_	_
a	_	_
single	_	_
reference	_	_
,	_	_
PRO	_	_
performs	_	_
best	_	_
when	_	_
tuning	_	_
on	_	_
short	_	_
sentences	_	_
,	_	_
but	_	_
with	_	_
multiple	_	_
references	_	_
,	_	_
it	_	_
works	_	_
best	_	_
with	_	_
mid-length	_	_
sentences	_	_
.	_	_

#125
With	_	_
multiple	_	_
references	_	_
,	_	_
MERT	_	_
performs	_	_
best	_	_
when	_	_
tuning	_	_
on	_	_
high-verbosity	_	_
datasets	_	_
;	_	_
however	_	_
,	_	_
with	_	_
a	_	_
single	_	_
reference	_	_
,	_	_
it	_	_
prefers	_	_
mid-verbosity	_	_
.	_	_

#126
Based	_	_
on	_	_
the	_	_
above	_	_
results	_	_
,	_	_
we	_	_
recommend	_	_
that	_	_
,	_	_
whenever	_	_
we	_	_
have	_	_
no	_	_
access	_	_
to	_	_
the	_	_
input	_	_
side	_	_
of	_	_
the	_	_
testing	_	_
dataset	_	_
beforehand	_	_
,	_	_
we	_	_
should	deontic	_
tune	_	_
on	_	_
datasets	_	_
with	_	_
high	_	_
verbosity	_	_
.	_	_

#127
4.4	_	_
Test	_	_
vs	_	_
.	_	_

#133
test	_	_
hypothesis	_	_
verbosity	_	_
when	_	_
using	_	_
MERT	_	_
to	_	_
tune	_	_
under	_	_
different	_	_
conditions	_	_
,	_	_
and	_	_
testing	_	_
on	_	_
each	_	_
of	_	_
the	_	_
unseen	_	_
full	_	_
datasets	_	_
.	_	_

#134
We	_	_
test	_	_
on	_	_
full	_	_
datasets	_	_
to	_	_
avoid	_	_
the	_	_
verbosity	_	_
bias	_	_
that	_	_
might	speculation	_
occur	_	_
for	_	_
specific	_	_
conditions	_	_
(	_	_
see	_	_
Section	_	_
3	_	_
)	_	_
.	_	_

#135
We	_	_
can	feasibility-rhetorical	_
see	_	_
strong	_	_
positive	_	_
correlation	_	_
between	_	_
the	_	_
tuning	_	_
set	_	_
verbosity	_	_
and	_	_
the	_	_
hypothesis	_	_
verbosity	_	_
on	_	_
the	_	_
test	_	_
datasets	_	_
.	_	_

#136
The	_	_
average	_	_
correlation	_	_
for	_	_
Arabic-English	_	_
is	_	_
r=0.95	_	_
with	_	_
multiple	_	_
references	_	_
and	_	_
r=0.98	_	_
with	_	_
a	_	_
single	_	_
reference	_	_
;	_	_
for	_	_
Spanish-English	_	_
,	_	_
it	_	_
is	_	_
r=0.97	_	_
.	_	_

#146
In	_	_
other	_	_
words	_	_
,	_	_
MERT	_	_
learns	_	_
to	_	_
generate	_	_
a	_	_
fixed	_	_
number	_	_
of	_	_
words	_	_
per	_	_
input	_	_
word	_	_
.	_	_

#147
This	_	_
can	feasibility	_
be	_	_
explained	_	_
by	_	_
the	_	_
fact	_	_
that	_	_
MERT	_	_
optimizes	_	_
BLEU	_	_
score	_	_
directly	_	_
,	_	_
and	_	_
thus	_	_
learns	_	_
to	_	_
output	_	_
the	_	_
“right”	_	_
verbosity	_	_
on	_	_
the	_	_
tuning	_	_
dataset	_	_
(	_	_
in	_	_
contrast	_	_
,	_	_
PRO	_	_
optimizes	_	_
sentence-level	_	_
BLEU+1	_	_
,	_	_
which	_	_
is	_	_
an	_	_
approximation	_	_
to	_	_
BLEU	_	_
,	_	_
but	_	_
it	_	_
is	_	_
not	_	_
the	_	_
actual	_	_
BLEU	_	_
)	_	_
.	_	_

#148
This	_	_
explains	_	_
why	_	_
MERT	_	_
performs	_	_
best	_	_
when	_	_
the	_	_
tuning	_	_
conditions	_	_
and	_	_
the	_	_
testing	_	_
conditions	_	_
are	_	_
in	_	_
sync	_	_
.	_	_

#151
the	_	_
testing	_	_
hypothesis/reference	_	_
length	_	_
ratio	_	_
when	_	_
using	_	_
PRO	_	_
to	_	_
tune	_	_
on	_	_
short	_	_
,	_	_
middle	_	_
,	_	_
and	_	_
long	_	_
and	_	_
testing	_	_
on	_	_
each	_	_
of	_	_
the	_	_
unseen	_	_
full	_	_
datasets	_	_
,	_	_
as	_	_
in	_	_
the	_	_
previous	_	_
subsection	_	_
.	_	_

#152
We	_	_
can	feasibility-rhetorical	_
see	_	_
that	_	_
there	_	_
is	_	_
positive	_	_
correlation	_	_
between	_	_
the	_	_
tuning	_	_
set	_	_
average	_	_
source	_	_
side	_	_
length	_	_
and	_	_
the	_	_
testing	_	_
hypothesis/reference	_	_
length	_	_
ratio	_	_
.	_	_

#153
For	_	_
SpanishEnglish	_	_
,	_	_
it	_	_
is	_	_
quite	_	_
strong	_	_
(	_	_
r=0.64	_	_
)	_	_
,	_	_
and	_	_
for	_	_
ArabicEnglish	_	_
,	_	_
it	_	_
is	_	_
more	_	_
clearly	_	_
expressed	_	_
with	_	_
one	_	_
(	_	_
r=0.42	_	_
)	_	_
than	_	_
with	_	_
multiple	_	_
references	_	_
(	_	_
r=0.34	_	_
)	_	_
.	_	_

#164
5	_	_
Discussion	_	_
We	_	_
have	_	_
observed	_	_
that	_	_
high-verbosity	_	_
tuning	_	_
sets	_	_
yield	_	_
better	_	_
results	_	_
with	_	_
PRO	_	_
.	_	_

#165
We	_	_
have	_	_
further	_	_
seen	_	_
that	_	_
we	_	_
can	feasibility	_
manipulate	_	_
verbosity	_	_
by	_	_
adjusting	_	_
the	_	_
average	_	_
length	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
.	_	_

#166
This	_	_
leads	_	_
to	_	_
the	_	_
natural	_	_
question	_	_
:	_	_
can	capability	_
this	_	_
yield	_	_
better	_	_
BLEU	_	_
?	_	_

#167
It	_	_
turns	_	_
out	_	_
that	_	_
the	_	_
answer	_	_
is	_	_
“yes”	_	_
.	_	_

#170
Moreover	_	_
,	_	_
our	_	_
previous	_	_
findings	_	_
suggest	_	_
that	_	_
for	_	_
PRO	_	_
,	_	_
higherverbosity	_	_
tuning	_	_
datasets	_	_
will	_	_
perform	_	_
better	_	_
in	_	_
this	_	_
situation	_	_
.	_	_

#171
Therefore	_	_
,	_	_
we	_	_
should	deontic-rhetorical	_
expect	_	_
that	_	_
longer	_	_
tuning	_	_
datasets	_	_
could	speculation	_
yield	_	_
better	_	_
BLEU	_	_
.	_	_

#172
Table	_	_
3	_	_
presents	_	_
the	_	_
results	_	_
for	_	_
PRO	_	_
with	_	_
ArabicEnglish	_	_
when	_	_
tuning	_	_
on	_	_
MT06	_	_
,	_	_
or	_	_
subsets	_	_
thereof	_	_
,	_	_
and	_	_
testing	_	_
on	_	_
MT09	_	_
.	_	_

#175
Line	_	_
4	_	_
shows	_	_
that	_	_
selecting	_	_
a	_	_
random-50	_	_
%	_	_
subset	_	_
(	_	_
included	_	_
here	_	_
to	_	_
show	_	_
the	_	_
effect	_	_
of	_	_
using	_	_
mixed-length	_	_
sentences	_	_
)	_	_
yields	_	_
results	_	_
that	_	_
are	_	_
very	_	_
close	_	_
to	_	_
those	_	_
for	_	_
middle	_	_
.	_	_

#176
Comparing	_	_
line	_	_
3	_	_
to	_	_
lines	_	_
4	_	_
and	_	_
5	_	_
,	_	_
we	_	_
can	rhetorical	_
see	_	_
that	_	_
tuning	_	_
on	_	_
long	_	_
yields	_	_
longer	_	_
translations	_	_
and	_	_
also	_	_
higher	_	_
BLEU	_	_
,	_	_
compared	_	_
to	_	_
tuning	_	_
on	_	_
the	_	_
full	_	_
dataset	_	_
or	_	_
on	_	_
random	_	_
.	_	_

#177
Next	_	_
,	_	_
lines	_	_
6	_	_
and	_	_
7	_	_
show	_	_
the	_	_
results	_	_
when	_	_
applying	_	_
our	_	_
smoothing	_	_
fix	_	_
for	_	_
sentence-level	_	_
BLEU+1	_	_
(	_	_
Nakov	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
,	_	_
which	_	_
prevents	_	_
translations	_	_
from	_	_
becoming	_	_
too	_	_
short	_	_
;	_	_
we	_	_
can	rhetorical	_
see	_	_
that	_	_
long	_	_
yields	_	_
very	_	_
comparable	_	_
results	_	_
.	_	_

#178
Yet	_	_
,	_	_
manipulating	_	_
the	_	_
tuning	_	_
dataset	_	_
might	speculation	_
be	_	_
preferable	_	_
since	_	_
it	_	_
allows	_	_
(	_	_
i	_	_
)	_	_
faster	_	_
tuning	_	_
,	_	_
by	_	_
using	_	_
part	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
,	_	_
(	_	_
ii	_	_
)	_	_
flexibility	_	_
in	_	_
the	_	_
selection	_	_
of	_	_
the	_	_
desired	_	_
verbosity	_	_
,	_	_
and	_	_
(	_	_
iii	_	_
)	_	_
applicability	_	_
to	_	_
other	_	_
MT	_	_
evaluation	_	_
measures	_	_
.	_	_

#179
Point	_	_
(	_	_
ii	_	_
)	_	_
is	_	_
illustrated	_	_
on	_	_
Figure	_	_
5	_	_
,	_	_
which	_	_
shows	_	_
that	_	_
there	_	_
is	_	_
direct	_	_
positive	_	_
correlation	_	_
between	_	_
verbosity	_	_
,	_	_
length	_	_
ratio	_	_
,	_	_
and	_	_
BLEU	_	_
;	_	_
note	_	_
that	_	_
the	_	_
tuning	_	_
set	_	_
size	_	_
does	_	_
not	_	_
matter	_	_
much	_	_
:	_	_
in	_	_
fact	_	_
,	_	_
better	_	_
results	_	_
are	_	_
obtained	_	_
when	_	_
using	_	_
less	_	_
tuning	_	_
data	_	_
.	_	_

#187
This	_	_
leaves	_	_
us	_	_
with	_	_
the	_	_
situation	_	_
that	_	_
getting	_	_
lucky	_	_
in	_	_
the	_	_
selection	_	_
of	_	_
tuning	_	_
datasets	_	_
and	_	_
optimization	_	_
strategy	_	_
overshadows	_	_
scientific	_	_
advances	_	_
in	_	_
modeling	_	_
or	_	_
decoding	_	_
.	_	_

#188
Understanding	_	_
these	_	_
dependencies	_	_
in	_	_
detail	_	_
puts	_	_
us	_	_
in	_	_
a	_	_
better	_	_
position	_	_
to	_	_
construct	_	_
tuning	_	_
sets	_	_
that	_	_
match	_	_
the	_	_
test	_	_
datasets	_	_
in	_	_
such	_	_
a	_	_
way	_	_
that	_	_
improvements	_	_
in	_	_
models	_	_
,	_	_
training	_	_
,	_	_
and	_	_
decoding	_	_
algorithms	_	_
can	feasibility	_
be	_	_
measured	_	_
more	_	_
reliably	_	_
.	_	_

#189
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
have	_	_
studied	_	_
the	_	_
impact	_	_
that	_	_
source-side	_	_
length	_	_
and	_	_
verbosity	_	_
of	_	_
tuning	_	_
sets	_	_
have	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
translation	_	_
system	_	_
when	_	_
tuning	_	_
the	_	_
system	_	_
with	_	_
different	_	_
optimizers	_	_
such	_	_
as	_	_
MERT	_	_
and	_	_
PRO	_	_
.	_	_

#190
We	_	_
observed	_	_
that	_	_
MERT	_	_
learns	_	_
the	_	_
verbosity	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
very	_	_
well	_	_
,	_	_
but	_	_
this	_	_
can	options	_
be	_	_
a	_	_
disadvantage	_	_
because	_	_
we	_	_
do	_	_
not	_	_
know	_	_
the	_	_
verbosity	_	_
of	_	_
unseen	_	_
test	_	_
sentences	_	_
.	_	_

#191
In	_	_
contrast	_	_
,	_	_
PRO	_	_
is	_	_
affected	_	_
by	_	_
both	_	_
the	_	_
verbosity	_	_
and	_	_
the	_	_
source-side	_	_
length	_	_
of	_	_
the	_	_
tuning	_	_
dataset	_	_
.	_	_

#192
There	_	_
may	options	_
be	_	_
other	_	_
characteristics	_	_
of	_	_
test	_	_
datasets	_	_
,	_	_
e.g.	_	_
,	_	_
amount	_	_
of	_	_
reordering	_	_
,	_	_
number	_	_
of	_	_
unknown	_	_
words	_	_
,	_	_
complexity	_	_
of	_	_
the	_	_
sentences	_	_
in	_	_
terms	_	_
of	_	_
syntactic	_	_
structure	_	_
,	_	_
etc	_	_
.	_	_

#193
that	_	_
could	speculation	_
have	_	_
similar	_	_
effects	_	_
of	_	_
creating	_	_
good	_	_
or	_	_
bad	_	_
luck	_	_
when	_	_
deciding	_	_
how	_	_
to	_	_
tune	_	_
an	_	_
SMT	_	_
system	_	_
.	_	_

#194
Until	_	_
we	_	_
have	_	_
such	_	_
controlled	_	_
evaluation	_	_
scenarios	_	_
,	_	_
our	_	_
short-term	_	_
recommendations	_	_
are	_	_
as	_	_
follows	_	_
:	_	_
•	_	_
Know	_	_
your	_	_
tuning	_	_
datasets	_	_
:	_	_
Different	_	_
language	_	_
pairs	_	_
and	_	_
translation	_	_
directions	_	_
may	options	_
have	_	_
different	_	_
source-side	_	_
length	_	_
–	_	_
verbosity	_	_
dependencies	_	_
.	_	_

#195
•	_	_
When	_	_
optimizing	_	_
with	_	_
PRO	_	_
:	_	_
select	_	_
or	_	_
construct	_	_
a	_	_
high-verbosity	_	_
dataset	_	_
as	_	_
this	_	_
could	capability-speculation	_
potentially	_	_
compensate	_	_
for	_	_
PROs	_	_
tendency	_	_
to	_	_
yield	_	_
too	_	_
short	_	_
translations	_	_
.	_	_

#196
Note	_	_
that	_	_
for	_	_
Arabic-English	_	_
,	_	_
higher	_	_
verbosity	_	_
means	_	_
longer	_	_
tuning	_	_
sentences	_	_
,	_	_
while	_	_
for	_	_
SpanishEnglish	_	_
,	_	_
it	_	_
means	_	_
shorter	_	_
ones	_	_
;	_	_
translation	_	_
direction	_	_
might	speculation	_
matter	_	_
too	_	_
.	_	_

#197
•	_	_
When	_	_
optimizing	_	_
with	_	_
MERT	_	_
:	_	_
If	_	_
you	_	_
know	_	_
beforehand	_	_
the	_	_
test	_	_
set	_	_
,	_	_
select	_	_
the	_	_
closest	_	_
tuning	_	_
set	_	_
.	_	_