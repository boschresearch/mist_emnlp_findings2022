#2
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
explore	_	_
n-gram	_	_
models	_	_
over	_	_
Minimal	_	_
Translation	_	_
Units	_	_
(	_	_
MTUs	_	_
)	_	_
to	_	_
explicitly	_	_
capture	_	_
contextual	_	_
dependencies	_	_
across	_	_
phrase	_	_
boundaries	_	_
in	_	_
the	_	_
channel	_	_
model	_	_
.	_	_

#3
As	_	_
there	_	_
is	_	_
no	_	_
single	_	_
best	_	_
direction	_	_
in	_	_
which	_	_
contextual	_	_
information	_	_
should	deontic	_
flow	_	_
,	_	_
we	_	_
explore	_	_
multiple	_	_
decomposition	_	_
structures	_	_
as	_	_
well	_	_
as	_	_
dynamic	_	_
bidirectional	_	_
decomposition	_	_
.	_	_

#4
The	_	_
resulting	_	_
models	_	_
are	_	_
evaluated	_	_
in	_	_
an	_	_
intrinsic	_	_
task	_	_
of	_	_
lexical	_	_
selection	_	_
for	_	_
MT	_	_
as	_	_
well	_	_
as	_	_
a	_	_
full	_	_
MT	_	_
system	_	_
,	_	_
through	_	_
n-best	_	_
reranking	_	_
.	_	_

#12
Much	_	_
of	_	_
their	_	_
work	_	_
is	_	_
limited	_	_
by	_	_
the	_	_
requirement	_	_
“that	_	_
the	_	_
source	_	_
and	_	_
target	_	_
side	_	_
of	_	_
a	_	_
tuple	_	_
of	_	_
words	_	_
are	_	_
synchronized	_	_
,	_	_
i.e	_	_
.	_	_
that	_	_
they	_	_
occur	_	_
in	_	_
the	_	_
same	_	_
order	_	_
in	_	_
their	_	_
respective	_	_
languages”	_	_
(	_	_
Crego	_	_
and	_	_
Yvon	_	_
,	_	_
2010	_	_
)	_	_
.	_	_

#13
For	_	_
language	_	_
pairs	_	_
with	_	_
significant	_	_
typological	_	_
divergences	_	_
,	_	_
such	_	_
as	_	_
Chinese-English	_	_
,	_	_
it	_	_
is	_	_
quite	_	_
difficult	_	_
to	_	_
extract	_	_
a	_	_
synchronized	_	_
sequence	_	_
of	_	_
units	_	_
;	_	_
in	_	_
the	_	_
limit	_	_
,	_	_
the	_	_
smallest	_	_
synchronized	_	_
unit	_	_
may	options	_
be	_	_
the	_	_
whole	_	_
sentence	_	_
.	_	_

#14
Other	_	_
approaches	_	_
explore	_	_
incorporation	_	_
into	_	_
syntax-based	_	_
MT	_	_
systems	_	_
or	_	_
replacing	_	_
the	_	_
phrasal	_	_
translation	_	_
system	_	_
altogether	_	_
.	_	_

#15
We	_	_
investigate	_	_
the	_	_
addition	_	_
of	_	_
MTUs	_	_
to	_	_
a	_	_
phrasal	_	_
translation	_	_
system	_	_
to	_	_
improve	_	_
modeling	_	_
of	_	_
context	_	_
and	_	_
to	_	_
provide	_	_
more	_	_
robust	_	_
estimation	_	_
of	_	_
long	_	_
phrases	_	_
.	_	_

#16
However	_	_
,	_	_
in	_	_
a	_	_
phrase-based	_	_
system	_	_
there	_	_
is	_	_
no	_	_
single	_	_
synchronized	_	_
traversal	_	_
order	_	_
;	_	_
instead	_	_
,	_	_
we	_	_
may	feasibility-options	_
consider	_	_
the	_	_
translation	_	_
units	_	_
in	_	_
many	_	_
possible	_	_
orders	_	_
:	_	_
left-to-right	_	_
or	_	_
right-to-left	_	_
according	_	_
to	_	_
either	_	_
the	_	_
source	_	_
or	_	_
the	_	_
target	_	_
are	_	_
natural	_	_
choices	_	_
.	_	_

#17
Alternatively	_	_
we	_	_
consider	_	_
translating	_	_
a	_	_
particularly	_	_
unambiguous	_	_
unit	_	_
in	_	_
the	_	_
middle	_	_
of	_	_
the	_	_
sentence	_	_
and	_	_
building	_	_
outwards	_	_
from	_	_
there	_	_
.	_	_

#23
MTUs	_	_
have	_	_
been	_	_
used	_	_
in	_	_
dependency	_	_
translation	_	_
models	_	_
(	_	_
Quirk	_	_
and	_	_
Menezes	_	_
,	_	_
2006	_	_
)	_	_
to	_	_
augment	_	_
syntax	_	_
directed	_	_
translation	_	_
systems	_	_
.	_	_

#24
Likewise	_	_
in	_	_
target	_	_
language	_	_
syntax	_	_
systems	_	_
,	_	_
one	_	_
can	feasibility	_
consider	_	_
Markov	_	_
models	_	_
over	_	_
minimal	_	_
rules	_	_
,	_	_
where	_	_
the	_	_
translation	_	_
probability	_	_
of	_	_
each	_	_
rule	_	_
is	_	_
adjusted	_	_
to	_	_
include	_	_
context	_	_
information	_	_
from	_	_
parent	_	_
rules	_	_
(	_	_
Vaswani	_	_
et	_	_
al.	_	_
,	_	_
2011	_	_
)	_	_
.	_	_

#25
Most	_	_
prior	_	_
work	_	_
tends	_	_
to	_	_
replace	_	_
the	_	_
existing	_	_
probabilities	_	_
rather	_	_
than	_	_
augmenting	_	_
them	_	_
.	_	_

#26
We	_	_
believe	_	_
that	_	_
Markov	_	_
rules	_	_
provide	_	_
an	_	_
additional	_	_
signal	_	_
but	_	_
are	_	_
not	_	_
a	_	_
replacement	_	_
.	_	_

#27
Their	_	_
distributions	_	_
should	deontic	_
be	_	_
more	_	_
informative	_	_
than	_	_
the	_	_
so-called	_	_
“lexical	_	_
weighting”	_	_
models	_	_
,	_	_
and	_	_
less	_	_
sparse	_	_
than	_	_
relative	_	_
frequency	_	_
estimates	_	_
,	_	_
though	_	_
potentially	_	_
not	_	_
as	_	_
effective	_	_
for	_	_
truly	_	_
non-compositional	_	_
units	_	_
.	_	_

#28
Therefore	_	_
,	_	_
we	_	_
explore	_	_
the	_	_
inclusion	_	_
of	_	_
all	_	_
such	_	_
information	_	_
.	_	_

#39
null	_	_
Figure	_	_
1	_	_
:	_	_
Word	_	_
alignment	_	_
and	_	_
minimum	_	_
translation	_	_
units	_	_
.	_	_

#40
3.1	_	_
Definition	_	_
of	_	_
an	_	_
MTU	_	_
Informally	_	_
,	_	_
the	_	_
notion	_	_
of	_	_
a	_	_
minimal	_	_
translation	_	_
unit	_	_
is	_	_
simple	_	_
:	_	_
it	_	_
is	_	_
a	_	_
translation	_	_
rule	_	_
that	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
broken	_	_
down	_	_
any	_	_
further	_	_
without	_	_
violating	_	_
the	_	_
constraints	_	_
of	_	_
the	_	_
rules	_	_
.	_	_

#41
We	_	_
restrict	_	_
ourselves	_	_
to	_	_
contiguous	_	_
MTUs	_	_
.	_	_

#42
They	_	_
are	_	_
similar	_	_
to	_	_
small	_	_
phrase	_	_
pairs	_	_
,	_	_
though	_	_
unlike	_	_
phrase	_	_
pairs	_	_
we	_	_
allow	_	_
MTUs	_	_
to	_	_
have	_	_
either	_	_
an	_	_
empty	_	_
source	_	_
or	_	_
empty	_	_
target	_	_
side	_	_
,	_	_
thereby	_	_
allowing	_	_
insertion	_	_
and	_	_
deletion	_	_
phrases	_	_
.	_	_

#43
Conventional	_	_
phrase	_	_
pairs	_	_
may	capability-feasibility	_
be	_	_
viewed	_	_
as	_	_
compositions	_	_
of	_	_
these	_	_
MTUs	_	_
up	_	_
to	_	_
a	_	_
given	_	_
size	_	_
limit	_	_
.	_	_

#44
Consider	_	_
a	_	_
word-aligned	_	_
sentence	_	_
pair	_	_
consisting	_	_
of	_	_
a	_	_
sequence	_	_
of	_	_
source	_	_
words	_	_
s	_	_
=	_	_
s1	_	_
.	_	_

#50
tn	_	_
,	_	_
and	_	_
a	_	_
word	_	_
alignment	_	_
relation	_	_
between	_	_
the	_	_
source	_	_
and	_	_
target	_	_
words	_	_
∼	_	_
⊆	_	_
{	_	_
1..m	_	_
}	_	_
×	_	_
{	_	_
1..n	_	_
}	_	_
.	_	_

#51
A	_	_
translation	_	_
unit	_	_
is	_	_
a	_	_
sequence	_	_
of	_	_
source	_	_
words	_	_
si..s	_	_
j	_	_
and	_	_
a	_	_
sequence	_	_
of	_	_
target	_	_
words	_	_
tk..tl	_	_
(	_	_
one	_	_
of	_	_
which	_	_
may	options	_
be	_	_
empty	_	_
)	_	_
such	_	_
that	_	_
for	_	_
all	_	_
aligned	_	_
pairs	_	_
i′	_	_
∼	_	_
k′	_	_
,	_	_
we	_	_
have	_	_
i	_	_
≤	_	_
i′	_	_
≤	_	_
j	_	_
if	_	_
and	_	_
only	_	_
if	_	_
k	_	_
≤	_	_
k′	_	_
≤	_	_
l.	_	_
This	_	_
definition	_	_
,	_	_
nearly	_	_
identical	_	_
to	_	_
that	_	_
of	_	_
a	_	_
phrase	_	_
pair	_	_
(	_	_
Koehn	_	_
et	_	_
al.	_	_
,	_	_
2003	_	_
)	_	_
,	_	_
relaxes	_	_
the	_	_
constraint	_	_
that	_	_
one	_	_
aligned	_	_
word	_	_
must	deontic	_
be	_	_
present	_	_
.	_	_

#52
A	_	_
set	_	_
of	_	_
translation	_	_
units	_	_
is	_	_
a	_	_
partition	_	_
of	_	_
the	_	_
sentence	_	_
pair	_	_
if	_	_
each	_	_
source	_	_
and	_	_
target	_	_
word	_	_
is	_	_
covered	_	_
exactly	_	_
once	_	_
.	_	_

#59
Therefore	_	_
,	_	_
training	_	_
an	_	_
n-gram	_	_
model	_	_
over	_	_
minimal	_	_
translation	_	_
units	_	_
turns	_	_
out	_	_
to	_	_
be	_	_
a	_	_
simple	_	_
and	_	_
clean	_	_
choice	_	_
:	_	_
the	_	_
resulting	_	_
segmentation	_	_
is	_	_
unique	_	_
,	_	_
and	_	_
the	_	_
distribution	_	_
is	_	_
smooth	_	_
.	_	_

#60
If	_	_
we	_	_
want	_	_
to	_	_
capture	_	_
more	_	_
context	_	_
,	_	_
we	_	_
can	feasibility	_
simply	_	_
increase	_	_
the	_	_
order	_	_
of	_	_
the	_	_
Markov	_	_
model	_	_
.	_	_

#61
Such	_	_
Markov	_	_
models	_	_
address	_	_
issues	_	_
in	_	_
large	_	_
phrase-based	_	_
translation	_	_
approaches	_	_
.	_	_

#65
Different	_	_
decomposition	_	_
orders	_	_
have	_	_
been	_	_
used	_	_
in	_	_
part-of-speech	_	_
tagging	_	_
and	_	_
named	_	_
entity	_	_
recognition	_	_
(	_	_
Tsuruoka	_	_
and	_	_
Tsujii	_	_
,	_	_
2005	_	_
)	_	_
.	_	_

#66
Intuitively	_	_
,	_	_
information	_	_
from	_	_
the	_	_
left	_	_
or	_	_
right	_	_
could	speculation	_
be	_	_
more	_	_
useful	_	_
for	_	_
particular	_	_
disambiguation	_	_
choices	_	_
.	_	_

#67
Our	_	_
research	_	_
on	_	_
different	_	_
decomposition	_	_
orders	_	_
was	_	_
motivated	_	_
by	_	_
this	_	_
work	_	_
.	_	_

#69
The	_	_
task	_	_
exhibits	_	_
much	_	_
more	_	_
ambiguity	_	_
–	_	_
the	_	_
number	_	_
of	_	_
possible	_	_
MTUs	_	_
is	_	_
in	_	_
the	_	_
millions	_	_
.	_	_

#70
An	_	_
opportunity	_	_
arises	_	_
from	_	_
the	_	_
reordering	_	_
phenomenon	_	_
in	_	_
machine	_	_
translation	_	_
:	_	_
while	_	_
in	_	_
POS	_	_
tagging	_	_
the	_	_
natural	_	_
decomposition	_	_
orders	_	_
to	_	_
study	_	_
are	_	_
only	_	_
left-to-right	_	_
and	_	_
right-to-left	_	_
,	_	_
in	_	_
machine	_	_
translation	_	_
we	_	_
can	feasibility	_
further	_	_
distinguish	_	_
source	_	_
and	_	_
target	_	_
sentence	_	_
orders	_	_
.	_	_

#71
We	_	_
first	_	_
define	_	_
the	_	_
source	_	_
left-to-right	_	_
and	_	_
the	_	_
target	_	_
left-to-right	_	_
orders	_	_
of	_	_
the	_	_
aligned	_	_
sets	_	_
of	_	_
MTUs	_	_
.	_	_

#77
The	_	_
sequence	_	_
in	_	_
target	_	_
left-to-right	_	_
order	_	_
is	_	_
M3-M4-M5-M1-M2	_	_
.	_	_

#78
This	_	_
illustrates	_	_
that	_	_
decomposition	_	_
structure	_	_
may	options	_
differ	_	_
significantly	_	_
depending	_	_
on	_	_
which	_	_
language	_	_
is	_	_
used	_	_
to	_	_
define	_	_
the	_	_
enumeration	_	_
order	_	_
.	_	_

#79
Once	_	_
a	_	_
sentence	_	_
pair	_	_
is	_	_
represented	_	_
as	_	_
a	_	_
sequence	_	_
of	_	_
MTUs	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
define	_	_
the	_	_
probability	_	_
of	_	_
the	_	_
sentence	_	_
pair	_	_
using	_	_
a	_	_
conventional	_	_
n-gram	_	_
Markov	_	_
model	_	_
(	_	_
MM	_	_
)	_	_
over	_	_
MTUs	_	_
.	_	_

#80
For	_	_
example	_	_
,	_	_
the	_	_
3-gram	_	_
MM	_	_
probability	_	_
of	_	_
the	_	_
sentence	_	_
pair	_	_
in	_	_
Figure	_	_
1	_	_
under	_	_
the	_	_
source	_	_
left-to-right	_	_
order	_	_
is	_	_
as	_	_
follows	_	_
:	_	_
P	_	_
(	_	_
M1	_	_
)	_	_
·P	_	_
(	_	_
M2|M1	_	_
)	_	_
·P	_	_
(	_	_
M3|M1	_	_
,	_	_
M2	_	_
)	_	_
·P	_	_
(	_	_
M4|M2	_	_
,	_	_
M3	_	_
)	_	_
·	_	_
P	_	_
(	_	_
M5|M3	_	_
,	_	_
M4	_	_
)	_	_
.	_	_

#82
We	_	_
compare	_	_
all	_	_
four	_	_
decomposition	_	_
orders	_	_
(	_	_
source	_	_
order	_	_
left-to-right	_	_
and	_	_
right-to-left	_	_
,	_	_
and	_	_
target	_	_
order	_	_
left-to-right	_	_
and	_	_
righttoleft	_	_
)	_	_
.	_	_

#83
Although	_	_
the	_	_
independence	_	_
assumptions	_	_
of	_	_
left-to-right	_	_
and	_	_
right-to-left	_	_
are	_	_
the	_	_
same	_	_
,	_	_
the	_	_
resulting	_	_
models	_	_
may	options	_
be	_	_
different	_	_
due	_	_
to	_	_
smoothing	_	_
.	_	_

#84
In	_	_
addition	_	_
to	_	_
studying	_	_
these	_	_
four	_	_
basic	_	_
decomposition	_	_
orders	_	_
,	_	_
we	_	_
report	_	_
performance	_	_
of	_	_
two	_	_
cyclic	_	_
orders	_	_
:	_	_
cyclic	_	_
in	_	_
source	_	_
or	_	_
target	_	_
sentence	_	_
order	_	_
.	_	_

#113
The	_	_
top	_	_
scoring	_	_
complete	_	_
hypotheses	_	_
covering	_	_
the	_	_
first	_	_
m	_	_
MTUs	_	_
are	_	_
maintained	_	_
in	_	_
a	_	_
beam	_	_
.	_	_

#114
When	_	_
scoring	_	_
with	_	_
a	_	_
target	_	_
left-to-right	_	_
MTU	_	_
Markov	_	_
model	_	_
(	_	_
L2RT	_	_
)	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
score	_	_
each	_	_
partial	_	_
hypothesis	_	_
exactly	_	_
at	_	_
each	_	_
step	_	_
.	_	_

#115
When	_	_
scoring	_	_
using	_	_
a	_	_
R2LT	_	_
model	_	_
or	_	_
a	_	_
source	_	_
order	_	_
model	_	_
,	_	_
we	_	_
use	_	_
lower-order	_	_
approximations	_	_
to	_	_
the	_	_
trigram	_	_
MTU	_	_
Markov	_	_
model	_	_
scores	_	_
as	_	_
future	_	_
scores	_	_
,	_	_
since	_	_
not	_	_
all	_	_
needed	_	_
context	_	_
is	_	_
available	_	_
for	_	_
a	_	_
hypothesis	_	_
at	_	_
the	_	_
time	_	_
of	_	_
construction	_	_
.	_	_

#116
As	_	_
additional	_	_
context	_	_
becomes	_	_
available	_	_
,	_	_
the	_	_
exact	_	_
score	_	_
can	feasibility	_
be	_	_
computed	_	_
.	_	_

#117
2	_	_
4.1	_	_
Basic	_	_
decomposition	_	_
order	_	_
combinations	_	_
We	_	_
first	_	_
introduce	_	_
two	_	_
methods	_	_
of	_	_
combining	_	_
different	_	_
decomposition	_	_
orders	_	_
:	_	_
product	_	_
and	_	_
system	_	_
combination	_	_
.	_	_

#118
comThe	_	_
product	_	_
method	_	_
arises	_	_
naturally	_	_
in	_	_
the	_	_
machine	_	_
translation	_	_
setting	_	_
,	_	_
where	_	_
probabilities	_	_
from	_	_
different	_	_
models	_	_
are	_	_
multiplied	_	_
together	_	_
and	_	_
further	_	_
weighted	_	_
to	_	_
form	_	_
the	_	_
log-linear	_	_
model	_	_
for	_	_
machine	_	_
translation	_	_
(	_	_
Och	_	_
and	_	_
Ney	_	_
,	_	_
2002	_	_
)	_	_
.	_	_

#119
We	_	_
define	_	_
a	_	_
similar	_	_
scoring	_	_
function	_	_
using	_	_
a	_	_
set	_	_
of	_	_
MTU	_	_
Markov	_	_
models	_	_
MM1	_	_
,	_	_
...	_	_
,	_	_
MMk	_	_
for	_	_
a	_	_
hypothesis	_	_
h	_	_
as	_	_
follows	_	_
:	_	_
Score	_	_
(	_	_
h	_	_
)	_	_
=	_	_
λ1logPMM1	_	_
(	_	_
h	_	_
)	_	_
+	_	_
...	_	_
+	_	_
λklogPMMk	_	_
(	_	_
h	_	_
)	_	_
2We	_	_
apply	_	_
hypothesis	_	_
recombination	_	_
,	_	_
which	_	_
can	capability	_
merge	_	_
hypotheses	_	_
that	_	_
are	_	_
indistinguishable	_	_
with	_	_
respect	_	_
to	_	_
future	_	_
continuations	_	_
.	_	_

#120
This	_	_
is	_	_
similar	_	_
to	_	_
recombination	_	_
in	_	_
a	_	_
standard-phrase	_	_
based	_	_
decoder	_	_
with	_	_
the	_	_
difference	_	_
that	_	_
it	_	_
is	_	_
not	_	_
always	_	_
the	_	_
last	_	_
two	_	_
target	_	_
MTUs	_	_
that	_	_
define	_	_
the	_	_
context	_	_
needed	_	_
by	_	_
future	_	_
extensions	_	_
.	_	_

#142
They	_	_
define	_	_
the	_	_
set	_	_
of	_	_
parents	_	_
(	_	_
context	_	_
)	_	_
used	_	_
to	_	_
predict	_	_
each	_	_
target	_	_
MTU	_	_
.	_	_

#143
The	_	_
decomposition	_	_
structures	_	_
we	_	_
consider	_	_
are	_	_
limited	_	_
to	_	_
acyclic	_	_
graphs	_	_
where	_	_
each	_	_
node	_	_
can	deontic	_
have	_	_
one	_	_
of	_	_
the	_	_
following	_	_
parent	_	_
configurations	_	_
:	_	_
no	_	_
parents	_	_
(	_	_
C	_	_
=	_	_
0	_	_
in	_	_
the	_	_
Figure	_	_
)	_	_
,	_	_
one	_	_
left	_	_
parent	_	_
(	_	_
C	_	_
=	_	_
1L	_	_
)	_	_
,	_	_
one	_	_
right	_	_
parent	_	_
(	_	_
C	_	_
=	_	_
1R	_	_
)	_	_
,	_	_
one	_	_
left	_	_
and	_	_
one	_	_
right	_	_
parent	_	_
(	_	_
C	_	_
=	_	_
LR	_	_
)	_	_
,	_	_
two	_	_
left	_	_
parents	_	_
(	_	_
C	_	_
=	_	_
2L	_	_
)	_	_
,	_	_
and	_	_
two	_	_
right	_	_
parents	_	_
(	_	_
C	_	_
=	_	_
2R	_	_
)	_	_
.	_	_

#144
If	_	_
all	_	_
nodes	_	_
have	_	_
two	_	_
left	_	_
parents	_	_
,	_	_
we	_	_
recover	_	_
the	_	_
left-to-right	_	_
decomposition	_	_
order	_	_
,	_	_
and	_	_
if	_	_
all	_	_
nodes	_	_
have	_	_
two	_	_
right	_	_
parents	_	_
,	_	_
the	_	_
right-to-left	_	_
decomposition	_	_
order	_	_
.	_	_

#157
The	_	_
final	_	_
score	_	_
of	_	_
the	_	_
second	_	_
decomposition	_	_
and	_	_
assignment	_	_
in	_	_
Figure	_	_
3	_	_
is	_	_
:	_	_
Score	_	_
(	_	_
h	_	_
)	_	_
=	_	_
2	_	_
∗	_	_
wC0	_	_
+	_	_
wCLR	_	_
+	_	_
wC1R	_	_
+	_	_
wL2RlogPLR	_	_
(	_	_
m1	_	_
)	_	_
+	_	_
wCyclogPCyc	_	_
(	_	_
m2|m1	_	_
,	_	_
m3	_	_
)	_	_
+	_	_
wR2LlogPRL	_	_
(	_	_
m3|m4	_	_
)	_	_
+	_	_
wL2RlogPLR	_	_
(	_	_
m4	_	_
)	_	_
There	_	_
are	_	_
two	_	_
main	_	_
differences	_	_
between	_	_
our	_	_
approach	_	_
and	_	_
that	_	_
of	_	_
Tsuruoka	_	_
and	_	_
Tsujii	_	_
(	_	_
2005	_	_
)	_	_
:	_	_
we	_	_
perform	_	_
beam	_	_
search	_	_
with	_	_
hypothesis	_	_
recombination	_	_
instead	_	_
of	_	_
exact	_	_
decoding	_	_
(	_	_
due	_	_
to	_	_
the	_	_
larger	_	_
size	_	_
of	_	_
the	_	_
hypothesis	_	_
set	_	_
)	_	_
,	_	_
and	_	_
we	_	_
use	_	_
parameters	_	_
to	_	_
be	_	_
able	_	_
to	_	_
globally	_	_
weight	_	_
the	_	_
probabilities	_	_
from	_	_
different	_	_
models	_	_
and	_	_
to	_	_
develop	_	_
preferences	_	_
for	_	_
using	_	_
certain	_	_
types	_	_
of	_	_
decompositions	_	_
.	_	_

#158
For	_	_
example	_	_
,	_	_
the	_	_
model	_	_
can	capability	_
learn	_	_
to	_	_
prefer	_	_
right-to-left	_	_
decompositions	_	_
for	_	_
one	_	_
language	_	_
pair	_	_
,	_	_
and	_	_
left-to-right	_	_
decompositions	_	_
for	_	_
another	_	_
.	_	_

#159
An	_	_
additional	_	_
difference	_	_
from	_	_
prior	_	_
work	_	_
is	_	_
the	_	_
definition	_	_
of	_	_
the	_	_
possible	_	_
decomposition	_	_
orders	_	_
that	_	_
are	_	_
searched	_	_
over	_	_
.	_	_

#161
We	_	_
train	_	_
and	_	_
use	_	_
only	_	_
three	_	_
n-gram	_	_
Markov	_	_
models	_	_
to	_	_
assign	_	_
probabilities	_	_
:	_	_
L2RT	_	_
,	_	_
R2LT	_	_
,	_	_
and	_	_
CycT	_	_
,	_	_
whereas	_	_
the	_	_
prior	_	_
work	_	_
used	_	_
sixteen	_	_
models	_	_
.	_	_

#162
One	_	_
could	speculation	_
potentially	_	_
see	_	_
additional	_	_
gains	_	_
from	_	_
considering	_	_
a	_	_
larger	_	_
space	_	_
of	_	_
structures	_	_
but	_	_
the	_	_
training	_	_
time	_	_
and	_	_
runtime	_	_
memory	_	_
requirements	_	_
might	speculation	_
become	_	_
prohibitive	_	_
for	_	_
the	_	_
machine	_	_
translation	_	_
task	_	_
.	_	_

#163
Because	_	_
of	_	_
the	_	_
maximization	_	_
over	_	_
decomposition	_	_
structures	_	_
,	_	_
the	_	_
score	_	_
of	_	_
a	_	_
translation	_	_
is	_	_
not	_	_
a	_	_
simple	_	_
linear	_	_
function	_	_
of	_	_
the	_	_
features	_	_
,	_	_
but	_	_
rather	_	_
a	_	_
maximum	_	_
over	_	_
linear	_	_
functions	_	_
.	_	_

#171
Thus	_	_
,	_	_
each	_	_
source	_	_
sentence	_	_
and	_	_
its	_	_
candidate	_	_
translation	_	_
form	_	_
a	_	_
word-aligned	_	_
parallel	_	_
sentence	_	_
pair	_	_
.	_	_

#172
We	_	_
can	feasibility	_
extract	_	_
MTU	_	_
sequences	_	_
from	_	_
this	_	_
sentence	_	_
pair	_	_
and	_	_
compute	_	_
its	_	_
probability	_	_
according	_	_
to	_	_
MTU	_	_
Markov	_	_
models	_	_
.	_	_

#173
These	_	_
MTU	_	_
MM	_	_
log-probabilities	_	_
are	_	_
appended	_	_
to	_	_
the	_	_
original	_	_
MT	_	_
features	_	_
and	_	_
used	_	_
to	_	_
rerank	_	_
the	_	_
1000-best	_	_
list	_	_
.	_	_

#177
6.1	_	_
Lexical	_	_
selection	_	_
experiments	_	_
The	_	_
data	_	_
used	_	_
for	_	_
the	_	_
lexical	_	_
selection	_	_
experiments	_	_
consists	_	_
of	_	_
the	_	_
training	_	_
portion	_	_
of	_	_
the	_	_
datasets	_	_
used	_	_
for	_	_
MT	_	_
.	_	_

#178
These	_	_
training	_	_
sets	_	_
are	_	_
split	_	_
into	_	_
three	_	_
sections	_	_
:	_	_
lex-train	_	_
,	_	_
for	_	_
training	_	_
MTU	_	_
Markov	_	_
models	_	_
and	_	_
extracting	_	_
possible	_	_
translations	_	_
for	_	_
each	_	_
source	_	_
3If	_	_
we	_	_
include	_	_
the	_	_
decompositions	_	_
in	_	_
the	_	_
hypotheses	_	_
we	_	_
could	feasibility-options	_
use	_	_
MERT	_	_
but	_	_
then	_	_
the	_	_
n-best	_	_
lists	_	_
used	_	_
for	_	_
training	_	_
might	speculation	negation
not	_	_
contain	_	_
much	_	_
variety	_	_
in	_	_
terms	_	_
of	_	_
translation	_	_
options	_	_
.	_	_

#179
This	_	_
is	_	_
an	_	_
interesting	_	_
direction	_	_
for	_	_
future	_	_
research	_	_
.	_	_

#187
The	_	_
oracle	_	_
looks	_	_
at	_	_
the	_	_
correct	_	_
translation	_	_
and	_	_
always	_	_
chooses	_	_
the	_	_
correct	_	_
target	_	_
MTU	_	_
if	_	_
it	_	_
is	_	_
in	_	_
the	_	_
vocabulary	_	_
of	_	_
available	_	_
MTUs	_	_
.	_	_

#188
We	_	_
can	rhetorical	_
see	_	_
that	_	_
there	_	_
is	_	_
a	_	_
large	_	_
difference	_	_
between	_	_
the	_	_
baseline	_	_
and	_	_
oracle	_	_
performance	_	_
,	_	_
underscoring	_	_
the	_	_
importance	_	_
of	_	_
modeling	_	_
context	_	_
for	_	_
accurate	_	_
prediction	_	_
.	_	_

#189
The	_	_
best	_	_
decomposition	_	_
order	_	_
varies	_	_
from	_	_
language	_	_
to	_	_
language	_	_
:	_	_
right-to-left	_	_
in	_	_
source	_	_
order	_	_
is	_	_
best	_	_
for	_	_
Chinese-English	_	_
,	_	_
right-to-left	_	_
in	_	_
target	_	_
order	_	_
is	_	_
best	_	_
for	_	_
German-English	_	_
and	_	_
left-to-right	_	_
or	_	_
righttoleft	_	_
in	_	_
target	_	_
order	_	_
are	_	_
best	_	_
in	_	_
English-Bulgarian	_	_
.	_	_

#199
Baseline-2	_	_
in	_	_
the	_	_
lower	_	_
part	_	_
of	_	_
the	_	_
table	_	_
is	_	_
the	_	_
best	_	_
individual	_	_
model	_	_
out	_	_
of	_	_
all	_	_
six	_	_
.	_	_

#200
We	_	_
can	rhetorical	_
see	_	_
that	_	_
the	_	_
product	_	_
models	_	_
TgtProduct	_	_
(	_	_
a	_	_
product	_	_
of	_	_
the	_	_
three	_	_
target-order	_	_
MTU	_	_
MMs	_	_
)	_	_
and	_	_
AllProduct	_	_
(	_	_
a	_	_
product	_	_
of	_	_
all	_	_
six	_	_
MTU	_	_
MMs	_	_
)	_	_
are	_	_
consistently	_	_
best	_	_
.	_	_

#201
The	_	_
dynamic	_	_
decomposition	_	_
models	_	_
TgtDynamic	_	_
achieve	_	_
slight	_	_
but	_	_
not	_	_
significant	_	_
gains	_	_
over	_	_
the	_	_
baseline	_	_
.	_	_

#231
The	_	_
last	_	_
row	_	_
of	_	_
the	_	_
table	_	_
reports	_	_
the	_	_
results	_	_
achieved	_	_
by	_	_
a	_	_
combination	_	_
of	_	_
MTU	_	_
MMs	_	_
that	_	_
do	_	_
not	_	_
use	_	_
context	_	_
across	_	_
the	_	_
phrasal	_	_
boundaries	_	_
.	_	_

#232
Since	_	_
an	_	_
MTU	_	_
MM	_	_
limited	_	_
to	_	_
look	_	_
only	_	_
inside	_	_
phrases	_	_
can	capability-options	negation
provide	_	_
improved	_	_
smoothing	_	_
compared	_	_
to	_	_
whole	_	_
phrase	_	_
relative	_	_
frequency	_	_
counts	_	_
,	_	_
it	_	_
is	_	_
conceivable	_	_
it	_	_
could	capability-speculation	_
provide	_	_
a	_	_
large	_	_
improvement	_	_
.	_	_

#233
However	_	_
,	_	_
there	_	_
is	_	_
no	_	_
improvement	_	_
in	_	_
practice	_	_
for	_	_
this	_	_
language	_	_
pair	_	_
;	_	_
the	_	_
additional	_	_
improvements	_	_
from	_	_
MTU	_	_
MMs	_	_
stem	_	_
from	_	_
modeling	_	_
cross-phrase	_	_
context	_	_
.	_	_