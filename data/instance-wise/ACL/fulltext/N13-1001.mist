#31
A	_	_
higher	_	_
beam	_	_
is	_	_
required	_	_
to	_	_
prevent	_	_
it	_	_
from	_	_
getting	_	_
pruned	_	_
.	_	_

#32
Phrase-based	_	_
systems	_	_
,	_	_
on	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
are	_	_
likely	_	_
to	_	_
have	_	_
access	_	_
to	_	_
the	_	_
phrasal	_	_
unit	_	_
“schoß	_	_
ein	_	_
Tor	_	_
–	_	_
scored	_	_
a	_	_
goal”	_	_
and	_	_
can	capability	_
generate	_	_
it	_	_
in	_	_
a	_	_
single	_	_
step	_	_
.	_	_

#33
Moreover	_	_
,	_	_
a	_	_
more	_	_
accurate	_	_
future-cost	_	_
estimate	_	_
can	feasibility	_
be	_	_
computed	_	_
because	_	_
of	_	_
the	_	_
available	_	_
context	_	_
internal	_	_
to	_	_
the	_	_
phrase	_	_
.	_	_

#34
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
extend	_	_
the	_	_
N-gram	_	_
model	_	_
,	_	_
based	_	_
on	_	_
operation	_	_
sequences	_	_
(	_	_
Durrani	_	_
et	_	_
al.	_	_
,	_	_
2011	_	_
)	_	_
,	_	_
to	_	_
use	_	_
phrases	_	_
during	_	_
decoding	_	_
.	_	_

#40
Section	_	_
4	_	_
analyzes	_	_
the	_	_
search	_	_
problem	_	_
when	_	_
decoding	_	_
with	_	_
Figure	_	_
1	_	_
:	_	_
Different	_	_
Segmentations	_	_
of	_	_
a	_	_
Bilingual	_	_
Sentence	_	_
Pair	_	_
minimal	_	_
units	_	_
.	_	_

#41
Section	_	_
5	_	_
discusses	_	_
how	_	_
information	_	_
available	_	_
in	_	_
phrases	_	_
can	feasibility	_
be	_	_
used	_	_
to	_	_
improve	_	_
search	_	_
performance	_	_
.	_	_

#42
Section	_	_
6	_	_
presents	_	_
the	_	_
results	_	_
of	_	_
this	_	_
work	_	_
.	_	_

#50
This	_	_
problem	_	_
is	_	_
corrected	_	_
by	_	_
the	_	_
monolingual	_	_
language	_	_
model	_	_
that	_	_
takes	_	_
context	_	_
into	_	_
account	_	_
.	_	_

#51
But	_	_
often	_	_
the	_	_
language	_	_
model	_	_
can	capability	negation
not	_	_
compensate	_	_
for	_	_
the	_	_
dispreference	_	_
of	_	_
the	_	_
translation	_	_
model	_	_
for	_	_
non-	_	_
local	_	_
dependencies	_	_
.	_	_

#52
The	_	_
second	_	_
problem	_	_
is	_	_
that	_	_
the	_	_
model	_	_
is	_	_
unaware	_	_
of	_	_
the	_	_
actual	_	_
phrasal	_	_
segmentation	_	_
of	_	_
a	_	_
sentence	_	_
during	_	_
training	_	_
.	_	_

#62
The	_	_
second	_	_
segmentation	_	_
allows	_	_
the	_	_
model	_	_
to	_	_
capture	_	_
the	_	_
reordering	_	_
of	_	_
the	_	_
complex	_	_
verb	_	_
predicate	_	_
“würden	_	_
–	_	_
would”	_	_
and	_	_
“abstimmen	_	_
–	_	_
vote”	_	_
by	_	_
learning	_	_
that	_	_
the	_	_
verb	_	_
“abstimmen	_	_
–	_	_
vote”	_	_
is	_	_
discontinuous	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
auxiliary	_	_
.	_	_

#63
This	_	_
information	_	_
can	feasibility	negation
not	_	_
be	_	_
captured	_	_
in	_	_
the	_	_
first	_	_
segmentation	_	_
because	_	_
of	_	_
the	_	_
phrasal	_	_
independence	_	_
assumption	_	_
and	_	_
stiff	_	_
phrasal	_	_
boundaries	_	_
.	_	_

#64
The	_	_
model	_	_
loses	_	_
one	_	_
of	_	_
the	_	_
dependencies	_	_
depending	_	_
upon	_	_
which	_	_
segmentation	_	_
it	_	_
chooses	_	_
during	_	_
decoding	_	_
.	_	_

#65
N-gram-based	_	_
SMT	_	_
is	_	_
an	_	_
instance	_	_
of	_	_
a	_	_
joint	_	_
model	_	_
that	_	_
generates	_	_
source	_	_
and	_	_
target	_	_
strings	_	_
together	_	_
in	_	_
bilingual	_	_
translation	_	_
units	_	_
called	_	_
tuples	_	_
.	_	_

#66
Tuples	_	_
are	_	_
essentially	_	_
phrases	_	_
but	_	_
they	_	_
are	_	_
atomic	_	_
units	_	_
that	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
decomposed	_	_
any	_	_
further	_	_
.	_	_

#67
This	_	_
condition	_	_
of	_	_
atomicity	_	_
results	_	_
in	_	_
a	_	_
unique	_	_
segmentation	_	_
of	_	_
the	_	_
bilingual	_	_
sentence	_	_
pair	_	_
given	_	_
its	_	_
alignments	_	_
.	_	_

#85
Representing	_	_
bilingual	_	_
sentences	_	_
as	_	_
a	_	_
sequence	_	_
of	_	_
operations	_	_
enables	_	_
them	_	_
to	_	_
memorize	_	_
phrases	_	_
and	_	_
lexical	_	_
re-	_	_
ordering	_	_
triggers	_	_
like	_	_
PBSMT	_	_
.	_	_

#86
However	_	_
,	_	_
using	_	_
minimal	_	_
units	_	_
during	_	_
decoding	_	_
and	_	_
searching	_	_
over	_	_
all	_	_
possible	_	_
reorderings	_	_
means	_	_
that	_	_
hypotheses	_	_
can	feasibility	negation
no	_	_
longer	_	_
be	_	_
arranged	_	_
in	_	_
2m	_	_
stacks	_	_
.	_	_

#87
The	_	_
problem	_	_
of	_	_
inaccurate	_	_
future-cost	_	_
estimates	_	_
resurfaces	_	_
resulting	_	_
in	_	_
more	_	_
search	_	_
errors	_	_
.	_	_

#93
3	_	_
Operation	_	_
Sequence	_	_
Model	_	_
The	_	_
N-gram	_	_
model	_	_
with	_	_
integrated	_	_
reordering	_	_
models	_	_
a	_	_
sequence	_	_
of	_	_
operations	_	_
obtained	_	_
through	_	_
the	_	_
transformation	_	_
of	_	_
a	_	_
bilingual	_	_
sentence	_	_
pair	_	_
.	_	_

#94
An	_	_
operation	_	_
can	deontic-options	_
either	_	_
be	_	_
to	_	_
i	_	_
)	_	_
generate	_	_
a	_	_
sequence	_	_
of	_	_
source	_	_
and	_	_
target	_	_
words	_	_
,	_	_
ii	_	_
)	_	_
to	_	_
insert	_	_
a	_	_
gap	_	_
as	_	_
a	_	_
placeholder	_	_
for	_	_
skipped	_	_
words	_	_
,	_	_
iii	_	_
)	_	_
or	_	_
to	_	_
jump	_	_
forward	_	_
and	_	_
backward	_	_
in	_	_
a	_	_
sentence	_	_
to	_	_
translate	_	_
words	_	_
discontinuously	_	_
.	_	_

#95
The	_	_
translate	_	_
operation	_	_
Generate	_	_
(	_	_
X	_	_
,	_	_
Y	_	_
)	_	_
encapsulates	_	_
the	_	_
translation	_	_
tuple	_	_
(	_	_
X	_	_
,	_	_
Y	_	_
)	_	_
.	_	_

#109
The	_	_
ultimate	_	_
goal	_	_
is	_	_
to	_	_
find	_	_
the	_	_
best	_	_
scoring	_	_
hypothesis	_	_
,	_	_
that	_	_
has	_	_
translated	_	_
all	_	_
the	_	_
words	_	_
in	_	_
the	_	_
foreign	_	_
sentence	_	_
.	_	_

#110
The	_	_
overall	_	_
process	_	_
can	feasibility	_
be	_	_
roughly	_	_
divided	_	_
into	_	_
the	_	_
following	_	_
steps	_	_
:	_	_
i	_	_
)	_	_
extraction	_	_
of	_	_
translation	_	_
units	_	_
ii	_	_
)	_	_
future-cost	_	_
estimation	_	_
,	_	_
iii	_	_
)	_	_
hypothesis	_	_
extension	_	_
iv	_	_
)	_	_
recombination	_	_
and	_	_
pruning	_	_
.	_	_

#111
During	_	_
the	_	_
hypothesis	_	_
extension	_	_
each	_	_
extracted	_	_
phrase	_	_
is	_	_
translated	_	_
into	_	_
a	_	_
sequence	_	_
of	_	_
operations	_	_
.	_	_

#117
4.2	_	_
Drawbacks	_	_
of	_	_
Cept-based	_	_
Decoding	_	_
One	_	_
of	_	_
the	_	_
main	_	_
drawbacks	_	_
of	_	_
the	_	_
operation	_	_
sequence	_	_
model	_	_
is	_	_
that	_	_
it	_	_
has	_	_
a	_	_
more	_	_
difficult	_	_
search	_	_
problem	_	_
than	_	_
the	_	_
phrase-based	_	_
model	_	_
.	_	_

#118
The	_	_
operation	_	_
model	_	_
,	_	_
although	_	_
based	_	_
on	_	_
minimal	_	_
translation	_	_
units	_	_
,	_	_
can	capability	_
learn	_	_
larger	_	_
translation	_	_
chunks	_	_
by	_	_
memorizing	_	_
a	_	_
sequence	_	_
of	_	_
operations	_	_
.	_	_

#119
However	_	_
,	_	_
using	_	_
cepts	_	_
during	_	_
decoding	_	_
has	_	_
the	_	_
following	_	_
drawbacks	_	_
:	_	_
i	_	_
)	_	_
the	_	_
cept-based	_	_
decoder	_	_
does	_	_
not	_	_
have	_	_
access	_	_
to	_	_
all	_	_
the	_	_
translation	_	_
units	_	_
that	_	_
a	_	_
phrase-based	_	_
decoder	_	_
uses	_	_
as	_	_
part	_	_
of	_	_
a	_	_
larger	_	_
phrase	_	_
.	_	_

#128
Typically	_	_
only	_	_
the	_	_
20	_	_
best	_	_
translation	_	_
options	_	_
are	_	_
used	_	_
,	_	_
to	_	_
reduce	_	_
the	_	_
decoding	_	_
time	_	_
,	_	_
and	_	_
such	_	_
phrasal	_	_
units	_	_
with	_	_
less	_	_
frequent	_	_
cept	_	_
translations	_	_
are	_	_
never	_	_
hypothesized	_	_
in	_	_
the	_	_
N-gram-based	_	_
systems	_	_
.	_	_

#129
The	_	_
phrase-based	_	_
system	_	_
on	_	_
the	_	_
other	_	_
hand	_	_
can	capability	_
extract	_	_
the	_	_
phrase	_	_
“Wie	_	_
heißen	_	_
Sie	_	_
–	_	_
what	_	_
is	_	_
your	_	_
name”	_	_
even	_	_
if	_	_
it	_	_
is	_	_
observed	_	_
only	_	_
once	_	_
during	_	_
training	_	_
.	_	_

#130
A	_	_
similar	_	_
problem	_	_
is	_	_
also	_	_
reported	_	_
in	_	_
Costa-jussà	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2007	_	_
)	_	_
.	_	_

#133
Larger	_	_
Beam	_	_
Size	_	_
:	_	_
A	_	_
related	_	_
problem	_	_
is	_	_
that	_	_
a	_	_
higher	_	_
beam	_	_
size	_	_
is	_	_
required	_	_
in	_	_
cept-based	_	_
decoding	_	_
to	_	_
prevent	_	_
uncommon	_	_
translations	_	_
from	_	_
getting	_	_
pruned	_	_
.	_	_

#134
The	_	_
phrase-based	_	_
system	_	_
can	capability	_
generate	_	_
the	_	_
phrase-pair	_	_
“Wie	_	_
heißen	_	_
Sie	_	_
–	_	_
what	_	_
is	_	_
your	_	_
name”	_	_
in	_	_
a	_	_
single	_	_
step	_	_
placing	_	_
it	_	_
directly	_	_
into	_	_
the	_	_
stack	_	_
three	_	_
words	_	_
to	_	_
the	_	_
right	_	_
.	_	_

#135
The	_	_
cept-based	_	_
decoder	_	_
generates	_	_
this	_	_
phrase	_	_
in	_	_
three	_	_
stacks	_	_
with	_	_
the	_	_
tuple	_	_
translations	_	_
“Wie	_	_
–	_	_
What	_	_
is”	_	_
,	_	_
“Sie	_	_
–	_	_
your”	_	_
and	_	_
“heißen	_	_
–	_	_
name”	_	_
.	_	_

#143
The	_	_
future-cost	_	_
estimate	_	_
for	_	_
the	_	_
phrase	_	_
pair	_	_
“Wie	_	_
heißen	_	_
Sie	_	_
–	_	_
What	_	_
is	_	_
your	_	_
name”	_	_
is	_	_
estimated	_	_
by	_	_
calculating	_	_
the	_	_
cost	_	_
of	_	_
each	_	_
feature	_	_
.	_	_

#144
The	_	_
language	_	_
model	_	_
cost	_	_
,	_	_
for	_	_
example	_	_
,	_	_
is	_	_
estimated	_	_
in	_	_
the	_	_
phrase-based	_	_
system	_	_
as	_	_
follows	_	_
:	_	_
plm	_	_
=	_	_
p	_	_
(	_	_
What	_	_
)	_	_
×	_	_
p	_	_
(	_	_
is|What	_	_
)	_	_
×	_	_
p	_	_
(	_	_
your|What	_	_
is	_	_
)	_	_
×	_	_
p	_	_
(	_	_
name|What	_	_
is	_	_
your	_	_
)	_	_
The	_	_
cost	_	_
of	_	_
the	_	_
direct	_	_
phrase	_	_
translation	_	_
probability	_	_
,	_	_
one	_	_
of	_	_
the	_	_
features	_	_
used	_	_
in	_	_
the	_	_
phrase	_	_
translation	_	_
model	_	_
,	_	_
is	_	_
estimated	_	_
as	_	_
:	_	_
ptm	_	_
=	_	_
p	_	_
(	_	_
What	_	_
is	_	_
your	_	_
name|Wie	_	_
heißen	_	_
Sie	_	_
)	_	_
Phrase-based	_	_
SMT	_	_
is	_	_
aware	_	_
during	_	_
the	_	_
preprocessing	_	_
step	_	_
that	_	_
the	_	_
words	_	_
“Wie	_	_
heißen	_	_
Sie”	_	_
may	capability-options	_
be	_	_
translated	_	_
as	_	_
a	_	_
phrase	_	_
.	_	_

#145
This	_	_
is	_	_
helpful	_	_
for	_	_
estimating	_	_
a	_	_
more	_	_
accurate	_	_
future	_	_
cost	_	_
because	_	_
the	_	_
phrase-	_	_
internal	_	_
context	_	_
is	_	_
already	_	_
available	_	_
.	_	_

#146
The	_	_
same	_	_
is	_	_
not	_	_
true	_	_
for	_	_
the	_	_
operation	_	_
sequence	_	_
model	_	_
,	_	_
to	_	_
which	_	_
only	_	_
minimal	_	_
units	_	_
are	_	_
available	_	_
.	_	_

#147
The	_	_
operation	_	_
model	_	_
does	_	_
not	_	_
have	_	_
the	_	_
information	_	_
that	_	_
“Wie	_	_
heißen	_	_
Sie”	_	_
may	capability-options	_
be	_	_
translated	_	_
as	_	_
a	_	_
phrase	_	_
during	_	_
decoding	_	_
.	_	_

#148
The	_	_
future-cost	_	_
estimate	_	_
available	_	_
to	_	_
the	_	_
operation	_	_
model	_	_
for	_	_
the	_	_
span	_	_
covering	_	_
“Wie	_	_
heißen	_	_
Sie”	_	_
will	_	_
have	_	_
unigram	_	_
probabilities	_	_
for	_	_
both	_	_
the	_	_
translation	_	_
and	_	_
language	_	_
model	_	_
:	_	_
plm	_	_
=	_	_
p	_	_
(	_	_
What	_	_
)	_	_
×	_	_
p	_	_
(	_	_
is|What	_	_
)	_	_
×	_	_
p	_	_
(	_	_
your	_	_
)	_	_
×	_	_
p	_	_
(	_	_
name	_	_
)	_	_
ptm	_	_
=	_	_
p	_	_
(	_	_
Generate	_	_
(	_	_
Wie	_	_
,	_	_
What	_	_
is	_	_
)	_	_
)	_	_
×	_	_
p	_	_
(	_	_
Generate	_	_
(	_	_
heißen	_	_
,	_	_
name	_	_
)	_	_
)	_	_
×	_	_
p	_	_
(	_	_
Generate	_	_
(	_	_
Sie	_	_
,	_	_
your	_	_
)	_	_
)	_	_
Thus	_	_
the	_	_
future-cost	_	_
estimate	_	_
in	_	_
the	_	_
operation	_	_
model	_	_
is	_	_
much	_	_
worse	_	_
than	_	_
that	_	_
of	_	_
the	_	_
phrase-based	_	_
model	_	_
.	_	_

#152
5	_	_
N-gram	_	_
Model	_	_
with	_	_
Phrase-based	_	_
Decoding	_	_
In	_	_
the	_	_
last	_	_
section	_	_
we	_	_
discussed	_	_
the	_	_
disadvantages	_	_
of	_	_
using	_	_
cepts	_	_
during	_	_
search	_	_
in	_	_
a	_	_
left-to-right	_	_
decoding	_	_
framework	_	_
.	_	_

#153
We	_	_
now	_	_
define	_	_
a	_	_
method	_	_
to	_	_
empirically	_	_
study	_	_
the	_	_
mentioned	_	_
drawbacks	_	_
and	_	_
whether	_	_
using	_	_
information	_	_
available	_	_
in	_	_
phrase-pairs	_	_
during	_	_
decoding	_	_
can	capability-feasibility	_
help	_	_
improve	_	_
search	_	_
accuracy	_	_
and	_	_
translation	_	_
quality	_	_
.	_	_

#154
5.1	_	_
Training	_	_
We	_	_
extended	_	_
the	_	_
training	_	_
steps	_	_
in	_	_
Durrani	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2011	_	_
)	_	_
to	_	_
extract	_	_
a	_	_
phrase	_	_
lexicon	_	_
from	_	_
the	_	_
parallel	_	_
data	_	_
.	_	_

#160
are	_	_
also	_	_
estimated	_	_
.	_	_

#161
The	_	_
estimates	_	_
of	_	_
the	_	_
count-	_	_
based	_	_
reordering	_	_
penalties	_	_
(	_	_
gap	_	_
penalty	_	_
and	_	_
open	_	_
gap	_	_
penalty	_	_
)	_	_
and	_	_
the	_	_
distance-based	_	_
features	_	_
(	_	_
gapwidth	_	_
and	_	_
reordering	_	_
distance	_	_
)	_	_
could	feasibility	negation
not	_	_
be	_	_
estimated	_	_
previously	_	_
with	_	_
cepts	_	_
but	_	_
are	_	_
available	_	_
when	_	_
using	_	_
phrases	_	_
.	_	_

#162
5.2	_	_
Decoding	_	_
We	_	_
extended	_	_
the	_	_
decoder	_	_
developed	_	_
by	_	_
Durrani	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2011	_	_
)	_	_
and	_	_
tried	_	_
three	_	_
ideas	_	_
.	_	_

#167
We	_	_
therefore	_	_
do	_	_
not	_	_
create	_	_
any	_	_
alternative	_	_
compositions	_	_
of	_	_
the	_	_
same	_	_
phrasal	_	_
unit	_	_
during	_	_
decoding	_	_
.	_	_

#168
This	_	_
option	_	_
is	_	_
not	_	_
available	_	_
in	_	_
phrase-based	_	_
decoding	_	_
,	_	_
because	_	_
an	_	_
alternative	_	_
composition	_	_
may	speculation	_
lead	_	_
towards	_	_
a	_	_
better	_	_
model	_	_
score	_	_
.	_	_

#169
In	_	_
our	_	_
secondary	_	_
set	_	_
of	_	_
experiments	_	_
,	_	_
we	_	_
used	_	_
cept-based	_	_
decoding	_	_
but	_	_
modified	_	_
the	_	_
decoder	_	_
to	_	_
use	_	_
information	_	_
available	_	_
from	_	_
the	_	_
phrases	_	_
extracted	_	_
for	_	_
the	_	_
test	_	_
sentences	_	_
.	_	_

#170
Firstly	_	_
,	_	_
we	_	_
used	_	_
future-cost	_	_
estimates	_	_
from	_	_
the	_	_
extracted	_	_
phrases	_	_
(	_	_
see	_	_
system	_	_
cept.500.fc	_	_
in	_	_
Table1	_	_
)	_	_
.	_	_

#171
This	_	_
however	_	_
,	_	_
leads	_	_
to	_	_
inconsistency	_	_
in	_	_
the	_	_
cases	_	_
where	_	_
the	_	_
future	_	_
cost	_	_
is	_	_
estimated	_	_
from	_	_
some	_	_
phrasal	_	_
unit	_	_
that	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
generated	_	_
through	_	_
the	_	_
available	_	_
cept	_	_
translations	_	_
.	_	_

#172
For	_	_
example	_	_
,	_	_
say	_	_
the	_	_
best	_	_
cost	_	_
to	_	_
cover	_	_
the	_	_
sequence	_	_
“Wie	_	_
heißen	_	_
Sie”	_	_
is	_	_
given	_	_
by	_	_
the	_	_
phrase	_	_
“What	_	_
is	_	_
your	_	_
name”	_	_
.	_	_

#185
For	_	_
each	_	_
input	_	_
sentence	_	_
we	_	_
select	_	_
a	_	_
single	_	_
best	_	_
scoring	_	_
hypothesis	_	_
.	_	_

#186
The	_	_
best	_	_
scoring	_	_
hypothesis	_	_
can	capability-feasibility	_
be	_	_
contributed	_	_
from	_	_
several	_	_
runs	_	_
.	_	_

#187
In	_	_
this	_	_
case	_	_
all	_	_
these	_	_
runs	_	_
will	_	_
be	_	_
given	_	_
a	_	_
credit	_	_
for	_	_
that	_	_
particular	_	_
sentence	_	_
when	_	_
computing	_	_
the	_	_
search	_	_
accuracy	_	_
.	_	_

#202
Using	_	_
phrases	_	_
in	_	_
search	_	_
reduces	_	_
the	_	_
decoding	_	_
speed	_	_
.	_	_

#203
In	_	_
order	_	_
to	_	_
make	_	_
a	_	_
fair	_	_
comparison	_	_
,	_	_
both	_	_
the	_	_
phrase-based	_	_
and	_	_
the	_	_
baseline	_	_
cept-based	_	_
decoders	_	_
should	deontic	_
be	_	_
allowed	_	_
to	_	_
run	_	_
for	_	_
the	_	_
same	_	_
amount	_	_
of	_	_
time	_	_
.	_	_

#204
We	_	_
therefore	_	_
reduced	_	_
the	_	_
stack	_	_
size	_	_
in	_	_
the	_	_
phrase-based	_	_
decoder	_	_
so	_	_
that	_	_
it	_	_
runs	_	_
in	_	_
the	_	_
same	_	_
amount	_	_
of	_	_
time	_	_
as	_	_
the	_	_
cept-based	_	_
decoder	_	_
.	_	_

#230
We	_	_
found	_	_
future	_	_
cost	_	_
to	_	_
be	_	_
more	_	_
critical	_	_
in	_	_
G-E	_	_
than	_	_
F-E	_	_
experiments	_	_
.	_	_

#231
This	_	_
can	feasibility	_
be	_	_
explained	_	_
by	_	_
the	_	_
fact	_	_
that	_	_
more	_	_
reordering	_	_
is	_	_
required	_	_
in	_	_
G-E	_	_
and	_	_
it	_	_
is	_	_
necessary	_	_
to	_	_
account	_	_
for	_	_
the	_	_
reordering	_	_
operations	_	_
and	_	_
jump-based	_	_
features	_	_
(	_	_
gapbased	_	_
penalties	_	_
,	_	_
reordering	_	_
distance	_	_
and	_	_
gap-width	_	_
)	_	_
in	_	_
the	_	_
future-cost	_	_
estimation	_	_
.	_	_

#232
F-E	_	_
on	_	_
the	_	_
other	_	_
hand	_	_
is	_	_
largely	_	_
monotonic	_	_
except	_	_
for	_	_
a	_	_
few	_	_
short	_	_
distance	_	_
reorderings	_	_
such	_	_
as	_	_
flipping	_	_
noun	_	_
and	_	_
adjective	_	_
.	_	_

#261
However	_	_
,	_	_
the	_	_
drawback	_	_
of	_	_
the	_	_
phrase-based	_	_
model	_	_
is	_	_
the	_	_
phrasal	_	_
independence	_	_
assumption	_	_
,	_	_
spurious	_	_
ambiguity	_	_
in	_	_
segmentation	_	_
and	_	_
a	_	_
weak	_	_
mechanism	_	_
to	_	_
handle	_	_
non-local	_	_
reorderings	_	_
.	_	_

#262
We	_	_
showed	_	_
that	_	_
combining	_	_
a	_	_
model	_	_
based	_	_
on	_	_
minimal	_	_
units	_	_
with	_	_
phrase-	_	_
based	_	_
decoding	_	_
can	capability-options	_
improve	_	_
both	_	_
search	_	_
accuracy	_	_
and	_	_
translation	_	_
quality	_	_
.	_	_

#263
We	_	_
also	_	_
showed	_	_
that	_	_
the	_	_
phrasal	_	_
information	_	_
can	feasibility	_
be	_	_
indirectly	_	_
used	_	_
in	_	_
cept-	_	_
based	_	_
decoding	_	_
with	_	_
improved	_	_
results	_	_
.	_	_

#264
We	_	_
tested	_	_
our	_	_
system	_	_
against	_	_
the	_	_
state-of-the-art	_	_
phrase-based	_	_
and	_	_
N-gram-based	_	_
systems	_	_
,	_	_
for	_	_
German-to-English	_	_
,	_	_
French-to-English	_	_
,	_	_
and	_	_
Spanish-to-English	_	_
for	_	_
three	_	_
standard	_	_
test	_	_
sets	_	_
.	_	_

#266
We	_	_
have	_	_
shown	_	_
the	_	_
benefits	_	_
of	_	_
using	_	_
phrase-based	_	_
search	_	_
with	_	_
a	_	_
model	_	_
based	_	_
on	_	_
minimal	_	_
units	_	_
.	_	_

#267
In	_	_
future	_	_
work	_	_
,	_	_
we	_	_
would	_	_
like	_	_
to	_	_
study	_	_
whether	_	_
a	_	_
phrase-based	_	_
system	_	_
like	_	_
Moses	_	_
or	_	_
Phrasal	_	_
can	capability	_
profit	_	_
from	_	_
an	_	_
OSM-style	_	_
or	_	_
Ngramstyle	_	_
feature	_	_
.	_	_

#268
Feng	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2010	_	_
)	_	_
previously	_	_
showed	_	_
that	_	_
adding	_	_
a	_	_
linearized	_	_
source-side	_	_
language	_	_
model	_	_
in	_	_
a	_	_
phrase-based	_	_
system	_	_
helped	_	_
.	_	_

#269
It	_	_
would	_	_
also	_	_
be	_	_
interesting	_	_
to	_	_
study	_	_
whether	_	_
the	_	_
insight	_	_
of	_	_
using	_	_
minimal	_	_
units	_	_
for	_	_
modeling	_	_
and	_	_
phrase-based	_	_
search	_	_
would	_	_
hold	_	_
for	_	_
hierarchical	_	_
SMT	_	_
.	_	_

#270
Vaswani	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2011	_	_
)	_	_
recently	_	_
showed	_	_
that	_	_
a	_	_
Markov	_	_
model	_	_
over	_	_
the	_	_
derivation	_	_
history	_	_
of	_	_
minimal	_	_
rules	_	_
can	capability	_
obtain	_	_
the	_	_
same	_	_
translation	_	_
quality	_	_
as	_	_
using	_	_
grammars	_	_
formed	_	_
with	_	_
composed	_	_
rules	_	_
.	_	_

#271
Acknowledgments	_	_
We	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
the	_	_
anonymous	_	_
reviewers	_	_
for	_	_
their	_	_
helpful	_	_
feedback	_	_
and	_	_
suggestions	_	_
.	_	_