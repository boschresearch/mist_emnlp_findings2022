#16
The	_	_
task	_	_
is	_	_
challenging	_	_
because	_	_
the	_	_
same	_	_
keywords	_	_
often	_	_
refer	_	_
to	_	_
different	_	_
entities	_	_
,	_	_
and	_	_
interesting	_	_
supplementary	_	_
information	_	_
to	_	_
the	_	_
highlighted	_	_
entity	_	_
is	_	_
highly	_	_
sensitive	_	_
to	_	_
the	_	_
semantic	_	_
context	_	_
.	_	_

#17
For	_	_
example	_	_
,	_	_
“Paul	_	_
Simon”	_	_
can	options	_
refer	_	_
to	_	_
many	_	_
people	_	_
,	_	_
such	_	_
as	_	_
the	_	_
singer	_	_
and	_	_
the	_	_
senator	_	_
.	_	_

#18
Consider	_	_
an	_	_
article	_	_
about	_	_
the	_	_
music	_	_
of	_	_
Paul	_	_
Simon	_	_
and	_	_
another	_	_
about	_	_
his	_	_
life	_	_
.	_	_

#31
It	_	_
is	_	_
often	_	_
the	_	_
case	_	_
that	_	_
a	_	_
user	_	_
is	_	_
interested	_	_
in	_	_
a	_	_
document	_	_
because	_	_
it	_	_
provides	_	_
supplementary	_	_
information	_	_
about	_	_
the	_	_
entities	_	_
or	_	_
concepts	_	_
she	_	_
encounters	_	_
when	_	_
reading	_	_
another	_	_
document	_	_
although	_	_
the	_	_
overall	_	_
contents	_	_
of	_	_
the	_	_
second	_	_
documents	_	_
is	_	_
not	_	_
highly	_	_
relevant	_	_
.	_	_

#32
For	_	_
example	_	_
,	_	_
a	_	_
user	_	_
may	speculation	_
be	_	_
interested	_	_
in	_	_
knowing	_	_
more	_	_
about	_	_
the	_	_
history	_	_
of	_	_
University	_	_
of	_	_
Washington	_	_
after	_	_
reading	_	_
the	_	_
news	_	_
about	_	_
President	_	_
Obama’s	_	_
visit	_	_
to	_	_
Seattle	_	_
.	_	_

#33
To	_	_
better	_	_
model	_	_
interestingness	_	_
,	_	_
we	_	_
extend	_	_
the	_	_
model	_	_
of	_	_
Huang	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2013	_	_
)	_	_
in	_	_
two	_	_
significant	_	_
aspects	_	_
.	_	_

#34
First	_	_
,	_	_
while	_	_
Huang	_	_
et	_	_
al	_	_
.	_	_
treat	_	_
a	_	_
document	_	_
as	_	_
a	_	_
bag	_	_
of	_	_
words	_	_
for	_	_
semantic	_	_
mapping	_	_
,	_	_
the	_	_
DSSM	_	_
treats	_	_
a	_	_
document	_	_
as	_	_
a	_	_
sequence	_	_
of	_	_
words	_	_
and	_	_
tries	_	_
to	_	_
discover	_	_
prominent	_	_
keywords	_	_
.	_	_

#35
These	_	_
keywords	_	_
represent	_	_
the	_	_
entities	_	_
or	_	_
concepts	_	_
that	_	_
might	speculation	_
interest	_	_
users	_	_
,	_	_
via	_	_
the	_	_
convolutional	_	_
and	_	_
max-pooling	_	_
layers	_	_
which	_	_
are	_	_
related	_	_
to	_	_
the	_	_
deep	_	_
models	_	_
used	_	_
for	_	_
computer	_	_
vision	_	_
(	_	_
Krizhevsky	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
)	_	_
and	_	_
speech	_	_
recognition	_	_
(	_	_
Deng	_	_
et	_	_
al.	_	_
,	_	_
2013a	_	_
)	_	_
but	_	_
are	_	_
not	_	_
used	_	_
in	_	_
Huang	_	_
et	_	_
al.’s	_	_
model	_	_
.	_	_

#36
The	_	_
DSSM	_	_
then	_	_
forms	_	_
the	_	_
high-level	_	_
semantic	_	_
representation	_	_
of	_	_
the	_	_
whole	_	_
document	_	_
based	_	_
on	_	_
these	_	_
keywords	_	_
.	_	_

#37
Second	_	_
,	_	_
instead	_	_
of	_	_
directly	_	_
computing	_	_
the	_	_
document	_	_
relevance	_	_
score	_	_
using	_	_
cosine	_	_
similarity	_	_
in	_	_
the	_	_
learned	_	_
semantic	_	_
space	_	_
,	_	_
as	_	_
in	_	_
Huang	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2013	_	_
)	_	_
,	_	_
we	_	_
feed	_	_
the	_	_
features	_	_
derived	_	_
from	_	_
the	_	_
semantic	_	_
representations	_	_
of	_	_
documents	_	_
to	_	_
a	_	_
ranker	_	_
which	_	_
is	_	_
trained	_	_
in	_	_
a	_	_
supervised	_	_
manner	_	_
.	_	_

#38
As	_	_
a	_	_
result	_	_
,	_	_
a	_	_
document	_	_
that	_	_
is	_	_
not	_	_
highly	_	_
relevant	_	_
to	_	_
another	_	_
document	_	_
a	_	_
user	_	_
is	_	_
reading	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
distance	_	_
between	_	_
their	_	_
derived	_	_
feature	_	_
1	_	_
We	_	_
stress	_	_
here	_	_
that	_	_
,	_	_
although	_	_
the	_	_
click	_	_
signal	_	_
is	_	_
available	_	_
to	_	_
form	_	_
a	_	_
dataset	_	_
and	_	_
a	_	_
gold	_	_
standard	_	_
ranker	_	_
(	_	_
to	_	_
be	_	_
described	_	_
in	_	_
vectors	_	_
is	_	_
big	_	_
)	_	_
may	speculation	_
still	_	_
have	_	_
a	_	_
high	_	_
score	_	_
of	_	_
interestingness	_	_
because	_	_
the	_	_
former	_	_
provides	_	_
useful	_	_
information	_	_
about	_	_
an	_	_
entity	_	_
mentioned	_	_
in	_	_
the	_	_
latter	_	_
.	_	_

#39
Such	_	_
information	_	_
and	_	_
entity	_	_
are	_	_
encoded	_	_
,	_	_
respectively	_	_
,	_	_
by	_	_
(	_	_
some	_	_
subsets	_	_
of	_	_
)	_	_
the	_	_
semantic	_	_
features	_	_
in	_	_
their	_	_
corresponding	_	_
documents	_	_
.	_	_

#44
Our	_	_
notion	_	_
of	_	_
a	_	_
document	_	_
is	_	_
meant	_	_
in	_	_
its	_	_
most	_	_
general	_	_
form	_	_
as	_	_
a	_	_
string	_	_
of	_	_
raw	_	_
unstructured	_	_
text	_	_
.	_	_

#45
That	_	_
is	_	_
,	_	_
the	_	_
interestingness	_	_
function	_	_
should	deontic	negation
not	_	_
rely	_	_
on	_	_
any	_	_
document	_	_
structure	_	_
such	_	_
as	_	_
title	_	_
tags	_	_
,	_	_
hyperlinks	_	_
,	_	_
etc.	_	_
,	_	_
or	_	_
Web	_	_
interaction	_	_
data	_	_
.	_	_

#46
In	_	_
our	_	_
tasks	_	_
,	_	_
documents	_	_
can	feasibility	_
be	_	_
formed	_	_
either	_	_
from	_	_
the	_	_
plain	_	_
text	_	_
of	_	_
a	_	_
webpage	_	_
or	_	_
as	_	_
a	_	_
text	_	_
span	_	_
in	_	_
that	_	_
plain	_	_
text	_	_
,	_	_
as	_	_
will	_	_
be	_	_
discussed	_	_
in	_	_
Sections	_	_
4	_	_
and	_	_
5	_	_
.	_	_

#47
2.1	_	_
Data	_	_
We	_	_
can	feasibility-rhetorical	_
observe	_	_
many	_	_
naturally	_	_
occurring	_	_
manifestations	_	_
of	_	_
interestingness	_	_
on	_	_
the	_	_
Web	_	_
.	_	_

#48
For	_	_
example	_	_
,	_	_
on	_	_
Twitter	_	_
,	_	_
users	_	_
follow	_	_
shared	_	_
links	_	_
embedded	_	_
in	_	_
tweets	_	_
.	_	_

#50
When	_	_
a	_	_
user	_	_
clicks	_	_
on	_	_
a	_	_
hyperlink	_	_
,	_	_
it	_	_
is	_	_
reasonable	_	_
to	_	_
assume	_	_
that	_	_
she	_	_
is	_	_
interested	_	_
in	_	_
learning	_	_
more	_	_
about	_	_
the	_	_
anchor	_	_
,	_	_
modulo	_	_
cases	_	_
of	_	_
erroneous	_	_
clicks	_	_
.	_	_

#51
Aggregate	_	_
clicks	_	_
can	capability	_
therefore	_	_
serve	_	_
as	_	_
a	_	_
proxy	_	_
for	_	_
interestingness	_	_
.	_	_

#52
That	_	_
is	_	_
,	_	_
for	_	_
a	_	_
given	_	_
source	_	_
document	_	_
,	_	_
target	_	_
documents	_	_
that	_	_
attract	_	_
the	_	_
most	_	_
clicks	_	_
are	_	_
more	_	_
interesting	_	_
than	_	_
documents	_	_
that	_	_
attract	_	_
fewer	_	_
clicks1	_	_
.	_	_

#72
It	_	_
is	_	_
worth	_	_
noting	_	_
that	_	_
the	_	_
tri-letter	_	_
vector	_	_
complements	_	_
the	_	_
one-hot	_	_
vector	_	_
representation	_	_
in	_	_
two	_	_
aspects	_	_
.	_	_

#73
First	_	_
,	_	_
different	_	_
OOV	_	_
(	_	_
out	_	_
of	_	_
vocabulary	_	_
)	_	_
words	_	_
can	feasibility	_
be	_	_
represented	_	_
by	_	_
tri-letter	_	_
vectors	_	_
with	_	_
few	_	_
collisions	_	_
.	_	_

#74
Second	_	_
,	_	_
spelling	_	_
variations	_	_
of	_	_
the	_	_
same	_	_
word	_	_
can	feasibility	_
be	_	_
mapped	_	_
to	_	_
the	_	_
points	_	_
that	_	_
are	_	_
close	_	_
to	_	_
each	_	_
other	_	_
in	_	_
the	_	_
tri-letter	_	_
space	_	_
.	_	_

#75
Although	_	_
the	_	_
number	_	_
of	_	_
unique	_	_
English	_	_
words	_	_
on	_	_
the	_	_
Web	_	_
is	_	_
extremely	_	_
large	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
distinct	_	_
triletters	_	_
in	_	_
English	_	_
is	_	_
limited	_	_
(	_	_
restricted	_	_
to	_	_
the	_	_
most	_	_
frequent	_	_
30K	_	_
in	_	_
this	_	_
study	_	_
)	_	_
.	_	_

#89
We	_	_
design	_	_
by	_	_
adopting	_	_
the	_	_
max	_	_
operation	_	_
over	_	_
each	_	_
“time”	_	_
of	_	_
the	_	_
sequence	_	_
of	_	_
vectors	_	_
computed	_	_
by	_	_
(	_	_
1	_	_
)	_	_
,	_	_
which	_	_
forces	_	_
the	_	_
network	_	_
to	_	_
retain	_	_
only	_	_
the	_	_
most	_	_
useful	_	_
,	_	_
partially	_	_
invariant	_	_
local	_	_
features	_	_
produced	_	_
by	_	_
the	_	_
convolutional	_	_
layer	_	_
:	_	_
max	_	_
,	_	_
…	_	_
,	_	_
u	_	_
(	_	_
2	_	_
)	_	_
where	_	_
the	_	_
max	_	_
operation	_	_
is	_	_
performed	_	_
for	_	_
each	_	_
dimension	_	_
of	_	_
across	_	_
1	_	_
,	_	_
…	_	_
,	_	_
respectively	_	_
.	_	_

#90
That	_	_
convolutional	_	_
and	_	_
max-pooling	_	_
layers	_	_
are	_	_
able	_	_
to	_	_
discover	_	_
prominent	_	_
keywords	_	_
of	_	_
a	_	_
document	_	_
can	feasibility	_
be	_	_
demonstrated	_	_
using	_	_
the	_	_
procedure	_	_
in	_	_
Figure	_	_
2	_	_
using	_	_
a	_	_
toy	_	_
example	_	_
.	_	_

#91
First	_	_
,	_	_
the	_	_
convolutional	_	_
layer	_	_
of	_	_
(	_	_
1	_	_
)	_	_
generates	_	_
for	_	_
each	_	_
word	_	_
in	_	_
a	_	_
5-	_	_
word	_	_
document	_	_
a	_	_
4-dimensional	_	_
local	_	_
feature	_	_
vector	_	_
,	_	_
which	_	_
represents	_	_
a	_	_
distribution	_	_
of	_	_
four	_	_
topics	_	_
.	_	_

#103
Consider	_	_
a	_	_
source	_	_
document	_	_
and	_	_
two	_	_
candidate	_	_
target	_	_
documents	_	_
and	_	_
,	_	_
where	_	_
is	_	_
more	_	_
interesting	_	_
than	_	_
to	_	_
a	_	_
user	_	_
when	_	_
reading	_	_
.	_	_

#104
We	_	_
construct	_	_
two	_	_
pairs	_	_
of	_	_
documents	_	_
,	_	_
and	_	_
,	_	_
,	_	_
where	_	_
the	_	_
former	_	_
is	_	_
preferred	_	_
and	_	_
should	deontic	_
have	_	_
a	_	_
higher	_	_
u1	_	_
u2	_	_
u3	_	_
u4	_	_
u5	_	_
w1	_	_
w2	_	_
w3	_	_
w4	_	_
w5	_	_
w1	_	_
w2	_	_
w3	_	_
w4	_	_
w5	_	_
v	_	_
Figure	_	_
2	_	_
:	_	_
Toy	_	_
example	_	_
of	_	_
(	_	_
upper	_	_
)	_	_
a	_	_
5-word	_	_
document	_	_
and	_	_
its	_	_
local	_	_
feature	_	_
vectors	_	_
extracted	_	_
using	_	_
a	_	_
convolutional	_	_
layer	_	_
,	_	_
and	_	_
(	_	_
bottom	_	_
)	_	_
the	_	_
global	_	_
feature	_	_
vector	_	_
of	_	_
the	_	_
document	_	_
generated	_	_
after	_	_
max-pooling	_	_
.	_	_

#105
interestingness	_	_
score	_	_
.	_	_

#108
That	_	_
is	_	_
,	_	_
the	_	_
DSSM	_	_
is	_	_
learned	_	_
to	_	_
represent	_	_
documents	_	_
as	_	_
points	_	_
in	_	_
a	_	_
hidden	_	_
interestingness	_	_
space	_	_
,	_	_
where	_	_
the	_	_
similarity	_	_
between	_	_
a	_	_
document	_	_
and	_	_
its	_	_
interesting	_	_
documents	_	_
is	_	_
maximized	_	_
.	_	_

#109
We	_	_
use	_	_
the	_	_
following	_	_
logistic	_	_
loss	_	_
over	_	_
∆	_	_
,	_	_
which	_	_
can	feasibility	_
be	_	_
shown	_	_
to	_	_
upper	_	_
bound	_	_
the	_	_
pairwise	_	_
accuracy	_	_
:	_	_
∆	_	_
;	_	_
log	_	_
1	_	_
exp	_	_
∆	_	_
(	_	_
6	_	_
)	_	_
3	_	_
In	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
observed	_	_
better	_	_
results	_	_
by	_	_
sampling	_	_
more	_	_
negative	_	_
training	_	_
examples	_	_
(	_	_
e.g.	_	_
,	_	_
up	_	_
to	_	_
100	_	_
)	_	_
although	_	_
this	_	_
makes	_	_
the	_	_
training	_	_
much	_	_
slower	_	_
.	_	_

#110
An	_	_
alternative	_	_
approach	_	_
The	_	_
loss	_	_
function	_	_
in	_	_
(	_	_
6	_	_
)	_	_
has	_	_
a	_	_
shape	_	_
similar	_	_
to	_	_
the	_	_
hinge	_	_
loss	_	_
used	_	_
in	_	_
SVMs	_	_
.	_	_

#113
In	_	_
the	_	_
experiments	_	_
,	_	_
we	_	_
set	_	_
10	_	_
.	_	_

#114
Because	_	_
the	_	_
loss	_	_
function	_	_
is	_	_
differentiable	_	_
,	_	_
optimizing	_	_
the	_	_
model	_	_
parameters	_	_
can	feasibility	_
be	_	_
done	_	_
using	_	_
gradient-based	_	_
methods	_	_
.	_	_

#115
Due	_	_
to	_	_
space	_	_
limitations	_	_
,	_	_
we	_	_
omit	_	_
the	_	_
derivation	_	_
of	_	_
the	_	_
gradient	_	_
of	_	_
the	_	_
loss	_	_
function	_	_
,	_	_
for	_	_
which	_	_
readers	_	_
are	_	_
referred	_	_
to	_	_
related	_	_
derivations	_	_
(	_	_
e.g.	_	_
,	_	_
Collobert	_	_
et	_	_
al	_	_
.	_	_
2011	_	_
;	_	_
Huang	_	_
et	_	_
al	_	_
.	_	_
2013	_	_
;	_	_
Shen	_	_
et	_	_
al	_	_
.	_	_
2014	_	_
)	_	_
.	_	_

#120
Given	_	_
the	_	_
training	_	_
set	_	_
(	_	_
TRAIN_1	_	_
in	_	_
Section	_	_
2	_	_
)	_	_
,	_	_
it	_	_
takes	_	_
approximately	_	_
30	_	_
hours	_	_
to	_	_
train	_	_
a	_	_
DSSM	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
1	_	_
,	_	_
on	_	_
a	_	_
Xeon	_	_
E5-2670	_	_
2.60GHz	_	_
machine	_	_
with	_	_
one	_	_
Tesla	_	_
K20	_	_
GPU	_	_
card	_	_
.	_	_

#121
In	_	_
principle	_	_
,	_	_
the	_	_
loss	_	_
function	_	_
of	_	_
(	_	_
6	_	_
)	_	_
can	feasibility	_
be	_	_
further	_	_
regularized	_	_
(	_	_
e.g	_	_
.	_	_
by	_	_
adding	_	_
a	_	_
term	_	_
of	_	_
2	_	_
norm	_	_
)	_	_
to	_	_
deal	_	_
with	_	_
overfitting	_	_
.	_	_

#122
However	_	_
,	_	_
we	_	_
did	_	_
not	_	_
find	_	_
a	_	_
clear	_	_
empirical	_	_
advantage	_	_
over	_	_
the	_	_
simpler	_	_
early	_	_
stop	_	_
approach	_	_
in	_	_
a	_	_
pilot	_	_
study	_	_
,	_	_
hence	_	_
we	_	_
adopted	_	_
the	_	_
latter	_	_
in	_	_
the	_	_
experiments	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#124
Starting	_	_
with	_	_
1.0	_	_
,	_	_
after	_	_
each	_	_
epoch	_	_
(	_	_
a	_	_
pass	_	_
over	_	_
the	_	_
entire	_	_
training	_	_
data	_	_
)	_	_
,	_	_
the	_	_
learning	_	_
rate	_	_
is	_	_
adjusted	_	_
as	_	_
0.5	_	_
if	_	_
the	_	_
loss	_	_
on	_	_
validation	_	_
data	_	_
(	_	_
held-out	_	_
from	_	_
TRAIN_1	_	_
)	_	_
is	_	_
not	_	_
reduced	_	_
.	_	_

#125
The	_	_
training	_	_
stops	_	_
if	_	_
either	_	_
is	_	_
smaller	_	_
than	_	_
a	_	_
preset	_	_
threshold	_	_
(	_	_
0.0001	_	_
)	_	_
or	_	_
the	_	_
loss	_	_
on	_	_
training	_	_
data	_	_
can	feasibility	negation
no	_	_
longer	_	_
be	_	_
reduced	_	_
significantly	_	_
.	_	_

#126
In	_	_
our	_	_
experiments	_	_
,	_	_
the	_	_
DSSM	_	_
training	_	_
typically	_	_
converges	_	_
within	_	_
20	_	_
epochs	_	_
.	_	_

#128
First	_	_
,	_	_
we	_	_
use	_	_
the	_	_
DSSM	_	_
as	_	_
a	_	_
feature	_	_
generator	_	_
.	_	_

#129
The	_	_
output	_	_
layer	_	_
of	_	_
the	_	_
DSSM	_	_
can	capability-feasibility	_
be	_	_
seen	_	_
as	_	_
a	_	_
set	_	_
of	_	_
semantic	_	_
features	_	_
,	_	_
which	_	_
can	feasibility	_
be	_	_
incorporated	_	_
in	_	_
a	_	_
boosted	_	_
tree	_	_
is	_	_
to	_	_
approximate	_	_
the	_	_
partition	_	_
function	_	_
using	_	_
Noise	_	_
Contrastive	_	_
Estimation	_	_
(	_	_
Gutmann	_	_
and	_	_
Hyvarinen	_	_
2010	_	_
)	_	_
.	_	_

#130
We	_	_
leave	_	_
it	_	_
to	_	_
future	_	_
work	_	_
.	_	_

#141
Similarly	_	_
at	_	_
runtime	_	_
,	_	_
we	_	_
define	_	_
sim	_	_
,	_	_
as	_	_
(	_	_
5	_	_
)	_	_
.	_	_

#142
4	_	_
Experiments	_	_
on	_	_
Highlighting	_	_
Recall	_	_
from	_	_
Section	_	_
1	_	_
that	_	_
in	_	_
this	_	_
task	_	_
,	_	_
a	_	_
system	_	_
must	deontic	_
select	_	_
most	_	_
interesting	_	_
keywords	_	_
in	_	_
a	_	_
document	_	_
that	_	_
a	_	_
user	_	_
is	_	_
reading	_	_
.	_	_

#143
To	_	_
evaluate	_	_
our	_	_
models	_	_
using	_	_
the	_	_
click	_	_
transition	_	_
data	_	_
described	_	_
in	_	_
Section	_	_
2	_	_
,	_	_
we	_	_
simulate	_	_
the	_	_
task	_	_
as	_	_
follows	_	_
.	_	_

#144
We	_	_
use	_	_
the	_	_
set	_	_
of	_	_
anchors	_	_
in	_	_
a	_	_
source	_	_
document	_	_
to	_	_
simulate	_	_
the	_	_
set	_	_
of	_	_
candidate	_	_
keywords	_	_
that	_	_
may	speculation	_
be	_	_
of	_	_
interest	_	_
to	_	_
the	_	_
user	_	_
while	_	_
reading	_	_
,	_	_
and	_	_
treat	_	_
the	_	_
text	_	_
of	_	_
a	_	_
document	_	_
that	_	_
is	_	_
linked	_	_
by	_	_
an	_	_
anchor	_	_
in	_	_
as	_	_
a	_	_
target	_	_
document	_	_
.	_	_

#145
As	_	_
shown	_	_
in	_	_
Figure	_	_
1	_	_
,	_	_
to	_	_
apply	_	_
DSSM	_	_
to	_	_
a	_	_
specific	_	_
task	_	_
,	_	_
we	_	_
need	_	_
to	_	_
define	_	_
the	_	_
focus	_	_
in	_	_
source	_	_
and	_	_
target	_	_
documents	_	_
.	_	_

#176
We	_	_
also	_	_
distinguish	_	_
between	_	_
two	_	_
types	_	_
of	_	_
learned	_	_
rankers	_	_
:	_	_
those	_	_
which	_	_
draw	_	_
their	_	_
features	_	_
only	_	_
from	_	_
the	_	_
source	_	_
(	_	_
src	_	_
only	_	_
)	_	_
document	_	_
and	_	_
those	_	_
that	_	_
draw	_	_
their	_	_
features	_	_
from	_	_
both	_	_
the	_	_
source	_	_
and	_	_
target	_	_
(	_	_
src+tar	_	_
)	_	_
documents	_	_
.	_	_

#177
Although	_	_
our	_	_
task	_	_
setting	_	_
allows	_	_
access	_	_
to	_	_
the	_	_
content	_	_
of	_	_
both	_	_
source	_	_
and	_	_
target	_	_
documents	_	_
,	_	_
there	_	_
are	_	_
practical	_	_
scenarios	_	_
where	_	_
a	_	_
system	_	_
should	deontic	_
predict	_	_
what	_	_
interests	_	_
the	_	_
user	_	_
without	_	_
looking	_	_
at	_	_
the	_	_
target	_	_
document	_	_
because	_	_
the	_	_
extra	_	_
step	_	_
of	_	_
identifying	_	_
a	_	_
suitable	_	_
target	_	_
document	_	_
for	_	_
each	_	_
candidate	_	_
concept	_	_
or	_	_
entity	_	_
of	_	_
interest	_	_
is	_	_
computationally	_	_
expensive	_	_
.	_	_

#178
4.2	_	_
Analysis	_	_
of	_	_
Results	_	_
As	_	_
shown	_	_
in	_	_
Table	_	_
1	_	_
,	_	_
NSF+DSSM	_	_
,	_	_
which	_	_
incorporates	_	_
our	_	_
DSSM	_	_
,	_	_
is	_	_
the	_	_
overall	_	_
best	_	_
performing	_	_
system	_	_
across	_	_
test	_	_
sets	_	_
.	_	_

#259
This	_	_
model	_	_
treats	_	_
a	_	_
document	_	_
as	_	_
a	_	_
bag	_	_
of	_	_
words	_	_
,	_	_
just	_	_
like	_	_
BLTM	_	_
.	_	_

#260
These	_	_
results	_	_
demonstrate	_	_
that	_	_
the	_	_
effectiveness	_	_
of	_	_
DSSM	_	_
can	capability-feasibility	_
also	_	_
be	_	_
attributed	_	_
to	_	_
the	_	_
convolutional	_	_
architecture	_	_
in	_	_
the	_	_
neural	_	_
network	_	_
,	_	_
in	_	_
addition	_	_
to	_	_
being	_	_
deep	_	_
and	_	_
being	_	_
discriminative	_	_
.	_	_

#261
We	_	_
turn	_	_
now	_	_
to	_	_
discussing	_	_
the	_	_
ranker	_	_
results	_	_
in	_	_
Rows	_	_
7	_	_
to	_	_
9	_	_
.	_	_

#271
There	_	_
are	_	_
many	_	_
systems	_	_
that	_	_
identify	_	_
popular	_	_
content	_	_
in	_	_
the	_	_
Web	_	_
or	_	_
recommend	_	_
content	_	_
(	_	_
e.g.	_	_
,	_	_
Bandari	_	_
et	_	_
al	_	_
.	_	_
2012	_	_
;	_	_
Lerman	_	_
and	_	_
Hogg	_	_
2010	_	_
;	_	_
Szabo	_	_
and	_	_
Huberman	_	_
2010	_	_
)	_	_
,	_	_
which	_	_
is	_	_
closely	_	_
related	_	_
to	_	_
the	_	_
highlighting	_	_
task	_	_
.	_	_

#272
In	_	_
contrast	_	_
to	_	_
these	_	_
approaches	_	_
,	_	_
we	_	_
strive	_	_
to	_	_
predict	_	_
what	_	_
term	_	_
a	_	_
user	_	_
is	_	_
likely	_	_
to	_	_
be	_	_
interested	_	_
in	_	_
when	_	_
reading	_	_
content	_	_
,	_	_
which	_	_
may	options	_
or	_	_
may	options	negation
not	_	_
be	_	_
the	_	_
same	_	_
as	_	_
the	_	_
most	_	_
popular	_	_
content	_	_
that	_	_
is	_	_
related	_	_
to	_	_
the	_	_
current	_	_
document	_	_
.	_	_

#273
It	_	_
has	_	_
empirically	_	_
been	_	_
demonstrated	_	_
in	_	_
Gamon	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2013	_	_
)	_	_
that	_	_
popularity	_	_
is	_	_
in	_	_
fact	_	_
a	_	_
rather	_	_
poor	_	_
predictor	_	_
for	_	_
interestingness	_	_
.	_	_

#279
Hinton	_	_
and	_	_
Salakhutdinov	_	_
(	_	_
2010	_	_
)	_	_
propose	_	_
the	_	_
most	_	_
original	_	_
approach	_	_
based	_	_
on	_	_
an	_	_
unsupervised	_	_
version	_	_
of	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
to	_	_
discover	_	_
the	_	_
hierarchical	_	_
semantic	_	_
structure	_	_
embedded	_	_
in	_	_
queries	_	_
and	_	_
documents	_	_
.	_	_

#280
Huang	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2013	_	_
)	_	_
significantly	_	_
extends	_	_
the	_	_
approach	_	_
so	_	_
that	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
can	feasibility	_
be	_	_
trained	_	_
on	_	_
large-scale	_	_
query-document	_	_
pairs	_	_
giving	_	_
much	_	_
better	_	_
performance	_	_
.	_	_

#281
The	_	_
use	_	_
of	_	_
the	_	_
convolutional	_	_
neural	_	_
network	_	_
for	_	_
text	_	_
processing	_	_
,	_	_
central	_	_
to	_	_
our	_	_
DSSM	_	_
,	_	_
was	_	_
also	_	_
described	_	_
in	_	_
Collobert	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2011	_	_
)	_	_
and	_	_
Shen	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2014	_	_
)	_	_
but	_	_
with	_	_
very	_	_
different	_	_
applications	_	_
.	_	_

#282
The	_	_
DSSM	_	_
described	_	_
in	_	_
Section	_	_
3	_	_
can	capability-feasibility	_
be	_	_
viewed	_	_
as	_	_
a	_	_
variant	_	_
of	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
models	_	_
used	_	_
in	_	_
these	_	_
previous	_	_
studies	_	_
.	_	_

#283
7	_	_
Conclusions	_	_
Modeling	_	_
interestingness	_	_
is	_	_
fundamental	_	_
to	_	_
many	_	_
online	_	_
recommendation	_	_
systems	_	_
.	_	_

#292
To	_	_
capture	_	_
such	_	_
signals	_	_
,	_	_
our	_	_
model	_	_
needs	_	_
to	_	_
be	_	_
extended	_	_
to	_	_
adequately	_	_
represent	_	_
time	_	_
series	_	_
(	_	_
e.g.	_	_
,	_	_
causal	_	_
relations	_	_
and	_	_
consequences	_	_
of	_	_
actions	_	_
)	_	_
.	_	_

#293
One	_	_
potentially	_	_
effective	_	_
model	_	_
for	_	_
such	_	_
a	_	_
purpose	_	_
is	_	_
based	_	_
on	_	_
the	_	_
architecture	_	_
of	_	_
recurrent	_	_
neural	_	_
networks	_	_
(	_	_
e.g.	_	_
,	_	_
Mikolov	_	_
et	_	_
al	_	_
.	_	_
2010	_	_
;	_	_
Chen	_	_
and	_	_
Deng	_	_
,	_	_
2014	_	_
)	_	_
,	_	_
which	_	_
can	feasibility	_
be	_	_
incorporated	_	_
into	_	_
the	_	_
deep	_	_
semantic	_	_
model	_	_
proposed	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#294
Additional	_	_
Authors	_	_
Yelong	_	_
Shen	_	_
(	_	_
Microsoft	_	_
Research	_	_
,	_	_
One	_	_
Microsoft	_	_
Way	_	_
,	_	_
Redmond	_	_
,	_	_
WA	_	_
98052	_	_
,	_	_
USA	_	_
,	_	_
email	_	_
:	_	_
yeshen	_	_
@	_	_
microsoft.com	_	_
)	_	_
.	_	_