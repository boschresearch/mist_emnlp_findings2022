#5
For	_	_
a	_	_
start	_	_
,	_	_
it	_	_
implies	_	_
that	_	_
we	_	_
have	_	_
a	_	_
way	_	_
to	_	_
formally	_	_
represent	_	_
the	_	_
intended	_	_
meaning	_	_
of	_	_
all	_	_
texts	_	_
in	_	_
all	_	_
possible	_	_
contexts	_	_
,	_	_
and	_	_
furthermore	_	_
a	_	_
way	_	_
to	_	_
measure	_	_
the	_	_
degree	_	_
of	_	_
equivalence	_	_
between	_	_
two	_	_
such	_	_
representations	_	_
.	_	_

#6
This	_	_
goes	_	_
far	_	_
beyond	_	_
the	_	_
state-of-the-art	_	_
for	_	_
arbitrary	_	_
sentence	_	_
pairs	_	_
,	_	_
and	_	_
several	_	_
restrictions	_	_
must	deontic	_
be	_	_
imposed	_	_
.	_	_

#7
The	_	_
Semantic	_	_
Textual	_	_
Similarity	_	_
(	_	_
STS	_	_
)	_	_
task	_	_
(	_	_
Agirre	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
,	_	_
2013	_	_
)	_	_
limits	_	_
the	_	_
comparison	_	_
to	_	_
isolated	_	_
sentences	_	_
only	_	_
(	_	_
rather	_	_
than	_	_
complete	_	_
texts	_	_
)	_	_
,	_	_
and	_	_
defines	_	_
similarity	_	_
of	_	_
a	_	_
pair	_	_
of	_	_
sentences	_	_
as	_	_
the	_	_
one	_	_
assigned	_	_
by	_	_
human	_	_
judges	_	_
on	_	_
a	_	_
0–5	_	_
scale	_	_
(	_	_
with	_	_
0	_	_
implying	_	_
no	_	_
relation	_	_
and	_	_
5	_	_
complete	_	_
semantic	_	_
equivalence	_	_
)	_	_
.	_	_

#33
Since	_	_
word	_	_
sense	_	_
disambiguation	_	_
is	_	_
not	_	_
used	_	_
,	_	_
we	_	_
take	_	_
the	_	_
similarity	_	_
between	_	_
the	_	_
nearest	_	_
senses	_	_
of	_	_
two	_	_
words	_	_
.	_	_

#34
For	_	_
cases	_	_
when	_	_
the	_	_
WordNet-based	_	_
similarity	_	_
can	feasibility	negation
not	_	_
be	_	_
obtained	_	_
,	_	_
a	_	_
similarity	_	_
based	_	_
on	_	_
the	_	_
Levenshtein	_	_
distance	_	_
(	_	_
Levenshtein	_	_
,	_	_
1966	_	_
)	_	_
is	_	_
used	_	_
instead	_	_
.	_	_

#35
It	_	_
is	_	_
normalised	_	_
by	_	_
the	_	_
length	_	_
of	_	_
the	_	_
longest	_	_
word	_	_
in	_	_
the	_	_
pair	_	_
.	_	_

#44
A	_	_
novel	_	_
variant	_	_
,	_	_
which	_	_
we	_	_
have	_	_
called	_	_
“Multisense	_	_
Random	_	_
Indexing”	_	_
(	_	_
MSRI	_	_
)	_	_
,	_	_
inspired	_	_
by	_	_
Reisinger	_	_
and	_	_
Mooney	_	_
(	_	_
2010	_	_
)	_	_
,	_	_
attempts	_	_
to	_	_
capture	_	_
one	_	_
or	_	_
more	_	_
“senses”	_	_
per	_	_
unique	_	_
term	_	_
in	_	_
an	_	_
unsupervised	_	_
manner	_	_
,	_	_
each	_	_
sense	_	_
represented	_	_
as	_	_
an	_	_
individual	_	_
vector	_	_
in	_	_
the	_	_
model	_	_
.	_	_

#45
The	_	_
method	_	_
is	_	_
similar	_	_
to	_	_
classical	_	_
sliding	_	_
window	_	_
RI	_	_
,	_	_
but	_	_
each	_	_
term	_	_
can	capability	_
have	_	_
multiple	_	_
context	_	_
vectors	_	_
(	_	_
referred	_	_
to	_	_
as	_	_
“sense	_	_
vectors”	_	_
here	_	_
)	_	_
which	_	_
are	_	_
updated	_	_
individually	_	_
.	_	_

#46
When	_	_
updating	_	_
a	_	_
term	_	_
vector	_	_
,	_	_
instead	_	_
of	_	_
directly	_	_
adding	_	_
the	_	_
index	_	_
vectors	_	_
of	_	_
the	_	_
neighbouring	_	_
terms	_	_
in	_	_
the	_	_
window	_	_
to	_	_
its	_	_
context	_	_
vector	_	_
,	_	_
the	_	_
system	_	_
first	_	_
computes	_	_
a	_	_
separate	_	_
window	_	_
vector	_	_
consisting	_	_
of	_	_
the	_	_
sum	_	_
of	_	_
the	_	_
index	_	_
vectors	_	_
.	_	_

#95
Overall	_	_
the	_	_
character	_	_
n-gram	_	_
features	_	_
are	_	_
the	_	_
most	_	_
informative	_	_
,	_	_
particularly	_	_
for	_	_
HeadLine	_	_
and	_	_
SMT	_	_
.	_	_

#96
The	_	_
reason	_	_
may	speculation	_
be	_	_
that	_	_
these	_	_
not	_	_
only	_	_
capture	_	_
word	_	_
overlap	_	_
(	_	_
Ahn	_	_
,	_	_
2011	_	_
)	_	_
,	_	_
but	_	_
also	_	_
inflectional	_	_
forms	_	_
and	_	_
spelling	_	_
variants	_	_
.	_	_

#97
The	_	_
(	_	_
weighted	_	_
)	_	_
distributional	_	_
similarity	_	_
features	_	_
based	_	_
on	_	_
NYT	_	_
are	_	_
important	_	_
for	_	_
HeadLine	_	_
and	_	_
SMT	_	_
,	_	_
which	_	_
obviously	_	_
contain	_	_
sentence	_	_
pairs	_	_
from	_	_
the	_	_
news	_	_
genre	_	_
,	_	_
whereas	_	_
the	_	_
Wikipedia	_	_
based	_	_
feature	_	_
is	_	_
more	_	_
important	_	_
for	_	_
OnWN	_	_
and	_	_
FNWN	_	_
.	_	_

#103
Although	_	_
treated	_	_
as	_	_
a	_	_
single	_	_
feature	_	_
,	_	_
it	_	_
is	_	_
actually	_	_
a	_	_
combination	_	_
of	_	_
similarity	_	_
features	_	_
where	_	_
an	_	_
appropriate	_	_
feature	_	_
is	_	_
selected	_	_
for	_	_
each	_	_
word	_	_
pair	_	_
.	_	_

#104
This	_	_
“vertical”	_	_
way	_	_
of	_	_
combining	_	_
features	_	_
can	capability	_
potentially	_	_
provide	_	_
a	_	_
more	_	_
fine-grained	_	_
feature	_	_
selection	_	_
,	_	_
resulting	_	_
in	_	_
less	_	_
noise	_	_
.	_	_

#105
Indeed	_	_
,	_	_
if	_	_
two	_	_
words	_	_
are	_	_
matching	_	_
as	_	_
named	_	_
entities	_	_
or	_	_
as	_	_
close	_	_
synonyms	_	_
,	_	_
less	_	_
precise	_	_
types	_	_
of	_	_
features	_	_
such	_	_
as	_	_
character-based	_	_
and	_	_
data-driven	_	_
similarity	_	_
should	deontic	negation
not	_	_
dominate	_	_
the	_	_
overall	_	_
similarity	_	_
score	_	_
.	_	_

#106
It	_	_
is	_	_
interesting	_	_
to	_	_
find	_	_
that	_	_
MSRI	_	_
outperforms	_	_
both	_	_
classical	_	_
RI	_	_
and	_	_
ESA	_	_
(	_	_
Gabrilovich	_	_
and	_	_
Markovitch	_	_
,	_	_
2007	_	_
)	_	_
on	_	_
this	_	_
task	_	_
.	_	_

#107
Still	_	_
,	_	_
the	_	_
more	_	_
advanced	_	_
features	_	_
,	_	_
such	_	_
as	_	_
MSRI-Context	_	_
,	_	_
gave	_	_
inferior	_	_
results	_	_
compared	_	_
to	_	_
MSRI-Centroid	_	_
.	_	_

#108
This	_	_
suggests	_	_
that	_	_
more	_	_
research	_	_
on	_	_
MSRI	_	_
is	_	_
needed	_	_
to	_	_
understand	_	_
how	_	_
both	_	_
training	_	_
and	_	_
retrieval	_	_
can	feasibility	_
be	_	_
optimised	_	_
.	_	_

#109
Also	_	_
,	_	_
LSA-based	_	_
features	_	_
(	_	_
see	_	_
tl.weight-dist-sim-wiki	_	_
)	_	_
achieve	_	_
better	_	_
results	_	_
than	_	_
both	_	_
MSRI	_	_
,	_	_
RI	_	_
and	_	_
ESA	_	_
.	_	_

#110
Then	_	_
again	_	_
,	_	_
larger	_	_
corpora	_	_
were	_	_
used	_	_
for	_	_
training	_	_
the	_	_
LSA	_	_
models	_	_
.	_	_

#111
RI	_	_
has	_	_
been	_	_
shown	_	_
to	_	_
be	_	_
comparable	_	_
to	_	_
LSA	_	_
(	_	_
Karlgren	_	_
and	_	_
Sahlgren	_	_
,	_	_
2001	_	_
)	_	_
,	_	_
and	_	_
since	_	_
a	_	_
relatively	_	_
small	_	_
corpus	_	_
was	_	_
used	_	_
for	_	_
training	_	_
the	_	_
RI/MSRI	_	_
models	_	_
,	_	_
there	_	_
are	_	_
reasons	_	_
to	_	_
believe	_	_
that	_	_
better	_	_
scores	_	_
can	feasibility	_
be	_	_
achieved	_	_
by	_	_
both	_	_
RI-	_	_
and	_	_
MSRI-based	_	_
features	_	_
by	_	_
using	_	_
more	_	_
training	_	_
data	_	_
.	_	_

#112
HeadLine	_	_
OnWN	_	_
FNWN	_	_
SMT	_	_
Mean	_	_
Features	_	_
r	_	_
n	_	_
r	_	_
n	_	_
r	_	_
n	_	_
r	_	_
n	_	_
r	_	_
n	_	_
CharacterNGramMeasure-3	_	_
0.72	_	_
2	_	_
0.39	_	_
2	_	_
0.44	_	_
3	_	_
0.70	_	_
1	_	_
0.56	_	_
1	_	_
CharacterNGramMeasure-4	_	_
0.69	_	_
3	_	_
0.38	_	_
5	_	_
0.45	_	_
2	_	_
0.67	_	_
6	_	_
0.55	_	_
2	_	_
CharacterNGramMeasure-2	_	_
0.73	_	_
1	_	_
0.37	_	_
9	_	_
0.34	_	_
10	_	_
0.69	_	_
2	_	_
0.53	_	_
3	_	_
tl.weight-dist-sim-wiki	_	_
0.58	_	_
14	_	_
0.39	_	_
3	_	_
0.45	_	_
1	_	_
0.67	_	_
5	_	_
0.52	_	_
4	_	_
tl.wn-sim-lem	_	_
0.69	_	_
4	_	_
0.40	_	_
1	_	_
0.41	_	_
5	_	_
0.59	_	_
10	_	_
0.52	_	_
5	_	_
GateWordMatch	_	_
0.67	_	_
8	_	_
0.37	_	_
11	_	_
0.34	_	_
11	_	_
0.60	_	_
9	_	_
0.50	_	_
6	_	_
tl.dist-sim-nyt	_	_
0.69	_	_
5	_	_
0.34	_	_
28	_	_
0.26	_	_
23	_	_
0.65	_	_
8	_	_
0.49	_	_
7	_	_
tl.n-gram-match-lem-1	_	_
0.68	_	_
6	_	_
0.36	_	_
16	_	_
0.37	_	_
8	_	_
0.51	_	_
14	_	_
0.48	_	_
8	_	_
tl.weight-dist-sim-nyt	_	_
0.57	_	_
17	_	_
0.37	_	_
14	_	_
0.29	_	_
18	_	_
0.66	_	_
7	_	_
0.47	_	_
9	_	_
tl.n-gram-match-lc-1	_	_
0.68	_	_
7	_	_
0.37	_	_
10	_	_
0.32	_	_
13	_	_
0.50	_	_
17	_	_
0.47	_	_
10	_	_
MCS06-Resnik-WordNet	_	_
0.49	_	_
26	_	_
0.36	_	_
22	_	_
0.28	_	_
19	_	_
0.68	_	_
3	_	_
0.45	_	_
11	_	_
TWSI-Resnik-WordNet	_	_
0.49	_	_
27	_	_
0.36	_	_
23	_	_
0.28	_	_
20	_	_
0.68	_	_
4	_	_
0.45	_	_
12	_	_
tl.weight-word-match-lem	_	_
0.56	_	_
18	_	_
0.37	_	_
16	_	_
0.37	_	_
7	_	_
0.50	_	_
16	_	_
0.45	_	_
13	_	_
MSRI-Centroid	_	_
0.60	_	_
13	_	_
0.36	_	_
17	_	_
0.37	_	_
9	_	_
0.45	_	_
19	_	_
0.45	_	_
14	_	_
tl.weight-word-match-olc	_	_
0.56	_	_
19	_	_
0.38	_	_
8	_	_
0.32	_	_
12	_	_
0.51	_	_
15	_	_
0.44	_	_
15	_	_
MSRI-MaxSense	_	_
0.58	_	_
15	_	_
0.36	_	_
15	_	_
0.31	_	_
14	_	_
0.45	_	_
20	_	_
0.42	_	_
16	_	_
GreedyStringTiling-3	_	_
0.67	_	_
9	_	_
0.38	_	_
6	_	_
0.31	_	_
15	_	_
0.34	_	_
29	_	_
0.43	_	_
17	_	_
ESA-Wikipedia	_	_
0.50	_	_
25	_	_
0.30	_	_
38	_	_
0.32	_	_
14	_	_
0.54	_	_
12	_	_
0.42	_	_
18	_	_
WordNGramJaccard-1	_	_
0.64	_	_
10	_	_
0.37	_	_
12	_	_
0.25	_	_
25	_	_
0.33	_	_
30	_	_
0.40	_	_
19	_	_
WordNGramContainment-1-stopword	_	_
0.64	_	_
25	_	_
0.38	_	_
7	_	_
0.25	_	_
24	_	_
0.32	_	_
31	_	_
0.40	_	_
20	_	_
RI-Hungarian	_	_
0.58	_	_
16	_	_
0.33	_	_
31	_	_
0.10	_	_
34	_	_
0.42	_	_
22	_	_
0.36	_	_
24	_	_
RI-AvgTermTerm	_	_
0.56	_	_
20	_	_
0.33	_	_
32	_	_
0.11	_	_
33	_	_
0.37	_	_
28	_	_
0.34	_	_
25	_	_
LongestCommonSubstring	_	_
0.40	_	_
29	_	_
0.30	_	_
39	_	_
0.42	_	_
4	_	_
0.37	_	_
27	_	_
0.37	_	_
26	_	_
ESA-WordNet	_	_
0.11	_	_
43	_	_
0.30	_	_
40	_	_
0.41	_	_
6	_	_
0.49	_	_
18	_	_
0.33	_	_
29	_	_
LongestCommonSubsequenceNorm	_	_
0.53	_	_
21	_	_
0.39	_	_
4	_	_
0.19	_	_
27	_	_
0.18	_	_
37	_	_
0.32	_	_
30	_	_
MultisenseRI-ContextTermTerm	_	_
0.39	_	_
31	_	_
0.33	_	_
33	_	_
0.28	_	_
21	_	_
0.15	_	_
38	_	_
0.29	_	_
33	_	_
MultisenseRI-HASensesTermTerm	_	_
0.39	_	_
32	_	_
0.33	_	_
34	_	_
0.28	_	_
22	_	_
0.15	_	_
39	_	_
0.29	_	_
34	_	_
RI-SentVectors-Norm	_	_
0.34	_	_
35	_	_
0.35	_	_
26	_	_
-0.01	_	_
51	_	_
0.24	_	_
35	_	_
0.23	_	_
39	_	_
RelationSimilarity	_	_
0.31	_	_
39	_	_
0.35	_	_
27	_	_
0.24	_	_
26	_	_
0.02	_	_
41	_	_
0.23	_	_
40	_	_
RI-SentVectors-TFIDF	_	_
0.27	_	_
40	_	_
0.15	_	_
50	_	_
0.08	_	_
40	_	_
0.23	_	_
36	_	_
0.18	_	_
41	_	_
GraphEditDistance	_	_
0.33	_	_
38	_	_
0.25	_	_
46	_	_
0.13	_	_
31	_	_
-0.11	_	_
49	_	_
0.15	_	_
42	_	_
Table	_	_
3	_	_
:	_	_
Correlation	_	_
score	_	_
and	_	_
rank	_	_
of	_	_
the	_	_
best	_	_
features	_	_
8	_	_
Conclusion	_	_
and	_	_
Future	_	_
Work	_	_
The	_	_
NTNU	_	_
system	_	_
can	capability-feasibility	_
be	_	_
regarded	_	_
as	_	_
continuation	_	_
of	_	_
the	_	_
most	_	_
successful	_	_
systems	_	_
from	_	_
the	_	_
STS’12	_	_
shared	_	_
task	_	_
,	_	_
combining	_	_
shallow	_	_
textual	_	_
,	_	_
distributional	_	_
and	_	_
knowledge-based	_	_
features	_	_
into	_	_
a	_	_
support	_	_
vector	_	_
regression	_	_
model	_	_
.	_	_

#113
It	_	_
reuses	_	_
features	_	_
from	_	_
the	_	_
TakeLab	_	_
and	_	_
DKPro	_	_
systems	_	_
,	_	_
resulting	_	_
in	_	_
a	_	_
very	_	_
strong	_	_
baseline	_	_
.	_	_