#17
It	_	_
models	_	_
actual	_	_
phrasal	_	_
translation	_	_
probabilities	_	_
while	_	_
avoiding	_	_
sparsity	_	_
issues	_	_
by	_	_
using	_	_
single	_	_
words	_	_
as	_	_
input	_	_
and	_	_
output	_	_
units	_	_
.	_	_

#18
Furthermore	_	_
,	_	_
in	_	_
addition	_	_
to	_	_
the	_	_
unidirectional	_	_
formulation	_	_
,	_	_
we	_	_
are	_	_
the	_	_
first	_	_
to	_	_
propose	_	_
a	_	_
bidirectional	_	_
architecture	_	_
which	_	_
can	capability	_
take	_	_
the	_	_
full	_	_
source	_	_
sentence	_	_
into	_	_
account	_	_
for	_	_
all	_	_
predictions	_	_
.	_	_

#19
Our	_	_
experiments	_	_
show	_	_
that	_	_
these	_	_
models	_	_
can	capability-options	_
improve	_	_
state-of-the-art	_	_
baselines	_	_
containing	_	_
a	_	_
recurrent	_	_
language	_	_
model	_	_
on	_	_
three	_	_
tasks	_	_
.	_	_

#20
For	_	_
our	_	_
competitive	_	_
IWSLT	_	_
2013	_	_
German→English	_	_
system	_	_
,	_	_
we	_	_
observe	_	_
gains	_	_
of	_	_
up	_	_
to	_	_
1.6	_	_
%	_	_
BLEU	_	_
and	_	_
1.7	_	_
%	_	_
TER	_	_
.	_	_

#68
The	_	_
cross-entropy	_	_
error	_	_
criterion	_	_
is	_	_
used	_	_
for	_	_
training	_	_
.	_	_

#69
Further	_	_
details	_	_
on	_	_
LSTM	_	_
neural	_	_
networks	_	_
can	feasibility-rhetorical	_
be	_	_
found	_	_
in	_	_
(	_	_
Graves	_	_
and	_	_
Schmidhuber	_	_
,	_	_
2005	_	_
;	_	_
Sundermeyer	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
.	_	_

#70
4	_	_
Translation	_	_
Modeling	_	_
with	_	_
RNNs	_	_
In	_	_
the	_	_
following	_	_
we	_	_
describe	_	_
our	_	_
word-	_	_
and	_	_
phrase-based	_	_
translation	_	_
models	_	_
in	_	_
detail	_	_
.	_	_

#71
We	_	_
also	_	_
show	_	_
how	_	_
bidirectional	_	_
RNNs	_	_
can	capability	_
enable	_	_
such	_	_
models	_	_
to	_	_
include	_	_
full	_	_
source	_	_
information	_	_
.	_	_

#72
4.1	_	_
Resolving	_	_
Alignment	_	_
Ambiguities	_	_
Our	_	_
word-based	_	_
recurrent	_	_
models	_	_
are	_	_
only	_	_
defined	_	_
for	_	_
one-to-one-aligned	_	_
source-target	_	_
sentence	_	_
pairs	_	_
.	_	_

#84
Target	_	_
identity	_	_
is	_	_
defined	_	_
analogously	_	_
.	_	_

#85
The	_	_
results	_	_
can	rhetorical	_
be	_	_
found	_	_
in	_	_
Tab	_	_
.	_	_

#86
1	_	_
.	_	_

#95
(	_	_
3	_	_
)	_	_
We	_	_
denote	_	_
the	_	_
formulation	_	_
(	_	_
1	_	_
)	_	_
as	_	_
the	_	_
bidirectional	_	_
joint	_	_
model	_	_
(	_	_
BJM	_	_
)	_	_
.	_	_

#96
This	_	_
model	_	_
can	feasibility	_
be	_	_
simplified	_	_
by	_	_
several	_	_
independence	_	_
assumptions	_	_
.	_	_

#97
First	_	_
,	_	_
we	_	_
drop	_	_
the	_	_
dependency	_	_
on	_	_
the	_	_
future	_	_
source	_	_
information	_	_
,	_	_
receiving	_	_
what	_	_
we	_	_
denote	_	_
as	_	_
the	_	_
unidirectional	_	_
joint	_	_
model	_	_
(	_	_
JM	_	_
)	_	_
in	_	_
(	_	_
2	_	_
)	_	_
.	_	_

#99
Finally	_	_
,	_	_
assuming	_	_
conditional	_	_
independence	_	_
from	_	_
the	_	_
previous	_	_
target	_	_
sequence	_	_
,	_	_
we	_	_
receive	_	_
the	_	_
unidirectional	_	_
translation	_	_
model	_	_
(	_	_
TM	_	_
)	_	_
in	_	_
(	_	_
3	_	_
)	_	_
.	_	_

#100
Analogously	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
define	_	_
a	_	_
bidirectional	_	_
translation	_	_
model	_	_
(	_	_
BTM	_	_
)	_	_
by	_	_
keeping	_	_
the	_	_
dependency	_	_
on	_	_
the	_	_
full	_	_
source	_	_
sentence	_	_
f	_	_
I1	_	_
,	_	_
but	_	_
dropping	_	_
the	_	_
previous	_	_
target	_	_
sequence	_	_
ei−11	_	_
:	_	_
p	_	_
(	_	_
eI1|f	_	_
I1	_	_
)	_	_
≈	_	_
I∏	_	_
i=1	_	_
p	_	_
(	_	_
ei|f	_	_
I1	_	_
)	_	_
.	_	_

#101
(	_	_
4	_	_
)	_	_
Fig	_	_
.	_	_
2	_	_
shows	_	_
the	_	_
dependencies	_	_
of	_	_
the	_	_
word-	_	_
based	_	_
neural	_	_
translation	_	_
and	_	_
joint	_	_
models	_	_
.	_	_

#114
Finally	_	_
,	_	_
ϕ	_	_
is	_	_
the	_	_
widely-used	_	_
softmax	_	_
function	_	_
to	_	_
obtain	_	_
normalized	_	_
probabilities	_	_
,	_	_
and	_	_
c	_	_
denotes	_	_
a	_	_
word	_	_
class	_	_
mapping	_	_
from	_	_
any	_	_
target	_	_
word	_	_
to	_	_
its	_	_
unique	_	_
word	_	_
class	_	_
.	_	_

#115
For	_	_
the	_	_
bidirectional	_	_
model	_	_
,	_	_
the	_	_
equations	_	_
can	feasibility	_
be	_	_
defined	_	_
analogously	_	_
.	_	_

#116
Due	_	_
to	_	_
the	_	_
use	_	_
of	_	_
word	_	_
classes	_	_
,	_	_
the	_	_
output	_	_
layer	_	_
consists	_	_
of	_	_
two	_	_
parts	_	_
.	_	_

#138
As	_	_
there	_	_
is	_	_
no	_	_
one-to-one	_	_
correspondence	_	_
between	_	_
the	_	_
words	_	_
within	_	_
a	_	_
phrase	_	_
,	_	_
the	_	_
basic	_	_
idea	_	_
of	_	_
our	_	_
phrase-based	_	_
approach	_	_
is	_	_
to	_	_
let	_	_
the	_	_
neural	_	_
network	_	_
learn	_	_
the	_	_
dependencies	_	_
itself	_	_
,	_	_
and	_	_
present	_	_
the	_	_
full	_	_
source	_	_
side	_	_
of	_	_
the	_	_
phrase	_	_
to	_	_
the	_	_
network	_	_
before	_	_
letting	_	_
it	_	_
predict	_	_
target	_	_
side	_	_
words	_	_
.	_	_

#139
Then	_	_
the	_	_
probability	_	_
for	_	_
the	_	_
target	_	_
side	_	_
of	_	_
a	_	_
phrase	_	_
can	feasibility	_
be	_	_
computed	_	_
,	_	_
in	_	_
case	_	_
of	_	_
Eq	_	_
.	_	_
6	_	_
,	_	_
by	_	_
:	_	_
p	_	_
(	_	_
ẽi|ẽi−11	_	_
,	_	_
f̃	_	_
Ĩ1	_	_
)	_	_
=	_	_
|ẽi|∏	_	_
j=1	_	_
p	_	_
(	_	_
(	_	_
ẽi	_	_
)	_	_
j	_	_
|	_	_
(	_	_
ẽi	_	_
)	_	_
j−11	_	_
,	_	_
ẽi−11	_	_
,	_	_
f̃	_	_
i1	_	_
)	_	_
,	_	_
and	_	_
analogously	_	_
for	_	_
the	_	_
case	_	_
of	_	_
Eq	_	_
.	_	_
5	_	_
.	_	_

#140
Here	_	_
,	_	_
(	_	_
ẽi	_	_
)	_	_
j	_	_
denotes	_	_
the	_	_
j-th	_	_
word	_	_
of	_	_
the	_	_
i-th	_	_
aligned	_	_
target	_	_
phrase	_	_
.	_	_

#149
Similarly	_	_
,	_	_
the	_	_
presentation	_	_
of	_	_
the	_	_
source	_	_
side	_	_
of	_	_
the	_	_
next	_	_
phrase	_	_
only	_	_
starts	_	_
after	_	_
the	_	_
prediction	_	_
of	_	_
the	_	_
current	_	_
target	_	_
side	_	_
is	_	_
completed	_	_
.	_	_

#150
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
no-operation	_	_
token	_	_
,	_	_
denoted	_	_
by	_	_
ε	_	_
,	_	_
which	_	_
is	_	_
not	_	_
part	_	_
of	_	_
the	_	_
vocabulary	_	_
(	_	_
which	_	_
means	_	_
it	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
input	_	_
to	_	_
or	_	_
predicted	_	_
by	_	_
the	_	_
RNN	_	_
)	_	_
.	_	_

#151
When	_	_
the	_	_
ε	_	_
token	_	_
occurs	_	_
as	_	_
input	_	_
,	_	_
it	_	_
indicates	_	_
that	_	_
no	_	_
input	_	_
needs	_	_
to	_	_
be	_	_
processed	_	_
by	_	_
the	_	_
RNN	_	_
.	_	_

#152
When	_	_
the	_	_
ε	_	_
token	_	_
occurs	_	_
as	_	_
a	_	_
teacher	_	_
signal	_	_
for	_	_
the	_	_
RNN	_	_
,	_	_
the	_	_
output	_	_
layer	_	_
distribution	_	_
is	_	_
ignored	_	_
,	_	_
and	_	_
does	_	_
not	_	_
even	_	_
have	_	_
to	_	_
be	_	_
computed	_	_
.	_	_

#153
In	_	_
both	_	_
cases	_	_
,	_	_
all	_	_
the	_	_
other	_	_
layers	_	_
are	_	_
still	_	_
processed	_	_
during	_	_
forward	_	_
and	_	_
backward	_	_
passes	_	_
such	_	_
that	_	_
the	_	_
RNN	_	_
state	_	_
can	feasibility	_
be	_	_
advanced	_	_
even	_	_
without	_	_
additional	_	_
input	_	_
or	_	_
output	_	_
.	_	_

#154
Fig	_	_
.	_	_
5	_	_
depicts	_	_
the	_	_
evaluation	_	_
of	_	_
a	_	_
phrase-based	_	_
joint	_	_
model	_	_
for	_	_
the	_	_
example	_	_
alignment	_	_
from	_	_
Fig	_	_
.	_	_
4	_	_
.	_	_

#174
ward	_	_
and	_	_
backward	_	_
layers	_	_
converge	_	_
into	_	_
a	_	_
hidden	_	_
layer	_	_
.	_	_

#175
A	_	_
shallow	_	_
variant	_	_
can	feasibility	_
be	_	_
obtained	_	_
if	_	_
the	_	_
parallel	_	_
layers	_	_
converge	_	_
into	_	_
the	_	_
output	_	_
layer	_	_
directly1	_	_
.	_	_

#176
diDue	_	_
to	_	_
the	_	_
full	_	_
dependence	_	_
on	_	_
the	_	_
source	_	_
sequence	_	_
,	_	_
evaluating	_	_
bidirectional	_	_
networks	_	_
requires	_	_
computing	_	_
the	_	_
forward	_	_
pass	_	_
of	_	_
the	_	_
forward	_	_
and	_	_
backward	_	_
layers	_	_
for	_	_
the	_	_
full	_	_
sequence	_	_
,	_	_
before	_	_
being	_	_
able	_	_
to	_	_
evaluate	_	_
the	_	_
next	_	_
layers	_	_
.	_	_

#204
These	_	_
results	_	_
are	_	_
also	_	_
consistent	_	_
with	_	_
the	_	_
improvements	_	_
in	_	_
terms	_	_
of	_	_
TER	_	_
,	_	_
where	_	_
we	_	_
achieve	_	_
reductions	_	_
by	_	_
0.8	_	_
TER	_	_
up	_	_
to	_	_
1.8	_	_
TER	_	_
.	_	_

#205
These	_	_
numbers	_	_
can	feasibility	_
be	_	_
directly	_	_
compared	_	_
to	_	_
the	_	_
case	_	_
of	_	_
feedforward	_	_
neural	_	_
network-based	_	_
translation	_	_
modeling	_	_
as	_	_
proposed	_	_
in	_	_
(	_	_
Devlin	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
which	_	_
we	_	_
include	_	_
in	_	_
the	_	_
very	_	_
last	_	_
row	_	_
of	_	_
the	_	_
table	_	_
.	_	_

#206
Nearly	_	_
all	_	_
of	_	_
our	_	_
recurrent	_	_
models	_	_
outperform	_	_
the	_	_
feedforward	_	_
approach	_	_
,	_	_
where	_	_
the	_	_
RNN	_	_
model	_	_
performing	_	_
best	_	_
on	_	_
the	_	_
dev	_	_
data	_	_
is	_	_
better	_	_
on	_	_
test	_	_
by	_	_
0.3	_	_
BLEU	_	_
and	_	_
1.0	_	_
TER	_	_
.	_	_

#207
Interestingly	_	_
,	_	_
for	_	_
the	_	_
recurrent	_	_
word-based	_	_
models	_	_
,	_	_
on	_	_
the	_	_
test	_	_
data	_	_
it	_	_
can	feasibility-rhetorical	_
be	_	_
seen	_	_
that	_	_
TMs	_	_
perform	_	_
better	_	_
than	_	_
JMs	_	_
,	_	_
even	_	_
though	_	_
TMs	_	_
do	_	_
not	_	_
take	_	_
advantage	_	_
of	_	_
the	_	_
target	_	_
side	_	_
history	_	_
words	_	_
.	_	_

#208
However	_	_
,	_	_
exploiting	_	_
this	_	_
extra	_	_
information	_	_
does	_	_
not	_	_
always	_	_
need	_	_
to	_	_
result	_	_
in	_	_
a	_	_
better	_	_
model	_	_
,	_	_
as	_	_
the	_	_
target	_	_
side	_	_
words	_	_
are	_	_
only	_	_
derived	_	_
from	_	_
the	_	_
given	_	_
source	_	_
side	_	_
,	_	_
which	_	_
is	_	_
available	_	_
to	_	_
both	_	_
TMs	_	_
and	_	_
JMs	_	_
.	_	_

#219
Therefore	_	_
,	_	_
we	_	_
mainly	_	_
report	_	_
JM	_	_
results	_	_
for	_	_
the	_	_
phrase-based	_	_
networks	_	_
.	_	_

#220
A	_	_
phrase-based	_	_
model	_	_
can	feasibility	_
also	_	_
be	_	_
trained	_	_
on	_	_
multiple	_	_
variants	_	_
for	_	_
the	_	_
phrase	_	_
alignment	_	_
.	_	_

#221
For	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
tested	_	_
10-best	_	_
alignments	_	_
against	_	_
the	_	_
single	_	_
best	_	_
alignment	_	_
,	_	_
which	_	_
resulted	_	_
in	_	_
a	_	_
small	_	_
improvement	_	_
of	_	_
0.2	_	_
TER	_	_
on	_	_
both	_	_
dev	_	_
and	_	_
test	_	_
.	_	_

#234
Due	_	_
to	_	_
the	_	_
large	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
,	_	_
we	_	_
concentrate	_	_
on	_	_
models	_	_
of	_	_
high	_	_
performance	_	_
in	_	_
the	_	_
IWSLT	_	_
experiments	_	_
.	_	_

#235
The	_	_
results	_	_
can	rhetorical	_
be	_	_
found	_	_
in	_	_
Tab	_	_
.	_	_

#236
4	_	_
and	_	_
5	_	_
.	_	_

#237
In	_	_
both	_	_
cases	_	_
,	_	_
we	_	_
see	_	_
consistent	_	_
improvements	_	_
over	_	_
the	_	_
recurrent	_	_
neural	_	_
network	_	_
language	_	_
model	_	_
baseline	_	_
,	_	_
improving	_	_
the	_	_
Arabic→English	_	_
system	_	_
by	_	_
0.6	_	_
BLEU	_	_
and	_	_
0.5	_	_
TER	_	_
on	_	_
test1	_	_
.	_	_

#238
This	_	_
can	feasibility	_
be	_	_
compared	_	_
to	_	_
the	_	_
rescoring	_	_
results	_	_
for	_	_
the	_	_
same	_	_
task	_	_
reported	_	_
by	_	_
(	_	_
Devlin	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
,	_	_
where	_	_
they	_	_
achieved	_	_
0.3	_	_
BLEU	_	_
,	_	_
despite	_	_
the	_	_
fact	_	_
that	_	_
they	_	_
used	_	_
multiple	_	_
references	_	_
for	_	_
scoring	_	_
,	_	_
whereas	_	_
in	_	_
our	_	_
experiments	_	_
we	_	_
rely	_	_
on	_	_
a	_	_
single	_	_
reference	_	_
only	_	_
.	_	_

#239
The	_	_
models	_	_
are	_	_
also	_	_
able	_	_
to	_	_
improve	_	_
the	_	_
Chinese→English	_	_
system	_	_
by	_	_
0.5	_	_
BLEU	_	_
and	_	_
0.5	_	_
TER	_	_
on	_	_
test2	_	_
.	_	_