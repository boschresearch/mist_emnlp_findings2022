#9
Most	_	_
previous	_	_
work	_	_
assumed	_	_
that	_	_
each	_	_
instance	_	_
is	_	_
best	_	_
labeled	_	_
with	_	_
a	_	_
single	_	_
sense	_	_
,	_	_
and	_	_
therefore	_	_
,	_	_
that	_	_
each	_	_
instance	_	_
belongs	_	_
to	_	_
exactly	_	_
one	_	_
sense	_	_
cluster	_	_
.	_	_

#10
However	_	_
,	_	_
recent	_	_
work	_	_
(	_	_
Erk	_	_
and	_	_
McCarthy	_	_
,	_	_
2009	_	_
;	_	_
Jurgens	_	_
,	_	_
2013	_	_
)	_	_
has	_	_
shown	_	_
that	_	_
more	_	_
than	_	_
one	_	_
sense	_	_
can	feasibility	_
be	_	_
used	_	_
to	_	_
interpret	_	_
certain	_	_
instances	_	_
,	_	_
due	_	_
to	_	_
context	_	_
ambiguity	_	_
and	_	_
sense	_	_
relatedness	_	_
.	_	_

#11
To	_	_
handle	_	_
these	_	_
characteristics	_	_
of	_	_
WSI	_	_
(	_	_
unsupervised	_	_
,	_	_
senses	_	_
represented	_	_
by	_	_
token	_	_
clusters	_	_
,	_	_
multiple	_	_
senses	_	_
per	_	_
instance	_	_
)	_	_
,	_	_
we	_	_
consider	_	_
approaches	_	_
based	_	_
on	_	_
topic	_	_
models	_	_
.	_	_

#17
One	_	_
possible	_	_
approach	_	_
would	_	_
be	_	_
to	_	_
only	_	_
keep	_	_
the	_	_
local	_	_
context	_	_
of	_	_
each	_	_
ambiguous	_	_
word	_	_
,	_	_
discarding	_	_
the	_	_
global	_	_
context	_	_
.	_	_

#18
However	_	_
,	_	_
the	_	_
topical	_	_
information	_	_
contained	_	_
in	_	_
the	_	_
broader	_	_
context	_	_
,	_	_
though	_	_
it	_	_
may	options	concessive-negation
not	_	_
determine	_	_
the	_	_
sense	_	_
directly	_	_
,	_	_
might	speculation	_
still	_	_
be	_	_
useful	_	_
for	_	_
narrowing	_	_
down	_	_
the	_	_
likely	_	_
senses	_	_
of	_	_
the	_	_
ambiguous	_	_
word	_	_
.	_	_

#19
Consider	_	_
the	_	_
ambiguous	_	_
word	_	_
cold	_	_
.	_	_

#20
In	_	_
the	_	_
sentence	_	_
“His	_	_
reaction	_	_
to	_	_
the	_	_
experiments	_	_
was	_	_
cold”	_	_
,	_	_
the	_	_
possible	_	_
senses	_	_
for	_	_
cold	_	_
include	_	_
cold	_	_
temperature	_	_
,	_	_
a	_	_
cold	_	_
sensation	_	_
,	_	_
common	_	_
cold	_	_
,	_	_
or	_	_
a	_	_
negative	_	_
emotional	_	_
reaction	_	_
.	_	_

#21
However	_	_
,	_	_
if	_	_
we	_	_
know	_	_
that	_	_
the	_	_
topic	_	_
of	_	_
the	_	_
document	_	_
concerns	_	_
the	_	_
effects	_	_
of	_	_
low	_	_
temperatures	_	_
on	_	_
physical	_	_
health	_	_
,	_	_
then	_	_
the	_	_
negative	_	_
emotional	_	_
reaction	_	_
sense	_	_
should	inference	_
become	_	_
less	_	_
likely	_	_
.	_	_

#22
Therefore	_	_
,	_	_
in	_	_
this	_	_
case	_	_
,	_	_
knowing	_	_
the	_	_
topic	_	_
helps	_	_
narrow	_	_
down	_	_
the	_	_
set	_	_
of	_	_
plausible	_	_
senses	_	_
.	_	_

#23
At	_	_
the	_	_
same	_	_
time	_	_
,	_	_
knowing	_	_
the	_	_
sense	_	_
can	capability	_
also	_	_
help	_	_
determine	_	_
possible	_	_
topics	_	_
.	_	_

#24
Consider	_	_
a	_	_
set	_	_
of	_	_
texts	_	_
that	_	_
all	_	_
include	_	_
the	_	_
word	_	_
cold	_	_
.	_	_

#25
Without	_	_
further	_	_
information	_	_
,	_	_
the	_	_
texts	_	_
might	options	_
discuss	_	_
any	_	_
of	_	_
a	_	_
number	_	_
of	_	_
possible	_	_
topics	_	_
.	_	_

#26
However	_	_
,	_	_
if	_	_
the	_	_
sense	_	_
of	_	_
cold	_	_
is	_	_
that	_	_
of	_	_
cold	_	_
ischemia	_	_
,	_	_
then	_	_
the	_	_
most	_	_
probable	_	_
topics	_	_
would	_	_
be	_	_
those	_	_
related	_	_
to	_	_
organ	_	_
transplantation	_	_
.	_	_

#36
WSD	_	_
seeks	_	_
to	_	_
assign	_	_
a	_	_
particular	_	_
sense	_	_
label	_	_
to	_	_
each	_	_
target	_	_
word	_	_
instance	_	_
,	_	_
where	_	_
the	_	_
sense	_	_
labels	_	_
are	_	_
known	_	_
and	_	_
usually	_	_
drawn	_	_
from	_	_
an	_	_
existing	_	_
sense	_	_
inventory	_	_
like	_	_
WordNet	_	_
(	_	_
Miller	_	_
et	_	_
al.	_	_
,	_	_
1990	_	_
)	_	_
.	_	_

#37
Although	_	_
extensive	_	_
research	_	_
has	_	_
been	_	_
devoted	_	_
to	_	_
WSD	_	_
,	_	_
WSI	_	_
may	capability-speculation	_
be	_	_
more	_	_
useful	_	_
for	_	_
downstream	_	_
tasks	_	_
.	_	_

#38
WSD	_	_
relies	_	_
on	_	_
sense	_	_
inventories	_	_
whose	_	_
construction	_	_
is	_	_
time-intensive	_	_
,	_	_
expensive	_	_
,	_	_
and	_	_
subject	_	_
to	_	_
poor	_	_
inter-annotator	_	_
agreement	_	_
(	_	_
Passonneau	_	_
et	_	_
al.	_	_
,	_	_
2010	_	_
)	_	_
.	_	_

#39
Sense	_	_
inventories	_	_
also	_	_
impose	_	_
a	_	_
fixed	_	_
sense	_	_
granularity	_	_
for	_	_
each	_	_
ambiguous	_	_
word	_	_
,	_	_
which	_	_
may	options	negation
not	_	_
match	_	_
the	_	_
ideal	_	_
granularity	_	_
for	_	_
the	_	_
task	_	_
of	_	_
interest	_	_
.	_	_

#40
Finally	_	_
,	_	_
they	_	_
may	options	_
lack	_	_
domain-specific	_	_
senses	_	_
and	_	_
are	_	_
difficult	_	_
to	_	_
adapt	_	_
to	_	_
low-resource	_	_
domains	_	_
or	_	_
languages	_	_
.	_	_

#41
In	_	_
contrast	_	_
,	_	_
senses	_	_
induced	_	_
by	_	_
WSI	_	_
are	_	_
more	_	_
likely	_	_
to	_	_
represent	_	_
the	_	_
task	_	_
and	_	_
domain	_	_
of	_	_
interest	_	_
.	_	_

#42
Researchers	_	_
in	_	_
machine	_	_
translation	_	_
and	_	_
information	_	_
retrieval	_	_
have	_	_
found	_	_
that	_	_
predefined	_	_
senses	_	_
are	_	_
often	_	_
not	_	_
well-suited	_	_
for	_	_
these	_	_
tasks	_	_
(	_	_
Voorhees	_	_
,	_	_
1993	_	_
;	_	_
Carpuat	_	_
and	_	_
Wu	_	_
,	_	_
2005	_	_
)	_	_
,	_	_
while	_	_
induced	_	_
senses	_	_
can	options	_
lead	_	_
to	_	_
improved	_	_
performance	_	_
(	_	_
Véronis	_	_
,	_	_
2004	_	_
;	_	_
Vickrey	_	_
et	_	_
al.	_	_
,	_	_
2005	_	_
;	_	_
Carpuat	_	_
and	_	_
Wu	_	_
,	_	_
2007	_	_
)	_	_
.	_	_

#43
Topic	_	_
Modeling	_	_
for	_	_
WSI	_	_
:	_	_
Brody	_	_
and	_	_
Lapata	_	_
(	_	_
2009	_	_
)	_	_
proposed	_	_
a	_	_
topic	_	_
model	_	_
that	_	_
uses	_	_
a	_	_
weighted	_	_
combination	_	_
of	_	_
separate	_	_
LDA	_	_
models	_	_
based	_	_
on	_	_
different	_	_
feature	_	_
sets	_	_
(	_	_
e.g	_	_
.	_	_
word	_	_
tokens	_	_
,	_	_
parts	_	_
of	_	_
speech	_	_
,	_	_
and	_	_
dependency	_	_
relations	_	_
)	_	_
.	_	_

#44
They	_	_
only	_	_
used	_	_
smaller	_	_
units	_	_
of	_	_
text	_	_
surrounding	_	_
the	_	_
ambiguous	_	_
word	_	_
,	_	_
discarding	_	_
the	_	_
global	_	_
context	_	_
of	_	_
each	_	_
instance	_	_
.	_	_

#45
Yao	_	_
and	_	_
Van	_	_
Durme	_	_
(	_	_
2011	_	_
)	_	_
proposed	_	_
a	_	_
model	_	_
based	_	_
on	_	_
a	_	_
hierarchical	_	_
Dirichlet	_	_
process	_	_
(	_	_
HDP	_	_
;	_	_
Teh	_	_
et	_	_
al.	_	_
,	_	_
2006	_	_
)	_	_
,	_	_
which	_	_
has	_	_
the	_	_
advantage	_	_
that	_	_
it	_	_
can	capability	_
automatically	_	_
discover	_	_
the	_	_
number	_	_
of	_	_
senses	_	_
.	_	_

#46
Lau	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2012	_	_
)	_	_
described	_	_
a	_	_
model	_	_
based	_	_
on	_	_
an	_	_
HDP	_	_
with	_	_
positional	_	_
word	_	_
features	_	_
;	_	_
it	_	_
formed	_	_
the	_	_
basis	_	_
for	_	_
their	_	_
submission	_	_
(	_	_
unimelb	_	_
,	_	_
Lau	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
)	_	_
to	_	_
the	_	_
SemEval-	_	_
2013	_	_
WSI	_	_
task	_	_
(	_	_
Jurgens	_	_
and	_	_
Klapaftis	_	_
,	_	_
2013	_	_
)	_	_
.	_	_

#56
They	_	_
have	_	_
been	_	_
useful	_	_
as	_	_
features	_	_
in	_	_
many	_	_
NLP	_	_
tasks	_	_
(	_	_
Turian	_	_
et	_	_
al.	_	_
,	_	_
2010	_	_
;	_	_
Collobert	_	_
et	_	_
al.	_	_
,	_	_
2011	_	_
;	_	_
Dhillon	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
;	_	_
Hisamoto	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
;	_	_
Bansal	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
.	_	_

#57
The	_	_
similarity	_	_
between	_	_
two	_	_
words	_	_
can	feasibility	_
be	_	_
computed	_	_
using	_	_
cosine	_	_
similarity	_	_
of	_	_
their	_	_
embedding	_	_
vectors	_	_
.	_	_

#58
Word	_	_
embeddings	_	_
are	_	_
often	_	_
also	_	_
used	_	_
to	_	_
build	_	_
representations	_	_
for	_	_
larger	_	_
units	_	_
of	_	_
text	_	_
,	_	_
such	_	_
as	_	_
sentences	_	_
,	_	_
through	_	_
vector	_	_
operations	_	_
(	_	_
e.g.	_	_
,	_	_
summation	_	_
)	_	_
applied	_	_
to	_	_
the	_	_
vector	_	_
of	_	_
each	_	_
token	_	_
in	_	_
the	_	_
sentence	_	_
.	_	_

#65
For	_	_
each	_	_
target	_	_
word	_	_
,	_	_
we	_	_
have	_	_
a	_	_
set	_	_
of	_	_
instances	_	_
.	_	_

#66
Each	_	_
instance	_	_
provides	_	_
context	_	_
for	_	_
a	_	_
single	_	_
occurrence	_	_
of	_	_
the	_	_
target	_	_
word.1	_	_
For	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
use	_	_
the	_	_
1The	_	_
target	_	_
word	_	_
token	_	_
may	options	_
occur	_	_
multiple	_	_
times	_	_
in	_	_
an	_	_
instance	_	_
,	_	_
but	_	_
only	_	_
one	_	_
occurrence	_	_
is	_	_
chosen	_	_
as	_	_
the	_	_
target	_	_
word	_	_
occurrence	_	_
.	_	_

#67
Figure	_	_
1	_	_
:	_	_
Proposed	_	_
sense-topic	_	_
model	_	_
in	_	_
plate	_	_
notation	_	_
.	_	_

#90
This	_	_
helps	_	_
to	_	_
mitigate	_	_
data	_	_
sparsity	_	_
issues	_	_
arising	_	_
from	_	_
attempting	_	_
to	_	_
estimate	_	_
high-	_	_
dimensional	_	_
distributions	_	_
from	_	_
small	_	_
datasets	_	_
.	_	_

#91
A	_	_
secondary	_	_
benefit	_	_
is	_	_
that	_	_
we	_	_
can	feasibility	_
avoid	_	_
biases	_	_
caused	_	_
by	_	_
particular	_	_
choices	_	_
of	_	_
generative	_	_
directionality	_	_
in	_	_
3We	_	_
use	_	_
Pr	_	_
(	_	_
)	_	_
for	_	_
generic	_	_
probability	_	_
distributions	_	_
without	_	_
further	_	_
qualifiers	_	_
and	_	_
Pθ	_	_
(	_	_
)	_	_
for	_	_
distributions	_	_
parameterized	_	_
by	_	_
θ	_	_
.	_	_

#92
4For	_	_
clarity	_	_
,	_	_
we	_	_
drop	_	_
the	_	_
(	_	_
i	_	_
)	_	_
superscripts	_	_
in	_	_
these	_	_
and	_	_
the	_	_
following	_	_
equations	_	_
.	_	_

#127
We	_	_
write	_	_
the	_	_
conditional	_	_
posterior	_	_
distribution	_	_
over	_	_
topics	_	_
for	_	_
global	_	_
context	_	_
word	_	_
token	_	_
i	_	_
in	_	_
instance	_	_
d	_	_
as	_	_
Pr	_	_
(	_	_
t	_	_
(	_	_
i	_	_
)	_	_
g	_	_
=	_	_
j|d	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
,	_	_
where	_	_
t	_	_
(	_	_
i	_	_
)	_	_
g	_	_
=	_	_
j	_	_
is	_	_
the	_	_
topic	_	_
assignment	_	_
of	_	_
token	_	_
i	_	_
,	_	_
d	_	_
is	_	_
the	_	_
current	_	_
instance	_	_
,	_	_
t−i	_	_
is	_	_
the	_	_
set	_	_
of	_	_
topic	_	_
assignments	_	_
of	_	_
all	_	_
word	_	_
tokens	_	_
aside	_	_
from	_	_
i	_	_
for	_	_
instance	_	_
d	_	_
,	_	_
s	_	_
is	_	_
the	_	_
set	_	_
of	_	_
sense	_	_
assignments	_	_
for	_	_
all	_	_
local	_	_
word	_	_
tokens	_	_
in	_	_
instance	_	_
d	_	_
,	_	_
and	_	_
“·”	_	_
stands	_	_
for	_	_
all	_	_
other	_	_
observed	_	_
or	_	_
known	_	_
information	_	_
,	_	_
including	_	_
all	_	_
words	_	_
,	_	_
all	_	_
Dirichlet	_	_
hyperparameters	_	_
,	_	_
and	_	_
all	_	_
latent	_	_
variable	_	_
assignments	_	_
in	_	_
other	_	_
instances	_	_
.	_	_

#128
The	_	_
conditional	_	_
posterior	_	_
can	feasibility-rhetorical	_
be	_	_
computed	_	_
by	_	_
:	_	_
Pr	_	_
(	_	_
t	_	_
(	_	_
i	_	_
)	_	_
g	_	_
=	_	_
j|d	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
∝	_	_
CDTdj	_	_
+	_	_
α∑T	_	_
k=1	_	_
C	_	_
DT	_	_
dk	_	_
+	_	_
Tα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
t=j|d	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
CWTij	_	_
+	_	_
α∑Wt	_	_
k′=1	_	_
C	_	_
WT	_	_
k′j	_	_
+Wtα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
w	_	_
(	_	_
i	_	_
)	_	_
g	_	_
|t=j	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
(	_	_
4	_	_
)	_	_
where	_	_
we	_	_
use	_	_
the	_	_
superscriptDT	_	_
as	_	_
a	_	_
mnemonic	_	_
for	_	_
“instance/topic”	_	_
when	_	_
counting	_	_
topic	_	_
assignments	_	_
in	_	_
an	_	_
instance	_	_
and	_	_
WT	_	_
for	_	_
“word/topic”	_	_
when	_	_
counting	_	_
topic	_	_
assignments	_	_
for	_	_
a	_	_
word	_	_
.	_	_

#129
CDTdj	_	_
contains	_	_
the	_	_
number	_	_
of	_	_
times	_	_
topic	_	_
j	_	_
is	_	_
assigned	_	_
to	_	_
some	_	_
word	_	_
token	_	_
in	_	_
instance	_	_
d	_	_
,	_	_
excluding	_	_
the	_	_
current	_	_
word	_	_
token	_	_
w	_	_
(	_	_
i	_	_
)	_	_
g	_	_
;	_	_
CWTij	_	_
is	_	_
the	_	_
number	_	_
of	_	_
times	_	_
word	_	_
w	_	_
(	_	_
i	_	_
)	_	_
g	_	_
is	_	_
assigned	_	_
to	_	_
topic	_	_
j	_	_
,	_	_
across	_	_
all	_	_
instances	_	_
,	_	_
excluding	_	_
the	_	_
current	_	_
word	_	_
token	_	_
.	_	_

#131
We	_	_
show	_	_
the	_	_
corresponding	_	_
conditional	_	_
posterior	_	_
probabilities	_	_
underneath	_	_
each	_	_
term	_	_
;	_	_
the	_	_
count	_	_
ratios	_	_
are	_	_
obtained	_	_
using	_	_
standard	_	_
Dirichlet-multinomial	_	_
collapsing	_	_
.	_	_

#132
The	_	_
conditional	_	_
posterior	_	_
distribution	_	_
over	_	_
topic/sense	_	_
pairs	_	_
for	_	_
a	_	_
local	_	_
context	_	_
word	_	_
token	_	_
w	_	_
(	_	_
i	_	_
)	_	_
`	_	_
can	feasibility-rhetorical	_
be	_	_
computed	_	_
by	_	_
:	_	_
Pr	_	_
(	_	_
t	_	_
(	_	_
i	_	_
)	_	_
`	_	_
=	_	_
j	_	_
,	_	_
s	_	_
(	_	_
i	_	_
)	_	_
`	_	_
=	_	_
k|d	_	_
,	_	_
t−i	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
∝	_	_
CDTdj	_	_
+	_	_
α∑T	_	_
k′=1	_	_
C	_	_
DT	_	_
dk′	_	_
+	_	_
Tα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
t=j|d	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
CWTij	_	_
+	_	_
α∑Wt	_	_
k′=1	_	_
C	_	_
WT	_	_
k′j	_	_
+Wtα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
w	_	_
(	_	_
i	_	_
)	_	_
`	_	_
|t=j	_	_
,	_	_
t−i	_	_
,	_	_
s	_	_
,	_	_
·	_	_
)	_	_
CDSdk	_	_
+	_	_
α∑S	_	_
k′=1	_	_
C	_	_
DS	_	_
dk′	_	_
+	_	_
Sα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
s=k|d	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
CWSik	_	_
+	_	_
α∑Ws	_	_
k′=1	_	_
C	_	_
WS	_	_
k′k	_	_
+Wsα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
w	_	_
(	_	_
i	_	_
)	_	_
`	_	_
|s=k	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
CSTkj	_	_
+	_	_
α∑S	_	_
k′=1	_	_
C	_	_
ST	_	_
k′j	_	_
+	_	_
Sα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
s=k|t=j	_	_
,	_	_
t−i	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
CSTkj	_	_
+	_	_
α∑T	_	_
k′=1	_	_
C	_	_
ST	_	_
kk′	_	_
+	_	_
Tα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
t=j|s=k	_	_
,	_	_
t−i	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
CSTkj	_	_
+	_	_
α∑S	_	_
k′=1	_	_
∑T	_	_
j′=1	_	_
C	_	_
ST	_	_
k′j′	_	_
+	_	_
STα︸	_	_
︷︷	_	_
︸	_	_
Pr	_	_
(	_	_
s=k	_	_
,	_	_
t=j|t−i	_	_
,	_	_
s−i	_	_
,	_	_
·	_	_
)	_	_
(	_	_
5	_	_
)	_	_
where	_	_
CDSdk	_	_
contains	_	_
the	_	_
number	_	_
of	_	_
times	_	_
sense	_	_
k	_	_
is	_	_
assigned	_	_
to	_	_
some	_	_
local	_	_
word	_	_
token	_	_
in	_	_
instance	_	_
d	_	_
,	_	_
excluding	_	_
the	_	_
current	_	_
word	_	_
token	_	_
;	_	_
CWSik	_	_
contains	_	_
the	_	_
number	_	_
of	_	_
time	_	_
word	_	_
w	_	_
(	_	_
i	_	_
)	_	_
`	_	_
is	_	_
assigned	_	_
to	_	_
sense	_	_
k	_	_
,	_	_
excluding	_	_
the	_	_
current	_	_
time	_	_
;	_	_
CSTkj	_	_
contains	_	_
the	_	_
number	_	_
of	_	_
times	_	_
sense	_	_
k	_	_
and	_	_
topic	_	_
j	_	_
are	_	_
assigned	_	_
to	_	_
some	_	_
local	_	_
word	_	_
tokens	_	_
.	_	_

#133
Ws	_	_
is	_	_
the	_	_
number	_	_
of	_	_
distinct	_	_
local	_	_
context	_	_
word	_	_
types	_	_
across	_	_
the	_	_
collection	_	_
.	_	_

#180
Model	_	_
B-cubed	_	_
(	_	_
%	_	_
)	_	_
NMI	_	_
(	_	_
%	_	_
)	_	_
AVG	_	_
Drop	_	_
s→	_	_
t	_	_
52.1	_	_
6.84	_	_
18.88	_	_
Drop	_	_
t→	_	_
s	_	_
51.1	_	_
6.78	_	_
18.61	_	_
Full	_	_
53.5	_	_
6.96	_	_
19.30	_	_
Table	_	_
3	_	_
:	_	_
Performance	_	_
on	_	_
TEST	_	_
for	_	_
the	_	_
sense-topic	_	_
model	_	_
with	_	_
ablation	_	_
of	_	_
links	_	_
between	_	_
sense	_	_
and	_	_
topic	_	_
variables	_	_
.	_	_

#181
rich	_	_
the	_	_
instances	_	_
,	_	_
we	_	_
can	feasibility	_
have	_	_
more	_	_
robust	_	_
co-	_	_
occurrence	_	_
statistics	_	_
.	_	_

#182
The	_	_
SemEval-2013	_	_
dataset	_	_
may	speculation	_
be	_	_
too	_	_
small	_	_
to	_	_
induce	_	_
meaningful	_	_
senses	_	_
,	_	_
since	_	_
there	_	_
are	_	_
only	_	_
about	_	_
100	_	_
instances	_	_
for	_	_
each	_	_
target	_	_
word	_	_
,	_	_
and	_	_
each	_	_
instance	_	_
only	_	_
contains	_	_
one	_	_
sentence	_	_
.	_	_

#183
This	_	_
is	_	_
why	_	_
most	_	_
shared	_	_
task	_	_
systems	_	_
added	_	_
instances	_	_
from	_	_
external	_	_
corpora	_	_
.	_	_

#190
This	_	_
will	_	_
introduce	_	_
more	_	_
word	_	_
tokens	_	_
into	_	_
the	_	_
set	_	_
of	_	_
global	_	_
context	_	_
words	_	_
,	_	_
while	_	_
keeping	_	_
the	_	_
set	_	_
of	_	_
local	_	_
context	_	_
words	_	_
mostly	_	_
unchanged	_	_
,	_	_
as	_	_
the	_	_
window	_	_
size	_	_
we	_	_
use	_	_
is	_	_
typically	_	_
smaller	_	_
than	_	_
the	_	_
length	_	_
of	_	_
the	_	_
original	_	_
instance	_	_
.	_	_

#191
With	_	_
more	_	_
global	_	_
context	_	_
words	_	_
,	_	_
the	_	_
model	_	_
has	_	_
more	_	_
evidence	_	_
to	_	_
learn	_	_
coherent	_	_
topics	_	_
,	_	_
which	_	_
could	capability-speculation	_
also	_	_
improve	_	_
the	_	_
induced	_	_
senses	_	_
via	_	_
the	_	_
connection	_	_
between	_	_
sense	_	_
and	_	_
topic	_	_
.	_	_

#192
The	_	_
ideal	_	_
way	_	_
of	_	_
enriching	_	_
context	_	_
for	_	_
an	_	_
instance	_	_
is	_	_
to	_	_
add	_	_
its	_	_
actual	_	_
context	_	_
from	_	_
the	_	_
corpus	_	_
from	_	_
which	_	_
it	_	_
was	_	_
extracted	_	_
.	_	_

#194
While	_	_
not	_	_
provided	_	_
for	_	_
the	_	_
SemEval	_	_
task	_	_
,	_	_
it	_	_
is	_	_
reasonable	_	_
to	_	_
assume	_	_
this	_	_
larger	_	_
context	_	_
in	_	_
many	_	_
real-world	_	_
applications	_	_
,	_	_
such	_	_
as	_	_
information	_	_
retrieval	_	_
and	_	_
machine	_	_
translation	_	_
of	_	_
documents	_	_
.	_	_

#195
However	_	_
,	_	_
in	_	_
other	_	_
settings	_	_
,	_	_
the	_	_
corpus	_	_
may	options	_
only	_	_
have	_	_
a	_	_
single	_	_
sentence	_	_
containing	_	_
the	_	_
target	_	_
word	_	_
(	_	_
e.g.	_	_
,	_	_
search	_	_
queries	_	_
or	_	_
machine	_	_
translation	_	_
of	_	_
sentences	_	_
)	_	_
.	_	_

#196
To	_	_
address	_	_
this	_	_
,	_	_
we	_	_
find	_	_
a	_	_
semanticallysimilar	_	_
sentence	_	_
from	_	_
the	_	_
English	_	_
ukWac	_	_
corpus	_	_
and	_	_
append	_	_
it	_	_
to	_	_
the	_	_
instance	_	_
as	_	_
additional	_	_
context	_	_
.	_	_

#199
To	_	_
compute	_	_
similarity	_	_
,	_	_
we	_	_
first	_	_
represent	_	_
instances	_	_
and	_	_
ukWac	_	_
sentences	_	_
by	_	_
summing	_	_
the	_	_
word	_	_
embeddings	_	_
across	_	_
their	_	_
word	_	_
tokens	_	_
,	_	_
then	_	_
compute	_	_
cosine	_	_
similarity	_	_
.	_	_

#200
The	_	_
ukWac	_	_
sentence	_	_
(	_	_
s∗	_	_
)	_	_
with	_	_
the	_	_
highest	_	_
cosine	_	_
similarity	_	_
to	_	_
each	_	_
original	_	_
instance	_	_
(	_	_
d	_	_
)	_	_
is	_	_
appended	_	_
to	_	_
that	_	_
instance	_	_
:	_	_
s∗	_	_
=	_	_
argmaxs∈ukWac	_	_
sim	_	_
(	_	_
d	_	_
,	_	_
s	_	_
)	_	_
Results	_	_
Since	_	_
the	_	_
vocabulary	_	_
has	_	_
increased	_	_
,	_	_
we	_	_
expect	_	_
we	_	_
may	speculation	_
need	_	_
larger	_	_
values	_	_
for	_	_
S	_	_
and	_	_
T	_	_
.	_	_

#201
On	_	_
TRIAL	_	_
,	_	_
we	_	_
find	_	_
best	_	_
performance	_	_
for	_	_
S	_	_
=	_	_
10	_	_
,	_	_
so	_	_
we	_	_
run	_	_
on	_	_
TEST	_	_
with	_	_
this	_	_
value	_	_
.	_	_

#205
It	_	_
is	_	_
unsurprising	_	_
that	_	_
we	_	_
find	_	_
best	_	_
performance	_	_
with	_	_
actual	_	_
context	_	_
.	_	_

#206
Interestingly	_	_
,	_	_
however	_	_
,	_	_
we	_	_
can	feasibility	_
achieve	_	_
almost	_	_
the	_	_
same	_	_
gains	_	_
when	_	_
automatically	_	_
finding	_	_
relevant	_	_
context	_	_
from	_	_
a	_	_
different	_	_
corpus	_	_
.	_	_

#207
Thus	_	_
,	_	_
even	_	_
in	_	_
real-world	_	_
settings	_	_
where	_	_
we	_	_
only	_	_
have	_	_
a	_	_
single	_	_
sentence	_	_
of	_	_
context	_	_
,	_	_
we	_	_
can	feasibility	_
induce	_	_
substantially	_	_
better	_	_
senses	_	_
by	_	_
automatically	_	_
broadening	_	_
the	_	_
global	_	_
context	_	_
in	_	_
an	_	_
unsupervised	_	_
manner	_	_
.	_	_

#208
As	_	_
a	_	_
comparative	_	_
experiment	_	_
,	_	_
we	_	_
also	_	_
evaluate	_	_
the	_	_
performance	_	_
of	_	_
LDA	_	_
when	_	_
adding	_	_
actual	_	_
context	_	_
(	_	_
Table	_	_
2	_	_
,	_	_
row	_	_
7	_	_
)	_	_
.	_	_

#228
When	_	_
LDA	_	_
is	_	_
run	_	_
with	_	_
the	_	_
actual	_	_
context	_	_
,	_	_
it	_	_
leaves	_	_
(	_	_
4	_	_
)	_	_
and	_	_
(	_	_
5	_	_
)	_	_
in	_	_
the	_	_
same	_	_
topic	_	_
(	_	_
i.e.	_	_
,	_	_
sense	_	_
)	_	_
,	_	_
while	_	_
assigning	_	_
(	_	_
3	_	_
)	_	_
into	_	_
another	_	_
topic	_	_
with	_	_
high	_	_
probability	_	_
.	_	_

#229
This	_	_
could	speculation	_
be	_	_
because	_	_
(	_	_
4	_	_
)	_	_
and	_	_
(	_	_
5	_	_
)	_	_
both	_	_
relate	_	_
to	_	_
child	_	_
development	_	_
,	_	_
and	_	_
therefore	_	_
LDA	_	_
considers	_	_
them	_	_
as	_	_
sharing	_	_
the	_	_
same	_	_
topic	_	_
.	_	_

#230
However	_	_
,	_	_
topic	_	_
is	_	_
not	_	_
the	_	_
same	_	_
as	_	_
sense	_	_
,	_	_
especially	_	_
when	_	_
larger	_	_
contexts	_	_
are	_	_
available	_	_
.	_	_

#232
6.2	_	_
Adding	_	_
Instances	_	_
We	_	_
also	_	_
consider	_	_
a	_	_
way	_	_
to	_	_
augment	_	_
our	_	_
dataset	_	_
with	_	_
additional	_	_
instances	_	_
from	_	_
an	_	_
external	_	_
corpus	_	_
.	_	_

#233
We	_	_
have	_	_
no	_	_
gold	_	_
standard	_	_
senses	_	_
for	_	_
these	_	_
instances	_	_
,	_	_
so	_	_
we	_	_
will	_	_
not	_	_
evaluate	_	_
our	_	_
model	_	_
on	_	_
them	_	_
;	_	_
they	_	_
are	_	_
merely	_	_
used	_	_
to	_	_
provide	_	_
richer	_	_
co-occurrence	_	_
statistics	_	_
about	_	_
the	_	_
target	_	_
word	_	_
so	_	_
that	_	_
we	_	_
can	feasibility	_
perform	_	_
better	_	_
on	_	_
the	_	_
instances	_	_
on	_	_
which	_	_
we	_	_
evaluate	_	_
.	_	_

#234
If	_	_
we	_	_
added	_	_
randomly-chosen	_	_
instances	_	_
(	_	_
containing	_	_
the	_	_
target	_	_
word	_	_
)	_	_
,	_	_
we	_	_
would	_	_
be	_	_
concerned	_	_
that	_	_
the	_	_
learned	_	_
topics	_	_
and	_	_
senses	_	_
may	speculation	negation
not	_	_
reflect	_	_
the	_	_
distributions	_	_
of	_	_
the	_	_
original	_	_
instance	_	_
set	_	_
.	_	_

#235
So	_	_
we	_	_
only	_	_
add	_	_
instances	_	_
that	_	_
are	_	_
semantically	_	_
similar	_	_
to	_	_
instances	_	_
in	_	_
our	_	_
original	_	_
set	_	_
(	_	_
Moore	_	_
and	_	_
Lewis	_	_
,	_	_
2010	_	_
;	_	_
Chambers	_	_
and	_	_
Jurafsky	_	_
,	_	_
2011	_	_
)	_	_
.	_	_