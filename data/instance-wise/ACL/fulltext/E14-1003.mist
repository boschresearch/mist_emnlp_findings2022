#11
Similar	_	_
words	_	_
are	_	_
grouped	_	_
in	_	_
the	_	_
same	_	_
sub-space	_	_
rather	_	_
than	_	_
being	_	_
treated	_	_
as	_	_
separate	_	_
entities	_	_
.	_	_

#12
Neural	_	_
network	_	_
models	_	_
can	capability-feasibility	_
be	_	_
seen	_	_
as	_	_
functions	_	_
over	_	_
continuous	_	_
representations	_	_
exploiting	_	_
the	_	_
similarity	_	_
between	_	_
words	_	_
,	_	_
thereby	_	_
making	_	_
the	_	_
estimation	_	_
of	_	_
probabilities	_	_
over	_	_
higher-	_	_
order	_	_
n-grams	_	_
easier	_	_
.	_	_

#13
However	_	_
,	_	_
feed-forward	_	_
networks	_	_
do	_	_
not	_	_
directly	_	_
address	_	_
the	_	_
limited	_	_
context	_	_
issue	_	_
either	_	_
,	_	_
since	_	_
predictions	_	_
are	_	_
based	_	_
on	_	_
a	_	_
fixed-size	_	_
context	_	_
,	_	_
similar	_	_
to	_	_
back-off	_	_
n-gram	_	_
models	_	_
.	_	_

#24
Minimum	_	_
Translation	_	_
Units	_	_
(	_	_
Quirk	_	_
and	_	_
Menezes	_	_
,	_	_
2006	_	_
;	_	_
Zhang	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
)	_	_
are	_	_
an	_	_
extension	_	_
which	_	_
additionally	_	_
permit	_	_
tuples	_	_
with	_	_
empty	_	_
source	_	_
or	_	_
target	_	_
sides	_	_
,	_	_
thereby	_	_
allowing	_	_
insertion	_	_
or	_	_
deletion	_	_
phrase	_	_
pairs	_	_
.	_	_

#25
The	_	_
two	_	_
basic	_	_
requirements	_	_
for	_	_
MTUs	_	_
are	_	_
that	_	_
there	_	_
are	_	_
no	_	_
overlapping	_	_
word	_	_
alignment	_	_
links	_	_
between	_	_
phrase	_	_
pairs	_	_
and	_	_
it	_	_
should	deontic	negation
not	_	_
be	_	_
possible	_	_
to	_	_
extract	_	_
smaller	_	_
phrase	_	_
pairs	_	_
without	_	_
violating	_	_
the	_	_
word	_	_
alignment	_	_
constraints	_	_
.	_	_

#26
Informally	_	_
,	_	_
we	_	_
can	feasibility	_
think	_	_
of	_	_
MTUs	_	_
as	_	_
small	_	_
phrase	_	_
pairs	_	_
that	_	_
can	capability-feasibility	negation
not	_	_
be	_	_
broken	_	_
down	_	_
any	_	_
further	_	_
without	_	_
violating	_	_
the	_	_
two	_	_
requirements	_	_
.	_	_

#27
Minimum	_	_
Translation	_	_
Units	_	_
partition	_	_
a	_	_
sentence	_	_
pair	_	_
into	_	_
a	_	_
set	_	_
of	_	_
minimal	_	_
bilingual	_	_
units	_	_
or	_	_
tuWords	_	_
MTUs	_	_
Tokens	_	_
34,769,416	_	_
14,853,062	_	_
Types	_	_
143,524	_	_
1,315,512	_	_
Singleton	_	_
types	_	_
34.9	_	_
%	_	_
80.1	_	_
%	_	_
Table	_	_
1	_	_
:	_	_
Token	_	_
and	_	_
type	_	_
counts	_	_
for	_	_
both	_	_
source	_	_
and	_	_
target	_	_
words	_	_
as	_	_
well	_	_
as	_	_
MTUs	_	_
based	_	_
on	_	_
the	_	_
WMT	_	_
2006	_	_
German	_	_
to	_	_
English	_	_
data	_	_
set	_	_
(	_	_
cf	_	_
.	_	_
§5	_	_
)	_	_
.	_	_

#32
Second	_	_
,	_	_
minimal	_	_
units	_	_
result	_	_
in	_	_
smaller	_	_
models	_	_
with	_	_
a	_	_
smoother	_	_
distribution	_	_
than	_	_
models	_	_
based	_	_
on	_	_
composed	_	_
units	_	_
(	_	_
Zhang	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
)	_	_
.	_	_

#33
Sentence	_	_
pairs	_	_
can	feasibility	_
be	_	_
generated	_	_
in	_	_
multiple	_	_
orders	_	_
,	_	_
such	_	_
as	_	_
left-to-right	_	_
or	_	_
right-to-left	_	_
,	_	_
either	_	_
in	_	_
source	_	_
or	_	_
target	_	_
order	_	_
.	_	_

#34
For	_	_
example	_	_
,	_	_
the	_	_
source	_	_
left-to-right	_	_
order	_	_
of	_	_
the	_	_
sentence	_	_
pair	_	_
in	_	_
Figure	_	_
1	_	_
is	_	_
simply	_	_
M1	_	_
,	_	_
M2	_	_
,	_	_
M3	_	_
,	_	_
M4	_	_
,	_	_
M5	_	_
,	_	_
while	_	_
the	_	_
target	_	_
left-to-right	_	_
order	_	_
is	_	_
M3	_	_
,	_	_
M4	_	_
,	_	_
M5	_	_
,	_	_
M1	_	_
,	_	_
M2	_	_
.	_	_

#36
For	_	_
example	_	_
,	_	_
in	_	_
Figure	_	_
1	_	_
we	_	_
place	_	_
M4	_	_
straight	_	_
after	_	_
M3	_	_
because	_	_
“the”	_	_
,	_	_
the	_	_
aligned	_	_
target	_	_
phrase	_	_
,	_	_
is	_	_
after	_	_
“held”	_	_
,	_	_
the	_	_
previous	_	_
non-null	_	_
aligned	_	_
target	_	_
phrase	_	_
.	_	_

#37
We	_	_
can	feasibility	_
straightforwardly	_	_
estimate	_	_
an	_	_
n-gram	_	_
model	_	_
over	_	_
MTUs	_	_
to	_	_
estimate	_	_
the	_	_
probability	_	_
of	_	_
a	_	_
sentence	_	_
pair	_	_
using	_	_
standard	_	_
back-off	_	_
techniques	_	_
commonly	_	_
employed	_	_
in	_	_
language	_	_
modeling	_	_
.	_	_

#38
For	_	_
example	_	_
,	_	_
a	_	_
trigram	_	_
model	_	_
in	_	_
target	_	_
left-to-right	_	_
order	_	_
factors	_	_
the	_	_
sentence	_	_
pair	_	_
in	_	_
Figure	_	_
1	_	_
as	_	_
p	_	_
(	_	_
M3	_	_
)	_	_
p	_	_
(	_	_
M4|M3	_	_
)	_	_
p	_	_
(	_	_
M5|M3	_	_
,	_	_
M4	_	_
)	_	_
p	_	_
(	_	_
M1|M4	_	_
,	_	_
M5	_	_
)	_	_
p	_	_
(	_	_
M2|M5	_	_
,	_	_
M1	_	_
)	_	_
.	_	_

#47
For	_	_
example	_	_
,	_	_
the	_	_
distributional	_	_
representations	_	_
induced	_	_
by	_	_
recurrent	_	_
neural	_	_
networks	_	_
have	_	_
been	_	_
found	_	_
to	_	_
have	_	_
interesting	_	_
syntactic	_	_
and	_	_
semantic	_	_
regularities	_	_
(	_	_
Mikolov	_	_
et	_	_
al.	_	_
,	_	_
2013	_	_
)	_	_
.	_	_

#48
Furthermore	_	_
,	_	_
these	_	_
representations	_	_
can	feasibility	_
be	_	_
exploited	_	_
to	_	_
estimate	_	_
more	_	_
reliable	_	_
statistics	_	_
over	_	_
higher-order	_	_
n-grams	_	_
than	_	_
with	_	_
discrete	_	_
word	_	_
units	_	_
.	_	_

#49
Recurrent	_	_
neural	_	_
networks	_	_
go	_	_
beyond	_	_
fixed-size	_	_
contexts	_	_
and	_	_
allow	_	_
the	_	_
model	_	_
to	_	_
keep	_	_
track	_	_
of	_	_
long-span	_	_
dependencies	_	_
that	_	_
are	_	_
important	_	_
for	_	_
future	_	_
predictions	_	_
.	_	_

#75
4	_	_
Bag-of-words	_	_
MTU	_	_
RNN	_	_
Model	_	_
The	_	_
previous	_	_
model	_	_
treats	_	_
MTUs	_	_
as	_	_
atomic	_	_
symbols	_	_
which	_	_
leads	_	_
to	_	_
large	_	_
vocabularies	_	_
requiring	_	_
large	_	_
parameter	_	_
sets	_	_
and	_	_
expensive	_	_
inference	_	_
.	_	_

#76
However	_	_
,	_	_
similar	_	_
MTUs	_	_
may	options	_
share	_	_
the	_	_
same	_	_
words	_	_
,	_	_
or	_	_
words	_	_
which	_	_
are	_	_
related	_	_
in	_	_
continuous	_	_
space	_	_
.	_	_

#77
The	_	_
atomic	_	_
MTU	_	_
model	_	_
does	_	_
not	_	_
exploit	_	_
this	_	_
since	_	_
it	_	_
can	capability	negation
not	_	_
access	_	_
the	_	_
internal	_	_
structure	_	_
of	_	_
a	_	_
minimal	_	_
unit	_	_
.	_	_

#78
The	_	_
approach	_	_
we	_	_
pursue	_	_
next	_	_
is	_	_
to	_	_
break	_	_
MTUs	_	_
into	_	_
individual	_	_
source	_	_
and	_	_
target	_	_
words	_	_
(	_	_
Le	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
in	_	_
order	_	_
to	_	_
exploit	_	_
structural	_	_
similarities	_	_
between	_	_
infrequently	_	_
observed	_	_
minimal	_	_
units	_	_
.	_	_

#81
The	_	_
input	_	_
layer	_	_
of	_	_
this	_	_
model	_	_
accepts	_	_
the	_	_
current	_	_
minimal	_	_
unit	_	_
as	_	_
a	_	_
KofN	_	_
vector	_	_
representing	_	_
K	_	_
source	_	_
and	_	_
target	_	_
words	_	_
as	_	_
opposed	_	_
to	_	_
the	_	_
1-of-N	_	_
encoding	_	_
of	_	_
entire	_	_
MTUs	_	_
in	_	_
the	_	_
previous	_	_
model	_	_
(	_	_
Figure	_	_
4	_	_
)	_	_
.	_	_

#82
Larger	_	_
MTUs	_	_
may	options	_
contain	_	_
the	_	_
same	_	_
word	_	_
more	_	_
than	_	_
once	_	_
and	_	_
we	_	_
simply	_	_
adjust	_	_
their	_	_
count	_	_
to	_	_
one.3	_	_
Different	_	_
to	_	_
the	_	_
2Applying	_	_
the	_	_
same	_	_
technique	_	_
would	_	_
likely	_	_
result	_	_
in	_	_
too	_	_
many	_	_
collisions	_	_
since	_	_
we	_	_
are	_	_
dealing	_	_
with	_	_
multi-word	_	_
units	_	_
instead	_	_
of	_	_
single	_	_
words	_	_
.	_	_

#83
3We	_	_
found	_	_
no	_	_
effect	_	_
on	_	_
accuracy	_	_
when	_	_
using	_	_
the	_	_
unmodified	_	_
count	_	_
in	_	_
initial	_	_
experiments	_	_
.	_	_

#93
The	_	_
word	_	_
layer	_	_
is	_	_
connected	_	_
to	_	_
a	_	_
convolutional	_	_
output	_	_
layer	_	_
yt	_	_
by	_	_
weights	_	_
summarized	_	_
in	_	_
the	_	_
sparse	_	_
matrix	_	_
C.	_	_
The	_	_
output	_	_
layer	_	_
represents	_	_
all	_	_
possible	_	_
next	_	_
minimal	_	_
units	_	_
,	_	_
where	_	_
each	_	_
MTU	_	_
entry	_	_
is	_	_
only	_	_
connected	_	_
to	_	_
neurons	_	_
in	_	_
the	_	_
word	_	_
layer	_	_
representing	_	_
its	_	_
source	_	_
and	_	_
target	_	_
words	_	_
.	_	_

#94
The	_	_
word	_	_
and	_	_
MTU	_	_
layers	_	_
are	_	_
then	_	_
computed	_	_
as	_	_
follows	_	_
:	_	_
wt	_	_
=	_	_
s	_	_
(	_	_
Vht	_	_
)	_	_
yt	_	_
=	_	_
g	_	_
(	_	_
Cwt	_	_
)	_	_
However	_	_
,	_	_
there	_	_
are	_	_
a	_	_
number	_	_
of	_	_
computational	_	_
issues	_	_
with	_	_
this	_	_
model	_	_
:	_	_
First	_	_
,	_	_
we	_	_
can	feasibility	negation
not	_	_
efficiently	_	_
factor	_	_
the	_	_
word	_	_
layer	_	_
wt	_	_
into	_	_
classes	_	_
such	_	_
as	_	_
for	_	_
the	_	_
atomic	_	_
MTU	_	_
RNN	_	_
model	_	_
because	_	_
we	_	_
require	_	_
all	_	_
its	_	_
activations	_	_
to	_	_
compute	_	_
the	_	_
MTU	_	_
output	_	_
layer	_	_
yt	_	_
.	_	_

#95
This	_	_
reduces	_	_
the	_	_
best	_	_
case	_	_
complexity	_	_
of	_	_
computing	_	_
the	_	_
word	_	_
layer	_	_
from	_	_
O	_	_
(	_	_
√|V	_	_
|	_	_
)	_	_
back	_	_
to	_	_
linear	_	_
in	_	_
the	_	_
number	_	_
of	_	_
source	_	_
and	_	_
target	_	_
words	_	_
|V	_	_
|	_	_
.	_	_

#107
into	_	_
probabilities	_	_
easier	_	_
.	_	_

#108
Furthermore	_	_
,	_	_
the	_	_
output	_	_
layer	_	_
can	feasibility	_
be	_	_
factorized	_	_
into	_	_
classes	_	_
requiring	_	_
only	_	_
a	_	_
fraction	_	_
of	_	_
the	_	_
neurons	_	_
to	_	_
be	_	_
computed	_	_
,	_	_
a	_	_
much	_	_
more	_	_
efficient	_	_
solution	_	_
compared	_	_
to	_	_
the	_	_
original	_	_
model	_	_
which	_	_
required	_	_
calculation	_	_
of	_	_
the	_	_
entire	_	_
output	_	_
layer	_	_
.	_	_

#109
The	_	_
simplified	_	_
model	_	_
computes	_	_
the	_	_
probability	_	_
of	_	_
the	_	_
next	_	_
MTU	_	_
mt+1	_	_
as	_	_
a	_	_
product	_	_
of	_	_
individual	_	_
word	_	_
probabilities	_	_
:	_	_
p	_	_
(	_	_
mt+1|mtt−n+1	_	_
,	_	_
ht	_	_
)	_	_
=	_	_
(	_	_
1	_	_
)	_	_
∏	_	_
a1	_	_
,	_	_
...	_	_
,	_	_
au∈mt+1	_	_
p	_	_
(	_	_
ck|mtt−n+1	_	_
,	_	_
ht	_	_
)	_	_
p	_	_
(	_	_
ak|ck	_	_
,	_	_
mtt−n+1	_	_
,	_	_
ht	_	_
)	_	_
where	_	_
we	_	_
predict	_	_
a	_	_
sequence	_	_
of	_	_
source	_	_
and	_	_
target	_	_
words	_	_
a1	_	_
,	_	_
.	_	_

#155
The	_	_
atomic	_	_
MTU	_	_
RNN	_	_
model	_	_
improves	_	_
over	_	_
the	_	_
n-gram	_	_
MTU	_	_
model	_	_
on	_	_
all	_	_
test	_	_
sets	_	_
for	_	_
German	_	_
to	_	_
English	_	_
,	_	_
however	_	_
,	_	_
for	_	_
French	_	_
to	_	_
English	_	_
the	_	_
back-off	_	_
model	_	_
performs	_	_
better	_	_
on	_	_
two	_	_
out	_	_
of	_	_
four	_	_
test	_	_
sets	_	_
.	_	_

#156
The	_	_
next	_	_
question	_	_
we	_	_
answer	_	_
is	_	_
if	_	_
breaking	_	_
MTUs	_	_
into	_	_
individual	_	_
units	_	_
to	_	_
leverage	_	_
similarities	_	_
in	_	_
the	_	_
internal	_	_
structure	_	_
can	capability	_
help	_	_
accuracy	_	_
.	_	_

#157
The	_	_
results	_	_
(	_	_
Table	_	_
2	_	_
and	_	_
Table	_	_
3	_	_
)	_	_
for	_	_
the	_	_
bag-of-words	_	_
model	_	_
(	_	_
BoW	_	_
MTU	_	_
RNN	_	_
)	_	_
clearly	_	_
show	_	_
that	_	_
this	_	_
is	_	_
the	_	_
case	_	_
for	_	_
both	_	_
language	_	_
pairs	_	_
.	_	_

#168
For	_	_
German	_	_
to	_	_
English	_	_
translation	_	_
accuracy	_	_
improves	_	_
by	_	_
0.2	_	_
to	_	_
0.3	_	_
BLEU	_	_
over	_	_
the	_	_
RNNLM	_	_
alone	_	_
,	_	_
with	_	_
gains	_	_
of	_	_
up	_	_
to	_	_
1.3	_	_
BLEU	_	_
over	_	_
the	_	_
baseline	_	_
and	_	_
up	_	_
to	_	_
0.7	_	_
BLEU	_	_
over	_	_
the	_	_
n-gram	_	_
MTU	_	_
model	_	_
.	_	_

#169
Improvements	_	_
for	_	_
French	_	_
to	_	_
English	_	_
are	_	_
lower	_	_
but	_	_
we	_	_
can	rhetorical	_
see	_	_
some	_	_
gains	_	_
on	_	_
news2011	_	_
and	_	_
on	_	_
the	_	_
dev	_	_
set	_	_
.	_	_

#170
Overall	_	_
,	_	_
we	_	_
improve	_	_
accuracy	_	_
on	_	_
the	_	_
French	_	_
to	_	_
English	_	_
task	_	_
by	_	_
up	_	_
to	_	_
1.5	_	_
BLEU	_	_
over	_	_
the	_	_
decoder	_	_
1-best	_	_
,	_	_
and	_	_
by	_	_
up	_	_
to	_	_
0.8	_	_
BLEU	_	_
over	_	_
the	_	_
n-gram	_	_
MTU	_	_
model	_	_
.	_	_