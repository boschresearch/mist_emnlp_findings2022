#33
The	_	_
WT	_	_
yifi	_	_
is	_	_
also	_	_
called	_	_
as	_	_
the	_	_
target	_	_
logit	_	_
[	_	_
14	_	_
]	_	_
of	_	_
the	_	_
i-th	_	_
sample	_	_
.	_	_

#34
In	_	_
the	_	_
A-softmax	_	_
loss	_	_
,	_	_
the	_	_
authors	_	_
proposed	_	_
to	_	_
normalize	_	_
the	_	_
weight	_	_
vectors	_	_
(	_	_
making	_	_
‖Wi‖	_	_
to	_	_
be	_	_
1	_	_
)	_	_
and	_	_
generalize	_	_
the	_	_
target	_	_
logit	_	_
from	_	_
‖fi‖cos	_	_
(	_	_
θyi	_	_
)	_	_
to	_	_
‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
,	_	_
LAS	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
e‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
e‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
+	_	_
∑c	_	_
j=1	_	_
,	_	_
j	_	_
6=yi	_	_
e	_	_
‖fi‖cos	_	_
(	_	_
θj	_	_
)	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
the	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
is	_	_
usually	_	_
a	_	_
piece-wise	_	_
function	_	_
defined	_	_
as	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
(	_	_
−1	_	_
)	_	_
k	_	_
cos	_	_
(	_	_
mθ	_	_
)	_	_
−	_	_
2k	_	_
+	_	_
λcos	_	_
(	_	_
θ	_	_
)	_	_
1	_	_
+	_	_
λ	_	_
,	_	_
θ	_	_
∈	_	_
[	_	_
kπ	_	_
m	_	_
,	_	_
(	_	_
k	_	_
+	_	_
1	_	_
)	_	_
π	_	_
m	_	_
]	_	_
,	_	_
(	_	_
3	_	_
)	_	_
where	_	_
m	_	_
is	_	_
usually	_	_
an	_	_
integer	_	_
larger	_	_
than	_	_
1	_	_
and	_	_
λ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
to	_	_
control	_	_
how	_	_
hard	_	_
the	_	_
classification	_	_
boundary	_	_
should	deontic	_
be	_	_
pushed	_	_
.	_	_

#35
During	_	_
training	_	_
,	_	_
the	_	_
λ	_	_
is	_	_
annealing	_	_
from	_	_
1	_	_
,	_	_
000	_	_
to	_	_
a	_	_
small	_	_
value	_	_
to	_	_
make	_	_
the	_	_
angular	_	_
space	_	_
of	_	_
each	_	_
class	_	_
become	_	_
more	_	_
and	_	_
more	_	_
compact	_	_
.	_	_

#60
However	_	_
,	_	_
to	_	_
set	_	_
the	_	_
annealing	_	_
curve	_	_
of	_	_
λ	_	_
,	_	_
lots	_	_
of	_	_
extra	_	_
parameters	_	_
are	_	_
introduced	_	_
,	_	_
which	_	_
are	_	_
more	_	_
or	_	_
less	_	_
confusing	_	_
for	_	_
starters	_	_
.	_	_

#61
Although	_	_
properly	_	_
tuning	_	_
those	_	_
hyper-parameters	_	_
for	_	_
λ	_	_
could	speculation	_
lead	_	_
to	_	_
impressive	_	_
results	_	_
,	_	_
the	_	_
hyper-parameters	_	_
are	_	_
still	_	_
quite	_	_
difficult	_	_
to	_	_
tune	_	_
.	_	_

#62
With	_	_
our	_	_
margin	_	_
scheme	_	_
,	_	_
we	_	_
find	_	_
that	_	_
we	_	_
no	_	_
longer	_	_
need	_	_
the	_	_
help	_	_
of	_	_
the	_	_
annealing	_	_
strategy	_	_
.	_	_

#91
Although	_	_
usually	_	_
the	_	_
cosine	_	_
margin	_	_
has	_	_
an	_	_
one-to-one	_	_
mapping	_	_
to	_	_
the	_	_
angular	_	_
margin	_	_
,	_	_
there	_	_
will	_	_
still	_	_
be	_	_
some	_	_
difference	_	_
while	_	_
optimizing	_	_
them	_	_
due	_	_
to	_	_
the	_	_
non-linearity	_	_
induced	_	_
by	_	_
the	_	_
cosine	_	_
function	_	_
.	_	_

#92
Whether	_	_
we	_	_
should	deontic	_
use	_	_
the	_	_
cosine	_	_
margin	_	_
depends	_	_
on	_	_
which	_	_
similarity	_	_
measurement	_	_
(	_	_
or	_	_
distance	_	_
)	_	_
the	_	_
final	_	_
loss	_	_
function	_	_
is	_	_
optimizing	_	_
.	_	_

#93
Obviously	_	_
,	_	_
our	_	_
modified	_	_
softmax	_	_
loss	_	_
function	_	_
is	_	_
optimizing	_	_
the	_	_
cosine	_	_
similarity	_	_
,	_	_
not	_	_
the	_	_
angle	_	_
.	_	_

#94
This	_	_
may	options	_
not	_	_
be	_	_
a	_	_
problem	_	_
if	_	_
we	_	_
are	_	_
using	_	_
the	_	_
conventional	_	_
softmax	_	_
loss	_	_
because	_	_
the	_	_
decision	_	_
boundaries	_	_
are	_	_
the	_	_
same	_	_
in	_	_
these	_	_
two	_	_
forms	_	_
(	_	_
cos	_	_
θ1	_	_
=	_	_
cos	_	_
θ2	_	_
⇒	_	_
θ1	_	_
=	_	_
θ2	_	_
)	_	_
.	_	_

#95
However	_	_
,	_	_
when	_	_
we	_	_
are	_	_
trying	_	_
to	_	_
push	_	_
the	_	_
boundary	_	_
,	_	_
we	_	_
will	_	_
face	_	_
a	_	_
problem	_	_
that	_	_
these	_	_
two	_	_
similarities	_	_
(	_	_
distances	_	_
)	_	_
have	_	_
different	_	_
densities	_	_
.	_	_

#96
Cosine	_	_
values	_	_
are	_	_
more	_	_
dense	_	_
when	_	_
the	_	_
angles	_	_
are	_	_
near	_	_
0	_	_
or	_	_
π	_	_
.	_	_

#97
If	_	_
we	_	_
want	_	_
to	_	_
optimize	_	_
the	_	_
angle	_	_
,	_	_
an	_	_
arccos	_	_
operation	_	_
may	options	_
be	_	_
required	_	_
after	_	_
the	_	_
value	_	_
of	_	_
the	_	_
inner	_	_
product	_	_
WTf	_	_
is	_	_
obtained	_	_
.	_	_

#98
It	_	_
will	_	_
potentially	_	_
be	_	_
more	_	_
computationally	_	_
expensive	_	_
.	_	_

#99
In	_	_
general	_	_
,	_	_
angular	_	_
margin	_	_
is	_	_
conceptually	_	_
better	_	_
than	_	_
the	_	_
cosine	_	_
margin	_	_
,	_	_
but	_	_
considering	_	_
the	_	_
computational	_	_
cost	_	_
,	_	_
cosine	_	_
margin	_	_
is	_	_
more	_	_
appealing	_	_
in	_	_
the	_	_
sense	_	_
that	_	_
it	_	_
could	capability	_
achieve	_	_
the	_	_
same	_	_
goal	_	_
with	_	_
less	_	_
efforts	_	_
.	_	_

#100
3.2.3	_	_
Feature	_	_
Normalization	_	_

#102
Our	_	_
loss	_	_
function	_	_
,	_	_
following	_	_
[	_	_
19	_	_
,	_	_
12	_	_
,	_	_
15	_	_
]	_	_
,	_	_
applies	_	_
feature	_	_
normalization	_	_
and	_	_
uses	_	_
a	_	_
global	_	_
scale	_	_
factor	_	_
s	_	_
to	_	_
replace	_	_
the	_	_
sample-dependent	_	_
feature	_	_
norm	_	_
in	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#103
One	_	_
question	_	_
arises	_	_
:	_	_
when	_	_
should	deontic	_
we	_	_
add	_	_
the	_	_
feature	_	_
normalization	_	_
?	_	_

#104
Our	_	_
answer	_	_
is	_	_
that	_	_
it	_	_
depends	_	_
on	_	_
the	_	_
image	_	_
quality	_	_
.	_	_

#111
As	_	_
a	_	_
conclusion	_	_
,	_	_
feature	_	_
normalization	_	_
is	_	_
most	_	_
suitable	_	_
for	_	_
tasks	_	_
whose	_	_
image	_	_
quality	_	_
is	_	_
very	_	_
low	_	_
.	_	_

#112
From	_	_
Figure	_	_
5	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
gradient	_	_
norm	_	_
may	options	_
be	_	_
extremely	_	_
big	_	_
when	_	_
the	_	_
feature	_	_
norm	_	_
is	_	_
very	_	_
small	_	_
.	_	_

#113
This	_	_
potentially	_	_
increases	_	_
the	_	_
risk	_	_
of	_	_
gradient	_	_
explosion	_	_
,	_	_
even	_	_
though	_	_
we	_	_
may	speculation	_
not	_	_
come	_	_
across	_	_
many	_	_
samples	_	_
with	_	_
very	_	_
small	_	_
feature	_	_
norm	_	_
.	_	_

#114
Maybe	_	_
some	_	_
re-weighting	_	_
strategy	_	_
whose	_	_
feature-gradient	_	_
norm	_	_
curve	_	_
is	_	_
between	_	_
the	_	_
two	_	_
curves	_	_
in	_	_
Figure	_	_
5	_	_
could	speculation	_
potentially	_	_
work	_	_
better	_	_
.	_	_

#115
This	_	_
is	_	_
an	_	_
interesting	_	_
topic	_	_
to	_	_
be	_	_
studied	_	_
in	_	_
the	_	_
future	_	_
.	_	_

#122
Moreover	_	_
,	_	_
our	_	_
loss	_	_
function	_	_
can	_	_
further	_	_
shrink	_	_
the	_	_
intra-class	_	_
variance	_	_
by	_	_
setting	_	_
a	_	_
larger	_	_
m.	_	_
Compared	_	_
to	_	_
A-Softmax	_	_
[	_	_
9	_	_
]	_	_
,	_	_
the	_	_
AM-Softmax	_	_
loss	_	_
also	_	_
converges	_	_
easier	_	_
with	_	_
proper	_	_
scaling	_	_
factor	_	_
s	_	_
.	_	_

#123
The	_	_
visualized	_	_
3D	_	_
features	_	_
well	_	_
demonstrates	_	_
that	_	_
AM-Softmax	_	_
could	capability	_
bring	_	_
the	_	_
large	_	_
margin	_	_
property	_	_
to	_	_
the	_	_
features	_	_
without	_	_
tuning	_	_
too	_	_
many	_	_
hyper-parameters	_	_
.	_	_

#124
4.	_	_
Experiment	_	_

#175
There	_	_
is	_	_
still	_	_
lots	_	_
of	_	_
potentials	_	_
for	_	_
the	_	_
research	_	_
of	_	_
the	_	_
large	_	_
margin	_	_
strategies	_	_
.	_	_

#176
There	_	_
could	speculation	_
be	_	_
more	_	_
creative	_	_
way	_	_
of	_	_
specifying	_	_
the	_	_
function	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
other	_	_
than	_	_
multiplication	_	_
and	_	_
addition	_	_
.	_	_

#177
In	_	_
our	_	_
AM-Softmax	_	_
loss	_	_
,	_	_
the	_	_
margin	_	_
is	_	_
a	_	_
manually	_	_
tuned	_	_
global	_	_
hyper-parameter	_	_
.	_	_