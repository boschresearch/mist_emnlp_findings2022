#29
Nature	_	_
has	_	_
always	_	_
been	_	_
a	_	_
source	_	_
of	_	_
inspiration	_	_
for	_	_
scientific	_	_
advances	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#30
When	_	_
given	_	_
the	_	_
task	_	_
of	_	_
designing	_	_
an	_	_
algorithm	_	_
for	_	_
object	_	_
recognition	_	_
without	_	_
having	_	_
much	_	_
prior	_	_
knowledge	_	_
of	_	_
the	_	_
field	_	_
,	_	_
one	_	_
might	options	_
attempt	_	_
replicating	_	_
some	_	_
object	_	_
recognition	_	_
system	_	_
that	_	_
can	_	_
be	_	_
found	_	_
in	_	_
nature	_	_
.	_	_

#31
Within	_	_
the	_	_
human	_	_
brain	_	_
,	_	_
the	_	_
neocortex	_	_
is	_	_
responsible	_	_
for	_	_
recognizing	_	_
very	_	_
high-level	_	_
patterns	_	_
,	_	_
such	_	_
as	_	_
abstract	_	_
concepts	_	_
or	_	_
complicated	_	_
implications	_	_
,	_	_
which	_	_
is	_	_
performed	_	_
by	_	_
around	_	_
20	_	_
billion	_	_
small	_	_
processing	_	_
units	_	_
,	_	_
called	_	_
Neurons	_	_
,	_	_
that	_	_
are	_	_
connected	_	_
with	_	_
each	_	_
other	_	_
and	_	_
organized	_	_
hierarchically	_	_
[	_	_
10	_	_
]	_	_
,	_	_
[	_	_
11	_	_
]	_	_
.	_	_

#52
This	_	_
weight	_	_
determines	_	_
how	_	_
important	_	_
the	_	_
result	_	_
of	_	_
the	_	_
lower	_	_
level	_	_
neuron	_	_
is	_	_
for	_	_
the	_	_
outcome	_	_
of	_	_
the	_	_
higher	_	_
level	_	_
neuron	_	_
.	_	_

#53
Recognizing	_	_
eyes	_	_
might	_	_
,	_	_
for	_	_
instance	_	_
,	_	_
be	_	_
an	_	_
important	_	_
prerequisite	_	_
for	_	_
recognizing	_	_
a	_	_
face	_	_
,	_	_
so	_	_
the	_	_
corresponding	_	_
weight	_	_
should	deontic	_
be	_	_
high	_	_
.	_	_

#54
In	_	_
addition	_	_
to	_	_
that	_	_
,	_	_
each	_	_
neuron	_	_
contains	_	_
a	_	_
Bias	_	_
that	_	_
reflects	_	_
how	_	_
likely	_	_
it	_	_
is	_	_
in	_	_
general	_	_
that	_	_
the	_	_
corresponding	_	_
pattern	_	_
is	_	_
present	_	_
.	_	_

#64
It	_	_
determines	_	_
the	_	_
network’s	_	_
prediction	_	_
quality	_	_
and	_	_
is	_	_
dependent	_	_
on	_	_
the	_	_
type	_	_
of	_	_
training	_	_
.	_	_

#65
For	_	_
instance	_	_
,	_	_
in	_	_
Unsupervised	_	_
Learning	_	_
,	_	_
the	_	_
loss	_	_
aims	_	_
to	_	_
put	_	_
an	_	_
emphasis	_	_
on	_	_
constraints	_	_
that	_	_
the	_	_
network	_	_
should	deontic	_
have	_	_
e.g.	_	_
ability	_	_
to	_	_
reconstruct	_	_
its	_	_
input	_	_
,	_	_
as	_	_
performed	_	_
in	_	_
Autoencoders	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#66
In	_	_
Supervised	_	_
Learning	_	_
,	_	_
where	_	_
pairs	_	_
of	_	_
inputs	_	_
and	_	_
corresponding	_	_
target	_	_
outputs	_	_
are	_	_
available	_	_
,	_	_
the	_	_
loss	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
the	_	_
measure	_	_
of	_	_
similarity	_	_
between	_	_
the	_	_
network’s	_	_
predictions	_	_
and	_	_
the	_	_
target	_	_
outputs	_	_
.	_	_

#73
Since	_	_
it	_	_
is	_	_
often	_	_
hard	_	_
to	_	_
understand	_	_
what	_	_
a	_	_
specific	_	_
neuron	_	_
is	_	_
recognizing	_	_
,	_	_
it	_	_
is	_	_
difficult	_	_
to	_	_
tell	_	_
how	_	_
or	_	_
why	_	_
a	_	_
given	_	_
deep	_	_
neural	_	_
network	_	_
is	_	_
working	_	_
(	_	_
or	_	_
not	_	_
)	_	_
.	_	_

#74
Another	_	_
challenge	_	_
of	_	_
using	_	_
deep	_	_
architectures	_	_
is	_	_
that	_	_
different	_	_
layers	_	_
might	speculation	_
learn	_	_
and	_	_
adapt	_	_
at	_	_
a	_	_
different	_	_
pace	_	_
.	_	_

#75
Especially	_	_
the	_	_
earlier	_	_
hidden	_	_
layers	_	_
,	_	_
which	_	_
are	_	_
close	_	_
to	_	_
the	_	_
input	_	_
layer	_	_
,	_	_
do	_	_
frequently	_	_
either	_	_
learn	_	_
much	_	_
slower	_	_
(	_	_
caused	_	_
by	_	_
very	_	_
low	_	_
gradients	_	_
)	_	_
or	_	_
much	_	_
faster	_	_
(	_	_
caused	_	_
by	_	_
high	_	_
gradients	_	_
)	_	_
also	_	_
leading	_	_
to	_	_
an	_	_
oscillatory	_	_
behavior	_	_
.	_	_

#79
When	_	_
tackling	_	_
deep	_	_
learning	_	_
tasks	_	_
,	_	_
it	_	_
is	_	_
generally	_	_
recommended	_	_
to	_	_
train	_	_
and	_	_
run	_	_
the	_	_
models	_	_
on	_	_
raw	_	_
inputs	_	_
,	_	_
without	_	_
manually	_	_
extracting	_	_
any	_	_
features	_	_
before	_	_
.	_	_

#80
The	_	_
reason	_	_
for	_	_
this	_	_
is	_	_
that	_	_
a	_	_
network	_	_
trained	_	_
on	_	_
the	_	_
raw	_	_
input	_	_
could	capability-speculation	_
learn	_	_
to	_	_
extract	_	_
these	_	_
features	_	_
on	_	_
its	_	_
own	_	_
,	_	_
but	_	_
in	_	_
contrast	_	_
to	_	_
working	_	_
with	_	_
prebuilt	_	_
features	_	_
,	_	_
it	_	_
would	_	_
also	_	_
be	_	_
able	_	_
to	_	_
further	_	_
optimize	_	_
the	_	_
feature	_	_
extraction	_	_
as	_	_
the	_	_
network	_	_
improves	_	_
.	_	_

#81
If	_	_
the	_	_
input	_	_
is	_	_
an	_	_
image	_	_
,	_	_
it	_	_
would	_	_
,	_	_
therefore	_	_
,	_	_
be	_	_
desirable	_	_
to	_	_
work	_	_
directly	_	_
with	_	_
its	_	_
raw	_	_
pixel	_	_
values	_	_
.	_	_

#84
If	_	_
one	_	_
would	_	_
use	_	_
the	_	_
simple	_	_
fully-connected	_	_
network	_	_
architecture	_	_
described	_	_
in	_	_
Sect.	_	_
II-A	_	_
,	_	_
each	_	_
neuron	_	_
in	_	_
the	_	_
subsequent	_	_
layer	_	_
would	_	_
then	_	_
be	_	_
connected	_	_
to	_	_
about	_	_
six	_	_
million	_	_
neurons	_	_
and	_	_
if	_	_
the	_	_
first	_	_
fully-connected	_	_
layer	_	_
would	_	_
contain	_	_
just	_	_
1000	_	_
neurons	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
parameters	_	_
would	_	_
amount	_	_
to	_	_
over	_	_
six	_	_
billion	_	_
.	_	_

#85
Since	_	_
the	_	_
network	_	_
has	_	_
to	_	_
optimize	_	_
all	_	_
of	_	_
these	_	_
parameters	_	_
,	_	_
the	_	_
training	_	_
process	_	_
could	speculation	_
then	_	_
become	_	_
very	_	_
time	_	_
and	_	_
storage	_	_
intensive	_	_
.	_	_

#86
In	_	_
order	_	_
to	_	_
solve	_	_
this	_	_
computational	_	_
problem	_	_
,	_	_
a	_	_
different	_	_
kind	_	_
of	_	_
network	_	_
architecture	_	_
is	_	_
used	_	_
,	_	_
called	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
(	_	_
CNN	_	_
)	_	_
.	_	_

#97
Neurons	_	_
of	_	_
the	_	_
convolution	_	_
operator	_	_
can	_	_
recognize	_	_
certain	_	_
local	_	_
patterns	_	_
of	_	_
the	_	_
previous	_	_
layer’s	_	_
output	_	_
.	_	_

#98
Since	_	_
the	_	_
patterns	_	_
that	_	_
are	_	_
recognized	_	_
should	deontic	_
be	_	_
independent	_	_
of	_	_
their	_	_
position	_	_
in	_	_
the	_	_
image	_	_
,	_	_
all	_	_
neurons	_	_
will	_	_
be	_	_
forced	_	_
to	_	_
recognize	_	_
the	_	_
same	_	_
pattern	_	_
by	_	_
making	_	_
all	_	_
of	_	_
them	_	_
share	_	_
one	_	_
single	_	_
set	_	_
of	_	_
parameters	_	_
.	_	_

#99
This	_	_
concept	_	_
is	_	_
referred	_	_
to	_	_
as	_	_
Parameter	_	_
Sharing	_	_
.	_	_

#101
In	_	_
the	_	_
convolutional	_	_
layer	_	_
,	_	_
the	_	_
depth	_	_
dimension	_	_
is	_	_
then	_	_
specifying	_	_
to	_	_
which	_	_
filter	_	_
a	_	_
given	_	_
neuron	_	_
belongs	_	_
.	_	_

#102
Another	_	_
reason	_	_
for	_	_
why	_	_
the	_	_
convolution	_	_
operations	_	_
are	_	_
performed	_	_
across	_	_
all	_	_
depth	_	_
values	_	_
is	_	_
that	_	_
neurons	_	_
in	_	_
a	_	_
convolutional	_	_
layer	_	_
,	_	_
which	_	_
are	_	_
stacked	_	_
on	_	_
top	_	_
of	_	_
others	_	_
,	_	_
should	deontic	_
have	_	_
their	_	_
features	_	_
jointly	_	_
considered	_	_
in	_	_
the	_	_
next	_	_
layer	_	_
.	_	_

#103
A	_	_
neuron	_	_
in	_	_
a	_	_
convolutional	_	_
layer	_	_
will	_	_
,	_	_
therefore	_	_
,	_	_
be	_	_
connected	_	_
to	_	_
r	_	_
∗	_	_
d	_	_
neurons	_	_
of	_	_
the	_	_
underlying	_	_
layer	_	_
,	_	_
where	_	_
r	_	_
is	_	_
the	_	_
size	_	_
of	_	_
the	_	_
receptive	_	_
field	_	_
and	_	_
d	_	_
is	_	_
the	_	_
depth	_	_
of	_	_
the	_	_
previous	_	_
layer	_	_
.	_	_

#120
Other	_	_
pooling	_	_
variants	_	_
,	_	_
such	_	_
as	_	_
Average	_	_
Pooling	_	_
,	_	_
perform	_	_
a	_	_
different	_	_
function	_	_
but	_	_
can	_	_
be	_	_
interpreted	_	_
similarly	_	_
.	_	_

#121
How	_	_
many	_	_
neurons	_	_
are	_	_
combined	_	_
across	_	_
each	_	_
of	_	_
the	_	_
two	_	_
dimensions	_	_
and	_	_
how	_	_
large	_	_
the	_	_
gap	_	_
between	_	_
two	_	_
pooling	_	_
operations	_	_
should	deontic	_
be	_	_
is	_	_
again	_	_
defined	_	_
by	_	_
the	_	_
receptive	_	_
field	_	_
and	_	_
stride	_	_
hyperparameters	_	_
that	_	_
were	_	_
used	_	_
for	_	_
defining	_	_
convolutional	_	_
layers	_	_
.	_	_

#122
III.	_	_
APPLICATIONS	_	_
OF	_	_
DCNNS	_	_
FOR	_	_
OBJECT	_	_
RECOGNITION	_	_
TASKS	_	_

#126
The	_	_
task	_	_
of	_	_
Image	_	_
Classification	_	_
describes	_	_
the	_	_
challenge	_	_
of	_	_
categorizing	_	_
a	_	_
given	_	_
image	_	_
into	_	_
one	_	_
of	_	_
several	_	_
classes	_	_
.	_	_

#127
A	_	_
possible	_	_
application	_	_
of	_	_
this	_	_
could	speculation	_
be	_	_
the	_	_
recognition	_	_
of	_	_
hand-written	_	_
digits	_	_
,	_	_
where	_	_
the	_	_
input	_	_
image	_	_
is	_	_
classified	_	_
as	_	_
one	_	_
of	_	_
the	_	_
ten	_	_
classes	_	_
.	_	_

#128
For	_	_
this	_	_
task	_	_
[	_	_
7	_	_
]	_	_
developed	_	_
a	_	_
DCNN	_	_
architecture	_	_
in	_	_
1998	_	_
,	_	_
the	_	_
LeNet-5	_	_
,	_	_
which	_	_
we	_	_
will	_	_
inspect	_	_
more	_	_
closely	_	_
later	_	_
.	_	_

#319
For	_	_
pooling	_	_
layers	_	_
,	_	_
a	_	_
window	_	_
size	_	_
of	_	_
two	_	_
with	_	_
no	_	_
padding	_	_
and	_	_
a	_	_
stride	_	_
of	_	_
one	_	_
or	_	_
two	_	_
are	_	_
frequently	_	_
selected	_	_
.	_	_

#320
Finally	_	_
,	_	_
in	_	_
fully-connected	_	_
layers	_	_
,	_	_
the	_	_
amount	_	_
of	_	_
neurons	_	_
should	deontic	_
be	_	_
greater	_	_
or	_	_
equal	_	_
to	_	_
the	_	_
amount	_	_
of	_	_
neurons	_	_
in	_	_
the	_	_
following	_	_
layer	_	_
and	_	_
be	_	_
within	_	_
the	_	_
same	_	_
order	_	_
of	_	_
magnitude	_	_
.	_	_

#321
One	_	_
of	_	_
the	_	_
more	_	_
general	_	_
key	_	_
design	_	_
ideas	_	_
that	_	_
we	_	_
can	_	_
observe	_	_
in	_	_
recent	_	_
network	_	_
architectures	_	_
is	_	_
that	_	_
direct	_	_
connections	_	_
from	_	_
earlier	_	_
to	_	_
later	_	_
layers	_	_
seem	_	_
to	_	_
be	_	_
necessary	_	_
to	_	_
achieve	_	_
state-of-the-art	_	_
results	_	_
.	_	_