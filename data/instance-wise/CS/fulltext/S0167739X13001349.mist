#1
The	_	_
speedup	_	_
of	_	_
a	_	_
program	_	_
on	_	_
these	_	_
machines	_	_
depends	_	_
on	_	_
how	_	_
well	_	_
the	_	_
latency	_	_
is	_	_
hidden	_	_
.	_	_

#2
If	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
were	_	_
infinite	_	_
,	_	_
theoretically	_	_
,	_	_
these	_	_
machines	_	_
could	capability-speculation	_
provide	_	_
the	_	_
performance	_	_
predicted	_	_
by	_	_
the	_	_
PRAM	_	_
analysis	_	_
of	_	_
these	_	_
programs	_	_
.	_	_

#3
However	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
is	_	_
not	_	_
infinite	_	_
,	_	_
and	_	_
is	_	_
constrained	_	_
by	_	_
both	_	_
hardware	_	_
and	_	_
algorithmic	_	_
limits	_	_
.	_	_

#28
Aggarwal	_	_
and	_	_
Vitter	_	_
proposed	_	_
the	_	_
Disk	_	_
Access	_	_
Machine	_	_
(	_	_
DAM	_	_
)	_	_
model	_	_
[	_	_
22	_	_
]	_	_
which	_	_
counts	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
memory	_	_
instead	_	_
of	_	_
simply	_	_
counting	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
by	_	_
the	_	_
program	_	_
.	_	_

#29
Therefore	_	_
,	_	_
it	_	_
better	_	_
captures	_	_
the	_	_
fact	_	_
that	_	_
modern	_	_
machines	_	_
have	_	_
memory	_	_
hierarchies	_	_
and	_	_
exploiting	_	_
spatial	_	_
and	_	_
temporal	_	_
locality	_	_
on	_	_
these	_	_
machines	_	_
can	options	_
lead	_	_
to	_	_
better	_	_
performance	_	_
.	_	_

#30
There	_	_
are	_	_
also	_	_
a	_	_
number	_	_
of	_	_
other	_	_
models	_	_
that	_	_
consider	_	_
the	_	_
memory	_	_
access	_	_
costs	_	_
of	_	_
sequential	_	_
algorithms	_	_
in	_	_
different	_	_
ways	_	_
[	_	_
23-29	_	_
]	_	_
.	_	_

#42
In	_	_
contrast	_	_
,	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
are	_	_
explicitly	_	_
designed	_	_
to	_	_
have	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
and	_	_
a	_	_
fast	_	_
context	_	_
switching	_	_
mechanism	_	_
.	_	_

#43
Highly-threaded	_	_
many-cores	_	_
are	_	_
explicitly	_	_
designed	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
;	_	_
if	_	_
a	_	_
thread	_	_
stalls	_	_
on	_	_
a	_	_
memory	_	_
operation	_	_
,	_	_
some	_	_
other	_	_
thread	_	_
can	feasibility	_
be	_	_
scheduled	_	_
in	_	_
its	_	_
place	_	_
.	_	_

#44
In	_	_
principle	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
does	_	_
not	_	_
matter	_	_
as	_	_
long	_	_
as	_	_
there	_	_
are	_	_
enough	_	_
threads	_	_
to	_	_
hide	_	_
their	_	_
latency	_	_
.	_	_

#45
Therefore	_	_
,	_	_
if	_	_
there	_	_
are	_	_
enough	_	_
threads	_	_
,	_	_
we	_	_
should	inference	_
,	_	_
in	_	_
principle	_	_
,	_	_
be	_	_
able	_	_
to	_	_
use	_	_
PRAM	_	_
algorithms	_	_
on	_	_
such	_	_
machines	_	_
,	_	_
since	_	_
we	_	_
can	feasibility	_
ignore	_	_
the	_	_
effect	_	_
of	_	_
memory	_	_
transfers	_	_
which	_	_
is	_	_
exactly	_	_
what	_	_
PRAM	_	_
model	_	_
does	_	_
.	_	_

#46
However	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
required	_	_
to	_	_
reach	_	_
the	_	_
point	_	_
where	_	_
one	_	_
gets	_	_
PRAM	_	_
performance	_	_
depends	_	_
on	_	_
both	_	_
the	_	_
algorithm	_	_
and	_	_
the	_	_
hardware	_	_
.	_	_

#56
This	_	_
model	_	_
explicitly	_	_
models	_	_
the	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
and	_	_
the	_	_
memory	_	_
latency	_	_
to	_	_
slow	_	_
memory	_	_
.	_	_

#57
Note	_	_
that	_	_
while	_	_
we	_	_
motivate	_	_
this	_	_
model	_	_
for	_	_
highly-threaded	_	_
many-core	_	_
machines	_	_
with	_	_
synchronous	_	_
computations	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
it	_	_
can	feasibility	_
be	_	_
used	_	_
in	_	_
any	_	_
system	_	_
which	_	_
has	_	_
fast	_	_
context	_	_
switching	_	_
and	_	_
enough	_	_
threads	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
.	_	_

#58
Typical	_	_
examples	_	_
of	_	_
such	_	_
machines	_	_
include	_	_
both	_	_
NVIDIA	_	_
and	_	_
AMD/ATI	_	_
GPUs	_	_
and	_	_
the	_	_
YarcData	_	_
uRiKA	_	_
system	_	_
.	_	_

#63
(	_	_
1	_	_
)	_	_
Ideally	_	_
,	_	_
we	_	_
want	_	_
to	_	_
get	_	_
the	_	_
PRAM	_	_
performance	_	_
for	_	_
algorithms	_	_
using	_	_
the	_	_
fewest	_	_
number	_	_
of	_	_
threads	_	_
possible	_	_
,	_	_
since	_	_
threads	_	_
do	_	_
have	_	_
overhead	_	_
.	_	_

#64
This	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
pick	_	_
such	_	_
algorithms	_	_
.	_	_

#65
(	_	_
2	_	_
)	_	_
It	_	_
also	_	_
captures	_	_
the	_	_
reality	_	_
of	_	_
when	_	_
memory	_	_
latency	_	_
is	_	_
large	_	_
and	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
large	_	_
but	_	_
finite	_	_
.	_	_

#66
In	_	_
particular	_	_
,	_	_
it	_	_
can	capability	_
distinguish	_	_
between	_	_
algorithms	_	_
that	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
analysis	_	_
,	_	_
but	_	_
one	_	_
may	capability-speculation	_
be	_	_
better	_	_
at	_	_
hiding	_	_
latency	_	_
than	_	_
another	_	_
with	_	_
a	_	_
bounded	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#67
This	_	_
model	_	_
is	_	_
a	_	_
high-level	_	_
model	_	_
meant	_	_
to	_	_
be	_	_
generally	_	_
applicable	_	_
to	_	_
a	_	_
number	_	_
of	_	_
machines	_	_
which	_	_
allow	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
with	_	_
fast	_	_
context	_	_
switching	_	_
.	_	_

#71
In	_	_
particular	_	_
,	_	_
we	_	_
model	_	_
a	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
shared	_	_
by	_	_
one	_	_
core	_	_
group	_	_
.	_	_

#72
In	_	_
practice	_	_
,	_	_
these	_	_
machines	_	_
may	capability-options	_
have	_	_
many	_	_
levels	_	_
of	_	_
memory	_	_
.	_	_

#73
However	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
the	_	_
interplay	_	_
between	_	_
the	_	_
farthest	_	_
level	_	_
,	_	_
since	_	_
the	_	_
latencies	_	_
are	_	_
the	_	_
largest	_	_
at	_	_
that	_	_
level	_	_
,	_	_
and	_	_
therefore	_	_
have	_	_
the	_	_
biggest	_	_
impact	_	_
on	_	_
the	_	_
performance	_	_
.	_	_

#74
We	_	_
expect	_	_
that	_	_
the	_	_
model	_	_
can	capability-feasibility	_
be	_	_
extended	_	_
to	_	_
also	_	_
model	_	_
other	_	_
levels	_	_
of	_	_
the	_	_
memory	_	_
hierarchy	_	_
.	_	_

#75
We	_	_
analyze	_	_
4	_	_
classic	_	_
algorithms	_	_
for	_	_
the	_	_
problem	_	_
of	_	_
computing	_	_
All	_	_
Pairs	_	_
Shortest	_	_
Paths	_	_
(	_	_
APSP	_	_
)	_	_
on	_	_
a	_	_
weighted	_	_
graph	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
[	_	_
43	_	_
]	_	_
.	_	_

#76
We	_	_
compare	_	_
the	_	_
analysis	_	_
from	_	_
this	_	_
model	_	_
with	_	_
the	_	_
PRAM	_	_
analysis	_	_
of	_	_
these	_	_
4	_	_
algorithms	_	_
to	_	_
gain	_	_
intuition	_	_
about	_	_
the	_	_
usefulness	_	_
of	_	_
both	_	_
our	_	_
model	_	_
and	_	_
the	_	_
PRAM	_	_
model	_	_
for	_	_
analyzing	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#77
Our	_	_
results	_	_
validate	_	_
the	_	_
intuition	_	_
that	_	_
this	_	_
model	_	_
can	capability	_
provide	_	_
more	_	_
information	_	_
than	_	_
the	_	_
PRAM	_	_
model	_	_
for	_	_
the	_	_
large	_	_
latency	_	_
,	_	_
finite	_	_
thread	_	_
case	_	_
.	_	_

#78
In	_	_
particular	_	_
,	_	_
we	_	_
compare	_	_
these	_	_
algorithms	_	_
and	_	_
find	_	_
specific	_	_
relationships	_	_
between	_	_
hardware	_	_
parameters	_	_
(	_	_
latency	_	_
,	_	_
fast	_	_
memory	_	_
size	_	_
,	_	_
limits	_	_
on	_	_
number	_	_
of	_	_
threads	_	_
)	_	_
under	_	_
which	_	_
some	_	_
algorithms	_	_
are	_	_
better	_	_
than	_	_
others	_	_
even	_	_
if	_	_
they	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
cost	_	_
.	_	_

#94
It	_	_
differs	_	_
from	_	_
the	_	_
RAM	_	_
model	_	_
by	_	_
defining	_	_
that	_	_
access	_	_
to	_	_
location	_	_
x	_	_
takes	_	_
logx	_	_
time	_	_
,	_	_
but	_	_
it	_	_
does	_	_
not	_	_
consider	_	_
the	_	_
concept	_	_
of	_	_
block	_	_
transfers	_	_
,	_	_
which	_	_
collects	_	_
data	_	_
into	_	_
blocks	_	_
to	_	_
utilize	_	_
spatial	_	_
locality	_	_
of	_	_
reference	_	_
in	_	_
algorithms	_	_
.	_	_

#95
The	_	_
Block	_	_
Transfer	_	_
model	_	_
(	_	_
BT	_	_
)	_	_
[	_	_
27	_	_
]	_	_
addresses	_	_
this	_	_
deficiency	_	_
by	_	_
defining	_	_
that	_	_
a	_	_
block	_	_
of	_	_
consecutive	_	_
locations	_	_
can	feasibility	_
be	_	_
copied	_	_
from	_	_
memory	_	_
to	_	_
memory	_	_
,	_	_
taking	_	_
one	_	_
unit	_	_
of	_	_
time	_	_
per	_	_
element	_	_
after	_	_
the	_	_
initial	_	_
access	_	_
time	_	_
.	_	_

#96
Alpern	_	_
et	_	_
al	_	_
.	_	_
propose	_	_
the	_	_
Memory	_	_
Hierarchy	_	_
(	_	_
MH	_	_
)	_	_
Framework	_	_
[	_	_
26	_	_
]	_	_
that	_	_
reflects	_	_
important	_	_
practical	_	_
considerations	_	_
that	_	_
are	_	_
hidden	_	_
by	_	_
the	_	_
RAM	_	_
and	_	_
HMM	_	_
models	_	_
:	_	_
data	_	_
are	_	_
moved	_	_
in	_	_
fixed	_	_
size	_	_
blocks	_	_
simultaneously	_	_
at	_	_
different	_	_
levels	_	_
in	_	_
the	_	_
hierarchy	_	_
,	_	_
and	_	_
the	_	_
memory	_	_
capacity	_	_
as	_	_
well	_	_
as	_	_
bus	_	_
bandwidth	_	_
are	_	_
limited	_	_
at	_	_
each	_	_
level	_	_
.	_	_

#97
But	_	_
there	_	_
are	_	_
too	_	_
many	_	_
parameters	_	_
in	_	_
this	_	_
model	_	_
that	_	_
can	capability-options	_
obscure	_	_
algorithm	_	_
analysis	_	_
.	_	_

#98
Thus	_	_
,	_	_
they	_	_
simplified	_	_
and	_	_
reduced	_	_
the	_	_
MH	_	_
parameters	_	_
by	_	_
putting	_	_
forward	_	_
a	_	_
new	_	_
Uniform	_	_
Memory	_	_
Hierarchy	_	_
(	_	_
UMH	_	_
)	_	_
model	_	_
[	_	_
28,29	_	_
]	_	_
.	_	_

#125
Meantime	_	_
,	_	_
Baghsorkhi	_	_
et	_	_
al	_	_
.	_	_
[	_	_
16	_	_
]	_	_
measure	_	_
performance	_	_
factors	_	_
in	_	_
isolation	_	_
and	_	_
later	_	_
combine	_	_
them	_	_
to	_	_
model	_	_
the	_	_
overall	_	_
performance	_	_
via	_	_
workflow	_	_
graphs	_	_
so	_	_
that	_	_
the	_	_
interactive	_	_
effects	_	_
between	_	_
different	_	_
performance	_	_
factors	_	_
are	_	_
modeled	_	_
correctly	_	_
.	_	_

#126
The	_	_
model	_	_
can	capability	_
determine	_	_
data	_	_
access	_	_
patterns	_	_
,	_	_
branch	_	_
divergence	_	_
,	_	_
and	_	_
control	_	_
flow	_	_
patterns	_	_
only	_	_
for	_	_
a	_	_
restricted	_	_
class	_	_
of	_	_
kernels	_	_
on	_	_
traditional	_	_
GPU	_	_
architectures	_	_
.	_	_

#127
Zhang	_	_
and	_	_
Owens	_	_
[	_	_
15	_	_
]	_	_
present	_	_
a	_	_
quantitative	_	_
performance	_	_
model	_	_
that	_	_
characterizes	_	_
an	_	_
application	_	_
's	_	_
performance	_	_
as	_	_
being	_	_
primarily	_	_
bounded	_	_
by	_	_
one	_	_
of	_	_
three	_	_
potential	_	_
limits	_	_
:	_	_
instruction	_	_
pipeline	_	_
,	_	_
shared	_	_
memory	_	_
accesses	_	_
,	_	_
and	_	_
global	_	_
memory	_	_
accesses	_	_
.	_	_

#134
TMM	_	_
model	_	_
The	_	_
TMM	_	_
model	_	_
is	_	_
meant	_	_
to	_	_
model	_	_
the	_	_
asymptotic	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#135
The	_	_
model	_	_
should	deontic	_
abstract	_	_
away	_	_
the	_	_
details	_	_
of	_	_
particular	_	_
implementations	_	_
so	_	_
as	_	_
to	_	_
be	_	_
applicable	_	_
to	_	_
many	_	_
instantiations	_	_
of	_	_
these	_	_
machines	_	_
,	_	_
while	_	_
being	_	_
particular	_	_
enough	_	_
to	_	_
model	_	_
the	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
these	_	_
machines	_	_
with	_	_
reasonable	_	_
accuracy	_	_
.	_	_

#136
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
describe	_	_
the	_	_
important	_	_
characteristics	_	_
of	_	_
these	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
and	_	_
our	_	_
model	_	_
for	_	_
analyzing	_	_
algorithms	_	_
for	_	_
these	_	_
architectures	_	_
.	_	_

#137
Highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
The	_	_
most	_	_
important	_	_
high-level	_	_
characteristic	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
is	_	_
that	_	_
they	_	_
provide	_	_
a	_	_
large	_	_
number	_	_
of	_	_
hardware	_	_
threads	_	_
and	_	_
use	_	_
fast	_	_
and	_	_
low-overhead	_	_
context-switching	_	_
in	_	_
order	_	_
to	_	_
hide	_	_
the	_	_
memory	_	_
access	_	_
latency	_	_
from	_	_
slow	_	_
global	_	_
memory	_	_
.	_	_

#138
Highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
typically	_	_
consist	_	_
of	_	_
a	_	_
number	_	_
of	_	_
core	_	_
groups	_	_
,	_	_
each	_	_
containing	_	_
a	_	_
number	_	_
of	_	_
processors	_	_
(	_	_
or	_	_
cores	_	_
)	_	_
,11	_	_
A	_	_
core	_	_
group	_	_
can	options	_
also	_	_
have	_	_
a	_	_
single	_	_
core	_	_
.	_	_

#139
a	_	_
fixed	_	_
number	_	_
of	_	_
registers	_	_
,	_	_
and	_	_
a	_	_
fixed	_	_
quantity	_	_
of	_	_
fast	_	_
local	_	_
on-chip	_	_
memory	_	_
shared	_	_
within	_	_
a	_	_
core	_	_
group	_	_
.	_	_

#140
A	_	_
large	_	_
slow	_	_
global	_	_
memory	_	_
is	_	_
shared	_	_
by	_	_
all	_	_
the	_	_
core	_	_
groups	_	_
.	_	_

#141
Registers	_	_
and	_	_
local	_	_
on-chip	_	_
memory	_	_
are	_	_
the	_	_
fastest	_	_
to	_	_
access	_	_
,	_	_
while	_	_
accessing	_	_
the	_	_
global	_	_
memory	_	_
may	options	_
potentially	_	_
take	_	_
100s	_	_
of	_	_
cycles	_	_
.	_	_

#142
The	_	_
TMM	_	_
model	_	_
models	_	_
these	_	_
machines	_	_
as	_	_
having	_	_
a	_	_
memory	_	_
hierarchy	_	_
with	_	_
two	_	_
levels	_	_
of	_	_
memory	_	_
:	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
.	_	_

#143
In	_	_
addition	_	_
,	_	_
on	_	_
most	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
,	_	_
data	_	_
is	_	_
transferred	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
memory	_	_
in	_	_
chunks	_	_
;	_	_
instead	_	_
of	_	_
just	_	_
transferring	_	_
one	_	_
word	_	_
at	_	_
a	_	_
time	_	_
,	_	_
the	_	_
hardware	_	_
tries	_	_
to	_	_
transfer	_	_
a	_	_
large	_	_
number	_	_
of	_	_
words	_	_
during	_	_
a	_	_
memory	_	_
transfer	_	_
.	_	_

#144
The	_	_
chunk	_	_
can	options	_
either	_	_
be	_	_
a	_	_
cache	_	_
line	_	_
from	_	_
hardware	_	_
managed	_	_
caches	_	_
,	_	_
or	_	_
an	_	_
explicitly-managed	_	_
combined	_	_
read	_	_
from	_	_
multiple	_	_
threads	_	_
.	_	_

#145
Since	_	_
this	_	_
characteristic	_	_
of	_	_
using	_	_
high-bandwidth	_	_
transfers	_	_
in	_	_
order	_	_
to	_	_
counter	_	_
high	_	_
latencies	_	_
is	_	_
common	_	_
to	_	_
most	_	_
many-core	_	_
machines	_	_
(	_	_
and	_	_
even	_	_
most	_	_
multi-core	_	_
machines	_	_
)	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
captures	_	_
the	_	_
chunk	_	_
size	_	_
as	_	_
one	_	_
of	_	_
its	_	_
parameters	_	_
.	_	_

#153
P	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
cores	_	_
(	_	_
or	_	_
processors	_	_
)	_	_
in	_	_
the	_	_
machine	_	_
.	_	_

#154
C	_	_
is	_	_
the	_	_
maximum	_	_
chunk	_	_
size	_	_
;	_	_
the	_	_
number	_	_
of	_	_
words	_	_
that	_	_
can	feasibility	_
be	_	_
read	_	_
from	_	_
slow	_	_
memory	_	_
to	_	_
fast	_	_
memory	_	_
in	_	_
one	_	_
memory	_	_
transfer	_	_
.	_	_

#155
The	_	_
parameter	_	_
Z	_	_
represents	_	_
the	_	_
size	_	_
of	_	_
fast	_	_
local	_	_
memory	_	_
per	_	_
core	_	_
group	_	_
and	_	_
Q	_	_
represents	_	_
the	_	_
number	_	_
of	_	_
cores	_	_
per	_	_
core	_	_
group	_	_
.	_	_

#156
As	_	_
mentioned	_	_
earlier	_	_
,	_	_
in	_	_
some	_	_
instantiations	_	_
,	_	_
a	_	_
core	_	_
group	_	_
can	options	_
have	_	_
a	_	_
single	_	_
core	_	_
.	_	_

#157
In	_	_
this	_	_
case	_	_
,	_	_
a	_	_
many-core	_	_
machine	_	_
looks	_	_
very	_	_
much	_	_
like	_	_
a	_	_
multi-core	_	_
machine	_	_
with	_	_
a	_	_
large	_	_
number	_	_
of	_	_
low-overhead	_	_
hardware	_	_
threads	_	_
.	_	_

#161
We	_	_
unify	_	_
these	_	_
constraints	_	_
into	_	_
one	_	_
parameter	_	_
.	_	_

#162
In	_	_
addition	_	_
to	_	_
the	_	_
architecture	_	_
parameters	_	_
,	_	_
we	_	_
must	deontic	_
also	_	_
consider	_	_
the	_	_
parameters	_	_
which	_	_
are	_	_
determined	_	_
by	_	_
the	_	_
algorithm	_	_
.	_	_

#163
We	_	_
assume	_	_
that	_	_
the	_	_
programmer	_	_
has	_	_
written	_	_
a	_	_
correct	_	_
synchronous	_	_
program	_	_
and	_	_
taken	_	_
care	_	_
to	_	_
balance	_	_
the	_	_
workload	_	_
across	_	_
the	_	_
core	_	_
groups	_	_
.	_	_

#164
These	_	_
program	_	_
parameters	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
2	_	_
.	_	_

#165
T1	_	_
represents	_	_
the	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
,	_	_
that	_	_
is	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
that	_	_
the	_	_
program	_	_
must	deontic	_
perform	_	_
(	_	_
including	_	_
fast	_	_
memory	_	_
accesses	_	_
)	_	_
.T∞	_	_
represents	_	_
the	_	_
span	_	_
of	_	_
the	_	_
algorithm	_	_
,	_	_
that	_	_
is	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
on	_	_
the	_	_
critical	_	_
path	_	_
.	_	_

#166
These	_	_
are	_	_
similar	_	_
to	_	_
the	_	_
analogous	_	_
PRAM	_	_
parameters	_	_
of	_	_
work	_	_
and	_	_
time	_	_
(	_	_
or	_	_
depth	_	_
or	_	_
critical-path	_	_
length	_	_
)	_	_
.	_	_

#169
Note	_	_
that	_	_
this	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
,	_	_
not	_	_
total	_	_
number	_	_
of	_	_
accesses	_	_
.	_	_

#170
Since	_	_
many-core	_	_
machines	_	_
often	_	_
transfer	_	_
data	_	_
in	_	_
large	_	_
chunks	_	_
,	_	_
multiple	_	_
memory	_	_
accesses	_	_
can	capability-feasibility-options	_
combine	_	_
into	_	_
one	_	_
memory	_	_
transfer	_	_
.	_	_

#171
For	_	_
instance	_	_
,	_	_
if	_	_
the	_	_
many-core	_	_
machine	_	_
has	_	_
a	_	_
hardware	_	_
managed	_	_
cache	_	_
,	_	_
and	_	_
the	_	_
program	_	_
accesses	_	_
data	_	_
sequentially	_	_
,	_	_
then	_	_
there	_	_
is	_	_
only	_	_
one	_	_
memory	_	_
operation	_	_
for	_	_
C	_	_
memory	_	_
accesses	_	_
;	_	_
these	_	_
will	_	_
count	_	_
as	_	_
one	_	_
when	_	_
accounting	_	_
for	_	_
M.	_	_
T	_	_
is	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
created	_	_
by	_	_
the	_	_
program	_	_
per	_	_
core	_	_
.	_	_

#187
A	_	_
fast	_	_
hardware-supported	_	_
context-switching	_	_
mechanism	_	_
enables	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
to	_	_
execute	_	_
concurrently	_	_
.	_	_

#188
Transfers	_	_
between	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
can	capability	_
occur	_	_
in	_	_
chunks	_	_
of	_	_
at	_	_
most	_	_
32	_	_
words	_	_
;	_	_
these	_	_
chunks	_	_
can	capability-feasibility	_
only	_	_
be	_	_
created	_	_
if	_	_
the	_	_
memory	_	_
accesses	_	_
are	_	_
within	_	_
a	_	_
specified	_	_
range	_	_
.	_	_

#189
Accessing	_	_
the	_	_
off-chip	_	_
global	_	_
memory	_	_
usually	_	_
takes	_	_
20	_	_
to	_	_
40	_	_
times	_	_
more	_	_
clock	_	_
cycles	_	_
than	_	_
accessing	_	_
the	_	_
on-chip	_	_
shared	_	_
memory/L1	_	_
cache	_	_
[	_	_
51	_	_
]	_	_
.	_	_

#197
Low	_	_
context-switch	_	_
threading	_	_
is	_	_
well	_	_
supported	_	_
,	_	_
and	_	_
every	_	_
64	_	_
threads	_	_
are	_	_
grouped	_	_
into	_	_
a	_	_
wavefront	_	_
executing	_	_
the	_	_
same	_	_
instruction	_	_
.	_	_

#198
Basically	_	_
,	_	_
the	_	_
SIMD	_	_
engine	_	_
can	capability-feasibility	_
naturally	_	_
be	_	_
modeled	_	_
by	_	_
core	_	_
groups	_	_
.	_	_

#199
Each	_	_
SC	_	_
is	_	_
modeled	_	_
as	_	_
a	_	_
core	_	_
in	_	_
TMM	_	_
,	_	_
summing	_	_
up	_	_
to	_	_
1600	_	_
cores	_	_
totally	_	_
.	_	_

#204
Therefore	_	_
,	_	_
128	_	_
defines	_	_
parameter	_	_
X	_	_
,	_	_
the	_	_
hard	_	_
limit	_	_
of	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
.	_	_

#205
There	_	_
can	options	_
be	_	_
up	_	_
to	_	_
65,000	_	_
threads	_	_
in	_	_
a	_	_
512	_	_
processor	_	_
system	_	_
and	_	_
over	_	_
1	_	_
million	_	_
threads	_	_
at	_	_
the	_	_
maximum	_	_
system	_	_
size	_	_
of	_	_
8192	_	_
processors	_	_
,	_	_
so	_	_
that	_	_
the	_	_
latencies	_	_
are	_	_
hidden	_	_
by	_	_
accommodating	_	_
many	_	_
remote	_	_
memory	_	_
references	_	_
in	_	_
flight	_	_
.	_	_

#206
The	_	_
processor	_	_
's	_	_
instruction	_	_
execution	_	_
hardware	_	_
essentially	_	_
does	_	_
a	_	_
context	_	_
switch	_	_
every	_	_
instruction	_	_
cycle	_	_
,	_	_
finding	_	_
the	_	_
next	_	_
thread	_	_
that	_	_
is	_	_
ready	_	_
to	_	_
issue	_	_
an	_	_
instruction	_	_
into	_	_
the	_	_
execution	_	_
pipeline	_	_
.	_	_

#209
Conceptually	_	_
,	_	_
each	_	_
of	_	_
the	_	_
Threadstorm	_	_
processors	_	_
is	_	_
mapped	_	_
to	_	_
a	_	_
core	_	_
group	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
but	_	_
,	_	_
different	_	_
than	_	_
the	_	_
two	_	_
GPU	_	_
architectures	_	_
,	_	_
it	_	_
has	_	_
only	_	_
one	_	_
core	_	_
on-chip	_	_
,	_	_
thus	_	_
Q	_	_
equals	_	_
1	_	_
.	_	_

#210
TMM	_	_
analysis	_	_
structure	_	_
In	_	_
order	_	_
to	_	_
analyze	_	_
program	_	_
performance	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
we	_	_
must	deontic	_
first	_	_
calculate	_	_
the	_	_
program	_	_
parameters	_	_
for	_	_
the	_	_
particular	_	_
program	_	_
.	_	_

#211
Once	_	_
we	_	_
have	_	_
calculated	_	_
these	_	_
values	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
then	_	_
try	_	_
to	_	_
understand	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
algorithm	_	_
.	_	_

#212
We	_	_
first	_	_
calculate	_	_
the	_	_
effective	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
TE	_	_
.	_	_

#213
The	_	_
effective	_	_
work	_	_
should	deontic	_
consider	_	_
both	_	_
work	_	_
due	_	_
to	_	_
computation	_	_
and	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
.	_	_

#214
Total	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
is	_	_
M⋅L	_	_
,	_	_
but	_	_
since	_	_
this	_	_
work	_	_
is	_	_
hidden	_	_
by	_	_
using	_	_
threads	_	_
,	_	_
the	_	_
real	_	_
effective	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
is	_	_
(	_	_
M⋅L	_	_
)	_	_
/T	_	_
Therefore	_	_
,	_	_
we	_	_
have	_	_
(	_	_
1	_	_
)	_	_
TE=O	_	_
(	_	_
max	_	_
(	_	_
T1	_	_
,	_	_
M⋅LT	_	_
)	_	_
)	_	_
.	_	_

#217
Therefore	_	_
,	_	_
speedup	_	_
on	_	_
P	_	_
cores	_	_
,	_	_
SP	_	_
,	_	_
is	_	_
(	_	_
3	_	_
)	_	_
SP=T1TP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
T1T∞	_	_
,	_	_
P⋅T1⋅TM⋅L	_	_
)	_	_
)	_	_
.	_	_

#218
For	_	_
linear	_	_
speedup	_	_
,	_	_
SP	_	_
should	deontic	_
be	_	_
P.	_	_
More	_	_
precisely	_	_
,	_	_
for	_	_
PRAM	_	_
algorithms	_	_
,	_	_
SP=min	_	_
(	_	_
P	_	_
,	_	_
T1/T∞	_	_
)	_	_
.	_	_

#219
Therefore	_	_
,	_	_
if	_	_
the	_	_
first	_	_
two	_	_
terms	_	_
in	_	_
the	_	_
min	_	_
of	_	_
Eq	_	_
.	_	_
(	_	_
3	_	_
)	_	_
dominate	_	_
,	_	_
then	_	_
a	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
algorithm	_	_
's	_	_
performance	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
corresponding	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#220
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
if	_	_
the	_	_
last	_	_
term	_	_
dominates	_	_
,	_	_
then	_	_
the	_	_
algorithm	_	_
's	_	_
performance	_	_
depends	_	_
on	_	_
other	_	_
factors	_	_
.	_	_

#221
If	_	_
T	_	_
could	feasibility-speculation	_
be	_	_
unbounded	_	_
,	_	_
then	_	_
the	_	_
last	_	_
term	_	_
will	_	_
never	_	_
dominate	_	_
.	_	_

#222
However	_	_
,	_	_
as	_	_
we	_	_
explained	_	_
earlier	_	_
,	_	_
T	_	_
is	_	_
not	_	_
an	_	_
unlimited	_	_
resource	_	_
and	_	_
has	_	_
both	_	_
hardware	_	_
and	_	_
algorithmic	_	_
upper	_	_
bounds	_	_
.	_	_

#223
Therefore	_	_
,	_	_
based	_	_
on	_	_
the	_	_
machine	_	_
parameters	_	_
,	_	_
algorithms	_	_
that	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
can	options	_
have	_	_
different	_	_
real	_	_
performance	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#224
Therefore	_	_
,	_	_
this	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
pick	_	_
algorithms	_	_
that	_	_
provide	_	_
performance	_	_
as	_	_
close	_	_
as	_	_
possible	_	_
to	_	_
PRAM	_	_
algorithms	_	_
.	_	_

#225
Analysis	_	_
of	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
algorithms	_	_
using	_	_
the	_	_
TMM	_	_
model	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
demonstrate	_	_
the	_	_
usefulness	_	_
of	_	_
our	_	_
model	_	_
by	_	_
using	_	_
it	_	_
to	_	_
analyze	_	_
4	_	_
different	_	_
algorithms	_	_
for	_	_
calculating	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
in	_	_
graphs	_	_
.	_	_

#228
Each	_	_
edge	_	_
e	_	_
has	_	_
a	_	_
weight	_	_
w	_	_
(	_	_
e	_	_
)	_	_
.	_	_

#229
We	_	_
must	deontic	_
calculate	_	_
the	_	_
shortest	_	_
weighted	_	_
path	_	_
from	_	_
every	_	_
vertex	_	_
to	_	_
every	_	_
other	_	_
vertex	_	_
.	_	_

#230
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
asymptotic	_	_
insights	_	_
,	_	_
therefore	_	_
,	_	_
we	_	_
assume	_	_
that	_	_
the	_	_
graphs	_	_
are	_	_
large	_	_
graphs	_	_
.	_	_

#235
A1	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
adjacency	_	_
matrix	_	_
A	_	_
and	_	_
we	_	_
want	_	_
to	_	_
calculate	_	_
An-1	_	_
to	_	_
calculate	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
.	_	_

#236
A2	_	_
can	feasibility	_
be	_	_
calculated	_	_
from	_	_
A1	_	_
as	_	_
follows	_	_
:	_	_
(	_	_
4	_	_
)	_	_
Aij2=min0≤k	_	_
<	_	_
n	_	_
(	_	_
Aij1	_	_
,	_	_
Aik1+Akj1	_	_
)	_	_
.	_	_

#237
Note	_	_
that	_	_
,	_	_
the	_	_
structure	_	_
of	_	_
this	_	_
equation	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
matrix	_	_
multiplication	_	_
operation	_	_
where	_	_
the	_	_
sum	_	_
is	_	_
replaced	_	_
by	_	_
a	_	_
min	_	_
operation	_	_
and	_	_
the	_	_
multiplication	_	_
is	_	_
replaced	_	_
by	_	_
an	_	_
addition	_	_
operation	_	_
.	_	_

#238
Therefore	_	_
,	_	_
we	_	_
can	feasibility	_
use	_	_
repeated	_	_
matrix	_	_
multiplication	_	_
which	_	_
calculates	_	_
An	_	_
using	_	_
O	_	_
(	_	_
lgn	_	_
)	_	_
matrix	_	_
multiplications	_	_
.	_	_

#239
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
Parallelizing	_	_
this	_	_
algorithm	_	_
for	_	_
the	_	_
PRAM	_	_
model	_	_
simply	_	_
involves	_	_
parallelizing	_	_
the	_	_
matrix	_	_
multiplication	_	_
algorithm	_	_
such	_	_
that	_	_
each	_	_
element	_	_
in	_	_
the	_	_
matrix	_	_
is	_	_
calculated	_	_
in	_	_
parallel	_	_
.	_	_

#240
The	_	_
total	_	_
work	_	_
of	_	_
lgn	_	_
matrix	_	_
multiplications	_	_
using	_	_
a	_	_
PRAM	_	_
algorithm	_	_
is	_	_
T1=O	_	_
(	_	_
n3lgn	_	_
)	_	_
.33	_	_
This	_	_
can	feasibility	_
be	_	_
done	_	_
faster	_	_
using	_	_
Strassen	_	_
's	_	_
algorithm	_	_
.	_	_

#241
Using	_	_
Strassen	_	_
's	_	_
algorithm	_	_
will	_	_
impact	_	_
the	_	_
PRAM	_	_
and	_	_
the	_	_
TMM	_	_
algorithms	_	_
equally	_	_
.	_	_

#252
The	_	_
work	_	_
and	_	_
the	_	_
span	_	_
of	_	_
this	_	_
algorithm	_	_
remain	_	_
unchanged	_	_
from	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#253
However	_	_
,	_	_
we	_	_
must	deontic	_
also	_	_
calculate	_	_
M	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
.	_	_

#254
Let	_	_
us	_	_
first	_	_
consider	_	_
a	_	_
single	_	_
matrix	_	_
multiplication	_	_
operation	_	_
.	_	_

#255
There	_	_
are	_	_
a	_	_
total	_	_
of	_	_
n2	_	_
elements	_	_
and	_	_
each	_	_
element	_	_
is	_	_
read	_	_
for	_	_
the	_	_
calculation	_	_
of	_	_
O	_	_
(	_	_
n/B	_	_
)	_	_
other	_	_
blocks	_	_
.	_	_

#256
However	_	_
,	_	_
due	_	_
to	_	_
the	_	_
regularity	_	_
in	_	_
memory	_	_
accesses	_	_
,	_	_
each	_	_
block	_	_
can	feasibility	_
be	_	_
read	_	_
fully	_	_
coalesced	_	_
.	_	_

#257
Therefore	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
for	_	_
one	_	_
matrix	_	_
multiply	_	_
is	_	_
O	_	_
(	_	_
(	_	_
n2/C	_	_
)	_	_
⋅	_	_
(	_	_
n/B	_	_
)	_	_
)	_	_
=O	_	_
(	_	_
n3/	_	_
(	_	_
BC	_	_
)	_	_
)	_	_
.	_	_

#258
Also	_	_
note	_	_
that	_	_
since	_	_
we	_	_
must	deontic	_
fit	_	_
a	_	_
B×B	_	_
block	_	_
in	_	_
a	_	_
local	_	_
memory	_	_
of	_	_
size	_	_
Z	_	_
on	_	_
one	_	_
core	_	_
group	_	_
,	_	_
we	_	_
get	_	_
B=Θ	_	_
(	_	_
Z	_	_
)	_	_
.	_	_

#259
Therefore	_	_
,	_	_
for	_	_
lgn	_	_
matrix	_	_
multiplication	_	_
operations	_	_
,	_	_
M=O	_	_
(	_	_
n3lgn/	_	_
(	_	_
Z⋅C	_	_
)	_	_
)	_	_
.	_	_

#262
Therefore	_	_
,	_	_
the	_	_
speedup	_	_
on	_	_
P	_	_
processors	_	_
is	_	_
(	_	_
9	_	_
)	_	_
SP=T1/TP	_	_
(	_	_
10	_	_
)	_	_
=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n2	_	_
,	_	_
Z⋅C⋅TL⋅P	_	_
)	_	_
)	_	_
.	_	_

#263
We	_	_
can	feasibility-rhetorical	_
now	_	_
compare	_	_
the	_	_
PRAM	_	_
and	_	_
TMM	_	_
analysis	_	_
and	_	_
note	_	_
that	_	_
the	_	_
speedup	_	_
is	_	_
P	_	_
as	_	_
long	_	_
as	_	_
ZCT/L≥1	_	_
.	_	_

#264
We	_	_
also	_	_
know	_	_
that	_	_
T≤min	_	_
(	_	_
X	_	_
,	_	_
Z/	_	_
(	_	_
QS	_	_
)	_	_
)	_	_
,	_	_
and	_	_
S=O	_	_
(	_	_
1	_	_
)	_	_
,	_	_
since	_	_
each	_	_
thread	_	_
only	_	_
needs	_	_
constant	_	_
memory	_	_
.	_	_

#265
Therefore	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
conclude	_	_
that	_	_
the	_	_
algorithm	_	_
achieves	_	_
linear	_	_
speedup	_	_
as	_	_
long	_	_
as	_	_
L≤min	_	_
(	_	_
ZCX	_	_
,	_	_
Z3/2C/Q	_	_
)	_	_
.	_	_

#266
Johnson	_	_
's	_	_
algorithm	_	_
:	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
Johnson	_	_
's	_	_
algorithm	_	_
[	_	_
54	_	_
]	_	_
is	_	_
an	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
algorithm	_	_
that	_	_
uses	_	_
Dijkstra	_	_
's	_	_
single	_	_
source	_	_
algorithm	_	_
as	_	_
the	_	_
subroutine	_	_
and	_	_
calls	_	_
it	_	_
n	_	_
times	_	_
,	_	_
once	_	_
from	_	_
each	_	_
source	_	_
vertex	_	_
.	_	_

#280
TMM	_	_
algorithm	_	_
and	_	_
analysis	_	_
The	_	_
TMM	_	_
algorithm	_	_
is	_	_
very	_	_
similar	_	_
to	_	_
the	_	_
PRAM	_	_
algorithm	_	_
where	_	_
each	_	_
thread	_	_
computes	_	_
a	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
.	_	_

#281
Therefore	_	_
,	_	_
each	_	_
thread	_	_
requires	_	_
a	_	_
min-heap	_	_
of	_	_
size	_	_
n.	_	_
Since	_	_
n	_	_
may	capability-options	_
be	_	_
arbitrarily	_	_
large	_	_
compared	_	_
to	_	_
Z/QT	_	_
(	_	_
the	_	_
share	_	_
of	_	_
local	_	_
memory	_	_
for	_	_
each	_	_
thread	_	_
)	_	_
,	_	_
these	_	_
heaps	_	_
can	capability	negation
not	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
and	_	_
must	deontic	_
be	_	_
allocated	_	_
on	_	_
the	_	_
slow	_	_
global	_	_
memory	_	_
.	_	_

#282
The	_	_
work	_	_
and	_	_
span	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#283
We	_	_
must	deontic	_
now	_	_
compute	_	_
M.	_	_
Note	_	_
that	_	_
each	_	_
time	_	_
the	_	_
thread	_	_
does	_	_
a	_	_
heap	_	_
operation	_	_
,	_	_
it	_	_
must	deontic	_
access	_	_
global	_	_
memory	_	_
,	_	_
since	_	_
the	_	_
heaps	_	_
are	_	_
stored	_	_
in	_	_
global	_	_
memory	_	_
.	_	_

#284
In	_	_
addition	_	_
,	_	_
binary	_	_
heap	_	_
accesses	_	_
are	_	_
not	_	_
predictable	_	_
and	_	_
regular	_	_
,	_	_
so	_	_
the	_	_
heap	_	_
accesses	_	_
from	_	_
different	_	_
threads	_	_
can	feasibility	negation
not	_	_
be	_	_
coalesced	_	_
.	_	_

#285
Therefore	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
is	_	_
M=O	_	_
(	_	_
mnlgn	_	_
)	_	_
.44	_	_
There	_	_
are	_	_
other	_	_
accesses	_	_
that	_	_
are	_	_
not	_	_
heap	_	_
accesses	_	_
,	_	_
but	_	_
those	_	_
are	_	_
asymptotically	_	_
fewer	_	_
and	_	_
can	feasibility	_
be	_	_
ignored	_	_
.	_	_

#286
Now	_	_
we	_	_
are	_	_
ready	_	_
to	_	_
calculate	_	_
the	_	_
time	_	_
on	_	_
P	_	_
processors	_	_
.	_	_

#299
Therefore	_	_
,	_	_
there	_	_
are	_	_
n	_	_
arrays	_	_
of	_	_
size	_	_
n	_	_
,	_	_
one	_	_
for	_	_
each	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
calculation	_	_
.	_	_

#300
Each	_	_
decrease-key	_	_
now	_	_
takes	_	_
O	_	_
(	_	_
1	_	_
)	_	_
time	_	_
,	_	_
since	_	_
one	_	_
can	feasibility	_
simply	_	_
decrease	_	_
the	_	_
key	_	_
using	_	_
random	_	_
access	_	_
.	_	_

#301
Each	_	_
delete-min	_	_
,	_	_
however	_	_
,	_	_
takes	_	_
O	_	_
(	_	_
n	_	_
)	_	_
work	_	_
,	_	_
since	_	_
one	_	_
must	deontic	_
look	_	_
at	_	_
the	_	_
entire	_	_
array	_	_
to	_	_
find	_	_
the	_	_
minimum	_	_
element	_	_
.	_	_

#302
Therefore	_	_
,	_	_
the	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
is	_	_
T1=O	_	_
(	_	_
n3+mn	_	_
)	_	_
and	_	_
the	_	_
span	_	_
is	_	_
O	_	_
(	_	_
n2+m	_	_
)	_	_
.	_	_

#303
We	_	_
can	feasibility	_
improve	_	_
the	_	_
span	_	_
by	_	_
doing	_	_
delete-min	_	_
in	_	_
parallel	_	_
,	_	_
since	_	_
one	_	_
can	feasibility	_
find	_	_
the	_	_
smallest	_	_
element	_	_
in	_	_
an	_	_
array	_	_
in	_	_
parallel	_	_
using	_	_
O	_	_
(	_	_
n	_	_
)	_	_
work	_	_
and	_	_
O	_	_
(	_	_
lgn	_	_
)	_	_
time	_	_
using	_	_
a	_	_
parallel	_	_
prefix	_	_
computation	_	_
.	_	_

#304
This	_	_
brings	_	_
the	_	_
total	_	_
span	_	_
to	_	_
T∞=O	_	_
(	_	_
nlgn+m	_	_
)	_	_
while	_	_
the	_	_
work	_	_
remains	_	_
the	_	_
same	_	_
.	_	_

#307
Therefore	_	_
,	_	_
all	_	_
the	_	_
threads	_	_
on	_	_
a	_	_
single	_	_
core	_	_
group	_	_
(	_	_
QT	_	_
in	_	_
number	_	_
)	_	_
cooperate	_	_
to	_	_
calculate	_	_
a	_	_
single	_	_
shortest	_	_
path	_	_
computation	_	_
.	_	_

#308
Since	_	_
we	_	_
assume	_	_
that	_	_
n	_	_
>	_	_
Z	_	_
,	_	_
the	_	_
entire	_	_
array	_	_
does	_	_
not	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
and	_	_
must	deontic	_
be	_	_
read	_	_
with	_	_
each	_	_
delete-min	_	_
operation	_	_
.	_	_

#309
Therefore	_	_
,	_	_
the	_	_
span	_	_
of	_	_
the	_	_
delete-min	_	_
operation	_	_
changes	_	_
.	_	_

#313
The	_	_
work	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
work	_	_
.	_	_

#314
We	_	_
must	deontic	_
now	_	_
compute	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
,	_	_
M.	_	_
There	_	_
are	_	_
n2	_	_
delete-min	_	_
operations	_	_
in	_	_
total	_	_
,	_	_
and	_	_
each	_	_
reads	_	_
the	_	_
array	_	_
of	_	_
size	_	_
n	_	_
coalesced	_	_
.	_	_

#315
In	_	_
addition	_	_
,	_	_
there	_	_
are	_	_
a	_	_
total	_	_
of	_	_
mn	_	_
decrease	_	_
key	_	_
operations	_	_
,	_	_
but	_	_
these	_	_
reads	_	_
can	feasibility	negation
not	_	_
be	_	_
coalesced	_	_
.	_	_

#316
Therefore	_	_
,	_	_
M=O	_	_
(	_	_
n3/C+mn	_	_
)	_	_
.	_	_

#322
The	_	_
algorithm	_	_
is	_	_
given	_	_
in	_	_
Algorithm	_	_
2	_	_
[	_	_
56,57	_	_
]	_	_
.	_	_

#323
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
Again	_	_
,	_	_
one	_	_
can	feasibility	_
do	_	_
each	_	_
single	_	_
source	_	_
computation	_	_
in	_	_
parallel	_	_
.	_	_

#324
Each	_	_
single	_	_
source	_	_
computation	_	_
takes	_	_
O	_	_
(	_	_
mn	_	_
)	_	_
work	_	_
,	_	_
making	_	_
the	_	_
total	_	_
work	_	_
of	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
O	_	_
(	_	_
mn2	_	_
)	_	_
and	_	_
the	_	_
total	_	_
span	_	_
O	_	_
(	_	_
mn	_	_
)	_	_
.	_	_

#325
One	_	_
can	feasibility	_
improve	_	_
the	_	_
span	_	_
by	_	_
relaxing	_	_
all	_	_
edges	_	_
in	_	_
one	_	_
iteration	_	_
in	_	_
parallel	_	_
making	_	_
the	_	_
span	_	_
O	_	_
(	_	_
n	_	_
)	_	_
.	_	_

#326
(	_	_
23	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
mn2P	_	_
,	_	_
n	_	_
)	_	_
)	_	_
.	_	_
(	_	_
24	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
mn	_	_
)	_	_
)	_	_
.	_	_

#329
For	_	_
each	_	_
single	_	_
source	_	_
calculation	_	_
,	_	_
we	_	_
maintain	_	_
three	_	_
arrays	_	_
,	_	_
A	_	_
,	_	_
B	_	_
and	_	_
W	_	_
,	_	_
of	_	_
size	_	_
m	_	_
,	_	_
and	_	_
one	_	_
array	_	_
D	_	_
of	_	_
size	_	_
n.	_	_
D	_	_
contains	_	_
the	_	_
current	_	_
guess	_	_
of	_	_
the	_	_
shortest	_	_
path	_	_
to	_	_
vertex	_	_
i.	_	_
B	_	_
contains	_	_
ending	_	_
vertices	_	_
of	_	_
edges	_	_
,	_	_
sorted	_	_
by	_	_
vertex	_	_
ID	_	_
.	_	_

#330
Therefore	_	_
B	_	_
may	options	_
contain	_	_
multiple	_	_
instances	_	_
of	_	_
the	_	_
same	_	_
vertex	_	_
if	_	_
that	_	_
vertex	_	_
has	_	_
multiple	_	_
incident	_	_
edges	_	_
.	_	_

#331
A	_	_
[	_	_
i	_	_
]	_	_
contains	_	_
the	_	_
starting	_	_
vertex	_	_
of	_	_
the	_	_
edge	_	_
that	_	_
ends	_	_
at	_	_
B	_	_
[	_	_
i	_	_
]	_	_
and	_	_
W	_	_
[	_	_
i	_	_
]	_	_
contains	_	_
the	_	_
weight	_	_
of	_	_
that	_	_
edge	_	_
.	_	_

#334
All	_	_
threads	_	_
relax	_	_
edges	_	_
in	_	_
parallel	_	_
in	_	_
order	_	_
of	_	_
B.	_	_
The	_	_
total	_	_
work	_	_
and	_	_
span	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#335
We	_	_
can	feasibility	_
now	_	_
calculate	_	_
the	_	_
time	_	_
and	_	_
speedup	_	_
assuming	_	_
threads	_	_
can	capability	_
read	_	_
all	_	_
the	_	_
arrays	_	_
coalesced	_	_
,	_	_
M=O	_	_
(	_	_
mn2/C+n3/C	_	_
)	_	_
=O	_	_
(	_	_
mn2/C	_	_
)	_	_
for	_	_
connected	_	_
graphs	_	_
.	_	_

#336
(	_	_
25	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
26	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
mn2P	_	_
,	_	_
n	_	_
,	_	_
mn2⋅LC⋅T⋅P	_	_
)	_	_
)	_	_
.	_	_

#344
We	_	_
have	_	_
ignored	_	_
the	_	_
span	_	_
term	_	_
,	_	_
since	_	_
the	_	_
span	_	_
is	_	_
small	_	_
relative	_	_
to	_	_
work	_	_
in	_	_
all	_	_
of	_	_
these	_	_
algorithms	_	_
.	_	_

#345
As	_	_
we	_	_
can	feasibility-rhetorical	_
see	_	_
,	_	_
if	_	_
L	_	_
is	_	_
small	_	_
,	_	_
then	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
provide	_	_
PRAM	_	_
performance	_	_
.	_	_

#346
However	_	_
,	_	_
the	_	_
cut-off	_	_
value	_	_
for	_	_
L	_	_
is	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
where	_	_
the	_	_
performance	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
differs	_	_
from	_	_
the	_	_
PRAM	_	_
model	_	_
is	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
.	_	_

#347
Therefore	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability-options	_
be	_	_
informative	_	_
when	_	_
comparing	_	_
between	_	_
algorithms	_	_
.	_	_

#348
We	_	_
will	_	_
perform	_	_
two	_	_
types	_	_
of	_	_
comparison	_	_
between	_	_
these	_	_
algorithms	_	_
in	_	_
this	_	_
section	_	_
.	_	_

#349
The	_	_
first	_	_
one	_	_
considers	_	_
the	_	_
direct	_	_
influence	_	_
of	_	_
machine	_	_
parameters	_	_
on	_	_
asymptotic	_	_
performance	_	_
.	_	_

#350
Since	_	_
machine	_	_
parameters	_	_
do	_	_
not	_	_
scale	_	_
with	_	_
problem	_	_
size	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
machine	_	_
parameters	_	_
can	capability	negation
not	_	_
change	_	_
the	_	_
asymptotic	_	_
performance	_	_
of	_	_
algorithms	_	_
in	_	_
terms	_	_
of	_	_
problem	_	_
size	_	_
.	_	_

#351
That	_	_
is	_	_
,	_	_
if	_	_
the	_	_
PRAM	_	_
analysis	_	_
indicates	_	_
that	_	_
some	_	_
algorithm	_	_
has	_	_
a	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
n	_	_
)	_	_
and	_	_
another	_	_
one	_	_
has	_	_
the	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
nlgn	_	_
)	_	_
,	_	_
for	_	_
large	_	_
enough	_	_
n	_	_
,	_	_
the	_	_
first	_	_
algorithm	_	_
is	_	_
always	_	_
asymptotically	_	_
better	_	_
since	_	_
eventually	_	_
lgn	_	_
will	_	_
dominate	_	_
whatever	_	_
machine	_	_
parameter	_	_
advantage	_	_
the	_	_
second	_	_
algorithm	_	_
may	options	_
have	_	_
.	_	_

#352
Therefore	_	_
,	_	_
for	_	_
this	_	_
first	_	_
comparison	_	_
,	_	_
we	_	_
only	_	_
compare	_	_
algorithms	_	_
which	_	_
have	_	_
the	_	_
same	_	_
asymptotic	_	_
performance	_	_
under	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#354
In	_	_
particular	_	_
,	_	_
we	_	_
look	_	_
at	_	_
the	_	_
case	_	_
when	_	_
lgn	_	_
<	_	_
Z	_	_
.	_	_

#355
In	_	_
this	_	_
case	_	_
,	_	_
even	_	_
algorithms	_	_
that	_	_
are	_	_
asymptotically	_	_
worse	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
can	options	_
be	_	_
better	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
for	_	_
large	_	_
latency	_	_
L.	_	_
In	_	_
the	_	_
next	_	_
section	_	_
,	_	_
we	_	_
will	_	_
look	_	_
at	_	_
even	_	_
smaller	_	_
problem	_	_
sizes	_	_
where	_	_
the	_	_
effects	_	_
are	_	_
even	_	_
more	_	_
dramatic	_	_
.	_	_

#356
Influence	_	_
of	_	_
machine	_	_
parameters	_	_
As	_	_
the	_	_
table	_	_
shows	_	_
,	_	_
the	_	_
limits	_	_
on	_	_
machine	_	_
parameters	_	_
to	_	_
get	_	_
linear	_	_
speedup	_	_
are	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
.	_	_

#357
Therefore	_	_
,	_	_
even	_	_
when	_	_
two	_	_
algorithms	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
,	_	_
their	_	_
performance	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
may	options	_
vary	_	_
significantly	_	_
.	_	_

#358
Let	_	_
us	_	_
consider	_	_
a	_	_
few	_	_
examples	_	_
:	_	_
Dynamic	_	_
programming	_	_
vs	_	_
.	_	_

#366
Influence	_	_
of	_	_
graph	_	_
size	_	_
The	_	_
previous	_	_
section	_	_
shows	_	_
the	_	_
asymptotic	_	_
power	_	_
of	_	_
the	_	_
model	_	_
;	_	_
the	_	_
results	_	_
there	_	_
hold	_	_
for	_	_
large	_	_
sizes	_	_
of	_	_
graphs	_	_
asymptotically	_	_
.	_	_

#367
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
also	_	_
help	_	_
decide	_	_
on	_	_
what	_	_
algorithm	_	_
to	_	_
use	_	_
based	_	_
on	_	_
the	_	_
size	_	_
of	_	_
the	_	_
graph	_	_
.	_	_

#368
In	_	_
particular	_	_
for	_	_
certain	_	_
sizes	_	_
of	_	_
graphs	_	_
,	_	_
algorithm	_	_
A	_	_
can	options	_
be	_	_
better	_	_
than	_	_
algorithm	_	_
B	_	_
even	_	_
if	_	_
it	_	_
is	_	_
asymptotically	_	_
worse	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#369
Therefore	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
give	_	_
us	_	_
information	_	_
that	_	_
the	_	_
PRAM	_	_
model	_	_
can	capability	negation
not	_	_
.	_	_

#370
Consider	_	_
the	_	_
example	_	_
of	_	_
dynamic	_	_
programming	_	_
vs	_	_
.	_	_

#372
In	_	_
the	_	_
PRAM	_	_
model	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
is	_	_
unquestionably	_	_
worse	_	_
than	_	_
Johnson	_	_
's	_	_
.	_	_

#373
However	_	_
,	_	_
if	_	_
lgn	_	_
<	_	_
Z	_	_
,	_	_
we	_	_
may	feasibility-options	_
have	_	_
a	_	_
different	_	_
conclusion	_	_
.	_	_

#374
In	_	_
this	_	_
case	_	_
,	_	_
dynamic	_	_
programming	_	_
has	_	_
runtime	_	_
:	_	_
(	_	_
28	_	_
)	_	_
n3lgn⋅LZCTP=n2LTP⋅nlgnZC	_	_
<	_	_
n2LTP⋅nC	_	_
.	_	_

#378
This	_	_
indicates	_	_
that	_	_
when	_	_
for	_	_
small	_	_
enough	_	_
graphs	_	_
where	_	_
lgn	_	_
<	_	_
Z	_	_
,	_	_
there	_	_
is	_	_
a	_	_
dichotomy	_	_
.	_	_

#379
For	_	_
dense	_	_
graphs	_	_
n2/m	_	_
<	_	_
C	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
should	deontic	_
be	_	_
preferred	_	_
,	_	_
while	_	_
for	_	_
sparse	_	_
graphs	_	_
,	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
is	_	_
better	_	_
.	_	_

#380
We	_	_
illustrate	_	_
this	_	_
performance	_	_
dependence	_	_
on	_	_
sparsity	_	_
with	_	_
experiments	_	_
in	_	_
Section	_	_
7	_	_
.	_	_

#383
Our	_	_
model	_	_
therefore	_	_
allows	_	_
us	_	_
to	_	_
do	_	_
two	_	_
things	_	_
.	_	_

#384
First	_	_
,	_	_
for	_	_
a	_	_
particular	_	_
machine	_	_
,	_	_
given	_	_
two	_	_
algorithms	_	_
which	_	_
are	_	_
asymptotically	_	_
similar	_	_
,	_	_
we	_	_
can	feasibility	_
pick	_	_
the	_	_
more	_	_
appropriate	_	_
algorithm	_	_
for	_	_
that	_	_
particular	_	_
machine	_	_
given	_	_
its	_	_
machine	_	_
parameters	_	_
.	_	_

#385
Second	_	_
,	_	_
if	_	_
we	_	_
also	_	_
consider	_	_
the	_	_
problem	_	_
size	_	_
,	_	_
then	_	_
we	_	_
can	feasibility	_
do	_	_
more	_	_
.	_	_

#386
For	_	_
small	_	_
problem	_	_
sizes	_	_
,	_	_
the	_	_
asymptotically	_	_
worse	_	_
algorithm	_	_
may	capability-options	_
in	_	_
fact	_	_
be	_	_
better	_	_
because	_	_
it	_	_
interacts	_	_
better	_	_
with	_	_
the	_	_
machine	_	_
.	_	_

#387
We	_	_
will	_	_
draw	_	_
more	_	_
insights	_	_
of	_	_
this	_	_
type	_	_
in	_	_
the	_	_
next	_	_
section	_	_
.	_	_

#388
Effect	_	_
of	_	_
problem	_	_
size	_	_
In	_	_
Section	_	_
5	_	_
,	_	_
we	_	_
explored	_	_
the	_	_
asymptotic	_	_
insights	_	_
that	_	_
can	feasibility	_
be	_	_
drawn	_	_
from	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#389
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
also	_	_
inform	_	_
insights	_	_
based	_	_
on	_	_
problem	_	_
size	_	_
.	_	_

#390
In	_	_
particular	_	_
,	_	_
some	_	_
algorithms	_	_
can	capability	_
take	_	_
advantage	_	_
of	_	_
smaller	_	_
problems	_	_
better	_	_
than	_	_
others	_	_
,	_	_
since	_	_
they	_	_
can	capability	_
use	_	_
fast	_	_
local	_	_
memory	_	_
more	_	_
effectively	_	_
.	_	_

#391
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
explore	_	_
the	_	_
insights	_	_
that	_	_
the	_	_
TMM	_	_
model	_	_
provides	_	_
in	_	_
these	_	_
cases	_	_
.	_	_

#392
Vertices	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
When	_	_
n	_	_
<	_	_
Z	_	_
,	_	_
all	_	_
the	_	_
vertices	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
.	_	_

#393
Note	_	_
that	_	_
this	_	_
does	_	_
not	_	_
mean	_	_
that	_	_
the	_	_
entire	_	_
problem	_	_
fits	_	_
in	_	_
local	_	_
memory	_	_
,	_	_
since	_	_
the	_	_
number	_	_
of	_	_
edges	_	_
can	options	_
still	_	_
be	_	_
much	_	_
larger	_	_
than	_	_
the	_	_
number	_	_
of	_	_
vertices	_	_
.	_	_

#394
In	_	_
this	_	_
scenario	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
by	_	_
the	_	_
first	_	_
,	_	_
second	_	_
,	_	_
and	_	_
fourth	_	_
algorithms	_	_
is	_	_
not	_	_
affected	_	_
at	_	_
all	_	_
.	_	_

#396
In	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
,	_	_
each	_	_
thread	_	_
does	_	_
its	_	_
own	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
.	_	_

#397
Since	_	_
the	_	_
local	_	_
memory	_	_
Z	_	_
is	_	_
shared	_	_
among	_	_
QT	_	_
threads	_	_
,	_	_
each	_	_
thread	_	_
can	capability	negation
not	_	_
hold	_	_
its	_	_
entire	_	_
vertex	_	_
array	_	_
in	_	_
local	_	_
memory	_	_
.	_	_

#398
In	_	_
the	_	_
Bellman-Ford	_	_
algorithm	_	_
,	_	_
the	_	_
cost	_	_
is	_	_
dominated	_	_
by	_	_
the	_	_
cost	_	_
of	_	_
reading	_	_
the	_	_
edges	_	_
.	_	_

#400
For	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
,	_	_
the	_	_
cost	_	_
is	_	_
lower	_	_
.	_	_

#401
Now	_	_
each	_	_
core	_	_
group	_	_
can	capability	_
store	_	_
the	_	_
vertex	_	_
array	_	_
and	_	_
does	_	_
not	_	_
need	_	_
to	_	_
access	_	_
it	_	_
from	_	_
slow	_	_
memory	_	_
.	_	_

#402
Therefore	_	_
the	_	_
bound	_	_
on	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
changes	_	_
to	_	_
M=O	_	_
(	_	_
n2/C+mn	_	_
)	_	_
=O	_	_
(	_	_
mn	_	_
)	_	_
for	_	_
connected	_	_
graphs	_	_
.	_	_

#403
For	_	_
these	_	_
small	_	_
problem	_	_
sizes	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability-options	_
provide	_	_
even	_	_
more	_	_
insight	_	_
.	_	_

#404
As	_	_
an	_	_
example	_	_
,	_	_
compare	_	_
the	_	_
two	_	_
versions	_	_
of	_	_
Johnson	_	_
's	_	_
algorithm	_	_
,	_	_
the	_	_
one	_	_
that	_	_
uses	_	_
arrays	_	_
and	_	_
the	_	_
one	_	_
that	_	_
uses	_	_
heaps	_	_
.	_	_

#407
Therefore	_	_
,	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
arrays	_	_
is	_	_
better	_	_
.	_	_

#408
Note	_	_
that	_	_
asymptotic	_	_
analysis	_	_
is	_	_
a	_	_
little	_	_
dubious	_	_
when	_	_
we	_	_
are	_	_
talking	_	_
about	_	_
small	_	_
problem	_	_
sizes	_	_
;	_	_
therefore	_	_
,	_	_
this	_	_
analysis	_	_
should	deontic	_
be	_	_
considered	_	_
skeptically	_	_
.	_	_

#409
However	_	_
,	_	_
the	_	_
analysis	_	_
is	_	_
rigorous	_	_
when	_	_
we	_	_
consider	_	_
the	_	_
circumstance	_	_
that	_	_
local	_	_
memory	_	_
size	_	_
grows	_	_
with	_	_
problem	_	_
size	_	_
(	_	_
i.e.	_	_
,	_	_
Z	_	_
is	_	_
asymptotic	_	_
)	_	_
.	_	_

#410
Moreover	_	_
,	_	_
this	_	_
type	_	_
of	_	_
analysis	_	_
can	capability-feasibility	_
still	_	_
provide	_	_
enough	_	_
insight	_	_
that	_	_
it	_	_
might	capability-speculation	_
guide	_	_
implementation	_	_
decisions	_	_
under	_	_
the	_	_
more	_	_
realistic	_	_
circumstance	_	_
of	_	_
bounded	_	_
(	_	_
but	_	_
potentially	_	_
large	_	_
)	_	_
Z.	_	_
Edges	_	_
fit	_	_
in	_	_
the	_	_
combined	_	_
local	_	_
memories	_	_
When	_	_
m=O	_	_
(	_	_
PZ/Q	_	_
)	_	_
,	_	_
the	_	_
edges	_	_
fit	_	_
in	_	_
all	_	_
the	_	_
memories	_	_
of	_	_
the	_	_
core	_	_
groups	_	_
combined	_	_
.	_	_

#411
Again	_	_
,	_	_
the	_	_
running	_	_
time	_	_
of	_	_
the	_	_
first	_	_
,	_	_
second	_	_
,	_	_
and	_	_
third	_	_
algorithms	_	_
do	_	_
not	_	_
change	_	_
,	_	_
since	_	_
they	_	_
can	capability	negation
not	_	_
take	_	_
advantage	_	_
of	_	_
this	_	_
property	_	_
.	_	_

#412
However	_	_
,	_	_
the	_	_
Bellman-Ford	_	_
algorithm	_	_
can	capability	_
take	_	_
advantage	_	_
of	_	_
this	_	_
property	_	_
and	_	_
each	_	_
thread	_	_
across	_	_
all	_	_
core	_	_
groups	_	_
is	_	_
responsible	_	_
for	_	_
relaxing	_	_
a	_	_
single	_	_
edge	_	_
.	_	_

#413
Now	_	_
a	_	_
portion	_	_
of	_	_
the	_	_
arrays	_	_
A	_	_
,	_	_
B	_	_
and	_	_
W	_	_
fit	_	_
in	_	_
each	_	_
core	_	_
group	_	_
's	_	_
local	_	_
memory	_	_
and	_	_
they	_	_
never	_	_
have	_	_
to	_	_
be	_	_
read	_	_
again	_	_
.	_	_

#429
•	_	_
Comparison	_	_
of	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
:	_	_
for	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
,	_	_
the	_	_
PRAM	_	_
performance	_	_
does	_	_
not	_	_
depend	_	_
on	_	_
the	_	_
graph	_	_
's	_	_
density	_	_
.	_	_

#430
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
performance	_	_
can	options	_
depend	_	_
on	_	_
the	_	_
graph	_	_
's	_	_
density	_	_
,	_	_
when	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
insufficient	_	_
for	_	_
the	_	_
performance	_	_
to	_	_
be	_	_
equivalent	_	_
to	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#431
Therefore	_	_
,	_	_
even	_	_
though	_	_
Johnson	_	_
's	_	_
algorithm	_	_
is	_	_
always	_	_
faster	_	_
than	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
according	_	_
to	_	_
the	_	_
PRAM	_	_
model	_	_
(	_	_
since	_	_
its	_	_
work	_	_
is	_	_
n3	_	_
while	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
has	_	_
work	_	_
n3lgn	_	_
)	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
when	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
small	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
may	capability-speculation	_
do	_	_
better	_	_
,	_	_
especially	_	_
for	_	_
dense	_	_
graphs	_	_
.	_	_

#432
We	_	_
demonstrate	_	_
through	_	_
experiments	_	_
that	_	_
,	_	_
this	_	_
is	_	_
a	_	_
true	_	_
indicator	_	_
of	_	_
performance	_	_
.	_	_

#433
Experimental	_	_
Setup	_	_
The	_	_
experiments	_	_
are	_	_
carried	_	_
out	_	_
on	_	_
an	_	_
NVIDIA	_	_
GTX	_	_
480	_	_
,	_	_
which	_	_
has	_	_
15	_	_
multiprocessors	_	_
,	_	_
each	_	_
with	_	_
32	_	_
cores	_	_
.	_	_

#434
As	_	_
a	_	_
typical	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machine	_	_
,	_	_
it	_	_
also	_	_
features	_	_
a	_	_
1.5	_	_
GB	_	_
global	_	_
memory	_	_
and	_	_
16	_	_
kB/48	_	_
kB	_	_
of	_	_
configurable	_	_
on-chip	_	_
shared	_	_
memory	_	_
per	_	_
multiprocessor	_	_
,	_	_
which	_	_
can	capability-feasibility	_
be	_	_
accessed	_	_
with	_	_
latency	_	_
significantly	_	_
lower	_	_
than	_	_
the	_	_
global	_	_
memory	_	_
.	_	_

#435
Runtimes	_	_
are	_	_
measured	_	_
across	_	_
various	_	_
configurations	_	_
of	_	_
each	_	_
problem	_	_
,	_	_
including	_	_
graph	_	_
size	_	_
,	_	_
thread	_	_
count	_	_
,	_	_
shared	_	_
memory	_	_
size	_	_
,	_	_
and	_	_
graph	_	_
density	_	_
.	_	_

#444
For	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
,	_	_
we	_	_
generate	_	_
random	_	_
graphs	_	_
with	_	_
{	_	_
1k,2k,4k,8k,16k	_	_
}	_	_
vertices	_	_
.	_	_

#445
To	_	_
better	_	_
utilize	_	_
fast	_	_
local	_	_
memory	_	_
,	_	_
the	_	_
problem	_	_
is	_	_
decomposed	_	_
into	_	_
sub-blocks	_	_
,	_	_
and	_	_
we	_	_
must	deontic	_
also	_	_
pick	_	_
a	_	_
block	_	_
size	_	_
.	_	_

#446
Since	_	_
we	_	_
only	_	_
care	_	_
about	_	_
the	_	_
effect	_	_
of	_	_
threads	_	_
and	_	_
not	_	_
the	_	_
effect	_	_
of	_	_
shared	_	_
memory	_	_
(	_	_
to	_	_
be	_	_
considered	_	_
in	_	_
the	_	_
next	_	_
subsection	_	_
)	_	_
,	_	_
here	_	_
we	_	_
show	_	_
the	_	_
results	_	_
with	_	_
a	_	_
block	_	_
size	_	_
of	_	_
64	_	_
,	_	_
as	_	_
it	_	_
allows	_	_
us	_	_
to	_	_
generate	_	_
the	_	_
maximum	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#472
Fig	_	_
.	_	_
6	_	_
illustrates	_	_
how	_	_
this	_	_
change	_	_
has	_	_
an	_	_
impact	_	_
on	_	_
speedup	_	_
across	_	_
a	_	_
range	_	_
of	_	_
threads/core	_	_
.	_	_

#473
For	_	_
a	_	_
fixed	_	_
Z	_	_
(	_	_
fast	_	_
memory	_	_
size	_	_
)	_	_
,	_	_
the	_	_
maximum	_	_
sub-block	_	_
size	_	_
B	_	_
can	capability-feasibility	_
be	_	_
determined	_	_
.	_	_

#474
Then	_	_
,	_	_
varying	_	_
thread	_	_
counts	_	_
has	_	_
the	_	_
same	_	_
effect	_	_
as	_	_
previously	_	_
illustrated	_	_
in	_	_
Fig	_	_
.	_	_
3	_	_
,	_	_
increasing	_	_
threads/core	_	_
increases	_	_
performance	_	_
until	_	_
the	_	_
PRAM	_	_
range	_	_
is	_	_
reached	_	_
.	_	_

#475
But	_	_
as	_	_
we	_	_
can	feasibility-rhetorical	_
see	_	_
from	_	_
the	_	_
figure	_	_
,	_	_
different	_	_
block	_	_
sizes	_	_
have	_	_
different	_	_
performance	_	_
for	_	_
the	_	_
same	_	_
number	_	_
of	_	_
threads/core	_	_
.	_	_

#476
This	_	_
effect	_	_
is	_	_
predicted	_	_
by	_	_
Eq	_	_
.	_	_
(	_	_
10	_	_
)	_	_
.	_	_

#477
As	_	_
we	_	_
increase	_	_
the	_	_
size	_	_
of	_	_
local	_	_
memory	_	_
,	_	_
the	_	_
performance	_	_
improves	_	_
,	_	_
since	_	_
we	_	_
can	feasibility	_
use	_	_
bigger	_	_
blocks	_	_
.	_	_

#478
In	_	_
order	_	_
to	_	_
isolate	_	_
the	_	_
effect	_	_
of	_	_
block	_	_
size	_	_
from	_	_
the	_	_
effects	_	_
of	_	_
other	_	_
parameters	_	_
,	_	_
we	_	_
also	_	_
plot	_	_
this	_	_
data	_	_
in	_	_
a	_	_
pair	_	_
of	_	_
different	_	_
formats	_	_
in	_	_
Figs	_	_
.	_	_
7	_	_
and	_	_
8	_	_
,	_	_
in	_	_
both	_	_
cases	_	_
limiting	_	_
the	_	_
number	_	_
of	_	_
threads/core	_	_
to	_	_
below	_	_
the	_	_
PRAM	_	_
range	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
range	_	_
where	_	_
speedup	_	_
is	_	_
linear	_	_
in	_	_
threads/core	_	_
)	_	_
.	_	_

#481
Comparison	_	_
between	_	_
the	_	_
dynamic	_	_
programming	_	_
and	_	_
Johnson	_	_
's	_	_
algorithms	_	_
It	_	_
is	_	_
interesting	_	_
to	_	_
compare	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
,	_	_
since	_	_
the	_	_
PRAM	_	_
and	_	_
the	_	_
TMM	_	_
model	_	_
differ	_	_
in	_	_
predicting	_	_
the	_	_
relative	_	_
performance	_	_
of	_	_
these	_	_
algorithms	_	_
.	_	_

#482
The	_	_
PRAM	_	_
model	_	_
predicts	_	_
that	_	_
Johnson	_	_
's	_	_
algorithm	_	_
should	inference	_
always	_	_
be	_	_
better	_	_
.	_	_

#483
However	_	_
,	_	_
from	_	_
Section	_	_
5.2	_	_
,	_	_
for	_	_
a	_	_
small	_	_
number	_	_
of	_	_
threads/core	_	_
working	_	_
on	_	_
a	_	_
dense	_	_
graph	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
dynamic	_	_
programming	_	_
may	capability-speculation	_
be	_	_
better	_	_
.	_	_

#484
For	_	_
the	_	_
graphs	_	_
with	_	_
8k	_	_
vertices	_	_
that	_	_
we	_	_
explored	_	_
earlier	_	_
,	_	_
lgn	_	_
<	_	_
Z	_	_
.	_	_

#494
In	_	_
particular	_	_
,	_	_
it	_	_
requires	_	_
the	_	_
work	_	_
and	_	_
depth	_	_
(	_	_
like	_	_
PRAM	_	_
algorithms	_	_
)	_	_
,	_	_
but	_	_
also	_	_
requires	_	_
the	_	_
analysis	_	_
of	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
.	_	_

#495
Using	_	_
these	_	_
three	_	_
values	_	_
,	_	_
we	_	_
can	feasibility	_
properly	_	_
order	_	_
algorithms	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
for	_	_
many	_	_
different	_	_
settings	_	_
of	_	_
machine	_	_
parameters	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#496
We	_	_
analyzed	_	_
4	_	_
shortest	_	_
paths	_	_
algorithms	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
and	_	_
compared	_	_
the	_	_
analysis	_	_
with	_	_
the	_	_
PRAM	_	_
analysis	_	_
.	_	_

#497
We	_	_
find	_	_
that	_	_
algorithms	_	_
with	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
can	options	_
have	_	_
different	_	_
TMM	_	_
performance	_	_
under	_	_
certain	_	_
machine	_	_
parameter	_	_
settings	_	_
.	_	_

#498
In	_	_
addition	_	_
,	_	_
for	_	_
certain	_	_
problem	_	_
sizes	_	_
which	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
,	_	_
algorithms	_	_
which	_	_
are	_	_
faster	_	_
on	_	_
PRAM	_	_
may	capability-options	_
be	_	_
slower	_	_
under	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#499
Further	_	_
,	_	_
we	_	_
implemented	_	_
a	_	_
pair	_	_
of	_	_
the	_	_
algorithms	_	_
and	_	_
showed	_	_
empirical	_	_
performance	_	_
is	_	_
effectively	_	_
predicted	_	_
by	_	_
the	_	_
TMM	_	_
model	_	_
under	_	_
a	_	_
variety	_	_
of	_	_
circumstances	_	_
.	_	_

#503
One	_	_
obvious	_	_
direction	_	_
is	_	_
to	_	_
design	_	_
more	_	_
algorithms	_	_
under	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#504
Ideally	_	_
,	_	_
this	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
come	_	_
up	_	_
with	_	_
new	_	_
algorithms	_	_
for	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#505
Empirical	_	_
validation	_	_
of	_	_
the	_	_
TMM	_	_
model	_	_
across	_	_
a	_	_
wider	_	_
number	_	_
of	_	_
physical	_	_
machines	_	_
and	_	_
manufacturers	_	_
is	_	_
also	_	_
worth	_	_
doing	_	_
.	_	_

#507
While	_	_
in	_	_
this	_	_
paper	_	_
we	_	_
assume	_	_
that	_	_
it	_	_
is	_	_
global	_	_
memory	_	_
vs	_	_
.	_	_

#508
memory	_	_
local	_	_
to	_	_
core	_	_
groups	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
it	_	_
can	options	_
be	_	_
any	_	_
two	_	_
levels	_	_
of	_	_
fast	_	_
and	_	_
slow	_	_
memory	_	_
.	_	_

#509
We	_	_
would	_	_
like	_	_
to	_	_
extend	_	_
it	_	_
to	_	_
multi-level	_	_
hierarchies	_	_
which	_	_
are	_	_
becoming	_	_
increasingly	_	_
common	_	_
.	_	_

#511
Other	_	_
than	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
,	_	_
all	_	_
of	_	_
the	_	_
algorithms	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
are	_	_
,	_	_
in	_	_
fact	_	_
,	_	_
parameter-oblivious	_	_
.	_	_

#512
And	_	_
matrix	_	_
multiplication	_	_
in	_	_
the	_	_
dynamic	_	_
programming	_	_
can	feasibility	_
easily	_	_
be	_	_
made	_	_
parameter-oblivious	_	_
.	_	_

#513
In	_	_
this	_	_
case	_	_
,	_	_
the	_	_
algorithms	_	_
should	inference	_
perform	_	_
well	_	_
under	_	_
all	_	_
settings	_	_
of	_	_
parameters	_	_
,	_	_
allowing	_	_
us	_	_
to	_	_
apply	_	_
the	_	_
model	_	_
at	_	_
any	_	_
two	_	_
levels	_	_
and	_	_
get	_	_
the	_	_
same	_	_
results	_	_
.	_	_

#514
Acknowledgments	_	_
This	_	_
work	_	_
was	_	_
supported	_	_
by	_	_
NSF	_	_
grants	_	_
CNS-0905368	_	_
and	_	_
CNS-0931693	_	_
and	_	_
Exegy	_	_
,	_	_
Inc	_	_
.	_	_