#0
A	_	_
memory	_	_
access	_	_
model	_	_
for	_	_
highly-threaded	_	_
many-core	_	_
architectures	_	_
Abstract	_	_
A	_	_
number	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
hide	_	_
memory-access	_	_
latency	_	_
by	_	_
low-overhead	_	_
context	_	_
switching	_	_
among	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#1
The	_	_
speedup	_	_
of	_	_
a	_	_
program	_	_
on	_	_
these	_	_
machines	_	_
depends	_	_
on	_	_
how	_	_
well	_	_
the	_	_
latency	_	_
is	_	_
hidden	_	_
.	_	_

#2
If	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
were	_	_
infinite	_	_
,	_	_
theoretically	_	_
,	_	_
these	_	_
machines	_	_
could	capability-speculation	_
provide	_	_
the	_	_
performance	_	_
predicted	_	_
by	_	_
the	_	_
PRAM	_	_
analysis	_	_
of	_	_
these	_	_
programs	_	_
.	_	_

#3
However	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
is	_	_
not	_	_
infinite	_	_
,	_	_
and	_	_
is	_	_
constrained	_	_
by	_	_
both	_	_
hardware	_	_
and	_	_
algorithmic	_	_
limits	_	_
.	_	_

#4
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
introduce	_	_
the	_	_
Threaded	_	_
Many-core	_	_
Memory	_	_
(	_	_
TMM	_	_
)	_	_
model	_	_
which	_	_
is	_	_
meant	_	_
to	_	_
capture	_	_
the	_	_
important	_	_
characteristics	_	_
of	_	_
these	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#5
Since	_	_
we	_	_
model	_	_
some	_	_
important	_	_
machine	_	_
parameters	_	_
of	_	_
these	_	_
machines	_	_
,	_	_
we	_	_
expect	_	_
analysis	_	_
under	_	_
this	_	_
model	_	_
to	_	_
provide	_	_
a	_	_
more	_	_
fine-grained	_	_
and	_	_
accurate	_	_
performance	_	_
prediction	_	_
than	_	_
the	_	_
PRAM	_	_
analysis	_	_
.	_	_

#6
We	_	_
analyze	_	_
4	_	_
algorithms	_	_
for	_	_
the	_	_
classic	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
problem	_	_
under	_	_
this	_	_
model	_	_
.	_	_

#7
We	_	_
find	_	_
that	_	_
even	_	_
when	_	_
two	_	_
algorithms	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
,	_	_
our	_	_
model	_	_
predicts	_	_
different	_	_
performance	_	_
for	_	_
some	_	_
settings	_	_
of	_	_
machine	_	_
parameters	_	_
.	_	_

#8
For	_	_
example	_	_
,	_	_
for	_	_
dense	_	_
graphs	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
have	_	_
the	_	_
same	_	_
performance	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#9
However	_	_
,	_	_
our	_	_
model	_	_
predicts	_	_
different	_	_
performance	_	_
for	_	_
large	_	_
enough	_	_
memory-access	_	_
latency	_	_
and	_	_
validates	_	_
the	_	_
intuition	_	_
that	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
performs	_	_
better	_	_
on	_	_
these	_	_
machines	_	_
.	_	_

#10
We	_	_
validate	_	_
several	_	_
predictions	_	_
made	_	_
by	_	_
our	_	_
model	_	_
using	_	_
empirical	_	_
measurements	_	_
on	_	_
an	_	_
instantiation	_	_
of	_	_
a	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machine	_	_
,	_	_
namely	_	_
the	_	_
NVIDIA	_	_
GTX	_	_
480	_	_
.	_	_

#11
Highlights	_	_
•	_	_
We	_	_
design	_	_
a	_	_
memory	_	_
model	_	_
to	_	_
analyze	_	_
algorithms	_	_
for	_	_
highly-threaded	_	_
many-core	_	_
systems	_	_
.	_	_

#12
•	_	_
The	_	_
model	_	_
captures	_	_
significant	_	_
factors	_	_
of	_	_
performance	_	_
:	_	_
work	_	_
,	_	_
span	_	_
,	_	_
and	_	_
memory	_	_
accesses	_	_
.	_	_

#13
•	_	_
We	_	_
show	_	_
the	_	_
model	_	_
is	_	_
better	_	_
than	_	_
PRAM	_	_
by	_	_
applying	_	_
both	_	_
to	_	_
4	_	_
shortest	_	_
paths	_	_
algorithms	_	_
.	_	_

#14
•	_	_
Empirical	_	_
performance	_	_
is	_	_
effectively	_	_
predicted	_	_
by	_	_
our	_	_
model	_	_
in	_	_
many	_	_
circumstances	_	_
.	_	_

#15
•	_	_
It	_	_
is	_	_
the	_	_
first	_	_
formalized	_	_
asymptotic	_	_
model	_	_
helpful	_	_
for	_	_
algorithm	_	_
design	_	_
on	_	_
many-cores	_	_
.	_	_

#16
Introduction	_	_
Highly-threaded	_	_
,	_	_
many-core	_	_
devices	_	_
such	_	_
as	_	_
GPUs	_	_
have	_	_
gained	_	_
popularity	_	_
in	_	_
the	_	_
last	_	_
decade	_	_
;	_	_
both	_	_
NVIDIA	_	_
and	_	_
AMD	_	_
manufacture	_	_
general	_	_
purpose	_	_
GPUs	_	_
that	_	_
fall	_	_
in	_	_
this	_	_
category	_	_
.	_	_

#17
The	_	_
important	_	_
distinction	_	_
between	_	_
these	_	_
machines	_	_
and	_	_
traditional	_	_
multi-core	_	_
machines	_	_
is	_	_
that	_	_
these	_	_
devices	_	_
provide	_	_
a	_	_
large	_	_
number	_	_
of	_	_
low-overhead	_	_
hardware	_	_
threads	_	_
with	_	_
low-overhead	_	_
context	_	_
switching	_	_
between	_	_
them	_	_
;	_	_
this	_	_
fast	_	_
context-switch	_	_
mechanism	_	_
is	_	_
used	_	_
to	_	_
hide	_	_
the	_	_
memory	_	_
access	_	_
latency	_	_
of	_	_
transferring	_	_
data	_	_
from	_	_
slow	_	_
large	_	_
(	_	_
and	_	_
often	_	_
global	_	_
)	_	_
memory	_	_
to	_	_
fast	_	_
,	_	_
small	_	_
(	_	_
and	_	_
typically	_	_
local	_	_
)	_	_
memory	_	_
.	_	_

#18
Researchers	_	_
have	_	_
designed	_	_
algorithms	_	_
to	_	_
solve	_	_
many	_	_
interesting	_	_
problems	_	_
for	_	_
these	_	_
devices	_	_
,	_	_
such	_	_
as	_	_
GPU	_	_
sorting	_	_
or	_	_
hashing	_	_
[	_	_
1-4	_	_
]	_	_
,	_	_
linear	_	_
algebra	_	_
[	_	_
5-7	_	_
]	_	_
,	_	_
dynamic	_	_
programming	_	_
[	_	_
8,9	_	_
]	_	_
,	_	_
graph	_	_
algorithms	_	_
[	_	_
10-13	_	_
]	_	_
,	_	_
and	_	_
many	_	_
other	_	_
classic	_	_
algorithms	_	_
[	_	_
14,15	_	_
]	_	_
.	_	_

#19
These	_	_
projects	_	_
generally	_	_
report	_	_
impressive	_	_
gains	_	_
in	_	_
performance	_	_
.	_	_

#20
These	_	_
devices	_	_
appear	_	_
to	_	_
be	_	_
here	_	_
to	_	_
stay	_	_
.	_	_

#21
While	_	_
there	_	_
is	_	_
a	_	_
lot	_	_
of	_	_
folk	_	_
wisdom	_	_
on	_	_
how	_	_
to	_	_
design	_	_
good	_	_
algorithms	_	_
for	_	_
these	_	_
highly-threaded	_	_
machines	_	_
,	_	_
in	_	_
addition	_	_
to	_	_
a	_	_
significant	_	_
body	_	_
of	_	_
work	_	_
on	_	_
performance	_	_
analysis	_	_
[	_	_
16-20	_	_
]	_	_
,	_	_
there	_	_
are	_	_
no	_	_
systematic	_	_
theoretical	_	_
models	_	_
to	_	_
analyze	_	_
the	_	_
performance	_	_
of	_	_
programs	_	_
on	_	_
these	_	_
machines	_	_
.	_	_

#22
We	_	_
are	_	_
interested	_	_
in	_	_
analyzing	_	_
and	_	_
characterizing	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
these	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
in	_	_
a	_	_
more	_	_
abstract	_	_
,	_	_
algorithmic	_	_
,	_	_
and	_	_
systematic	_	_
manner	_	_
.	_	_

#23
Theoretical	_	_
analysis	_	_
relies	_	_
upon	_	_
models	_	_
that	_	_
represent	_	_
underlying	_	_
assumptions	_	_
;	_	_
if	_	_
a	_	_
model	_	_
does	_	_
not	_	_
capture	_	_
the	_	_
important	_	_
aspects	_	_
of	_	_
target	_	_
machines	_	_
and	_	_
programs	_	_
,	_	_
then	_	_
the	_	_
analysis	_	_
is	_	_
not	_	_
predictive	_	_
of	_	_
real	_	_
performance	_	_
.	_	_

#24
Over	_	_
the	_	_
years	_	_
,	_	_
computer	_	_
scientists	_	_
have	_	_
designed	_	_
various	_	_
models	_	_
to	_	_
capture	_	_
important	_	_
aspects	_	_
of	_	_
the	_	_
machines	_	_
that	_	_
we	_	_
use	_	_
.	_	_

#25
The	_	_
most	_	_
fundamental	_	_
model	_	_
that	_	_
is	_	_
used	_	_
to	_	_
analyze	_	_
sequential	_	_
algorithms	_	_
is	_	_
the	_	_
Random	_	_
Access	_	_
Machine	_	_
(	_	_
RAM	_	_
)	_	_
model	_	_
[	_	_
21	_	_
]	_	_
,	_	_
which	_	_
we	_	_
teach	_	_
undergraduates	_	_
in	_	_
their	_	_
first	_	_
algorithms	_	_
class	_	_
.	_	_

#26
This	_	_
model	_	_
assumes	_	_
that	_	_
all	_	_
operations	_	_
,	_	_
including	_	_
memory	_	_
accesses	_	_
,	_	_
take	_	_
unit	_	_
time	_	_
.	_	_

#27
While	_	_
this	_	_
model	_	_
is	_	_
a	_	_
good	_	_
predictor	_	_
of	_	_
performance	_	_
on	_	_
computationally	_	_
intensive	_	_
programs	_	_
,	_	_
it	_	_
does	_	_
not	_	_
properly	_	_
capture	_	_
the	_	_
important	_	_
characteristics	_	_
of	_	_
the	_	_
memory	_	_
hierarchy	_	_
of	_	_
modern	_	_
machines	_	_
.	_	_

#28
Aggarwal	_	_
and	_	_
Vitter	_	_
proposed	_	_
the	_	_
Disk	_	_
Access	_	_
Machine	_	_
(	_	_
DAM	_	_
)	_	_
model	_	_
[	_	_
22	_	_
]	_	_
which	_	_
counts	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
memory	_	_
instead	_	_
of	_	_
simply	_	_
counting	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
by	_	_
the	_	_
program	_	_
.	_	_

#29
Therefore	_	_
,	_	_
it	_	_
better	_	_
captures	_	_
the	_	_
fact	_	_
that	_	_
modern	_	_
machines	_	_
have	_	_
memory	_	_
hierarchies	_	_
and	_	_
exploiting	_	_
spatial	_	_
and	_	_
temporal	_	_
locality	_	_
on	_	_
these	_	_
machines	_	_
can	options	_
lead	_	_
to	_	_
better	_	_
performance	_	_
.	_	_

#30
There	_	_
are	_	_
also	_	_
a	_	_
number	_	_
of	_	_
other	_	_
models	_	_
that	_	_
consider	_	_
the	_	_
memory	_	_
access	_	_
costs	_	_
of	_	_
sequential	_	_
algorithms	_	_
in	_	_
different	_	_
ways	_	_
[	_	_
23-29	_	_
]	_	_
.	_	_

#31
For	_	_
parallel	_	_
computing	_	_
,	_	_
the	_	_
analogue	_	_
for	_	_
the	_	_
RAM	_	_
model	_	_
is	_	_
the	_	_
Parallel	_	_
Random	_	_
Access	_	_
Machine	_	_
(	_	_
PRAM	_	_
)	_	_
model	_	_
[	_	_
30	_	_
]	_	_
,	_	_
and	_	_
there	_	_
is	_	_
a	_	_
large	_	_
body	_	_
of	_	_
work	_	_
describing	_	_
and	_	_
analyzing	_	_
algorithms	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
[	_	_
31,32	_	_
]	_	_
.	_	_

#32
In	_	_
the	_	_
PRAM	_	_
model	_	_
,	_	_
the	_	_
algorithm	_	_
's	_	_
complexity	_	_
is	_	_
analyzed	_	_
in	_	_
terms	_	_
of	_	_
its	_	_
work-the	_	_
time	_	_
taken	_	_
by	_	_
the	_	_
algorithm	_	_
on	_	_
1	_	_
processor	_	_
,	_	_
and	_	_
span	_	_
(	_	_
also	_	_
called	_	_
depth	_	_
and	_	_
critical-path	_	_
length	_	_
)	_	_
-the	_	_
time	_	_
taken	_	_
by	_	_
the	_	_
algorithm	_	_
on	_	_
an	_	_
infinite	_	_
number	_	_
of	_	_
processors	_	_
.	_	_

#33
Given	_	_
a	_	_
machine	_	_
with	_	_
P	_	_
processors	_	_
,	_	_
a	_	_
PRAM	_	_
algorithm	_	_
with	_	_
work	_	_
W	_	_
and	_	_
span	_	_
S	_	_
completes	_	_
in	_	_
max	_	_
(	_	_
W/P	_	_
,	_	_
S	_	_
)	_	_
time	_	_
.	_	_

#34
The	_	_
PRAM	_	_
model	_	_
also	_	_
ignores	_	_
the	_	_
vagaries	_	_
of	_	_
the	_	_
memory	_	_
hierarchy	_	_
and	_	_
assumes	_	_
that	_	_
each	_	_
memory	_	_
access	_	_
by	_	_
the	_	_
algorithm	_	_
takes	_	_
unit	_	_
time	_	_
.	_	_

#35
For	_	_
modern	_	_
machines	_	_
,	_	_
however	_	_
,	_	_
this	_	_
assumption	_	_
seldom	_	_
holds	_	_
.	_	_

#36
Therefore	_	_
,	_	_
researchers	_	_
have	_	_
designed	_	_
various	_	_
models	_	_
that	_	_
capture	_	_
memory	_	_
hierarchies	_	_
for	_	_
various	_	_
types	_	_
of	_	_
machines	_	_
such	_	_
as	_	_
distributed	_	_
memory	_	_
machines	_	_
[	_	_
33-35	_	_
]	_	_
,	_	_
shared	_	_
memory	_	_
machines	_	_
and	_	_
multi-cores	_	_
[	_	_
36-40	_	_
]	_	_
,	_	_
or	_	_
the	_	_
combination	_	_
of	_	_
the	_	_
two	_	_
[	_	_
41,42	_	_
]	_	_
.	_	_

#37
All	_	_
of	_	_
these	_	_
models	_	_
capture	_	_
particular	_	_
capabilities	_	_
and	_	_
properties	_	_
of	_	_
the	_	_
respective	_	_
target	_	_
machines	_	_
,	_	_
namely	_	_
shared	_	_
memory	_	_
machines	_	_
or	_	_
distributed	_	_
memory	_	_
machines	_	_
.	_	_

#38
While	_	_
superficially	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
such	_	_
as	_	_
GPUs	_	_
are	_	_
shared	_	_
memory	_	_
machines	_	_
,	_	_
their	_	_
characteristics	_	_
are	_	_
very	_	_
different	_	_
from	_	_
traditional	_	_
multi-core	_	_
or	_	_
multiprocessor	_	_
shared	_	_
memory	_	_
machines	_	_
.	_	_

#39
The	_	_
most	_	_
important	_	_
distinction	_	_
between	_	_
the	_	_
multi-cores	_	_
and	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
is	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
.	_	_

#40
On	_	_
multi-core	_	_
machines	_	_
,	_	_
context	_	_
switch	_	_
cost	_	_
is	_	_
high	_	_
,	_	_
and	_	_
most	_	_
models	_	_
nominally	_	_
assume	_	_
that	_	_
only	_	_
one	_	_
(	_	_
or	_	_
a	_	_
small	_	_
constant	_	_
number	_	_
of	_	_
)	_	_
thread	_	_
(	_	_
s	_	_
)	_	_
are	_	_
running	_	_
on	_	_
each	_	_
machine	_	_
and	_	_
this	_	_
thread	_	_
blocks	_	_
when	_	_
there	_	_
is	_	_
a	_	_
memory	_	_
access	_	_
.	_	_

#41
Therefore	_	_
,	_	_
many	_	_
models	_	_
consider	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
from	_	_
slow	_	_
memory	_	_
to	_	_
fast	_	_
memory	_	_
as	_	_
a	_	_
performance	_	_
measure	_	_
,	_	_
and	_	_
algorithms	_	_
are	_	_
designed	_	_
to	_	_
minimize	_	_
these	_	_
,	_	_
since	_	_
memory	_	_
transfers	_	_
take	_	_
a	_	_
significant	_	_
amount	_	_
of	_	_
time	_	_
.	_	_

#42
In	_	_
contrast	_	_
,	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
are	_	_
explicitly	_	_
designed	_	_
to	_	_
have	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
and	_	_
a	_	_
fast	_	_
context	_	_
switching	_	_
mechanism	_	_
.	_	_

#43
Highly-threaded	_	_
many-cores	_	_
are	_	_
explicitly	_	_
designed	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
;	_	_
if	_	_
a	_	_
thread	_	_
stalls	_	_
on	_	_
a	_	_
memory	_	_
operation	_	_
,	_	_
some	_	_
other	_	_
thread	_	_
can	feasibility	_
be	_	_
scheduled	_	_
in	_	_
its	_	_
place	_	_
.	_	_

#44
In	_	_
principle	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
does	_	_
not	_	_
matter	_	_
as	_	_
long	_	_
as	_	_
there	_	_
are	_	_
enough	_	_
threads	_	_
to	_	_
hide	_	_
their	_	_
latency	_	_
.	_	_

#45
Therefore	_	_
,	_	_
if	_	_
there	_	_
are	_	_
enough	_	_
threads	_	_
,	_	_
we	_	_
should	inference	_
,	_	_
in	_	_
principle	_	_
,	_	_
be	_	_
able	_	_
to	_	_
use	_	_
PRAM	_	_
algorithms	_	_
on	_	_
such	_	_
machines	_	_
,	_	_
since	_	_
we	_	_
can	feasibility	_
ignore	_	_
the	_	_
effect	_	_
of	_	_
memory	_	_
transfers	_	_
which	_	_
is	_	_
exactly	_	_
what	_	_
PRAM	_	_
model	_	_
does	_	_
.	_	_

#46
However	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
required	_	_
to	_	_
reach	_	_
the	_	_
point	_	_
where	_	_
one	_	_
gets	_	_
PRAM	_	_
performance	_	_
depends	_	_
on	_	_
both	_	_
the	_	_
algorithm	_	_
and	_	_
the	_	_
hardware	_	_
.	_	_

#47
Since	_	_
no	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machine	_	_
allows	_	_
an	_	_
infinite	_	_
number	_	_
of	_	_
threads	_	_
,	_	_
it	_	_
is	_	_
important	_	_
to	_	_
understand	_	_
both	_	_
(	_	_
1	_	_
)	_	_
how	_	_
many	_	_
threads	_	_
does	_	_
a	_	_
particular	_	_
algorithm	_	_
need	_	_
to	_	_
achieve	_	_
PRAM	_	_
performance	_	_
,	_	_
and	_	_
(	_	_
2	_	_
)	_	_
how	_	_
does	_	_
an	_	_
algorithm	_	_
perform	_	_
when	_	_
it	_	_
has	_	_
fewer	_	_
threads	_	_
than	_	_
required	_	_
to	_	_
get	_	_
PRAM	_	_
performance	_	_
?	_	_

#48
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
attempt	_	_
to	_	_
characterize	_	_
these	_	_
properties	_	_
of	_	_
algorithms	_	_
.	_	_

#49
To	_	_
motivate	_	_
this	_	_
enterprise	_	_
and	_	_
to	_	_
understand	_	_
the	_	_
importance	_	_
of	_	_
high	_	_
thread	_	_
counts	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
,	_	_
let	_	_
us	_	_
consider	_	_
a	_	_
simple	_	_
application	_	_
that	_	_
performs	_	_
Bloom	_	_
filter	_	_
set	_	_
membership	_	_
tests	_	_
on	_	_
an	_	_
input	_	_
stream	_	_
of	_	_
biosequence	_	_
data	_	_
on	_	_
GPUs	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#50
The	_	_
problem	_	_
is	_	_
embarrassingly	_	_
parallel	_	_
,	_	_
each	_	_
set	_	_
membership	_	_
test	_	_
is	_	_
independent	_	_
of	_	_
every	_	_
other	_	_
membership	_	_
test	_	_
.	_	_

#51
Fig	_	_
.	_	_
1	_	_
shows	_	_
the	_	_
performance	_	_
of	_	_
this	_	_
application	_	_
,	_	_
varying	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
core	_	_
,	_	_
for	_	_
two	_	_
distinct	_	_
GPUs	_	_
.	_	_

#52
For	_	_
both	_	_
machines	_	_
,	_	_
the	_	_
pattern	_	_
is	_	_
quite	_	_
similar	_	_
,	_	_
at	_	_
low	_	_
thread	_	_
counts	_	_
,	_	_
the	_	_
performance	_	_
increases	_	_
(	_	_
roughly	_	_
linearly	_	_
)	_	_
with	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
,	_	_
up	_	_
until	_	_
a	_	_
transition	_	_
region	_	_
,	_	_
after	_	_
which	_	_
the	_	_
performance	_	_
no	_	_
longer	_	_
increases	_	_
with	_	_
increasing	_	_
thread	_	_
count	_	_
.	_	_

#53
While	_	_
the	_	_
location	_	_
of	_	_
the	_	_
transition	_	_
region	_	_
is	_	_
different	_	_
for	_	_
distinct	_	_
GPU	_	_
models	_	_
,	_	_
this	_	_
general	_	_
pattern	_	_
is	_	_
found	_	_
in	_	_
many	_	_
applications	_	_
.	_	_

#54
Once	_	_
sufficient	_	_
threads	_	_
are	_	_
present	_	_
,	_	_
the	_	_
PRAM	_	_
model	_	_
adequately	_	_
describes	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
application	_	_
and	_	_
increasing	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
no	_	_
longer	_	_
helps	_	_
.	_	_

#55
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
propose	_	_
the	_	_
Threaded	_	_
Many-core	_	_
Memory	_	_
(	_	_
TMM	_	_
)	_	_
model	_	_
that	_	_
captures	_	_
the	_	_
performance	_	_
characteristics	_	_
of	_	_
these	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#56
This	_	_
model	_	_
explicitly	_	_
models	_	_
the	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
and	_	_
the	_	_
memory	_	_
latency	_	_
to	_	_
slow	_	_
memory	_	_
.	_	_

#57
Note	_	_
that	_	_
while	_	_
we	_	_
motivate	_	_
this	_	_
model	_	_
for	_	_
highly-threaded	_	_
many-core	_	_
machines	_	_
with	_	_
synchronous	_	_
computations	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
it	_	_
can	feasibility	_
be	_	_
used	_	_
in	_	_
any	_	_
system	_	_
which	_	_
has	_	_
fast	_	_
context	_	_
switching	_	_
and	_	_
enough	_	_
threads	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
.	_	_

#58
Typical	_	_
examples	_	_
of	_	_
such	_	_
machines	_	_
include	_	_
both	_	_
NVIDIA	_	_
and	_	_
AMD/ATI	_	_
GPUs	_	_
and	_	_
the	_	_
YarcData	_	_
uRiKA	_	_
system	_	_
.	_	_

#59
We	_	_
do	_	_
not	_	_
try	_	_
to	_	_
model	_	_
the	_	_
Intel	_	_
Xeon	_	_
Phi	_	_
,	_	_
due	_	_
to	_	_
its	_	_
limited	_	_
use	_	_
of	_	_
threading	_	_
for	_	_
latency	_	_
hiding	_	_
.	_	_

#60
In	_	_
contrast	_	_
,	_	_
its	_	_
approach	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
is	_	_
primarily	_	_
based	_	_
on	_	_
strided	_	_
memory	_	_
access	_	_
patterns	_	_
associated	_	_
with	_	_
vector	_	_
computation	_	_
.	_	_

#61
If	_	_
the	_	_
latency	_	_
of	_	_
transfers	_	_
from	_	_
slow	_	_
memory	_	_
to	_	_
fast	_	_
memory	_	_
is	_	_
small	_	_
,	_	_
or	_	_
if	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
is	_	_
infinite	_	_
,	_	_
then	_	_
this	_	_
model	_	_
generally	_	_
provides	_	_
the	_	_
same	_	_
analysis	_	_
results	_	_
as	_	_
the	_	_
PRAM	_	_
analysis	_	_
.	_	_

#62
It	_	_
,	_	_
however	_	_
,	_	_
provides	_	_
more	_	_
intuition	_	_
.	_	_

#63
(	_	_
1	_	_
)	_	_
Ideally	_	_
,	_	_
we	_	_
want	_	_
to	_	_
get	_	_
the	_	_
PRAM	_	_
performance	_	_
for	_	_
algorithms	_	_
using	_	_
the	_	_
fewest	_	_
number	_	_
of	_	_
threads	_	_
possible	_	_
,	_	_
since	_	_
threads	_	_
do	_	_
have	_	_
overhead	_	_
.	_	_

#64
This	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
pick	_	_
such	_	_
algorithms	_	_
.	_	_

#65
(	_	_
2	_	_
)	_	_
It	_	_
also	_	_
captures	_	_
the	_	_
reality	_	_
of	_	_
when	_	_
memory	_	_
latency	_	_
is	_	_
large	_	_
and	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
large	_	_
but	_	_
finite	_	_
.	_	_

#66
In	_	_
particular	_	_
,	_	_
it	_	_
can	capability	_
distinguish	_	_
between	_	_
algorithms	_	_
that	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
analysis	_	_
,	_	_
but	_	_
one	_	_
may	capability-speculation	_
be	_	_
better	_	_
at	_	_
hiding	_	_
latency	_	_
than	_	_
another	_	_
with	_	_
a	_	_
bounded	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#67
This	_	_
model	_	_
is	_	_
a	_	_
high-level	_	_
model	_	_
meant	_	_
to	_	_
be	_	_
generally	_	_
applicable	_	_
to	_	_
a	_	_
number	_	_
of	_	_
machines	_	_
which	_	_
allow	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
with	_	_
fast	_	_
context	_	_
switching	_	_
.	_	_

#68
Therefore	_	_
,	_	_
it	_	_
abstracts	_	_
away	_	_
many	_	_
implementation	_	_
details	_	_
of	_	_
either	_	_
the	_	_
machine	_	_
or	_	_
the	_	_
algorithm	_	_
.	_	_

#69
We	_	_
also	_	_
assume	_	_
that	_	_
the	_	_
hardware	_	_
provides	_	_
0-cost	_	_
and	_	_
perfect	_	_
scheduling	_	_
between	_	_
threads	_	_
.	_	_

#70
In	_	_
addition	_	_
,	_	_
it	_	_
also	_	_
models	_	_
the	_	_
machine	_	_
as	_	_
having	_	_
only	_	_
2	_	_
levels	_	_
of	_	_
memory	_	_
.	_	_

#71
In	_	_
particular	_	_
,	_	_
we	_	_
model	_	_
a	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
shared	_	_
by	_	_
one	_	_
core	_	_
group	_	_
.	_	_

#72
In	_	_
practice	_	_
,	_	_
these	_	_
machines	_	_
may	capability-options	_
have	_	_
many	_	_
levels	_	_
of	_	_
memory	_	_
.	_	_

#73
However	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
the	_	_
interplay	_	_
between	_	_
the	_	_
farthest	_	_
level	_	_
,	_	_
since	_	_
the	_	_
latencies	_	_
are	_	_
the	_	_
largest	_	_
at	_	_
that	_	_
level	_	_
,	_	_
and	_	_
therefore	_	_
have	_	_
the	_	_
biggest	_	_
impact	_	_
on	_	_
the	_	_
performance	_	_
.	_	_

#74
We	_	_
expect	_	_
that	_	_
the	_	_
model	_	_
can	capability-feasibility	_
be	_	_
extended	_	_
to	_	_
also	_	_
model	_	_
other	_	_
levels	_	_
of	_	_
the	_	_
memory	_	_
hierarchy	_	_
.	_	_

#75
We	_	_
analyze	_	_
4	_	_
classic	_	_
algorithms	_	_
for	_	_
the	_	_
problem	_	_
of	_	_
computing	_	_
All	_	_
Pairs	_	_
Shortest	_	_
Paths	_	_
(	_	_
APSP	_	_
)	_	_
on	_	_
a	_	_
weighted	_	_
graph	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
[	_	_
43	_	_
]	_	_
.	_	_

#76
We	_	_
compare	_	_
the	_	_
analysis	_	_
from	_	_
this	_	_
model	_	_
with	_	_
the	_	_
PRAM	_	_
analysis	_	_
of	_	_
these	_	_
4	_	_
algorithms	_	_
to	_	_
gain	_	_
intuition	_	_
about	_	_
the	_	_
usefulness	_	_
of	_	_
both	_	_
our	_	_
model	_	_
and	_	_
the	_	_
PRAM	_	_
model	_	_
for	_	_
analyzing	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#77
Our	_	_
results	_	_
validate	_	_
the	_	_
intuition	_	_
that	_	_
this	_	_
model	_	_
can	capability	_
provide	_	_
more	_	_
information	_	_
than	_	_
the	_	_
PRAM	_	_
model	_	_
for	_	_
the	_	_
large	_	_
latency	_	_
,	_	_
finite	_	_
thread	_	_
case	_	_
.	_	_

#78
In	_	_
particular	_	_
,	_	_
we	_	_
compare	_	_
these	_	_
algorithms	_	_
and	_	_
find	_	_
specific	_	_
relationships	_	_
between	_	_
hardware	_	_
parameters	_	_
(	_	_
latency	_	_
,	_	_
fast	_	_
memory	_	_
size	_	_
,	_	_
limits	_	_
on	_	_
number	_	_
of	_	_
threads	_	_
)	_	_
under	_	_
which	_	_
some	_	_
algorithms	_	_
are	_	_
better	_	_
than	_	_
others	_	_
even	_	_
if	_	_
they	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
cost	_	_
.	_	_

#79
Following	_	_
the	_	_
formal	_	_
analysis	_	_
,	_	_
we	_	_
assess	_	_
the	_	_
utility	_	_
of	_	_
the	_	_
model	_	_
by	_	_
comparing	_	_
empirically	_	_
measured	_	_
performance	_	_
on	_	_
an	_	_
individual	_	_
machine	_	_
to	_	_
that	_	_
predicted	_	_
by	_	_
the	_	_
model	_	_
.	_	_

#80
For	_	_
two	_	_
of	_	_
the	_	_
APSP	_	_
algorithms	_	_
,	_	_
we	_	_
illustrate	_	_
the	_	_
impact	_	_
of	_	_
various	_	_
individual	_	_
parameters	_	_
on	_	_
performance	_	_
,	_	_
showing	_	_
the	_	_
effectiveness	_	_
of	_	_
the	_	_
model	_	_
at	_	_
predicting	_	_
measured	_	_
performance	_	_
.	_	_

#81
This	_	_
paper	_	_
is	_	_
organized	_	_
as	_	_
follows	_	_
.	_	_

#82
Section	_	_
2	_	_
presents	_	_
related	_	_
work	_	_
.	_	_

#83
Section	_	_
3	_	_
describes	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#84
Section	_	_
4	_	_
provides	_	_
the	_	_
4	_	_
shortest	_	_
paths	_	_
algorithms	_	_
and	_	_
their	_	_
analysis	_	_
in	_	_
both	_	_
the	_	_
PRAM	_	_
and	_	_
TMM	_	_
models	_	_
.	_	_

#85
Section	_	_
5	_	_
provides	_	_
the	_	_
lessons	_	_
learned	_	_
from	_	_
this	_	_
model	_	_
;	_	_
in	_	_
particular	_	_
,	_	_
we	_	_
see	_	_
that	_	_
algorithms	_	_
that	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
have	_	_
different	_	_
performance	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
since	_	_
they	_	_
are	_	_
better	_	_
at	_	_
hiding	_	_
memory	_	_
latency	_	_
with	_	_
fewer	_	_
threads	_	_
.	_	_

#86
Section	_	_
6	_	_
continues	_	_
the	_	_
discussion	_	_
of	_	_
lessons	_	_
learned	_	_
,	_	_
concentrating	_	_
on	_	_
the	_	_
effects	_	_
of	_	_
problem	_	_
size	_	_
.	_	_

#87
Section	_	_
7	_	_
shows	_	_
performance	_	_
measurements	_	_
for	_	_
a	_	_
pair	_	_
of	_	_
the	_	_
APSP	_	_
algorithms	_	_
executing	_	_
on	_	_
a	_	_
commercial	_	_
GPU	_	_
,	_	_
illustrating	_	_
correspondence	_	_
between	_	_
model	_	_
predictions	_	_
and	_	_
empirical	_	_
measurements	_	_
.	_	_

#88
Finally	_	_
,	_	_
Section	_	_
8	_	_
concludes	_	_
.	_	_

#89
Related	_	_
work	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
briefly	_	_
review	_	_
the	_	_
related	_	_
work	_	_
.	_	_

#90
We	_	_
first	_	_
review	_	_
the	_	_
work	_	_
on	_	_
abstract	_	_
models	_	_
of	_	_
computations	_	_
for	_	_
both	_	_
sequential	_	_
and	_	_
parallel	_	_
machines	_	_
.	_	_

#91
We	_	_
then	_	_
review	_	_
recent	_	_
work	_	_
on	_	_
algorithms	_	_
and	_	_
performance	_	_
analysis	_	_
of	_	_
GPUs	_	_
which	_	_
are	_	_
the	_	_
most	_	_
common	_	_
current	_	_
instantiations	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#92
Many	_	_
machine	_	_
and	_	_
memory	_	_
models	_	_
have	_	_
been	_	_
designed	_	_
for	_	_
various	_	_
types	_	_
of	_	_
parallel	_	_
and	_	_
sequential	_	_
machines	_	_
.	_	_

#93
In	_	_
an	_	_
early	_	_
work	_	_
,	_	_
Aggarwal	_	_
et	_	_
al	_	_
.	_	_
[	_	_
25	_	_
]	_	_
present	_	_
the	_	_
Hierarchical	_	_
Memory	_	_
Model	_	_
(	_	_
HMM	_	_
)	_	_
and	_	_
use	_	_
it	_	_
for	_	_
a	_	_
theoretical	_	_
investigation	_	_
of	_	_
the	_	_
inherent	_	_
complexity	_	_
of	_	_
solving	_	_
problems	_	_
in	_	_
RAM	_	_
with	_	_
a	_	_
memory	_	_
hierarchy	_	_
of	_	_
multiple	_	_
levels	_	_
.	_	_

#94
It	_	_
differs	_	_
from	_	_
the	_	_
RAM	_	_
model	_	_
by	_	_
defining	_	_
that	_	_
access	_	_
to	_	_
location	_	_
x	_	_
takes	_	_
logx	_	_
time	_	_
,	_	_
but	_	_
it	_	_
does	_	_
not	_	_
consider	_	_
the	_	_
concept	_	_
of	_	_
block	_	_
transfers	_	_
,	_	_
which	_	_
collects	_	_
data	_	_
into	_	_
blocks	_	_
to	_	_
utilize	_	_
spatial	_	_
locality	_	_
of	_	_
reference	_	_
in	_	_
algorithms	_	_
.	_	_

#95
The	_	_
Block	_	_
Transfer	_	_
model	_	_
(	_	_
BT	_	_
)	_	_
[	_	_
27	_	_
]	_	_
addresses	_	_
this	_	_
deficiency	_	_
by	_	_
defining	_	_
that	_	_
a	_	_
block	_	_
of	_	_
consecutive	_	_
locations	_	_
can	feasibility	_
be	_	_
copied	_	_
from	_	_
memory	_	_
to	_	_
memory	_	_
,	_	_
taking	_	_
one	_	_
unit	_	_
of	_	_
time	_	_
per	_	_
element	_	_
after	_	_
the	_	_
initial	_	_
access	_	_
time	_	_
.	_	_

#96
Alpern	_	_
et	_	_
al	_	_
.	_	_
propose	_	_
the	_	_
Memory	_	_
Hierarchy	_	_
(	_	_
MH	_	_
)	_	_
Framework	_	_
[	_	_
26	_	_
]	_	_
that	_	_
reflects	_	_
important	_	_
practical	_	_
considerations	_	_
that	_	_
are	_	_
hidden	_	_
by	_	_
the	_	_
RAM	_	_
and	_	_
HMM	_	_
models	_	_
:	_	_
data	_	_
are	_	_
moved	_	_
in	_	_
fixed	_	_
size	_	_
blocks	_	_
simultaneously	_	_
at	_	_
different	_	_
levels	_	_
in	_	_
the	_	_
hierarchy	_	_
,	_	_
and	_	_
the	_	_
memory	_	_
capacity	_	_
as	_	_
well	_	_
as	_	_
bus	_	_
bandwidth	_	_
are	_	_
limited	_	_
at	_	_
each	_	_
level	_	_
.	_	_

#97
But	_	_
there	_	_
are	_	_
too	_	_
many	_	_
parameters	_	_
in	_	_
this	_	_
model	_	_
that	_	_
can	capability-options	_
obscure	_	_
algorithm	_	_
analysis	_	_
.	_	_

#98
Thus	_	_
,	_	_
they	_	_
simplified	_	_
and	_	_
reduced	_	_
the	_	_
MH	_	_
parameters	_	_
by	_	_
putting	_	_
forward	_	_
a	_	_
new	_	_
Uniform	_	_
Memory	_	_
Hierarchy	_	_
(	_	_
UMH	_	_
)	_	_
model	_	_
[	_	_
28,29	_	_
]	_	_
.	_	_

#99
Later	_	_
,	_	_
an	_	_
'ideal-cache	_	_
'	_	_
model	_	_
was	_	_
introduced	_	_
in	_	_
[	_	_
23,24	_	_
]	_	_
allowing	_	_
analysis	_	_
of	_	_
cache-oblivious	_	_
algorithms	_	_
that	_	_
use	_	_
asymptotically	_	_
optimal	_	_
amounts	_	_
of	_	_
work	_	_
and	_	_
move	_	_
data	_	_
asymptotically	_	_
optimally	_	_
among	_	_
multiple	_	_
levels	_	_
of	_	_
cache	_	_
without	_	_
the	_	_
necessity	_	_
of	_	_
tuning	_	_
program	_	_
variables	_	_
according	_	_
to	_	_
hardware	_	_
configuration	_	_
parameters	_	_
.	_	_

#100
In	_	_
the	_	_
parallel	_	_
case	_	_
,	_	_
although	_	_
widely	_	_
used	_	_
,	_	_
the	_	_
PRAM	_	_
[	_	_
30	_	_
]	_	_
model	_	_
is	_	_
unrealistic	_	_
because	_	_
it	_	_
assumes	_	_
all	_	_
processors	_	_
work	_	_
synchronously	_	_
and	_	_
that	_	_
interprocessor	_	_
communication	_	_
is	_	_
free	_	_
.	_	_

#101
Quite	_	_
different	_	_
to	_	_
PRAM	_	_
,	_	_
the	_	_
Bulk-Synchronous	_	_
Parallel	_	_
(	_	_
BSP	_	_
)	_	_
model	_	_
[	_	_
34	_	_
]	_	_
attempts	_	_
to	_	_
bridge	_	_
theory	_	_
and	_	_
practice	_	_
by	_	_
allowing	_	_
processors	_	_
to	_	_
work	_	_
asynchronously	_	_
,	_	_
and	_	_
it	_	_
models	_	_
latency	_	_
and	_	_
limited	_	_
bandwidth	_	_
for	_	_
distributed	_	_
memory	_	_
machines	_	_
without	_	_
shared	_	_
memory	_	_
.	_	_

#102
Culler	_	_
et	_	_
al	_	_
.	_	_
[	_	_
33	_	_
]	_	_
offer	_	_
a	_	_
new	_	_
parallel	_	_
machine	_	_
model	_	_
called	_	_
LogP	_	_
based	_	_
on	_	_
BSP	_	_
,	_	_
characterizing	_	_
a	_	_
parallel	_	_
machine	_	_
by	_	_
four	_	_
parameters	_	_
:	_	_
number	_	_
of	_	_
processors	_	_
,	_	_
communication	_	_
bandwidth	_	_
,	_	_
delay	_	_
,	_	_
and	_	_
overhead	_	_
.	_	_

#103
It	_	_
reflects	_	_
the	_	_
convergence	_	_
towards	_	_
systems	_	_
formed	_	_
by	_	_
a	_	_
collection	_	_
of	_	_
computers	_	_
connected	_	_
by	_	_
a	_	_
communication	_	_
network	_	_
via	_	_
message	_	_
passing	_	_
.	_	_

#104
Vitter	_	_
et	_	_
al	_	_
.	_	_
[	_	_
35	_	_
]	_	_
present	_	_
a	_	_
two-level	_	_
memory	_	_
model	_	_
and	_	_
give	_	_
a	_	_
realistic	_	_
treatment	_	_
of	_	_
parallel	_	_
block	_	_
transfers	_	_
in	_	_
parallel	_	_
machines	_	_
.	_	_

#105
But	_	_
this	_	_
model	_	_
assumes	_	_
that	_	_
processors	_	_
are	_	_
interconnected	_	_
via	_	_
sharing	_	_
of	_	_
internal	_	_
memory	_	_
.	_	_

#106
More	_	_
recently	_	_
,	_	_
several	_	_
models	_	_
have	_	_
been	_	_
proposed	_	_
emphasizing	_	_
the	_	_
use	_	_
of	_	_
private-cache	_	_
chip	_	_
multiprocessors	_	_
(	_	_
CMPs	_	_
)	_	_
.	_	_

#107
Arge	_	_
et	_	_
al	_	_
.	_	_
[	_	_
36	_	_
]	_	_
present	_	_
the	_	_
Parallel	_	_
External	_	_
Memory	_	_
(	_	_
PEM	_	_
)	_	_
model	_	_
with	_	_
P	_	_
processors	_	_
and	_	_
a	_	_
two-level	_	_
memory	_	_
hierarchy	_	_
,	_	_
consisting	_	_
of	_	_
the	_	_
main	_	_
memory	_	_
as	_	_
external	_	_
memory	_	_
shared	_	_
by	_	_
all	_	_
processors	_	_
and	_	_
caches	_	_
as	_	_
internal	_	_
memory	_	_
exclusive	_	_
to	_	_
each	_	_
of	_	_
the	_	_
P	_	_
processors	_	_
.	_	_

#108
Blelloch	_	_
et	_	_
al	_	_
.	_	_
[	_	_
37	_	_
]	_	_
present	_	_
a	_	_
multi-core-cache	_	_
model	_	_
capturing	_	_
the	_	_
fact	_	_
that	_	_
multi-core	_	_
machines	_	_
have	_	_
both	_	_
per-processor	_	_
private	_	_
caches	_	_
and	_	_
a	_	_
large	_	_
shared	_	_
cache	_	_
on-chip	_	_
.	_	_

#109
Bender	_	_
et	_	_
al	_	_
.	_	_
[	_	_
44	_	_
]	_	_
present	_	_
a	_	_
concurrent	_	_
cache-oblivious	_	_
model	_	_
.	_	_

#110
Blelloch	_	_
et	_	_
al	_	_
.	_	_
[	_	_
38	_	_
]	_	_
also	_	_
propose	_	_
a	_	_
parallel	_	_
cache-oblivious	_	_
(	_	_
PCO	_	_
)	_	_
model	_	_
to	_	_
account	_	_
for	_	_
costs	_	_
of	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
cache	_	_
hierarchies	_	_
.	_	_

#111
Chowdhury	_	_
et	_	_
al	_	_
.	_	_
[	_	_
39	_	_
]	_	_
present	_	_
a	_	_
hierarchical	_	_
multi-level	_	_
caching	_	_
model	_	_
(	_	_
HM	_	_
)	_	_
,	_	_
consisting	_	_
of	_	_
a	_	_
collection	_	_
of	_	_
cores	_	_
sharing	_	_
an	_	_
arbitrarily	_	_
large	_	_
main	_	_
memory	_	_
through	_	_
a	_	_
hierarchy	_	_
of	_	_
caches	_	_
of	_	_
finite	_	_
but	_	_
increasing	_	_
sizes	_	_
that	_	_
are	_	_
successively	_	_
shared	_	_
by	_	_
larger	_	_
groups	_	_
of	_	_
cores	_	_
.	_	_

#112
They	_	_
in	_	_
[	_	_
42	_	_
]	_	_
consider	_	_
three	_	_
types	_	_
of	_	_
caching	_	_
systems	_	_
for	_	_
CMPs	_	_
:	_	_
D-CMP	_	_
with	_	_
a	_	_
private	_	_
cache	_	_
for	_	_
each	_	_
core	_	_
,	_	_
S-CMP	_	_
with	_	_
a	_	_
single	_	_
cache	_	_
shared	_	_
by	_	_
all	_	_
cores	_	_
,	_	_
and	_	_
multi-core	_	_
with	_	_
private	_	_
L1	_	_
caches	_	_
and	_	_
a	_	_
shared	_	_
L2	_	_
cache	_	_
.	_	_

#113
All	_	_
the	_	_
models	_	_
above	_	_
do	_	_
not	_	_
accurately	_	_
describe	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
systems	_	_
,	_	_
due	_	_
to	_	_
their	_	_
distinctive	_	_
architectures	_	_
,	_	_
i.e	_	_
.	_	_
the	_	_
explicit	_	_
use	_	_
of	_	_
many	_	_
threads	_	_
for	_	_
the	_	_
purpose	_	_
of	_	_
hiding	_	_
memory	_	_
latency	_	_
.	_	_

#114
While	_	_
there	_	_
has	_	_
not	_	_
been	_	_
much	_	_
work	_	_
on	_	_
abstract	_	_
machine	_	_
models	_	_
for	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
,	_	_
there	_	_
has	_	_
been	_	_
a	_	_
lot	_	_
of	_	_
recent	_	_
work	_	_
on	_	_
designing	_	_
calibrated	_	_
performance	_	_
models	_	_
for	_	_
particular	_	_
instantiations	_	_
of	_	_
these	_	_
machines	_	_
such	_	_
as	_	_
NVIDIA	_	_
GPUs	_	_
.	_	_

#115
We	_	_
review	_	_
some	_	_
of	_	_
that	_	_
work	_	_
here	_	_
.	_	_

#116
Liu	_	_
et	_	_
al	_	_
.	_	_
[	_	_
19	_	_
]	_	_
describe	_	_
a	_	_
general	_	_
performance	_	_
model	_	_
that	_	_
predicts	_	_
the	_	_
performance	_	_
of	_	_
a	_	_
biosequence	_	_
database	_	_
scanning	_	_
application	_	_
fairly	_	_
precisely	_	_
.	_	_

#117
Their	_	_
model	_	_
incorporates	_	_
the	_	_
relationship	_	_
between	_	_
problem	_	_
size	_	_
and	_	_
performance	_	_
,	_	_
but	_	_
only	_	_
targets	_	_
their	_	_
biosequence	_	_
application	_	_
.	_	_

#118
Govindaraju	_	_
et	_	_
al	_	_
.	_	_
[	_	_
45	_	_
]	_	_
propose	_	_
a	_	_
cache	_	_
model	_	_
for	_	_
efficiently	_	_
implementing	_	_
three	_	_
memory	_	_
intensive	_	_
scientific	_	_
applications	_	_
with	_	_
nested	_	_
loops	_	_
.	_	_

#119
It	_	_
is	_	_
helpful	_	_
for	_	_
applications	_	_
with	_	_
2D-block	_	_
representations	_	_
while	_	_
choosing	_	_
an	_	_
appropriate	_	_
block	_	_
size	_	_
by	_	_
estimating	_	_
cache	_	_
misses	_	_
,	_	_
but	_	_
is	_	_
not	_	_
completely	_	_
general	_	_
.	_	_

#120
Ryoo	_	_
et	_	_
al	_	_
.	_	_
[	_	_
46	_	_
]	_	_
summarize	_	_
five	_	_
categories	_	_
of	_	_
optimization	_	_
mechanisms	_	_
,	_	_
and	_	_
use	_	_
two	_	_
metrics	_	_
to	_	_
prune	_	_
the	_	_
GPU	_	_
performance	_	_
optimization	_	_
space	_	_
by	_	_
98	_	_
%	_	_
via	_	_
computing	_	_
the	_	_
utilization	_	_
and	_	_
efficiency	_	_
of	_	_
GPU	_	_
applications	_	_
.	_	_

#121
They	_	_
do	_	_
not	_	_
,	_	_
however	_	_
,	_	_
consider	_	_
memory	_	_
latency	_	_
and	_	_
multiple	_	_
conflicting	_	_
performance	_	_
indicators	_	_
.	_	_

#122
Kothapalli	_	_
et	_	_
al	_	_
.	_	_
are	_	_
the	_	_
first	_	_
to	_	_
define	_	_
a	_	_
general	_	_
GPU	_	_
analytical	_	_
performance	_	_
model	_	_
in	_	_
[	_	_
47	_	_
]	_	_
.	_	_

#123
They	_	_
propose	_	_
a	_	_
simple	_	_
yet	_	_
efficient	_	_
solution	_	_
combining	_	_
several	_	_
well-known	_	_
parallel	_	_
computation	_	_
models	_	_
:	_	_
PRAM	_	_
,	_	_
BSP	_	_
,	_	_
QRQW	_	_
,	_	_
but	_	_
they	_	_
do	_	_
not	_	_
model	_	_
global	_	_
memory	_	_
coalescing	_	_
.	_	_

#124
Using	_	_
a	_	_
different	_	_
approach	_	_
,	_	_
Hong	_	_
et	_	_
al	_	_
.	_	_
[	_	_
17	_	_
]	_	_
propose	_	_
another	_	_
analytical	_	_
model	_	_
to	_	_
capture	_	_
the	_	_
cost	_	_
of	_	_
memory	_	_
operations	_	_
by	_	_
counting	_	_
the	_	_
number	_	_
of	_	_
parallel	_	_
memory	_	_
requests	_	_
in	_	_
terms	_	_
of	_	_
memory-warp	_	_
parallelism	_	_
(	_	_
MWP	_	_
)	_	_
and	_	_
computation-warp	_	_
parallelism	_	_
(	_	_
CWP	_	_
)	_	_
.	_	_

#125
Meantime	_	_
,	_	_
Baghsorkhi	_	_
et	_	_
al	_	_
.	_	_
[	_	_
16	_	_
]	_	_
measure	_	_
performance	_	_
factors	_	_
in	_	_
isolation	_	_
and	_	_
later	_	_
combine	_	_
them	_	_
to	_	_
model	_	_
the	_	_
overall	_	_
performance	_	_
via	_	_
workflow	_	_
graphs	_	_
so	_	_
that	_	_
the	_	_
interactive	_	_
effects	_	_
between	_	_
different	_	_
performance	_	_
factors	_	_
are	_	_
modeled	_	_
correctly	_	_
.	_	_

#126
The	_	_
model	_	_
can	capability	_
determine	_	_
data	_	_
access	_	_
patterns	_	_
,	_	_
branch	_	_
divergence	_	_
,	_	_
and	_	_
control	_	_
flow	_	_
patterns	_	_
only	_	_
for	_	_
a	_	_
restricted	_	_
class	_	_
of	_	_
kernels	_	_
on	_	_
traditional	_	_
GPU	_	_
architectures	_	_
.	_	_

#127
Zhang	_	_
and	_	_
Owens	_	_
[	_	_
15	_	_
]	_	_
present	_	_
a	_	_
quantitative	_	_
performance	_	_
model	_	_
that	_	_
characterizes	_	_
an	_	_
application	_	_
's	_	_
performance	_	_
as	_	_
being	_	_
primarily	_	_
bounded	_	_
by	_	_
one	_	_
of	_	_
three	_	_
potential	_	_
limits	_	_
:	_	_
instruction	_	_
pipeline	_	_
,	_	_
shared	_	_
memory	_	_
accesses	_	_
,	_	_
and	_	_
global	_	_
memory	_	_
accesses	_	_
.	_	_

#128
More	_	_
recently	_	_
,	_	_
Sim	_	_
et	_	_
al	_	_
.	_	_
[	_	_
48	_	_
]	_	_
develop	_	_
a	_	_
performance	_	_
analysis	_	_
framework	_	_
that	_	_
consists	_	_
of	_	_
an	_	_
analytical	_	_
model	_	_
and	_	_
profiling	_	_
tools	_	_
.	_	_

#129
The	_	_
framework	_	_
does	_	_
a	_	_
good	_	_
job	_	_
in	_	_
performance	_	_
diagnostics	_	_
on	_	_
case	_	_
studies	_	_
of	_	_
real	_	_
codes	_	_
.	_	_

#130
Kim	_	_
et	_	_
al	_	_
.	_	_
[	_	_
49	_	_
]	_	_
also	_	_
design	_	_
a	_	_
tool	_	_
to	_	_
estimate	_	_
GPU	_	_
memory	_	_
performance	_	_
by	_	_
collecting	_	_
performance-critical	_	_
parameters	_	_
.	_	_

#131
Parakh	_	_
et	_	_
al	_	_
.	_	_
[	_	_
50	_	_
]	_	_
present	_	_
a	_	_
model	_	_
to	_	_
estimate	_	_
both	_	_
computation	_	_
time	_	_
by	_	_
precisely	_	_
counting	_	_
instructions	_	_
and	_	_
memory	_	_
access	_	_
time	_	_
by	_	_
a	_	_
method	_	_
to	_	_
generate	_	_
address	_	_
traces	_	_
.	_	_

#132
All	_	_
of	_	_
these	_	_
efforts	_	_
are	_	_
mainly	_	_
focused	_	_
on	_	_
the	_	_
practical	_	_
calibrated	_	_
performance	_	_
models	_	_
.	_	_

#133
No	_	_
attempts	_	_
have	_	_
been	_	_
made	_	_
to	_	_
develop	_	_
an	_	_
asymptotic	_	_
theoretical	_	_
model	_	_
applicable	_	_
to	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
highly-threaded	_	_
machines	_	_
.	_	_

#134
TMM	_	_
model	_	_
The	_	_
TMM	_	_
model	_	_
is	_	_
meant	_	_
to	_	_
model	_	_
the	_	_
asymptotic	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#135
The	_	_
model	_	_
should	deontic	_
abstract	_	_
away	_	_
the	_	_
details	_	_
of	_	_
particular	_	_
implementations	_	_
so	_	_
as	_	_
to	_	_
be	_	_
applicable	_	_
to	_	_
many	_	_
instantiations	_	_
of	_	_
these	_	_
machines	_	_
,	_	_
while	_	_
being	_	_
particular	_	_
enough	_	_
to	_	_
model	_	_
the	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
these	_	_
machines	_	_
with	_	_
reasonable	_	_
accuracy	_	_
.	_	_

#136
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
describe	_	_
the	_	_
important	_	_
characteristics	_	_
of	_	_
these	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
and	_	_
our	_	_
model	_	_
for	_	_
analyzing	_	_
algorithms	_	_
for	_	_
these	_	_
architectures	_	_
.	_	_

#137
Highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
The	_	_
most	_	_
important	_	_
high-level	_	_
characteristic	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
is	_	_
that	_	_
they	_	_
provide	_	_
a	_	_
large	_	_
number	_	_
of	_	_
hardware	_	_
threads	_	_
and	_	_
use	_	_
fast	_	_
and	_	_
low-overhead	_	_
context-switching	_	_
in	_	_
order	_	_
to	_	_
hide	_	_
the	_	_
memory	_	_
access	_	_
latency	_	_
from	_	_
slow	_	_
global	_	_
memory	_	_
.	_	_

#138
Highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
typically	_	_
consist	_	_
of	_	_
a	_	_
number	_	_
of	_	_
core	_	_
groups	_	_
,	_	_
each	_	_
containing	_	_
a	_	_
number	_	_
of	_	_
processors	_	_
(	_	_
or	_	_
cores	_	_
)	_	_
,11	_	_
A	_	_
core	_	_
group	_	_
can	options	_
also	_	_
have	_	_
a	_	_
single	_	_
core	_	_
.	_	_

#139
a	_	_
fixed	_	_
number	_	_
of	_	_
registers	_	_
,	_	_
and	_	_
a	_	_
fixed	_	_
quantity	_	_
of	_	_
fast	_	_
local	_	_
on-chip	_	_
memory	_	_
shared	_	_
within	_	_
a	_	_
core	_	_
group	_	_
.	_	_

#140
A	_	_
large	_	_
slow	_	_
global	_	_
memory	_	_
is	_	_
shared	_	_
by	_	_
all	_	_
the	_	_
core	_	_
groups	_	_
.	_	_

#141
Registers	_	_
and	_	_
local	_	_
on-chip	_	_
memory	_	_
are	_	_
the	_	_
fastest	_	_
to	_	_
access	_	_
,	_	_
while	_	_
accessing	_	_
the	_	_
global	_	_
memory	_	_
may	options	_
potentially	_	_
take	_	_
100s	_	_
of	_	_
cycles	_	_
.	_	_

#142
The	_	_
TMM	_	_
model	_	_
models	_	_
these	_	_
machines	_	_
as	_	_
having	_	_
a	_	_
memory	_	_
hierarchy	_	_
with	_	_
two	_	_
levels	_	_
of	_	_
memory	_	_
:	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
.	_	_

#143
In	_	_
addition	_	_
,	_	_
on	_	_
most	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
,	_	_
data	_	_
is	_	_
transferred	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
memory	_	_
in	_	_
chunks	_	_
;	_	_
instead	_	_
of	_	_
just	_	_
transferring	_	_
one	_	_
word	_	_
at	_	_
a	_	_
time	_	_
,	_	_
the	_	_
hardware	_	_
tries	_	_
to	_	_
transfer	_	_
a	_	_
large	_	_
number	_	_
of	_	_
words	_	_
during	_	_
a	_	_
memory	_	_
transfer	_	_
.	_	_

#144
The	_	_
chunk	_	_
can	options	_
either	_	_
be	_	_
a	_	_
cache	_	_
line	_	_
from	_	_
hardware	_	_
managed	_	_
caches	_	_
,	_	_
or	_	_
an	_	_
explicitly-managed	_	_
combined	_	_
read	_	_
from	_	_
multiple	_	_
threads	_	_
.	_	_

#145
Since	_	_
this	_	_
characteristic	_	_
of	_	_
using	_	_
high-bandwidth	_	_
transfers	_	_
in	_	_
order	_	_
to	_	_
counter	_	_
high	_	_
latencies	_	_
is	_	_
common	_	_
to	_	_
most	_	_
many-core	_	_
machines	_	_
(	_	_
and	_	_
even	_	_
most	_	_
multi-core	_	_
machines	_	_
)	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
captures	_	_
the	_	_
chunk	_	_
size	_	_
as	_	_
one	_	_
of	_	_
its	_	_
parameters	_	_
.	_	_

#146
These	_	_
architectures	_	_
support	_	_
a	_	_
large	_	_
number	_	_
of	_	_
hardware	_	_
threads	_	_
,	_	_
much	_	_
larger	_	_
than	_	_
the	_	_
number	_	_
of	_	_
cores	_	_
.	_	_

#147
Cores	_	_
on	_	_
a	_	_
single	_	_
core	_	_
group	_	_
execute	_	_
in	_	_
synchronous	_	_
style	_	_
where	_	_
groups	_	_
of	_	_
threads	_	_
execute	_	_
in	_	_
lock-step	_	_
.	_	_

#148
When	_	_
a	_	_
thread	_	_
group	_	_
executing	_	_
on	_	_
a	_	_
core	_	_
group	_	_
stalls	_	_
on	_	_
a	_	_
slow	_	_
memory	_	_
access	_	_
,	_	_
in	_	_
theory	_	_
,	_	_
a	_	_
context	_	_
switch	_	_
occurs	_	_
and	_	_
another	_	_
thread	_	_
group	_	_
is	_	_
scheduled	_	_
on	_	_
that	_	_
core	_	_
group	_	_
.	_	_

#149
The	_	_
abstract	_	_
architecture	_	_
is	_	_
shown	_	_
in	_	_
Fig	_	_
.	_	_
2	_	_
.	_	_

#150
Note	_	_
that	_	_
this	_	_
architecture	_	_
abstraction	_	_
ignores	_	_
a	_	_
number	_	_
of	_	_
details	_	_
about	_	_
the	_	_
physical	_	_
machine	_	_
,	_	_
including	_	_
thread	_	_
grouping	_	_
,	_	_
scheduling	_	_
,	_	_
etc	_	_
.	_	_

#151
TMM	_	_
model	_	_
parameters	_	_
The	_	_
TMM	_	_
model	_	_
captures	_	_
the	_	_
important	_	_
characteristics	_	_
of	_	_
a	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architecture	_	_
by	_	_
using	_	_
six	_	_
parameters	_	_
shown	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#152
L	_	_
is	_	_
the	_	_
latency	_	_
for	_	_
accessing	_	_
the	_	_
slow	_	_
memory	_	_
(	_	_
in	_	_
our	_	_
case	_	_
,	_	_
the	_	_
global	_	_
memory	_	_
which	_	_
is	_	_
shared	_	_
by	_	_
all	_	_
the	_	_
core	_	_
groups	_	_
)	_	_
.	_	_

#153
P	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
cores	_	_
(	_	_
or	_	_
processors	_	_
)	_	_
in	_	_
the	_	_
machine	_	_
.	_	_

#154
C	_	_
is	_	_
the	_	_
maximum	_	_
chunk	_	_
size	_	_
;	_	_
the	_	_
number	_	_
of	_	_
words	_	_
that	_	_
can	feasibility	_
be	_	_
read	_	_
from	_	_
slow	_	_
memory	_	_
to	_	_
fast	_	_
memory	_	_
in	_	_
one	_	_
memory	_	_
transfer	_	_
.	_	_

#155
The	_	_
parameter	_	_
Z	_	_
represents	_	_
the	_	_
size	_	_
of	_	_
fast	_	_
local	_	_
memory	_	_
per	_	_
core	_	_
group	_	_
and	_	_
Q	_	_
represents	_	_
the	_	_
number	_	_
of	_	_
cores	_	_
per	_	_
core	_	_
group	_	_
.	_	_

#156
As	_	_
mentioned	_	_
earlier	_	_
,	_	_
in	_	_
some	_	_
instantiations	_	_
,	_	_
a	_	_
core	_	_
group	_	_
can	options	_
have	_	_
a	_	_
single	_	_
core	_	_
.	_	_

#157
In	_	_
this	_	_
case	_	_
,	_	_
a	_	_
many-core	_	_
machine	_	_
looks	_	_
very	_	_
much	_	_
like	_	_
a	_	_
multi-core	_	_
machine	_	_
with	_	_
a	_	_
large	_	_
number	_	_
of	_	_
low-overhead	_	_
hardware	_	_
threads	_	_
.	_	_

#158
Note	_	_
that	_	_
we	_	_
do	_	_
not	_	_
have	_	_
a	_	_
parameter	_	_
for	_	_
the	_	_
number	_	_
of	_	_
core	_	_
groups	_	_
,	_	_
that	_	_
quantity	_	_
is	_	_
simply	_	_
P/Q	_	_
.	_	_

#159
Finally	_	_
X	_	_
is	_	_
the	_	_
hardware	_	_
limit	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
an	_	_
algorithm	_	_
is	_	_
allowed	_	_
to	_	_
generate	_	_
per	_	_
core	_	_
.	_	_

#160
This	_	_
limit	_	_
is	_	_
enforced	_	_
due	_	_
to	_	_
many	_	_
different	_	_
constraints	_	_
,	_	_
such	_	_
as	_	_
constraints	_	_
on	_	_
the	_	_
number	_	_
of	_	_
registers	_	_
each	_	_
thread	_	_
uses	_	_
and	_	_
an	_	_
explicit	_	_
constraint	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#161
We	_	_
unify	_	_
these	_	_
constraints	_	_
into	_	_
one	_	_
parameter	_	_
.	_	_

#162
In	_	_
addition	_	_
to	_	_
the	_	_
architecture	_	_
parameters	_	_
,	_	_
we	_	_
must	deontic	_
also	_	_
consider	_	_
the	_	_
parameters	_	_
which	_	_
are	_	_
determined	_	_
by	_	_
the	_	_
algorithm	_	_
.	_	_

#163
We	_	_
assume	_	_
that	_	_
the	_	_
programmer	_	_
has	_	_
written	_	_
a	_	_
correct	_	_
synchronous	_	_
program	_	_
and	_	_
taken	_	_
care	_	_
to	_	_
balance	_	_
the	_	_
workload	_	_
across	_	_
the	_	_
core	_	_
groups	_	_
.	_	_

#164
These	_	_
program	_	_
parameters	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
2	_	_
.	_	_

#165
T1	_	_
represents	_	_
the	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
,	_	_
that	_	_
is	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
that	_	_
the	_	_
program	_	_
must	deontic	_
perform	_	_
(	_	_
including	_	_
fast	_	_
memory	_	_
accesses	_	_
)	_	_
.T∞	_	_
represents	_	_
the	_	_
span	_	_
of	_	_
the	_	_
algorithm	_	_
,	_	_
that	_	_
is	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
on	_	_
the	_	_
critical	_	_
path	_	_
.	_	_

#166
These	_	_
are	_	_
similar	_	_
to	_	_
the	_	_
analogous	_	_
PRAM	_	_
parameters	_	_
of	_	_
work	_	_
and	_	_
time	_	_
(	_	_
or	_	_
depth	_	_
or	_	_
critical-path	_	_
length	_	_
)	_	_
.	_	_

#167
Next	_	_
,	_	_
we	_	_
come	_	_
to	_	_
program	_	_
parameters	_	_
that	_	_
are	_	_
specific	_	_
to	_	_
many-core	_	_
programs	_	_
.	_	_

#168
M	_	_
represents	_	_
the	_	_
total	_	_
number	_	_
of	_	_
global	_	_
memory	_	_
operations	_	_
performed	_	_
by	_	_
the	_	_
algorithm	_	_
.	_	_

#169
Note	_	_
that	_	_
this	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
operations	_	_
,	_	_
not	_	_
total	_	_
number	_	_
of	_	_
accesses	_	_
.	_	_

#170
Since	_	_
many-core	_	_
machines	_	_
often	_	_
transfer	_	_
data	_	_
in	_	_
large	_	_
chunks	_	_
,	_	_
multiple	_	_
memory	_	_
accesses	_	_
can	capability-feasibility-options	_
combine	_	_
into	_	_
one	_	_
memory	_	_
transfer	_	_
.	_	_

#171
For	_	_
instance	_	_
,	_	_
if	_	_
the	_	_
many-core	_	_
machine	_	_
has	_	_
a	_	_
hardware	_	_
managed	_	_
cache	_	_
,	_	_
and	_	_
the	_	_
program	_	_
accesses	_	_
data	_	_
sequentially	_	_
,	_	_
then	_	_
there	_	_
is	_	_
only	_	_
one	_	_
memory	_	_
operation	_	_
for	_	_
C	_	_
memory	_	_
accesses	_	_
;	_	_
these	_	_
will	_	_
count	_	_
as	_	_
one	_	_
when	_	_
accounting	_	_
for	_	_
M.	_	_
T	_	_
is	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
created	_	_
by	_	_
the	_	_
program	_	_
per	_	_
core	_	_
.	_	_

#172
We	_	_
assume	_	_
that	_	_
the	_	_
work	_	_
is	_	_
perfectly	_	_
distributed	_	_
among	_	_
cores	_	_
.	_	_

#173
Therefore	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
threads	_	_
in	_	_
the	_	_
system	_	_
is	_	_
TP	_	_
.	_	_

#174
On	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
,	_	_
thread	_	_
switching	_	_
is	_	_
used	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
.	_	_

#175
Therefore	_	_
,	_	_
it	_	_
is	_	_
beneficial	_	_
to	_	_
create	_	_
as	_	_
many	_	_
threads	_	_
as	_	_
possible	_	_
.	_	_

#176
However	_	_
,	_	_
the	_	_
maximum	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
limited	_	_
by	_	_
both	_	_
the	_	_
hardware	_	_
and	_	_
the	_	_
program	_	_
.	_	_

#177
The	_	_
software	_	_
limitation	_	_
has	_	_
to	_	_
do	_	_
with	_	_
parallelism	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
is	_	_
limited	_	_
by	_	_
T≤T1/	_	_
(	_	_
T∞⋅P	_	_
)	_	_
.	_	_

#178
The	_	_
hardware	_	_
limits	_	_
T≤X	_	_
.	_	_

#179
Finally	_	_
,	_	_
we	_	_
have	_	_
a	_	_
parameter	_	_
S	_	_
,	_	_
which	_	_
is	_	_
the	_	_
local	_	_
memory	_	_
used	_	_
per	_	_
thread	_	_
.	_	_

#180
S	_	_
and	_	_
T	_	_
are	_	_
related	_	_
parameters	_	_
,	_	_
since	_	_
there	_	_
is	_	_
a	_	_
limited	_	_
amount	_	_
of	_	_
local	_	_
memory	_	_
in	_	_
the	_	_
system	_	_
.	_	_

#181
The	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
is	_	_
at	_	_
most	_	_
T≤Z/	_	_
(	_	_
QS	_	_
)	_	_
.	_	_

#182
TMM	_	_
model	_	_
applicability	_	_
The	_	_
TMM	_	_
model	_	_
is	_	_
a	_	_
high-level	_	_
abstract	_	_
model	_	_
,	_	_
meant	_	_
to	_	_
be	_	_
applicable	_	_
to	_	_
many	_	_
instantiations	_	_
of	_	_
hardware	_	_
platforms	_	_
that	_	_
feature	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
with	_	_
fast	_	_
context	_	_
switching	_	_
and	_	_
a	_	_
hierarchical	_	_
memory	_	_
subsystem	_	_
of	_	_
at	_	_
least	_	_
two	_	_
levels	_	_
with	_	_
a	_	_
large	_	_
memory	_	_
latency	_	_
gap	_	_
in	_	_
between	_	_
.	_	_

#183
Typical	_	_
examples	_	_
of	_	_
this	_	_
set	_	_
include	_	_
NVIDIA	_	_
GPUs	_	_
,	_	_
AMD/ATI	_	_
GPUs	_	_
,	_	_
and	_	_
the	_	_
uRiKA	_	_
machine	_	_
from	_	_
YarcData	_	_
.	_	_

#184
For	_	_
NVIDIA	_	_
GPUs	_	_
,	_	_
a	_	_
number	_	_
of	_	_
streaming	_	_
multiprocessors	_	_
(	_	_
core	_	_
groups	_	_
in	_	_
our	_	_
terminology	_	_
)	_	_
share	_	_
the	_	_
same	_	_
global	_	_
memory	_	_
.	_	_

#185
On	_	_
each	_	_
of	_	_
these	_	_
core	_	_
groups	_	_
,	_	_
there	_	_
are	_	_
a	_	_
number	_	_
of	_	_
CUDA	_	_
cores22	_	_
CUDA	_	_
(	_	_
aka	_	_
Compute	_	_
Unified	_	_
Device	_	_
Architecture	_	_
)	_	_
is	_	_
a	_	_
parallel	_	_
computing	_	_
platform	_	_
and	_	_
programming	_	_
model	_	_
created	_	_
by	_	_
NVIDIA	_	_
.	_	_

#186
that	_	_
share	_	_
a	_	_
fixed	_	_
number	_	_
of	_	_
registers	_	_
and	_	_
on-chip	_	_
(	_	_
fast	_	_
)	_	_
memory	_	_
shared	_	_
among	_	_
the	_	_
cores	_	_
of	_	_
the	_	_
core	_	_
group	_	_
.	_	_

#187
A	_	_
fast	_	_
hardware-supported	_	_
context-switching	_	_
mechanism	_	_
enables	_	_
a	_	_
large	_	_
number	_	_
of	_	_
threads	_	_
to	_	_
execute	_	_
concurrently	_	_
.	_	_

#188
Transfers	_	_
between	_	_
slow	_	_
global	_	_
memory	_	_
and	_	_
fast	_	_
local	_	_
memory	_	_
can	capability	_
occur	_	_
in	_	_
chunks	_	_
of	_	_
at	_	_
most	_	_
32	_	_
words	_	_
;	_	_
these	_	_
chunks	_	_
can	capability-feasibility	_
only	_	_
be	_	_
created	_	_
if	_	_
the	_	_
memory	_	_
accesses	_	_
are	_	_
within	_	_
a	_	_
specified	_	_
range	_	_
.	_	_

#189
Accessing	_	_
the	_	_
off-chip	_	_
global	_	_
memory	_	_
usually	_	_
takes	_	_
20	_	_
to	_	_
40	_	_
times	_	_
more	_	_
clock	_	_
cycles	_	_
than	_	_
accessing	_	_
the	_	_
on-chip	_	_
shared	_	_
memory/L1	_	_
cache	_	_
[	_	_
51	_	_
]	_	_
.	_	_

#190
All	_	_
these	_	_
features	_	_
are	_	_
well	_	_
captured	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#191
Streaming	_	_
multiprocessors	_	_
serve	_	_
the	_	_
same	_	_
role	_	_
as	_	_
a	_	_
core	_	_
group	_	_
,	_	_
while	_	_
CUDA	_	_
cores	_	_
are	_	_
equivalent	_	_
to	_	_
the	_	_
cores	_	_
defined	_	_
in	_	_
TMM	_	_
.	_	_

#192
The	_	_
width	_	_
of	_	_
memory	_	_
access	_	_
C	_	_
is	_	_
32	_	_
due	_	_
to	_	_
the	_	_
coalescing	_	_
of	_	_
the	_	_
threads	_	_
in	_	_
a	_	_
warp	_	_
.	_	_

#193
Global	_	_
memory	_	_
latency	_	_
and	_	_
size	_	_
of	_	_
on-chip	_	_
shared	_	_
memory/L1	_	_
cache	_	_
are	_	_
also	_	_
depicted	_	_
by	_	_
L	_	_
and	_	_
Z	_	_
respectively	_	_
.	_	_

#194
Considering	_	_
AMD/ATI	_	_
GPUs	_	_
and	_	_
taking	_	_
Cypress	_	_
,	_	_
the	_	_
codename	_	_
for	_	_
Radeon	_	_
HD5800	_	_
series	_	_
GPUs	_	_
,	_	_
as	_	_
an	_	_
example	_	_
,	_	_
the	_	_
architecture	_	_
is	_	_
composed	_	_
of	_	_
20	_	_
Single-Instruction-Multiple-Data	_	_
(	_	_
SIMD	_	_
)	_	_
computation	_	_
engines	_	_
.	_	_

#195
In	_	_
each	_	_
SIMD	_	_
engine	_	_
,	_	_
there	_	_
are	_	_
16	_	_
Thread	_	_
Processors	_	_
(	_	_
TP	_	_
)	_	_
and	_	_
a	_	_
32	_	_
kB	_	_
Local	_	_
Data	_	_
Store	_	_
(	_	_
LDS	_	_
)	_	_
.	_	_

#196
Every	_	_
TP	_	_
is	_	_
arranged	_	_
as	_	_
a	_	_
five-way	_	_
or	_	_
four-way	_	_
Very	_	_
Long	_	_
Instruction	_	_
Word	_	_
(	_	_
VLIW	_	_
)	_	_
processor	_	_
,	_	_
and	_	_
consists	_	_
of	_	_
5	_	_
Stream	_	_
Cores	_	_
(	_	_
SC	_	_
)	_	_
.	_	_

#197
Low	_	_
context-switch	_	_
threading	_	_
is	_	_
well	_	_
supported	_	_
,	_	_
and	_	_
every	_	_
64	_	_
threads	_	_
are	_	_
grouped	_	_
into	_	_
a	_	_
wavefront	_	_
executing	_	_
the	_	_
same	_	_
instruction	_	_
.	_	_

#198
Basically	_	_
,	_	_
the	_	_
SIMD	_	_
engine	_	_
can	capability-feasibility	_
naturally	_	_
be	_	_
modeled	_	_
by	_	_
core	_	_
groups	_	_
.	_	_

#199
Each	_	_
SC	_	_
is	_	_
modeled	_	_
as	_	_
a	_	_
core	_	_
in	_	_
TMM	_	_
,	_	_
summing	_	_
up	_	_
to	_	_
1600	_	_
cores	_	_
totally	_	_
.	_	_

#200
LDS	_	_
is	_	_
straightforwardly	_	_
described	_	_
by	_	_
the	_	_
fast	_	_
local	_	_
memory	_	_
of	_	_
TMM	_	_
.	_	_

#201
The	_	_
width	_	_
of	_	_
memory	_	_
access	_	_
C	_	_
in	_	_
TMM	_	_
equals	_	_
to	_	_
the	_	_
wavefront	_	_
width	_	_
of	_	_
64	_	_
for	_	_
AMD/ATI	_	_
GPUs	_	_
.	_	_

#202
The	_	_
uRiKA	_	_
system	_	_
from	_	_
YarcData	_	_
is	_	_
also	_	_
a	_	_
potential	_	_
target	_	_
for	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#203
Based	_	_
on	_	_
the	_	_
description	_	_
from	_	_
Alverson	_	_
et	_	_
al	_	_
.	_	_
[	_	_
52	_	_
]	_	_
about	_	_
the	_	_
nature	_	_
of	_	_
the	_	_
computations	_	_
this	_	_
processor	_	_
was	_	_
designed	_	_
to	_	_
run	_	_
,	_	_
it	_	_
is	_	_
a	_	_
purpose-built	_	_
appliance	_	_
for	_	_
real-time	_	_
graph	_	_
analytics	_	_
featuring	_	_
graph-optimized	_	_
hardware	_	_
that	_	_
provides	_	_
up	_	_
to	_	_
512	_	_
terabytes	_	_
of	_	_
global	_	_
shared	_	_
memory	_	_
,	_	_
massively-multi-threaded	_	_
graph	_	_
processors	_	_
(	_	_
named	_	_
Threadstorm	_	_
)	_	_
supporting	_	_
128	_	_
threads/processor	_	_
,	_	_
and	_	_
highly	_	_
scalable	_	_
I/O	_	_
.	_	_

#204
Therefore	_	_
,	_	_
128	_	_
defines	_	_
parameter	_	_
X	_	_
,	_	_
the	_	_
hard	_	_
limit	_	_
of	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
.	_	_

#205
There	_	_
can	options	_
be	_	_
up	_	_
to	_	_
65,000	_	_
threads	_	_
in	_	_
a	_	_
512	_	_
processor	_	_
system	_	_
and	_	_
over	_	_
1	_	_
million	_	_
threads	_	_
at	_	_
the	_	_
maximum	_	_
system	_	_
size	_	_
of	_	_
8192	_	_
processors	_	_
,	_	_
so	_	_
that	_	_
the	_	_
latencies	_	_
are	_	_
hidden	_	_
by	_	_
accommodating	_	_
many	_	_
remote	_	_
memory	_	_
references	_	_
in	_	_
flight	_	_
.	_	_

#206
The	_	_
processor	_	_
's	_	_
instruction	_	_
execution	_	_
hardware	_	_
essentially	_	_
does	_	_
a	_	_
context	_	_
switch	_	_
every	_	_
instruction	_	_
cycle	_	_
,	_	_
finding	_	_
the	_	_
next	_	_
thread	_	_
that	_	_
is	_	_
ready	_	_
to	_	_
issue	_	_
an	_	_
instruction	_	_
into	_	_
the	_	_
execution	_	_
pipeline	_	_
.	_	_

#207
This	_	_
suggests	_	_
that	_	_
the	_	_
memory	_	_
access	_	_
width	_	_
or	_	_
chunk	_	_
size	_	_
C	_	_
is	_	_
1	_	_
on	_	_
these	_	_
machines	_	_
.	_	_

#208
Threads	_	_
do	_	_
not	_	_
share	_	_
anything	_	_
,	_	_
as	_	_
the	_	_
Threadstorm	_	_
processor	_	_
has	_	_
128	_	_
hardware	_	_
copies	_	_
of	_	_
the	_	_
register	_	_
set	_	_
,	_	_
program	_	_
counter	_	_
,	_	_
stack	_	_
pointer	_	_
,	_	_
etc.	_	_
,	_	_
necessary	_	_
to	_	_
hold	_	_
the	_	_
current	_	_
state	_	_
of	_	_
one	_	_
software	_	_
thread	_	_
that	_	_
is	_	_
executing	_	_
on	_	_
the	_	_
processor	_	_
.	_	_

#209
Conceptually	_	_
,	_	_
each	_	_
of	_	_
the	_	_
Threadstorm	_	_
processors	_	_
is	_	_
mapped	_	_
to	_	_
a	_	_
core	_	_
group	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
but	_	_
,	_	_
different	_	_
than	_	_
the	_	_
two	_	_
GPU	_	_
architectures	_	_
,	_	_
it	_	_
has	_	_
only	_	_
one	_	_
core	_	_
on-chip	_	_
,	_	_
thus	_	_
Q	_	_
equals	_	_
1	_	_
.	_	_

#210
TMM	_	_
analysis	_	_
structure	_	_
In	_	_
order	_	_
to	_	_
analyze	_	_
program	_	_
performance	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
we	_	_
must	deontic	_
first	_	_
calculate	_	_
the	_	_
program	_	_
parameters	_	_
for	_	_
the	_	_
particular	_	_
program	_	_
.	_	_

#211
Once	_	_
we	_	_
have	_	_
calculated	_	_
these	_	_
values	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
then	_	_
try	_	_
to	_	_
understand	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
algorithm	_	_
.	_	_

#212
We	_	_
first	_	_
calculate	_	_
the	_	_
effective	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
TE	_	_
.	_	_

#213
The	_	_
effective	_	_
work	_	_
should	deontic	_
consider	_	_
both	_	_
work	_	_
due	_	_
to	_	_
computation	_	_
and	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
.	_	_

#214
Total	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
is	_	_
M⋅L	_	_
,	_	_
but	_	_
since	_	_
this	_	_
work	_	_
is	_	_
hidden	_	_
by	_	_
using	_	_
threads	_	_
,	_	_
the	_	_
real	_	_
effective	_	_
work	_	_
due	_	_
to	_	_
memory	_	_
accesses	_	_
is	_	_
(	_	_
M⋅L	_	_
)	_	_
/T	_	_
Therefore	_	_
,	_	_
we	_	_
have	_	_
(	_	_
1	_	_
)	_	_
TE=O	_	_
(	_	_
max	_	_
(	_	_
T1	_	_
,	_	_
M⋅LT	_	_
)	_	_
)	_	_
.	_	_

#215
Note	_	_
that	_	_
this	_	_
expression	_	_
assumes	_	_
perfect	_	_
scheduling	_	_
(	_	_
the	_	_
threads	_	_
are	_	_
context	_	_
swapped	_	_
with	_	_
no	_	_
overhead	_	_
,	_	_
as	_	_
soon	_	_
as	_	_
they	_	_
are	_	_
stalled	_	_
)	_	_
and	_	_
perfect	_	_
load	_	_
balance	_	_
between	_	_
threads	_	_
.	_	_

#216
The	_	_
time	_	_
to	_	_
execute	_	_
on	_	_
P	_	_
cores	_	_
is	_	_
represented	_	_
by	_	_
TP	_	_
and	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
(	_	_
2	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
TEP	_	_
,	_	_
T∞	_	_
)	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
.	_	_

#217
Therefore	_	_
,	_	_
speedup	_	_
on	_	_
P	_	_
cores	_	_
,	_	_
SP	_	_
,	_	_
is	_	_
(	_	_
3	_	_
)	_	_
SP=T1TP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
T1T∞	_	_
,	_	_
P⋅T1⋅TM⋅L	_	_
)	_	_
)	_	_
.	_	_

#218
For	_	_
linear	_	_
speedup	_	_
,	_	_
SP	_	_
should	deontic	_
be	_	_
P.	_	_
More	_	_
precisely	_	_
,	_	_
for	_	_
PRAM	_	_
algorithms	_	_
,	_	_
SP=min	_	_
(	_	_
P	_	_
,	_	_
T1/T∞	_	_
)	_	_
.	_	_

#219
Therefore	_	_
,	_	_
if	_	_
the	_	_
first	_	_
two	_	_
terms	_	_
in	_	_
the	_	_
min	_	_
of	_	_
Eq	_	_
.	_	_
(	_	_
3	_	_
)	_	_
dominate	_	_
,	_	_
then	_	_
a	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
algorithm	_	_
's	_	_
performance	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
corresponding	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#220
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
if	_	_
the	_	_
last	_	_
term	_	_
dominates	_	_
,	_	_
then	_	_
the	_	_
algorithm	_	_
's	_	_
performance	_	_
depends	_	_
on	_	_
other	_	_
factors	_	_
.	_	_

#221
If	_	_
T	_	_
could	feasibility-speculation	_
be	_	_
unbounded	_	_
,	_	_
then	_	_
the	_	_
last	_	_
term	_	_
will	_	_
never	_	_
dominate	_	_
.	_	_

#222
However	_	_
,	_	_
as	_	_
we	_	_
explained	_	_
earlier	_	_
,	_	_
T	_	_
is	_	_
not	_	_
an	_	_
unlimited	_	_
resource	_	_
and	_	_
has	_	_
both	_	_
hardware	_	_
and	_	_
algorithmic	_	_
upper	_	_
bounds	_	_
.	_	_

#223
Therefore	_	_
,	_	_
based	_	_
on	_	_
the	_	_
machine	_	_
parameters	_	_
,	_	_
algorithms	_	_
that	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
can	options	_
have	_	_
different	_	_
real	_	_
performance	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#224
Therefore	_	_
,	_	_
this	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
pick	_	_
algorithms	_	_
that	_	_
provide	_	_
performance	_	_
as	_	_
close	_	_
as	_	_
possible	_	_
to	_	_
PRAM	_	_
algorithms	_	_
.	_	_

#225
Analysis	_	_
of	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
algorithms	_	_
using	_	_
the	_	_
TMM	_	_
model	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
demonstrate	_	_
the	_	_
usefulness	_	_
of	_	_
our	_	_
model	_	_
by	_	_
using	_	_
it	_	_
to	_	_
analyze	_	_
4	_	_
different	_	_
algorithms	_	_
for	_	_
calculating	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
in	_	_
graphs	_	_
.	_	_

#226
All	_	_
pairs	_	_
shortest	_	_
paths	_	_
is	_	_
a	_	_
classic	_	_
problem	_	_
for	_	_
which	_	_
there	_	_
are	_	_
many	_	_
algorithms	_	_
.	_	_

#227
We	_	_
are	_	_
given	_	_
a	_	_
graph	_	_
G=	_	_
(	_	_
V	_	_
,	_	_
E	_	_
)	_	_
with	_	_
n	_	_
vertices	_	_
and	_	_
m	_	_
edges	_	_
.	_	_

#228
Each	_	_
edge	_	_
e	_	_
has	_	_
a	_	_
weight	_	_
w	_	_
(	_	_
e	_	_
)	_	_
.	_	_

#229
We	_	_
must	deontic	_
calculate	_	_
the	_	_
shortest	_	_
weighted	_	_
path	_	_
from	_	_
every	_	_
vertex	_	_
to	_	_
every	_	_
other	_	_
vertex	_	_
.	_	_

#230
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
asymptotic	_	_
insights	_	_
,	_	_
therefore	_	_
,	_	_
we	_	_
assume	_	_
that	_	_
the	_	_
graphs	_	_
are	_	_
large	_	_
graphs	_	_
.	_	_

#231
In	_	_
particular	_	_
n	_	_
>	_	_
Z	_	_
.	_	_

#232
Dynamic	_	_
programming	_	_
via	_	_
matrix	_	_
multiplication	_	_
Our	_	_
first	_	_
algorithm	_	_
is	_	_
a	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
[	_	_
53	_	_
]	_	_
that	_	_
uses	_	_
repeated	_	_
matrix	_	_
multiplication	_	_
to	_	_
calculate	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
.	_	_

#233
The	_	_
graph	_	_
is	_	_
represented	_	_
as	_	_
an	_	_
adjacency	_	_
matrix	_	_
A	_	_
where	_	_
Aij	_	_
represents	_	_
the	_	_
weight	_	_
of	_	_
edge	_	_
(	_	_
i	_	_
,	_	_
j	_	_
)	_	_
.	_	_

#234
Al	_	_
is	_	_
a	_	_
transitive	_	_
matrix	_	_
where	_	_
Aijl	_	_
represents	_	_
the	_	_
shortest	_	_
path	_	_
from	_	_
vertex	_	_
i	_	_
to	_	_
vertex	_	_
j	_	_
using	_	_
at	_	_
most	_	_
l	_	_
intermediate	_	_
edges	_	_
.	_	_

#235
A1	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
adjacency	_	_
matrix	_	_
A	_	_
and	_	_
we	_	_
want	_	_
to	_	_
calculate	_	_
An-1	_	_
to	_	_
calculate	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
.	_	_

#236
A2	_	_
can	feasibility	_
be	_	_
calculated	_	_
from	_	_
A1	_	_
as	_	_
follows	_	_
:	_	_
(	_	_
4	_	_
)	_	_
Aij2=min0≤k	_	_
<	_	_
n	_	_
(	_	_
Aij1	_	_
,	_	_
Aik1+Akj1	_	_
)	_	_
.	_	_

#237
Note	_	_
that	_	_
,	_	_
the	_	_
structure	_	_
of	_	_
this	_	_
equation	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
matrix	_	_
multiplication	_	_
operation	_	_
where	_	_
the	_	_
sum	_	_
is	_	_
replaced	_	_
by	_	_
a	_	_
min	_	_
operation	_	_
and	_	_
the	_	_
multiplication	_	_
is	_	_
replaced	_	_
by	_	_
an	_	_
addition	_	_
operation	_	_
.	_	_

#238
Therefore	_	_
,	_	_
we	_	_
can	feasibility	_
use	_	_
repeated	_	_
matrix	_	_
multiplication	_	_
which	_	_
calculates	_	_
An	_	_
using	_	_
O	_	_
(	_	_
lgn	_	_
)	_	_
matrix	_	_
multiplications	_	_
.	_	_

#239
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
Parallelizing	_	_
this	_	_
algorithm	_	_
for	_	_
the	_	_
PRAM	_	_
model	_	_
simply	_	_
involves	_	_
parallelizing	_	_
the	_	_
matrix	_	_
multiplication	_	_
algorithm	_	_
such	_	_
that	_	_
each	_	_
element	_	_
in	_	_
the	_	_
matrix	_	_
is	_	_
calculated	_	_
in	_	_
parallel	_	_
.	_	_

#240
The	_	_
total	_	_
work	_	_
of	_	_
lgn	_	_
matrix	_	_
multiplications	_	_
using	_	_
a	_	_
PRAM	_	_
algorithm	_	_
is	_	_
T1=O	_	_
(	_	_
n3lgn	_	_
)	_	_
.33	_	_
This	_	_
can	feasibility	_
be	_	_
done	_	_
faster	_	_
using	_	_
Strassen	_	_
's	_	_
algorithm	_	_
.	_	_

#241
Using	_	_
Strassen	_	_
's	_	_
algorithm	_	_
will	_	_
impact	_	_
the	_	_
PRAM	_	_
and	_	_
the	_	_
TMM	_	_
algorithms	_	_
equally	_	_
.	_	_

#242
Therefore	_	_
,	_	_
we	_	_
demonstrate	_	_
our	_	_
point	_	_
using	_	_
the	_	_
simpler	_	_
algorithm	_	_
.	_	_

#243
The	_	_
span	_	_
of	_	_
a	_	_
single	_	_
matrix	_	_
multiplication	_	_
algorithm	_	_
is	_	_
O	_	_
(	_	_
n	_	_
)	_	_
.	_	_

#244
Therefore	_	_
,	_	_
the	_	_
total	_	_
span	_	_
of	_	_
the	_	_
algorithm	_	_
is	_	_
T∞=O	_	_
(	_	_
nlgn	_	_
)	_	_
.	_	_

#245
The	_	_
time	_	_
and	_	_
speedup	_	_
using	_	_
P	_	_
processors	_	_
are	_	_
(	_	_
5	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
n3lgnP	_	_
,	_	_
nlgn	_	_
)	_	_
)	_	_
(	_	_
6	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n2	_	_
)	_	_
)	_	_
.	_	_

#246
Therefore	_	_
,	_	_
the	_	_
PRAM	_	_
algorithm	_	_
gets	_	_
linear	_	_
speedup	_	_
as	_	_
long	_	_
asP≤n2	_	_
.	_	_

#247
TMM	_	_
algorithm	_	_
and	_	_
analysis	_	_
TMM	_	_
algorithms	_	_
are	_	_
tailored	_	_
to	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
generally	_	_
by	_	_
using	_	_
fast	_	_
on-chip	_	_
memory	_	_
to	_	_
avoid	_	_
accesses	_	_
to	_	_
slow	_	_
off-chip	_	_
global	_	_
memory	_	_
,	_	_
coalescing	_	_
to	_	_
diminish	_	_
the	_	_
time	_	_
required	_	_
to	_	_
access	_	_
slow	_	_
memory	_	_
,	_	_
and	_	_
threading	_	_
to	_	_
hide	_	_
the	_	_
latency	_	_
of	_	_
accesses	_	_
to	_	_
slow	_	_
memory	_	_
.	_	_

#248
Due	_	_
to	_	_
its	_	_
large	_	_
size	_	_
,	_	_
the	_	_
adjacency	_	_
matrix	_	_
is	_	_
stored	_	_
in	_	_
off-chip	_	_
global	_	_
memory	_	_
.	_	_

#249
Following	_	_
traditional	_	_
block-decomposition	_	_
techniques	_	_
,	_	_
sub-blocks	_	_
of	_	_
the	_	_
result	_	_
matrix	_	_
(	_	_
whose	_	_
size	_	_
is	_	_
denoted	_	_
by	_	_
B	_	_
)	_	_
are	_	_
assigned	_	_
to	_	_
core	_	_
groups	_	_
for	_	_
computation	_	_
.	_	_

#250
The	_	_
threads	_	_
in	_	_
a	_	_
core	_	_
group	_	_
read	_	_
in	_	_
the	_	_
required	_	_
input	_	_
sub-blocks	_	_
,	_	_
perform	_	_
the	_	_
computation	_	_
of	_	_
Eq	_	_
.	_	_
(	_	_
4	_	_
)	_	_
for	_	_
their	_	_
assigned	_	_
sub-block	_	_
,	_	_
and	_	_
write	_	_
the	_	_
sub-block	_	_
out	_	_
to	_	_
global	_	_
memory	_	_
.	_	_

#251
This	_	_
happens	_	_
lgn	_	_
times	_	_
by	_	_
repeated	_	_
squaring	_	_
.	_	_

#252
The	_	_
work	_	_
and	_	_
the	_	_
span	_	_
of	_	_
this	_	_
algorithm	_	_
remain	_	_
unchanged	_	_
from	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#253
However	_	_
,	_	_
we	_	_
must	deontic	_
also	_	_
calculate	_	_
M	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
.	_	_

#254
Let	_	_
us	_	_
first	_	_
consider	_	_
a	_	_
single	_	_
matrix	_	_
multiplication	_	_
operation	_	_
.	_	_

#255
There	_	_
are	_	_
a	_	_
total	_	_
of	_	_
n2	_	_
elements	_	_
and	_	_
each	_	_
element	_	_
is	_	_
read	_	_
for	_	_
the	_	_
calculation	_	_
of	_	_
O	_	_
(	_	_
n/B	_	_
)	_	_
other	_	_
blocks	_	_
.	_	_

#256
However	_	_
,	_	_
due	_	_
to	_	_
the	_	_
regularity	_	_
in	_	_
memory	_	_
accesses	_	_
,	_	_
each	_	_
block	_	_
can	feasibility	_
be	_	_
read	_	_
fully	_	_
coalesced	_	_
.	_	_

#257
Therefore	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
for	_	_
one	_	_
matrix	_	_
multiply	_	_
is	_	_
O	_	_
(	_	_
(	_	_
n2/C	_	_
)	_	_
⋅	_	_
(	_	_
n/B	_	_
)	_	_
)	_	_
=O	_	_
(	_	_
n3/	_	_
(	_	_
BC	_	_
)	_	_
)	_	_
.	_	_

#258
Also	_	_
note	_	_
that	_	_
since	_	_
we	_	_
must	deontic	_
fit	_	_
a	_	_
B×B	_	_
block	_	_
in	_	_
a	_	_
local	_	_
memory	_	_
of	_	_
size	_	_
Z	_	_
on	_	_
one	_	_
core	_	_
group	_	_
,	_	_
we	_	_
get	_	_
B=Θ	_	_
(	_	_
Z	_	_
)	_	_
.	_	_

#259
Therefore	_	_
,	_	_
for	_	_
lgn	_	_
matrix	_	_
multiplication	_	_
operations	_	_
,	_	_
M=O	_	_
(	_	_
n3lgn/	_	_
(	_	_
Z⋅C	_	_
)	_	_
)	_	_
.	_	_

#260
Now	_	_
we	_	_
are	_	_
ready	_	_
to	_	_
calculate	_	_
the	_	_
time	_	_
on	_	_
P	_	_
processors	_	_
.	_	_

#261
(	_	_
7	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
8	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
n3lgnP	_	_
,	_	_
nlgn	_	_
,	_	_
n3lgn⋅LZ⋅C⋅T⋅P	_	_
)	_	_
)	_	_
.	_	_

#262
Therefore	_	_
,	_	_
the	_	_
speedup	_	_
on	_	_
P	_	_
processors	_	_
is	_	_
(	_	_
9	_	_
)	_	_
SP=T1/TP	_	_
(	_	_
10	_	_
)	_	_
=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n2	_	_
,	_	_
Z⋅C⋅TL⋅P	_	_
)	_	_
)	_	_
.	_	_

#263
We	_	_
can	feasibility-rhetorical	_
now	_	_
compare	_	_
the	_	_
PRAM	_	_
and	_	_
TMM	_	_
analysis	_	_
and	_	_
note	_	_
that	_	_
the	_	_
speedup	_	_
is	_	_
P	_	_
as	_	_
long	_	_
as	_	_
ZCT/L≥1	_	_
.	_	_

#264
We	_	_
also	_	_
know	_	_
that	_	_
T≤min	_	_
(	_	_
X	_	_
,	_	_
Z/	_	_
(	_	_
QS	_	_
)	_	_
)	_	_
,	_	_
and	_	_
S=O	_	_
(	_	_
1	_	_
)	_	_
,	_	_
since	_	_
each	_	_
thread	_	_
only	_	_
needs	_	_
constant	_	_
memory	_	_
.	_	_

#265
Therefore	_	_
,	_	_
we	_	_
can	feasibility-rhetorical	_
conclude	_	_
that	_	_
the	_	_
algorithm	_	_
achieves	_	_
linear	_	_
speedup	_	_
as	_	_
long	_	_
as	_	_
L≤min	_	_
(	_	_
ZCX	_	_
,	_	_
Z3/2C/Q	_	_
)	_	_
.	_	_

#266
Johnson	_	_
's	_	_
algorithm	_	_
:	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
Johnson	_	_
's	_	_
algorithm	_	_
[	_	_
54	_	_
]	_	_
is	_	_
an	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
algorithm	_	_
that	_	_
uses	_	_
Dijkstra	_	_
's	_	_
single	_	_
source	_	_
algorithm	_	_
as	_	_
the	_	_
subroutine	_	_
and	_	_
calls	_	_
it	_	_
n	_	_
times	_	_
,	_	_
once	_	_
from	_	_
each	_	_
source	_	_
vertex	_	_
.	_	_

#267
Dijkstra	_	_
's	_	_
algorithm	_	_
is	_	_
a	_	_
greedy	_	_
algorithm	_	_
for	_	_
calculating	_	_
single	_	_
source	_	_
shortest	_	_
paths	_	_
.	_	_

#268
The	_	_
pseudo-code	_	_
for	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
is	_	_
given	_	_
in	_	_
Algorithm	_	_
1	_	_
[	_	_
55	_	_
]	_	_
.	_	_

#269
The	_	_
single	_	_
source	_	_
algorithm	_	_
consists	_	_
of	_	_
n	_	_
insert	_	_
operations	_	_
,	_	_
m	_	_
decrease-key	_	_
operations	_	_
and	_	_
n	_	_
delete-min	_	_
operations	_	_
from	_	_
a	_	_
min-priority	_	_
queue	_	_
.	_	_

#270
The	_	_
standard	_	_
way	_	_
of	_	_
implementing	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
is	_	_
to	_	_
use	_	_
a	_	_
binary	_	_
or	_	_
a	_	_
Fibonacci	_	_
heap	_	_
to	_	_
store	_	_
the	_	_
array	_	_
elements	_	_
.	_	_

#271
We	_	_
now	_	_
consider	_	_
a	_	_
binary	_	_
heap	_	_
implementation	_	_
so	_	_
that	_	_
each	_	_
operation	_	_
(	_	_
insert	_	_
,	_	_
decrease-key	_	_
,	_	_
and	_	_
delete-min	_	_
)	_	_
takes	_	_
O	_	_
(	_	_
lgn	_	_
)	_	_
time	_	_
.	_	_

#272
Note	_	_
that	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
does	_	_
not	_	_
work	_	_
when	_	_
there	_	_
are	_	_
negative	_	_
weight	_	_
edges	_	_
in	_	_
the	_	_
graph	_	_
.	_	_

#273
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
A	_	_
simple	_	_
parallel	_	_
implementation	_	_
of	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
consists	_	_
of	_	_
doing	_	_
each	_	_
single-source	_	_
shortest	_	_
path	_	_
calculation	_	_
in	_	_
parallel	_	_
.	_	_

#274
The	_	_
total	_	_
work	_	_
of	_	_
a	_	_
single-source	_	_
computation	_	_
is	_	_
O	_	_
(	_	_
mlgn+nlgn	_	_
)	_	_
.	_	_

#275
For	_	_
simplicity	_	_
,	_	_
we	_	_
assume	_	_
that	_	_
the	_	_
graph	_	_
is	_	_
connected	_	_
,	_	_
giving	_	_
us	_	_
O	_	_
(	_	_
mlgn	_	_
)	_	_
.	_	_

#276
Therefore	_	_
,	_	_
the	_	_
total	_	_
work	_	_
for	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
is	_	_
T1=O	_	_
(	_	_
mnlgn	_	_
)	_	_
.	_	_

#277
The	_	_
span	_	_
is	_	_
T∞=O	_	_
(	_	_
mlgn	_	_
)	_	_
since	_	_
each	_	_
single	_	_
source	_	_
computation	_	_
executes	_	_
sequentially	_	_
.	_	_

#278
The	_	_
time	_	_
and	_	_
speedup	_	_
using	_	_
P	_	_
processors	_	_
are	_	_
(	_	_
11	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
mnlgnP	_	_
,	_	_
mlgn	_	_
)	_	_
)	_	_
(	_	_
12	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n	_	_
)	_	_
)	_	_
.	_	_

#279
Therefore	_	_
,	_	_
the	_	_
PRAM	_	_
algorithm	_	_
gets	_	_
linear	_	_
speedup	_	_
as	_	_
long	_	_
as	_	_
P≤n	_	_
.	_	_

#280
TMM	_	_
algorithm	_	_
and	_	_
analysis	_	_
The	_	_
TMM	_	_
algorithm	_	_
is	_	_
very	_	_
similar	_	_
to	_	_
the	_	_
PRAM	_	_
algorithm	_	_
where	_	_
each	_	_
thread	_	_
computes	_	_
a	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
.	_	_

#281
Therefore	_	_
,	_	_
each	_	_
thread	_	_
requires	_	_
a	_	_
min-heap	_	_
of	_	_
size	_	_
n.	_	_
Since	_	_
n	_	_
may	capability-options	_
be	_	_
arbitrarily	_	_
large	_	_
compared	_	_
to	_	_
Z/QT	_	_
(	_	_
the	_	_
share	_	_
of	_	_
local	_	_
memory	_	_
for	_	_
each	_	_
thread	_	_
)	_	_
,	_	_
these	_	_
heaps	_	_
can	capability	negation
not	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
and	_	_
must	deontic	_
be	_	_
allocated	_	_
on	_	_
the	_	_
slow	_	_
global	_	_
memory	_	_
.	_	_

#282
The	_	_
work	_	_
and	_	_
span	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#283
We	_	_
must	deontic	_
now	_	_
compute	_	_
M.	_	_
Note	_	_
that	_	_
each	_	_
time	_	_
the	_	_
thread	_	_
does	_	_
a	_	_
heap	_	_
operation	_	_
,	_	_
it	_	_
must	deontic	_
access	_	_
global	_	_
memory	_	_
,	_	_
since	_	_
the	_	_
heaps	_	_
are	_	_
stored	_	_
in	_	_
global	_	_
memory	_	_
.	_	_

#284
In	_	_
addition	_	_
,	_	_
binary	_	_
heap	_	_
accesses	_	_
are	_	_
not	_	_
predictable	_	_
and	_	_
regular	_	_
,	_	_
so	_	_
the	_	_
heap	_	_
accesses	_	_
from	_	_
different	_	_
threads	_	_
can	feasibility	negation
not	_	_
be	_	_
coalesced	_	_
.	_	_

#285
Therefore	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
is	_	_
M=O	_	_
(	_	_
mnlgn	_	_
)	_	_
.44	_	_
There	_	_
are	_	_
other	_	_
accesses	_	_
that	_	_
are	_	_
not	_	_
heap	_	_
accesses	_	_
,	_	_
but	_	_
those	_	_
are	_	_
asymptotically	_	_
fewer	_	_
and	_	_
can	feasibility	_
be	_	_
ignored	_	_
.	_	_

#286
Now	_	_
we	_	_
are	_	_
ready	_	_
to	_	_
calculate	_	_
the	_	_
time	_	_
on	_	_
P	_	_
processors	_	_
.	_	_

#287
(	_	_
13	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
14	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
mnlgnP	_	_
,	_	_
mlgn	_	_
,	_	_
mnlgn⋅LT⋅P	_	_
)	_	_
)	_	_
.	_	_

#288
Therefore	_	_
,	_	_
the	_	_
speedup	_	_
on	_	_
P	_	_
processors	_	_
is	_	_
(	_	_
15	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n	_	_
,	_	_
TL⋅P	_	_
)	_	_
)	_	_
.	_	_

#289
Note	_	_
that	_	_
this	_	_
algorithm	_	_
gets	_	_
linear	_	_
speedup	_	_
only	_	_
if	_	_
T/L≥1	_	_
.	_	_

#290
Therefore	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
this	_	_
algorithm	_	_
needs	_	_
to	_	_
get	_	_
linear	_	_
speedup	_	_
is	_	_
very	_	_
large	_	_
.	_	_

#291
We	_	_
know	_	_
that	_	_
T≤min	_	_
(	_	_
X	_	_
,	_	_
Z/	_	_
(	_	_
QS	_	_
)	_	_
)	_	_
,	_	_
and	_	_
S=O	_	_
(	_	_
1	_	_
)	_	_
for	_	_
this	_	_
algorithm	_	_
.	_	_

#292
This	_	_
allows	_	_
us	_	_
to	_	_
conclude	_	_
that	_	_
this	_	_
algorithm	_	_
achieves	_	_
linear	_	_
speedup	_	_
only	_	_
if	_	_
L≤min	_	_
(	_	_
X	_	_
,	_	_
Z/Q	_	_
)	_	_
,	_	_
since	_	_
each	_	_
thread	_	_
needs	_	_
only	_	_
constant	_	_
memory	_	_
.	_	_

#293
These	_	_
conditions	_	_
are	_	_
much	_	_
stricter	_	_
than	_	_
those	_	_
imposed	_	_
by	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
.	_	_

#294
Johnson	_	_
's	_	_
algorithm	_	_
:	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
This	_	_
algorithm	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
previous	_	_
algorithm	_	_
in	_	_
that	_	_
it	_	_
still	_	_
uses	_	_
n	_	_
single-source	_	_
Dijkstra	_	_
's	_	_
algorithm	_	_
calculations	_	_
.	_	_

#295
However	_	_
,	_	_
instead	_	_
of	_	_
binary	_	_
heaps	_	_
,	_	_
we	_	_
use	_	_
arrays	_	_
to	_	_
do	_	_
delete-min	_	_
and	_	_
decrease-key	_	_
operations	_	_
.	_	_

#296
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
The	_	_
PRAM	_	_
algorithm	_	_
is	_	_
very	_	_
similar	_	_
to	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
binary	_	_
heaps	_	_
.	_	_

#297
Each	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
is	_	_
computed	_	_
in	_	_
parallel	_	_
.	_	_

#298
However	_	_
,	_	_
in	_	_
this	_	_
algorithm	_	_
,	_	_
we	_	_
simply	_	_
store	_	_
the	_	_
current	_	_
estimates	_	_
of	_	_
the	_	_
shortest	_	_
path	_	_
of	_	_
vertices	_	_
in	_	_
an	_	_
array	_	_
instead	_	_
of	_	_
a	_	_
binary	_	_
heap	_	_
.	_	_

#299
Therefore	_	_
,	_	_
there	_	_
are	_	_
n	_	_
arrays	_	_
of	_	_
size	_	_
n	_	_
,	_	_
one	_	_
for	_	_
each	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
calculation	_	_
.	_	_

#300
Each	_	_
decrease-key	_	_
now	_	_
takes	_	_
O	_	_
(	_	_
1	_	_
)	_	_
time	_	_
,	_	_
since	_	_
one	_	_
can	feasibility	_
simply	_	_
decrease	_	_
the	_	_
key	_	_
using	_	_
random	_	_
access	_	_
.	_	_

#301
Each	_	_
delete-min	_	_
,	_	_
however	_	_
,	_	_
takes	_	_
O	_	_
(	_	_
n	_	_
)	_	_
work	_	_
,	_	_
since	_	_
one	_	_
must	deontic	_
look	_	_
at	_	_
the	_	_
entire	_	_
array	_	_
to	_	_
find	_	_
the	_	_
minimum	_	_
element	_	_
.	_	_

#302
Therefore	_	_
,	_	_
the	_	_
work	_	_
of	_	_
the	_	_
algorithm	_	_
is	_	_
T1=O	_	_
(	_	_
n3+mn	_	_
)	_	_
and	_	_
the	_	_
span	_	_
is	_	_
O	_	_
(	_	_
n2+m	_	_
)	_	_
.	_	_

#303
We	_	_
can	feasibility	_
improve	_	_
the	_	_
span	_	_
by	_	_
doing	_	_
delete-min	_	_
in	_	_
parallel	_	_
,	_	_
since	_	_
one	_	_
can	feasibility	_
find	_	_
the	_	_
smallest	_	_
element	_	_
in	_	_
an	_	_
array	_	_
in	_	_
parallel	_	_
using	_	_
O	_	_
(	_	_
n	_	_
)	_	_
work	_	_
and	_	_
O	_	_
(	_	_
lgn	_	_
)	_	_
time	_	_
using	_	_
a	_	_
parallel	_	_
prefix	_	_
computation	_	_
.	_	_

#304
This	_	_
brings	_	_
the	_	_
total	_	_
span	_	_
to	_	_
T∞=O	_	_
(	_	_
nlgn+m	_	_
)	_	_
while	_	_
the	_	_
work	_	_
remains	_	_
the	_	_
same	_	_
.	_	_

#305
The	_	_
time	_	_
and	_	_
speedup	_	_
using	_	_
P	_	_
processors	_	_
is	_	_
(	_	_
16	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
n3P	_	_
,	_	_
nlgn+m	_	_
)	_	_
)	_	_
(	_	_
17	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
n3P	_	_
,	_	_
nlgn	_	_
,	_	_
m	_	_
)	_	_
)	_	_
(	_	_
18	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
n2lgn	_	_
,	_	_
n3m	_	_
)	_	_
)	_	_
.	_	_

#306
TMM	_	_
algorithm	_	_
and	_	_
analysis	_	_
The	_	_
TMM	_	_
algorithm	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
PRAM	_	_
algorithm	_	_
,	_	_
except	_	_
that	_	_
each	_	_
core	_	_
group	_	_
is	_	_
responsible	_	_
for	_	_
a	_	_
single-source	_	_
shortest	_	_
path	_	_
calculation	_	_
.	_	_

#307
Therefore	_	_
,	_	_
all	_	_
the	_	_
threads	_	_
on	_	_
a	_	_
single	_	_
core	_	_
group	_	_
(	_	_
QT	_	_
in	_	_
number	_	_
)	_	_
cooperate	_	_
to	_	_
calculate	_	_
a	_	_
single	_	_
shortest	_	_
path	_	_
computation	_	_
.	_	_

#308
Since	_	_
we	_	_
assume	_	_
that	_	_
n	_	_
>	_	_
Z	_	_
,	_	_
the	_	_
entire	_	_
array	_	_
does	_	_
not	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
and	_	_
must	deontic	_
be	_	_
read	_	_
with	_	_
each	_	_
delete-min	_	_
operation	_	_
.	_	_

#309
Therefore	_	_
,	_	_
the	_	_
span	_	_
of	_	_
the	_	_
delete-min	_	_
operation	_	_
changes	_	_
.	_	_

#310
For	_	_
each	_	_
delete-min	_	_
operation	_	_
,	_	_
elements	_	_
are	_	_
read	_	_
into	_	_
local	_	_
memory	_	_
in	_	_
chunks	_	_
of	_	_
size	_	_
Z.	_	_
For	_	_
each	_	_
chunk	_	_
,	_	_
the	_	_
minimum	_	_
is	_	_
computed	_	_
in	_	_
parallel	_	_
in	_	_
O	_	_
(	_	_
lgZ	_	_
)	_	_
time	_	_
.	_	_

#311
Therefore	_	_
,	_	_
the	_	_
span	_	_
of	_	_
each	_	_
delete-min	_	_
operation	_	_
is	_	_
O	_	_
(	_	_
(	_	_
n/Z	_	_
)	_	_
lgZ	_	_
)	_	_
.	_	_

#312
Therefore	_	_
,	_	_
the	_	_
total	_	_
span	_	_
is	_	_
T∞=O	_	_
(	_	_
n2lgZ/Z	_	_
)	_	_
.	_	_

#313
The	_	_
work	_	_
is	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
work	_	_
.	_	_

#314
We	_	_
must	deontic	_
now	_	_
compute	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
,	_	_
M.	_	_
There	_	_
are	_	_
n2	_	_
delete-min	_	_
operations	_	_
in	_	_
total	_	_
,	_	_
and	_	_
each	_	_
reads	_	_
the	_	_
array	_	_
of	_	_
size	_	_
n	_	_
coalesced	_	_
.	_	_

#315
In	_	_
addition	_	_
,	_	_
there	_	_
are	_	_
a	_	_
total	_	_
of	_	_
mn	_	_
decrease	_	_
key	_	_
operations	_	_
,	_	_
but	_	_
these	_	_
reads	_	_
can	feasibility	negation
not	_	_
be	_	_
coalesced	_	_
.	_	_

#316
Therefore	_	_
,	_	_
M=O	_	_
(	_	_
n3/C+mn	_	_
)	_	_
.	_	_

#317
(	_	_
19	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
20	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
n3P	_	_
,	_	_
n2lgZZ	_	_
,	_	_
(	_	_
n3C+mn	_	_
)	_	_
⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
21	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
n3P	_	_
,	_	_
n2lgZZ	_	_
,	_	_
n3⋅LC⋅T⋅P	_	_
,	_	_
mn⋅LT⋅P	_	_
)	_	_
)	_	_
.	_	_

#318
Speedup	_	_
is	_	_
(	_	_
22	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
nZlgZ	_	_
,	_	_
C⋅TL⋅P	_	_
,	_	_
n2⋅Tm⋅L⋅P	_	_
)	_	_
)	_	_
.	_	_

#319
Again	_	_
,	_	_
in	_	_
this	_	_
algorithm	_	_
,	_	_
T≤min	_	_
(	_	_
X	_	_
,	_	_
Z/	_	_
(	_	_
QS	_	_
)	_	_
)	_	_
,	_	_
and	_	_
S=O	_	_
(	_	_
1	_	_
)	_	_
since	_	_
each	_	_
thread	_	_
needs	_	_
only	_	_
constant	_	_
memory	_	_
.	_	_

#320
Therefore	_	_
,	_	_
the	_	_
PRAM	_	_
performance	_	_
dominates	_	_
if	_	_
L≤min	_	_
(	_	_
CX	_	_
,	_	_
CZ/Q	_	_
,	_	_
n2X/m	_	_
,	_	_
n2Z/	_	_
(	_	_
mQ	_	_
)	_	_
)	_	_
.	_	_

#321
n	_	_
iterations	_	_
of	_	_
Bellman-Ford	_	_
algorithm	_	_
This	_	_
is	_	_
another	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
algorithm	_	_
that	_	_
uses	_	_
a	_	_
single-source	_	_
Bellman-Ford	_	_
algorithm	_	_
as	_	_
a	_	_
subroutine	_	_
.	_	_

#322
The	_	_
algorithm	_	_
is	_	_
given	_	_
in	_	_
Algorithm	_	_
2	_	_
[	_	_
56,57	_	_
]	_	_
.	_	_

#323
PRAM	_	_
algorithm	_	_
and	_	_
analysis	_	_
Again	_	_
,	_	_
one	_	_
can	feasibility	_
do	_	_
each	_	_
single	_	_
source	_	_
computation	_	_
in	_	_
parallel	_	_
.	_	_

#324
Each	_	_
single	_	_
source	_	_
computation	_	_
takes	_	_
O	_	_
(	_	_
mn	_	_
)	_	_
work	_	_
,	_	_
making	_	_
the	_	_
total	_	_
work	_	_
of	_	_
all	_	_
pairs	_	_
shortest	_	_
paths	_	_
O	_	_
(	_	_
mn2	_	_
)	_	_
and	_	_
the	_	_
total	_	_
span	_	_
O	_	_
(	_	_
mn	_	_
)	_	_
.	_	_

#325
One	_	_
can	feasibility	_
improve	_	_
the	_	_
span	_	_
by	_	_
relaxing	_	_
all	_	_
edges	_	_
in	_	_
one	_	_
iteration	_	_
in	_	_
parallel	_	_
making	_	_
the	_	_
span	_	_
O	_	_
(	_	_
n	_	_
)	_	_
.	_	_

#326
(	_	_
23	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
mn2P	_	_
,	_	_
n	_	_
)	_	_
)	_	_
.	_	_
(	_	_
24	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
mn	_	_
)	_	_
)	_	_
.	_	_

#327
TMM	_	_
algorithm	_	_
and	_	_
analysis	_	_
The	_	_
TMM	_	_
algorithm	_	_
for	_	_
this	_	_
problem	_	_
is	_	_
more	_	_
complicated	_	_
and	_	_
requires	_	_
more	_	_
data	_	_
structure	_	_
support	_	_
.	_	_

#328
Each	_	_
core	_	_
group	_	_
is	_	_
responsible	_	_
for	_	_
one	_	_
single-source	_	_
shortest	_	_
path	_	_
calculation	_	_
.	_	_

#329
For	_	_
each	_	_
single	_	_
source	_	_
calculation	_	_
,	_	_
we	_	_
maintain	_	_
three	_	_
arrays	_	_
,	_	_
A	_	_
,	_	_
B	_	_
and	_	_
W	_	_
,	_	_
of	_	_
size	_	_
m	_	_
,	_	_
and	_	_
one	_	_
array	_	_
D	_	_
of	_	_
size	_	_
n.	_	_
D	_	_
contains	_	_
the	_	_
current	_	_
guess	_	_
of	_	_
the	_	_
shortest	_	_
path	_	_
to	_	_
vertex	_	_
i.	_	_
B	_	_
contains	_	_
ending	_	_
vertices	_	_
of	_	_
edges	_	_
,	_	_
sorted	_	_
by	_	_
vertex	_	_
ID	_	_
.	_	_

#330
Therefore	_	_
B	_	_
may	options	_
contain	_	_
multiple	_	_
instances	_	_
of	_	_
the	_	_
same	_	_
vertex	_	_
if	_	_
that	_	_
vertex	_	_
has	_	_
multiple	_	_
incident	_	_
edges	_	_
.	_	_

#331
A	_	_
[	_	_
i	_	_
]	_	_
contains	_	_
the	_	_
starting	_	_
vertex	_	_
of	_	_
the	_	_
edge	_	_
that	_	_
ends	_	_
at	_	_
B	_	_
[	_	_
i	_	_
]	_	_
and	_	_
W	_	_
[	_	_
i	_	_
]	_	_
contains	_	_
the	_	_
weight	_	_
of	_	_
that	_	_
edge	_	_
.	_	_

#332
Therefore	_	_
,	_	_
both	_	_
D	_	_
and	_	_
B	_	_
are	_	_
sorted	_	_
.	_	_

#333
Each	_	_
thread	_	_
deals	_	_
with	_	_
one	_	_
index	_	_
in	_	_
the	_	_
array	_	_
and	_	_
relaxes	_	_
that	_	_
edge	_	_
in	_	_
each	_	_
iteration	_	_
.	_	_

#334
All	_	_
threads	_	_
relax	_	_
edges	_	_
in	_	_
parallel	_	_
in	_	_
order	_	_
of	_	_
B.	_	_
The	_	_
total	_	_
work	_	_
and	_	_
span	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
algorithm	_	_
.	_	_

#335
We	_	_
can	feasibility	_
now	_	_
calculate	_	_
the	_	_
time	_	_
and	_	_
speedup	_	_
assuming	_	_
threads	_	_
can	capability	_
read	_	_
all	_	_
the	_	_
arrays	_	_
coalesced	_	_
,	_	_
M=O	_	_
(	_	_
mn2/C+n3/C	_	_
)	_	_
=O	_	_
(	_	_
mn2/C	_	_
)	_	_
for	_	_
connected	_	_
graphs	_	_
.	_	_

#336
(	_	_
25	_	_
)	_	_
TP=O	_	_
(	_	_
max	_	_
(	_	_
T1P	_	_
,	_	_
T∞	_	_
,	_	_
M⋅LT⋅P	_	_
)	_	_
)	_	_
(	_	_
26	_	_
)	_	_
=O	_	_
(	_	_
max	_	_
(	_	_
mn2P	_	_
,	_	_
n	_	_
,	_	_
mn2⋅LC⋅T⋅P	_	_
)	_	_
)	_	_
.	_	_

#337
Therefore	_	_
,	_	_
the	_	_
speedup	_	_
on	_	_
P	_	_
processors	_	_
is	_	_
(	_	_
27	_	_
)	_	_
SP=Ω	_	_
(	_	_
min	_	_
(	_	_
P	_	_
,	_	_
mn	_	_
,	_	_
C⋅TL⋅P	_	_
)	_	_
)	_	_
.	_	_

#338
In	_	_
this	_	_
case	_	_
,	_	_
we	_	_
get	_	_
linear	_	_
speedup	_	_
if	_	_
CT/L≥1	_	_
.	_	_

#339
Subject	_	_
to	_	_
the	_	_
limits	_	_
on	_	_
threads	_	_
of	_	_
T≤min	_	_
(	_	_
X	_	_
,	_	_
Z/	_	_
(	_	_
QS	_	_
)	_	_
)	_	_
and	_	_
S=O	_	_
(	_	_
1	_	_
)	_	_
for	_	_
constant	_	_
local	_	_
memory	_	_
usage	_	_
per	_	_
thread	_	_
,	_	_
this	_	_
requires	_	_
L≤min	_	_
(	_	_
CX	_	_
,	_	_
CZ/Q	_	_
)	_	_
.	_	_

#340
Comparison	_	_
of	_	_
the	_	_
various	_	_
algorithms	_	_
As	_	_
our	_	_
analysis	_	_
of	_	_
shortest	_	_
paths	_	_
algorithms	_	_
indicates	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
allows	_	_
us	_	_
to	_	_
take	_	_
the	_	_
unique	_	_
properties	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
architectures	_	_
into	_	_
consideration	_	_
while	_	_
analyzing	_	_
the	_	_
algorithms	_	_
.	_	_

#341
Therefore	_	_
,	_	_
the	_	_
model	_	_
provides	_	_
more	_	_
nuance	_	_
in	_	_
the	_	_
analysis	_	_
of	_	_
these	_	_
algorithms	_	_
for	_	_
the	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
than	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#342
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
compare	_	_
the	_	_
running	_	_
times	_	_
of	_	_
the	_	_
various	_	_
algorithms	_	_
and	_	_
see	_	_
what	_	_
interesting	_	_
things	_	_
this	_	_
analysis	_	_
tells	_	_
us	_	_
.	_	_

#343
Table	_	_
3	_	_
indicates	_	_
the	_	_
running	_	_
times	_	_
of	_	_
the	_	_
various	_	_
algorithms	_	_
in	_	_
both	_	_
the	_	_
PRAM	_	_
model	_	_
and	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
as	_	_
well	_	_
as	_	_
the	_	_
conditions	_	_
under	_	_
which	_	_
TMM	_	_
results	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
results	_	_
.	_	_

#344
We	_	_
have	_	_
ignored	_	_
the	_	_
span	_	_
term	_	_
,	_	_
since	_	_
the	_	_
span	_	_
is	_	_
small	_	_
relative	_	_
to	_	_
work	_	_
in	_	_
all	_	_
of	_	_
these	_	_
algorithms	_	_
.	_	_

#345
As	_	_
we	_	_
can	feasibility-rhetorical	_
see	_	_
,	_	_
if	_	_
L	_	_
is	_	_
small	_	_
,	_	_
then	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
provide	_	_
PRAM	_	_
performance	_	_
.	_	_

#346
However	_	_
,	_	_
the	_	_
cut-off	_	_
value	_	_
for	_	_
L	_	_
is	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
where	_	_
the	_	_
performance	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
differs	_	_
from	_	_
the	_	_
PRAM	_	_
model	_	_
is	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
.	_	_

#347
Therefore	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability-options	_
be	_	_
informative	_	_
when	_	_
comparing	_	_
between	_	_
algorithms	_	_
.	_	_

#348
We	_	_
will	_	_
perform	_	_
two	_	_
types	_	_
of	_	_
comparison	_	_
between	_	_
these	_	_
algorithms	_	_
in	_	_
this	_	_
section	_	_
.	_	_

#349
The	_	_
first	_	_
one	_	_
considers	_	_
the	_	_
direct	_	_
influence	_	_
of	_	_
machine	_	_
parameters	_	_
on	_	_
asymptotic	_	_
performance	_	_
.	_	_

#350
Since	_	_
machine	_	_
parameters	_	_
do	_	_
not	_	_
scale	_	_
with	_	_
problem	_	_
size	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
machine	_	_
parameters	_	_
can	capability	negation
not	_	_
change	_	_
the	_	_
asymptotic	_	_
performance	_	_
of	_	_
algorithms	_	_
in	_	_
terms	_	_
of	_	_
problem	_	_
size	_	_
.	_	_

#351
That	_	_
is	_	_
,	_	_
if	_	_
the	_	_
PRAM	_	_
analysis	_	_
indicates	_	_
that	_	_
some	_	_
algorithm	_	_
has	_	_
a	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
n	_	_
)	_	_
and	_	_
another	_	_
one	_	_
has	_	_
the	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
nlgn	_	_
)	_	_
,	_	_
for	_	_
large	_	_
enough	_	_
n	_	_
,	_	_
the	_	_
first	_	_
algorithm	_	_
is	_	_
always	_	_
asymptotically	_	_
better	_	_
since	_	_
eventually	_	_
lgn	_	_
will	_	_
dominate	_	_
whatever	_	_
machine	_	_
parameter	_	_
advantage	_	_
the	_	_
second	_	_
algorithm	_	_
may	options	_
have	_	_
.	_	_

#352
Therefore	_	_
,	_	_
for	_	_
this	_	_
first	_	_
comparison	_	_
,	_	_
we	_	_
only	_	_
compare	_	_
algorithms	_	_
which	_	_
have	_	_
the	_	_
same	_	_
asymptotic	_	_
performance	_	_
under	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#353
Second	_	_
,	_	_
we	_	_
will	_	_
also	_	_
do	_	_
a	_	_
non-asymptotic	_	_
comparison	_	_
where	_	_
we	_	_
compare	_	_
algorithms	_	_
when	_	_
the	_	_
problem	_	_
size	_	_
is	_	_
relatively	_	_
small	_	_
,	_	_
but	_	_
not	_	_
very	_	_
small	_	_
.	_	_

#354
In	_	_
particular	_	_
,	_	_
we	_	_
look	_	_
at	_	_
the	_	_
case	_	_
when	_	_
lgn	_	_
<	_	_
Z	_	_
.	_	_

#355
In	_	_
this	_	_
case	_	_
,	_	_
even	_	_
algorithms	_	_
that	_	_
are	_	_
asymptotically	_	_
worse	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
can	options	_
be	_	_
better	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
for	_	_
large	_	_
latency	_	_
L.	_	_
In	_	_
the	_	_
next	_	_
section	_	_
,	_	_
we	_	_
will	_	_
look	_	_
at	_	_
even	_	_
smaller	_	_
problem	_	_
sizes	_	_
where	_	_
the	_	_
effects	_	_
are	_	_
even	_	_
more	_	_
dramatic	_	_
.	_	_

#356
Influence	_	_
of	_	_
machine	_	_
parameters	_	_
As	_	_
the	_	_
table	_	_
shows	_	_
,	_	_
the	_	_
limits	_	_
on	_	_
machine	_	_
parameters	_	_
to	_	_
get	_	_
linear	_	_
speedup	_	_
are	_	_
different	_	_
for	_	_
different	_	_
algorithms	_	_
.	_	_

#357
Therefore	_	_
,	_	_
even	_	_
when	_	_
two	_	_
algorithms	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
,	_	_
their	_	_
performance	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
may	options	_
vary	_	_
significantly	_	_
.	_	_

#358
Let	_	_
us	_	_
consider	_	_
a	_	_
few	_	_
examples	_	_
:	_	_
Dynamic	_	_
programming	_	_
vs	_	_
.	_	_

#359
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
when	_	_
m=O	_	_
(	_	_
n2	_	_
)	_	_
If	_	_
m=O	_	_
(	_	_
n2	_	_
)	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
graph	_	_
is	_	_
dense	_	_
)	_	_
,	_	_
the	_	_
PRAM	_	_
performance	_	_
for	_	_
both	_	_
algorithms	_	_
is	_	_
the	_	_
same	_	_
.	_	_

#360
However	_	_
whenZ/Q	_	_
<	_	_
L	_	_
<	_	_
Z3/2C/Q	_	_
,	_	_
Johnson	_	_
's	_	_
algorithm	_	_
has	_	_
a	_	_
significantly	_	_
worse	_	_
running	_	_
time	_	_
.	_	_

#361
Take	_	_
the	_	_
example	_	_
of	_	_
L=O	_	_
(	_	_
Z3/2C/Q	_	_
)	_	_
.	_	_

#362
The	_	_
Johnson	_	_
running	_	_
time	_	_
is	_	_
O	_	_
(	_	_
n3lgnZC/P	_	_
)	_	_
while	_	_
the	_	_
running	_	_
time	_	_
of	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
is	_	_
simply	_	_
O	_	_
(	_	_
n3lgn/P	_	_
)	_	_
.	_	_

#363
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
vs	_	_
.	_	_

#364
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
when	_	_
m=O	_	_
(	_	_
n2/lgn	_	_
)	_	_
If	_	_
m=O	_	_
(	_	_
n2/lgn	_	_
)	_	_
(	_	_
i.e.	_	_
,	_	_
a	_	_
somewhat	_	_
sparse	_	_
graph	_	_
)	_	_
,	_	_
these	_	_
two	_	_
algorithms	_	_
have	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
,	_	_
but	_	_
if	_	_
Z/Q	_	_
<	_	_
L≤ZC/Q	_	_
,	_	_
then	_	_
the	_	_
array	_	_
implementation	_	_
is	_	_
better	_	_
.	_	_

#365
For	_	_
L=ZC/Q	_	_
,	_	_
the	_	_
binary	_	_
heap	_	_
implementation	_	_
has	_	_
a	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
n3C/P	_	_
)	_	_
,	_	_
while	_	_
the	_	_
array	_	_
implementation	_	_
has	_	_
a	_	_
running	_	_
time	_	_
of	_	_
simply	_	_
O	_	_
(	_	_
n3/P	_	_
)	_	_
.	_	_

#366
Influence	_	_
of	_	_
graph	_	_
size	_	_
The	_	_
previous	_	_
section	_	_
shows	_	_
the	_	_
asymptotic	_	_
power	_	_
of	_	_
the	_	_
model	_	_
;	_	_
the	_	_
results	_	_
there	_	_
hold	_	_
for	_	_
large	_	_
sizes	_	_
of	_	_
graphs	_	_
asymptotically	_	_
.	_	_

#367
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
also	_	_
help	_	_
decide	_	_
on	_	_
what	_	_
algorithm	_	_
to	_	_
use	_	_
based	_	_
on	_	_
the	_	_
size	_	_
of	_	_
the	_	_
graph	_	_
.	_	_

#368
In	_	_
particular	_	_
for	_	_
certain	_	_
sizes	_	_
of	_	_
graphs	_	_
,	_	_
algorithm	_	_
A	_	_
can	options	_
be	_	_
better	_	_
than	_	_
algorithm	_	_
B	_	_
even	_	_
if	_	_
it	_	_
is	_	_
asymptotically	_	_
worse	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#369
Therefore	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
give	_	_
us	_	_
information	_	_
that	_	_
the	_	_
PRAM	_	_
model	_	_
can	capability	negation
not	_	_
.	_	_

#370
Consider	_	_
the	_	_
example	_	_
of	_	_
dynamic	_	_
programming	_	_
vs	_	_
.	_	_

#371
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
.	_	_

#372
In	_	_
the	_	_
PRAM	_	_
model	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
is	_	_
unquestionably	_	_
worse	_	_
than	_	_
Johnson	_	_
's	_	_
.	_	_

#373
However	_	_
,	_	_
if	_	_
lgn	_	_
<	_	_
Z	_	_
,	_	_
we	_	_
may	feasibility-options	_
have	_	_
a	_	_
different	_	_
conclusion	_	_
.	_	_

#374
In	_	_
this	_	_
case	_	_
,	_	_
dynamic	_	_
programming	_	_
has	_	_
runtime	_	_
:	_	_
(	_	_
28	_	_
)	_	_
n3lgn⋅LZCTP=n2LTP⋅nlgnZC	_	_
<	_	_
n2LTP⋅nC	_	_
.	_	_

#375
While	_	_
Johnson	_	_
's	_	_
algorithm	_	_
has	_	_
runtime	_	_
:	_	_
(	_	_
29	_	_
)	_	_
min	_	_
(	_	_
n3LCTP	_	_
,	_	_
mnLTP	_	_
)	_	_
=n2LTP⋅min	_	_
(	_	_
nC	_	_
,	_	_
mn	_	_
)	_	_
.	_	_

#376
If	_	_
n2/m	_	_
<	_	_
C	_	_
,	_	_
i.e	_	_
.	_	_
dense	_	_
graphs	_	_
,	_	_
n/C	_	_
<	_	_
m/n	_	_
.	_	_

#377
Combine	_	_
(	_	_
28	_	_
)	_	_
and	_	_
(	_	_
29	_	_
)	_	_
,	_	_
we	_	_
have	_	_
(	_	_
30	_	_
)	_	_
n3lgn⋅LZCTP	_	_
<	_	_
n3LCTP	_	_
,	_	_
if	_	_
n2m	_	_
<	_	_
C	_	_
.	_	_

#378
This	_	_
indicates	_	_
that	_	_
when	_	_
for	_	_
small	_	_
enough	_	_
graphs	_	_
where	_	_
lgn	_	_
<	_	_
Z	_	_
,	_	_
there	_	_
is	_	_
a	_	_
dichotomy	_	_
.	_	_

#379
For	_	_
dense	_	_
graphs	_	_
n2/m	_	_
<	_	_
C	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
should	deontic	_
be	_	_
preferred	_	_
,	_	_
while	_	_
for	_	_
sparse	_	_
graphs	_	_
,	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
is	_	_
better	_	_
.	_	_

#380
We	_	_
illustrate	_	_
this	_	_
performance	_	_
dependence	_	_
on	_	_
sparsity	_	_
with	_	_
experiments	_	_
in	_	_
Section	_	_
7	_	_
.	_	_

#381
We	_	_
get	_	_
a	_	_
similar	_	_
result	_	_
when	_	_
comparing	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
with	_	_
Bellman-Ford	_	_
when	_	_
m=O	_	_
(	_	_
n	_	_
)	_	_
.	_	_

#382
In	_	_
spite	_	_
of	_	_
being	_	_
worse	_	_
in	_	_
the	_	_
PRAM	_	_
world	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
is	_	_
better	_	_
when	_	_
lgn	_	_
<	_	_
Z	_	_
.	_	_

#383
Our	_	_
model	_	_
therefore	_	_
allows	_	_
us	_	_
to	_	_
do	_	_
two	_	_
things	_	_
.	_	_

#384
First	_	_
,	_	_
for	_	_
a	_	_
particular	_	_
machine	_	_
,	_	_
given	_	_
two	_	_
algorithms	_	_
which	_	_
are	_	_
asymptotically	_	_
similar	_	_
,	_	_
we	_	_
can	feasibility	_
pick	_	_
the	_	_
more	_	_
appropriate	_	_
algorithm	_	_
for	_	_
that	_	_
particular	_	_
machine	_	_
given	_	_
its	_	_
machine	_	_
parameters	_	_
.	_	_

#385
Second	_	_
,	_	_
if	_	_
we	_	_
also	_	_
consider	_	_
the	_	_
problem	_	_
size	_	_
,	_	_
then	_	_
we	_	_
can	feasibility	_
do	_	_
more	_	_
.	_	_

#386
For	_	_
small	_	_
problem	_	_
sizes	_	_
,	_	_
the	_	_
asymptotically	_	_
worse	_	_
algorithm	_	_
may	capability-options	_
in	_	_
fact	_	_
be	_	_
better	_	_
because	_	_
it	_	_
interacts	_	_
better	_	_
with	_	_
the	_	_
machine	_	_
.	_	_

#387
We	_	_
will	_	_
draw	_	_
more	_	_
insights	_	_
of	_	_
this	_	_
type	_	_
in	_	_
the	_	_
next	_	_
section	_	_
.	_	_

#388
Effect	_	_
of	_	_
problem	_	_
size	_	_
In	_	_
Section	_	_
5	_	_
,	_	_
we	_	_
explored	_	_
the	_	_
asymptotic	_	_
insights	_	_
that	_	_
can	feasibility	_
be	_	_
drawn	_	_
from	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#389
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability	_
also	_	_
inform	_	_
insights	_	_
based	_	_
on	_	_
problem	_	_
size	_	_
.	_	_

#390
In	_	_
particular	_	_
,	_	_
some	_	_
algorithms	_	_
can	capability	_
take	_	_
advantage	_	_
of	_	_
smaller	_	_
problems	_	_
better	_	_
than	_	_
others	_	_
,	_	_
since	_	_
they	_	_
can	capability	_
use	_	_
fast	_	_
local	_	_
memory	_	_
more	_	_
effectively	_	_
.	_	_

#391
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
explore	_	_
the	_	_
insights	_	_
that	_	_
the	_	_
TMM	_	_
model	_	_
provides	_	_
in	_	_
these	_	_
cases	_	_
.	_	_

#392
Vertices	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
When	_	_
n	_	_
<	_	_
Z	_	_
,	_	_
all	_	_
the	_	_
vertices	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
.	_	_

#393
Note	_	_
that	_	_
this	_	_
does	_	_
not	_	_
mean	_	_
that	_	_
the	_	_
entire	_	_
problem	_	_
fits	_	_
in	_	_
local	_	_
memory	_	_
,	_	_
since	_	_
the	_	_
number	_	_
of	_	_
edges	_	_
can	options	_
still	_	_
be	_	_
much	_	_
larger	_	_
than	_	_
the	_	_
number	_	_
of	_	_
vertices	_	_
.	_	_

#394
In	_	_
this	_	_
scenario	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
by	_	_
the	_	_
first	_	_
,	_	_
second	_	_
,	_	_
and	_	_
fourth	_	_
algorithms	_	_
is	_	_
not	_	_
affected	_	_
at	_	_
all	_	_
.	_	_

#395
In	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
,	_	_
we	_	_
consider	_	_
the	_	_
array	_	_
of	_	_
size	_	_
n2	_	_
and	_	_
being	_	_
able	_	_
to	_	_
fit	_	_
a	_	_
row	_	_
into	_	_
local	_	_
memory	_	_
does	_	_
not	_	_
reduce	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
transfers	_	_
.	_	_

#396
In	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
,	_	_
each	_	_
thread	_	_
does	_	_
its	_	_
own	_	_
single	_	_
source	_	_
shortest	_	_
path	_	_
.	_	_

#397
Since	_	_
the	_	_
local	_	_
memory	_	_
Z	_	_
is	_	_
shared	_	_
among	_	_
QT	_	_
threads	_	_
,	_	_
each	_	_
thread	_	_
can	capability	negation
not	_	_
hold	_	_
its	_	_
entire	_	_
vertex	_	_
array	_	_
in	_	_
local	_	_
memory	_	_
.	_	_

#398
In	_	_
the	_	_
Bellman-Ford	_	_
algorithm	_	_
,	_	_
the	_	_
cost	_	_
is	_	_
dominated	_	_
by	_	_
the	_	_
cost	_	_
of	_	_
reading	_	_
the	_	_
edges	_	_
.	_	_

#399
Therefore	_	_
,	_	_
the	_	_
bounds	_	_
do	_	_
not	_	_
change	_	_
.	_	_

#400
For	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
,	_	_
the	_	_
cost	_	_
is	_	_
lower	_	_
.	_	_

#401
Now	_	_
each	_	_
core	_	_
group	_	_
can	capability	_
store	_	_
the	_	_
vertex	_	_
array	_	_
and	_	_
does	_	_
not	_	_
need	_	_
to	_	_
access	_	_
it	_	_
from	_	_
slow	_	_
memory	_	_
.	_	_

#402
Therefore	_	_
the	_	_
bound	_	_
on	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
changes	_	_
to	_	_
M=O	_	_
(	_	_
n2/C+mn	_	_
)	_	_
=O	_	_
(	_	_
mn	_	_
)	_	_
for	_	_
connected	_	_
graphs	_	_
.	_	_

#403
For	_	_
these	_	_
small	_	_
problem	_	_
sizes	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
can	capability-options	_
provide	_	_
even	_	_
more	_	_
insight	_	_
.	_	_

#404
As	_	_
an	_	_
example	_	_
,	_	_
compare	_	_
the	_	_
two	_	_
versions	_	_
of	_	_
Johnson	_	_
's	_	_
algorithm	_	_
,	_	_
the	_	_
one	_	_
that	_	_
uses	_	_
arrays	_	_
and	_	_
the	_	_
one	_	_
that	_	_
uses	_	_
heaps	_	_
.	_	_

#405
When	_	_
m=O	_	_
(	_	_
n2/lg2n	_	_
)	_	_
,	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
heaps	_	_
is	_	_
better	_	_
than	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
arrays	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#406
But	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
for	_	_
large	_	_
L	_	_
,	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
heaps	_	_
has	_	_
the	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
Lmnlgn/	_	_
(	_	_
TP	_	_
)	_	_
)	_	_
=O	_	_
(	_	_
Ln3/	_	_
(	_	_
TPlgn	_	_
)	_	_
)	_	_
,	_	_
while	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
arrays	_	_
has	_	_
the	_	_
running	_	_
time	_	_
of	_	_
O	_	_
(	_	_
Ln3/	_	_
(	_	_
TPlg2n	_	_
)	_	_
)	_	_
.	_	_

#407
Therefore	_	_
,	_	_
the	_	_
algorithm	_	_
that	_	_
uses	_	_
arrays	_	_
is	_	_
better	_	_
.	_	_

#408
Note	_	_
that	_	_
asymptotic	_	_
analysis	_	_
is	_	_
a	_	_
little	_	_
dubious	_	_
when	_	_
we	_	_
are	_	_
talking	_	_
about	_	_
small	_	_
problem	_	_
sizes	_	_
;	_	_
therefore	_	_
,	_	_
this	_	_
analysis	_	_
should	deontic	_
be	_	_
considered	_	_
skeptically	_	_
.	_	_

#409
However	_	_
,	_	_
the	_	_
analysis	_	_
is	_	_
rigorous	_	_
when	_	_
we	_	_
consider	_	_
the	_	_
circumstance	_	_
that	_	_
local	_	_
memory	_	_
size	_	_
grows	_	_
with	_	_
problem	_	_
size	_	_
(	_	_
i.e.	_	_
,	_	_
Z	_	_
is	_	_
asymptotic	_	_
)	_	_
.	_	_

#410
Moreover	_	_
,	_	_
this	_	_
type	_	_
of	_	_
analysis	_	_
can	capability-feasibility	_
still	_	_
provide	_	_
enough	_	_
insight	_	_
that	_	_
it	_	_
might	capability-speculation	_
guide	_	_
implementation	_	_
decisions	_	_
under	_	_
the	_	_
more	_	_
realistic	_	_
circumstance	_	_
of	_	_
bounded	_	_
(	_	_
but	_	_
potentially	_	_
large	_	_
)	_	_
Z.	_	_
Edges	_	_
fit	_	_
in	_	_
the	_	_
combined	_	_
local	_	_
memories	_	_
When	_	_
m=O	_	_
(	_	_
PZ/Q	_	_
)	_	_
,	_	_
the	_	_
edges	_	_
fit	_	_
in	_	_
all	_	_
the	_	_
memories	_	_
of	_	_
the	_	_
core	_	_
groups	_	_
combined	_	_
.	_	_

#411
Again	_	_
,	_	_
the	_	_
running	_	_
time	_	_
of	_	_
the	_	_
first	_	_
,	_	_
second	_	_
,	_	_
and	_	_
third	_	_
algorithms	_	_
do	_	_
not	_	_
change	_	_
,	_	_
since	_	_
they	_	_
can	capability	negation
not	_	_
take	_	_
advantage	_	_
of	_	_
this	_	_
property	_	_
.	_	_

#412
However	_	_
,	_	_
the	_	_
Bellman-Ford	_	_
algorithm	_	_
can	capability	_
take	_	_
advantage	_	_
of	_	_
this	_	_
property	_	_
and	_	_
each	_	_
thread	_	_
across	_	_
all	_	_
core	_	_
groups	_	_
is	_	_
responsible	_	_
for	_	_
relaxing	_	_
a	_	_
single	_	_
edge	_	_
.	_	_

#413
Now	_	_
a	_	_
portion	_	_
of	_	_
the	_	_
arrays	_	_
A	_	_
,	_	_
B	_	_
and	_	_
W	_	_
fit	_	_
in	_	_
each	_	_
core	_	_
group	_	_
's	_	_
local	_	_
memory	_	_
and	_	_
they	_	_
never	_	_
have	_	_
to	_	_
be	_	_
read	_	_
again	_	_
.	_	_

#414
Therefore	_	_
,	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
operations	_	_
reduces	_	_
to	_	_
M=O	_	_
(	_	_
n3/C	_	_
)	_	_
.	_	_

#415
And	_	_
the	_	_
run	_	_
time	_	_
under	_	_
the	_	_
TMM	_	_
model	_	_
reduces	_	_
to	_	_
O	_	_
(	_	_
n3L/	_	_
(	_	_
CTP	_	_
)	_	_
)	_	_
.	_	_

#416
Again	_	_
,	_	_
compare	_	_
Bellman-Ford	_	_
algorithm	_	_
with	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
binary	_	_
heaps	_	_
.	_	_

#417
When	_	_
m=O	_	_
(	_	_
n2/lgn	_	_
)	_	_
,	_	_
Johnson	_	_
's	_	_
algorithm	_	_
is	_	_
better	_	_
than	_	_
the	_	_
Bellman-Ford	_	_
algorithm	_	_
in	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#418
However	_	_
,	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
,	_	_
Johnson	_	_
's	_	_
has	_	_
run	_	_
time	_	_
of	_	_
O	_	_
(	_	_
Lmnlgn/	_	_
(	_	_
TP	_	_
)	_	_
)	_	_
=O	_	_
(	_	_
Ln3/	_	_
(	_	_
TP	_	_
)	_	_
)	_	_
,	_	_
while	_	_
Bellman-Ford	_	_
with	_	_
a	_	_
run	_	_
time	_	_
of	_	_
O	_	_
(	_	_
Ln3/	_	_
(	_	_
CTP	_	_
)	_	_
)	_	_
flips	_	_
to	_	_
be	_	_
the	_	_
better	_	_
one	_	_
.	_	_

#419
Empirical	_	_
investigation	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
conduct	_	_
experiments	_	_
to	_	_
understand	_	_
the	_	_
extent	_	_
of	_	_
the	_	_
applicability	_	_
of	_	_
our	_	_
model	_	_
in	_	_
explaining	_	_
the	_	_
performance	_	_
of	_	_
algorithms	_	_
on	_	_
a	_	_
real	_	_
machine	_	_
.	_	_

#420
This	_	_
evaluation	_	_
is	_	_
a	_	_
proof-of-concept	_	_
that	_	_
the	_	_
model	_	_
successfully	_	_
predicts	_	_
performance	_	_
on	_	_
one	_	_
example	_	_
of	_	_
a	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machine	_	_
.	_	_

#421
It	_	_
is	_	_
not	_	_
meant	_	_
to	_	_
be	_	_
an	_	_
exhaustive	_	_
empirical	_	_
study	_	_
of	_	_
the	_	_
model	_	_
's	_	_
applicability	_	_
for	_	_
all	_	_
instances	_	_
of	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#422
We	_	_
implemented	_	_
two	_	_
all-pairs	_	_
shortest	_	_
paths	_	_
algorithms	_	_
:	_	_
the	_	_
dynamic	_	_
programming	_	_
using	_	_
matrix	_	_
multiplication	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
,	_	_
on	_	_
an	_	_
NVIDIA	_	_
GPU	_	_
.	_	_

#423
In	_	_
these	_	_
experiments	_	_
,	_	_
we	_	_
investigate	_	_
the	_	_
following	_	_
aspects	_	_
of	_	_
the	_	_
TMM	_	_
model	_	_
:	_	_
•	_	_
Effect	_	_
of	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
:	_	_
the	_	_
fact	_	_
that	_	_
the	_	_
TMM	_	_
model	_	_
incorporates	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
processor	_	_
in	_	_
the	_	_
model	_	_
is	_	_
the	_	_
primary	_	_
differentiator	_	_
between	_	_
the	_	_
PRAM	_	_
and	_	_
TMM	_	_
models	_	_
.	_	_

#424
The	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
as	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
increases	_	_
the	_	_
performance	_	_
increases	_	_
,	_	_
up	_	_
to	_	_
a	_	_
certain	_	_
point	_	_
.	_	_

#425
After	_	_
this	_	_
point	_	_
,	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
does	_	_
not	_	_
matter	_	_
,	_	_
and	_	_
the	_	_
TMM	_	_
model	_	_
behaves	_	_
the	_	_
same	_	_
as	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#426
In	_	_
this	_	_
set	_	_
of	_	_
experiments	_	_
,	_	_
we	_	_
will	_	_
use	_	_
both	_	_
the	_	_
dynamic	_	_
programming	_	_
and	_	_
Johnson	_	_
's	_	_
algorithms	_	_
to	_	_
demonstrate	_	_
this	_	_
dependence	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#427
•	_	_
Effect	_	_
of	_	_
fast	_	_
local	_	_
memory	_	_
size	_	_
:	_	_
in	_	_
some	_	_
algorithms	_	_
,	_	_
including	_	_
the	_	_
dynamic	_	_
programming	_	_
via	_	_
matrix	_	_
multiplication	_	_
,	_	_
the	_	_
size	_	_
of	_	_
the	_	_
fast	_	_
memory	_	_
affects	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
algorithm	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#428
We	_	_
investigate	_	_
this	_	_
dependence	_	_
.	_	_

#429
•	_	_
Comparison	_	_
of	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
:	_	_
for	_	_
Johnson	_	_
's	_	_
algorithm	_	_
using	_	_
arrays	_	_
,	_	_
the	_	_
PRAM	_	_
performance	_	_
does	_	_
not	_	_
depend	_	_
on	_	_
the	_	_
graph	_	_
's	_	_
density	_	_
.	_	_

#430
However	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
performance	_	_
can	options	_
depend	_	_
on	_	_
the	_	_
graph	_	_
's	_	_
density	_	_
,	_	_
when	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
insufficient	_	_
for	_	_
the	_	_
performance	_	_
to	_	_
be	_	_
equivalent	_	_
to	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#431
Therefore	_	_
,	_	_
even	_	_
though	_	_
Johnson	_	_
's	_	_
algorithm	_	_
is	_	_
always	_	_
faster	_	_
than	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
according	_	_
to	_	_
the	_	_
PRAM	_	_
model	_	_
(	_	_
since	_	_
its	_	_
work	_	_
is	_	_
n3	_	_
while	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
has	_	_
work	_	_
n3lgn	_	_
)	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
when	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
small	_	_
,	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
may	capability-speculation	_
do	_	_
better	_	_
,	_	_
especially	_	_
for	_	_
dense	_	_
graphs	_	_
.	_	_

#432
We	_	_
demonstrate	_	_
through	_	_
experiments	_	_
that	_	_
,	_	_
this	_	_
is	_	_
a	_	_
true	_	_
indicator	_	_
of	_	_
performance	_	_
.	_	_

#433
Experimental	_	_
Setup	_	_
The	_	_
experiments	_	_
are	_	_
carried	_	_
out	_	_
on	_	_
an	_	_
NVIDIA	_	_
GTX	_	_
480	_	_
,	_	_
which	_	_
has	_	_
15	_	_
multiprocessors	_	_
,	_	_
each	_	_
with	_	_
32	_	_
cores	_	_
.	_	_

#434
As	_	_
a	_	_
typical	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machine	_	_
,	_	_
it	_	_
also	_	_
features	_	_
a	_	_
1.5	_	_
GB	_	_
global	_	_
memory	_	_
and	_	_
16	_	_
kB/48	_	_
kB	_	_
of	_	_
configurable	_	_
on-chip	_	_
shared	_	_
memory	_	_
per	_	_
multiprocessor	_	_
,	_	_
which	_	_
can	capability-feasibility	_
be	_	_
accessed	_	_
with	_	_
latency	_	_
significantly	_	_
lower	_	_
than	_	_
the	_	_
global	_	_
memory	_	_
.	_	_

#435
Runtimes	_	_
are	_	_
measured	_	_
across	_	_
various	_	_
configurations	_	_
of	_	_
each	_	_
problem	_	_
,	_	_
including	_	_
graph	_	_
size	_	_
,	_	_
thread	_	_
count	_	_
,	_	_
shared	_	_
memory	_	_
size	_	_
,	_	_
and	_	_
graph	_	_
density	_	_
.	_	_

#436
When	_	_
plotted	_	_
as	_	_
execution	_	_
time	_	_
,	_	_
the	_	_
performance	_	_
units	_	_
are	_	_
in	_	_
seconds	_	_
.	_	_

#437
In	_	_
many	_	_
cases	_	_
,	_	_
however	_	_
,	_	_
the	_	_
trends	_	_
we	_	_
wish	_	_
to	_	_
see	_	_
are	_	_
more	_	_
readily	_	_
apparent	_	_
when	_	_
performance	_	_
is	_	_
shown	_	_
in	_	_
terms	_	_
of	_	_
speedup	_	_
rather	_	_
than	_	_
execution	_	_
time	_	_
.	_	_

#438
This	_	_
poses	_	_
a	_	_
problem	_	_
,	_	_
however	_	_
,	_	_
as	_	_
it	_	_
is	_	_
arguably	_	_
meaningless	_	_
to	_	_
attempt	_	_
to	_	_
realistically	_	_
measure	_	_
the	_	_
single-core	_	_
execution	_	_
time	_	_
of	_	_
an	_	_
application	_	_
deployed	_	_
on	_	_
a	_	_
modern	_	_
GPU	_	_
.	_	_

#439
We	_	_
address	_	_
this	_	_
issue	_	_
using	_	_
the	_	_
following	_	_
technique	_	_
:	_	_
all	_	_
speedup	_	_
plots	_	_
compare	_	_
the	_	_
measured	_	_
,	_	_
empirical	_	_
execution	_	_
time	_	_
on	_	_
P	_	_
cores	_	_
to	_	_
the	_	_
theoretical	_	_
,	_	_
asymptotic	_	_
execution	_	_
time	_	_
on	_	_
1	_	_
core	_	_
using	_	_
the	_	_
PRAM	_	_
model	_	_
.	_	_

#440
As	_	_
a	_	_
result	_	_
,	_	_
the	_	_
speedup	_	_
axis	_	_
does	_	_
not	_	_
represent	_	_
a	_	_
quantitatively	_	_
meaningful	_	_
scale	_	_
,	_	_
and	_	_
the	_	_
scale	_	_
is	_	_
labeled	_	_
"	_	_
arbitrary	_	_
"	_	_
on	_	_
the	_	_
graphs	_	_
to	_	_
reflect	_	_
this	_	_
fact	_	_
;	_	_
however	_	_
,	_	_
the	_	_
shape	_	_
of	_	_
the	_	_
curves	_	_
are	_	_
representative	_	_
of	_	_
the	_	_
speedup	_	_
achievable	_	_
relative	_	_
to	_	_
a	_	_
fixed	_	_
serial	_	_
execution	_	_
time	_	_
.	_	_

#441
Effect	_	_
of	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
The	_	_
TMM	_	_
model	_	_
indicates	_	_
that	_	_
when	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
is	_	_
small	_	_
,	_	_
the	_	_
performance	_	_
of	_	_
algorithms	_	_
depends	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#442
With	_	_
sufficient	_	_
number	_	_
of	_	_
threads	_	_
,	_	_
the	_	_
performance	_	_
converges	_	_
to	_	_
the	_	_
PRAM	_	_
performance	_	_
and	_	_
only	_	_
depends	_	_
on	_	_
the	_	_
problem	_	_
size	_	_
and	_	_
the	_	_
number	_	_
of	_	_
processors	_	_
.	_	_

#443
We	_	_
verify	_	_
this	_	_
result	_	_
using	_	_
both	_	_
the	_	_
dynamic	_	_
programming	_	_
and	_	_
Johnson	_	_
's	_	_
algorithms	_	_
.	_	_

#444
For	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
,	_	_
we	_	_
generate	_	_
random	_	_
graphs	_	_
with	_	_
{	_	_
1k,2k,4k,8k,16k	_	_
}	_	_
vertices	_	_
.	_	_

#445
To	_	_
better	_	_
utilize	_	_
fast	_	_
local	_	_
memory	_	_
,	_	_
the	_	_
problem	_	_
is	_	_
decomposed	_	_
into	_	_
sub-blocks	_	_
,	_	_
and	_	_
we	_	_
must	deontic	_
also	_	_
pick	_	_
a	_	_
block	_	_
size	_	_
.	_	_

#446
Since	_	_
we	_	_
only	_	_
care	_	_
about	_	_
the	_	_
effect	_	_
of	_	_
threads	_	_
and	_	_
not	_	_
the	_	_
effect	_	_
of	_	_
shared	_	_
memory	_	_
(	_	_
to	_	_
be	_	_
considered	_	_
in	_	_
the	_	_
next	_	_
subsection	_	_
)	_	_
,	_	_
here	_	_
we	_	_
show	_	_
the	_	_
results	_	_
with	_	_
a	_	_
block	_	_
size	_	_
of	_	_
64	_	_
,	_	_
as	_	_
it	_	_
allows	_	_
us	_	_
to	_	_
generate	_	_
the	_	_
maximum	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#447
We	_	_
increase	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
until	_	_
we	_	_
reach	_	_
either	_	_
the	_	_
hardware	_	_
limit	_	_
or	_	_
the	_	_
limit	_	_
imposed	_	_
by	_	_
the	_	_
algorithm	_	_
.	_	_

#448
Fig	_	_
.	_	_
3	_	_
shows	_	_
the	_	_
speedup	_	_
while	_	_
varying	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
.	_	_

#449
We	_	_
see	_	_
that	_	_
the	_	_
speedup	_	_
increases	_	_
approximately	_	_
linearly	_	_
with	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
per	_	_
core	_	_
(	_	_
as	_	_
predicted	_	_
by	_	_
Eq	_	_
.	_	_
(	_	_
10	_	_
)	_	_
)	_	_
and	_	_
then	_	_
flattens	_	_
out	_	_
.	_	_

#450
This	_	_
indicates	_	_
that	_	_
for	_	_
this	_	_
experiment	_	_
,	_	_
16	_	_
is	_	_
an	_	_
estimated	_	_
threshold	_	_
of	_	_
threads/core	_	_
where	_	_
the	_	_
TMM	_	_
model	_	_
switches	_	_
to	_	_
the	_	_
"	_	_
PRAM	_	_
range	_	_
"	_	_
and	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
no	_	_
longer	_	_
matters	_	_
.	_	_

#451
Note	_	_
that	_	_
the	_	_
expression	_	_
for	_	_
this	_	_
threshold	_	_
does	_	_
not	_	_
depend	_	_
on	_	_
the	_	_
graph	_	_
size	_	_
,	_	_
as	_	_
it	_	_
is	_	_
equal	_	_
to	_	_
L/ZC	_	_
.	_	_

#452
Also	_	_
note	_	_
that	_	_
the	_	_
speedup	_	_
(	_	_
both	_	_
in	_	_
and	_	_
out	_	_
of	_	_
the	_	_
PRAM	_	_
range	_	_
)	_	_
is	_	_
not	_	_
impacted	_	_
by	_	_
the	_	_
size	_	_
of	_	_
the	_	_
graph	_	_
(	_	_
again	_	_
as	_	_
predicted	_	_
by	_	_
Eq	_	_
.	_	_
(	_	_
10	_	_
)	_	_
)	_	_
.	_	_

#453
We	_	_
see	_	_
a	_	_
similar	_	_
performance	_	_
dependence	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
in	_	_
Johnson	_	_
's	_	_
algorithm	_	_
.	_	_

#454
Here	_	_
we	_	_
ran	_	_
experiments	_	_
with	_	_
8k	_	_
vertices	_	_
and	_	_
varied	_	_
the	_	_
number	_	_
of	_	_
edges	_	_
(	_	_
ranging	_	_
between	_	_
32k	_	_
and	_	_
32	_	_
M	_	_
)	_	_
.	_	_

#455
The	_	_
speedup	_	_
graph	_	_
is	_	_
shown	_	_
in	_	_
Fig	_	_
.	_	_
4	_	_
.	_	_

#456
As	_	_
we	_	_
increase	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
,	_	_
the	_	_
speedup	_	_
increases	_	_
.	_	_

#457
We	_	_
see	_	_
two	_	_
other	_	_
interesting	_	_
things	_	_
,	_	_
however	_	_
.	_	_

#458
First	_	_
,	_	_
we	_	_
never	_	_
see	_	_
the	_	_
flattening	_	_
of	_	_
performance	_	_
with	_	_
increasing	_	_
thread	_	_
counts	_	_
that	_	_
is	_	_
seen	_	_
with	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
.	_	_

#459
Therefore	_	_
,	_	_
it	_	_
appears	_	_
that	_	_
Johnson	_	_
's	_	_
algorithm	_	_
requires	_	_
more	_	_
threads	_	_
to	_	_
reach	_	_
the	_	_
PRAM	_	_
range	_	_
where	_	_
the	_	_
performance	_	_
no	_	_
longer	_	_
depends	_	_
on	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
.	_	_

#460
This	_	_
is	_	_
also	_	_
predicted	_	_
by	_	_
our	_	_
model	_	_
as	_	_
the	_	_
number	_	_
of	_	_
threads/core	_	_
required	_	_
by	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
to	_	_
reach	_	_
PRAM	_	_
range	_	_
is	_	_
T≥L/ZC	_	_
while	_	_
the	_	_
corresponding	_	_
number	_	_
of	_	_
threads	_	_
required	_	_
by	_	_
Johnson	_	_
's	_	_
is	_	_
T≥L/C	_	_
,	_	_
clearly	_	_
a	_	_
larger	_	_
threshold	_	_
.	_	_

#461
Johnson	_	_
's	_	_
algorithm	_	_
is	_	_
not	_	_
taking	_	_
advantage	_	_
of	_	_
the	_	_
fast	_	_
local	_	_
memory	_	_
,	_	_
and	_	_
this	_	_
factor	_	_
influences	_	_
the	_	_
number	_	_
of	_	_
threads	_	_
required	_	_
to	_	_
hide	_	_
the	_	_
latency	_	_
to	_	_
global	_	_
memory	_	_
.	_	_

#462
Second	_	_
,	_	_
we	_	_
see	_	_
that	_	_
the	_	_
performance	_	_
depends	_	_
on	_	_
the	_	_
number	_	_
of	_	_
edges	_	_
.	_	_

#463
This	_	_
is	_	_
consistent	_	_
with	_	_
the	_	_
fact	_	_
that	_	_
we	_	_
are	_	_
in	_	_
the	_	_
TMM	_	_
range	_	_
where	_	_
the	_	_
runtime	_	_
is	_	_
(	_	_
mnL/TP	_	_
)	_	_
and	_	_
not	_	_
in	_	_
the	_	_
PRAM	_	_
range	_	_
where	_	_
the	_	_
runtime	_	_
only	_	_
depends	_	_
on	_	_
the	_	_
number	_	_
of	_	_
vertices	_	_
.	_	_

#464
The	_	_
dependence	_	_
on	_	_
graph	_	_
density	_	_
is	_	_
explored	_	_
further	_	_
in	_	_
Fig	_	_
.	_	_
5	_	_
.	_	_

#465
Here	_	_
,	_	_
the	_	_
runtime	_	_
is	_	_
plotted	_	_
vs	_	_
.	_	_

#466
number	_	_
of	_	_
graph	_	_
edges	_	_
for	_	_
varying	_	_
threads/core	_	_
.	_	_

#467
The	_	_
linear	_	_
relationship	_	_
predicted	_	_
by	_	_
the	_	_
last	_	_
term	_	_
of	_	_
Eq	_	_
.	_	_
(	_	_
21	_	_
)	_	_
(	_	_
for	_	_
dense	_	_
graphs	_	_
)	_	_
is	_	_
illustrated	_	_
clearly	_	_
in	_	_
the	_	_
figure	_	_
.	_	_

#468
Effect	_	_
of	_	_
fast	_	_
local	_	_
memory	_	_
size	_	_
In	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
,	_	_
access	_	_
to	_	_
local	_	_
memory	_	_
is	_	_
faster	_	_
than	_	_
access	_	_
to	_	_
slow	_	_
global	_	_
memory	_	_
.	_	_

#469
Among	_	_
our	_	_
shortest	_	_
paths	_	_
algorithms	_	_
,	_	_
only	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
makes	_	_
use	_	_
of	_	_
the	_	_
local	_	_
memory	_	_
and	_	_
the	_	_
running	_	_
time	_	_
depends	_	_
on	_	_
this	_	_
fast	_	_
memory	_	_
size	_	_
.	_	_

#470
In	_	_
this	_	_
experiment	_	_
we	_	_
verify	_	_
the	_	_
effect	_	_
of	_	_
this	_	_
fast	_	_
memory	_	_
size	_	_
on	_	_
algorithm	_	_
performance	_	_
.	_	_

#471
We	_	_
set	_	_
the	_	_
fast	_	_
memory	_	_
size	_	_
on	_	_
our	_	_
machine	_	_
and	_	_
measure	_	_
its	_	_
effect	_	_
.	_	_

#472
Fig	_	_
.	_	_
6	_	_
illustrates	_	_
how	_	_
this	_	_
change	_	_
has	_	_
an	_	_
impact	_	_
on	_	_
speedup	_	_
across	_	_
a	_	_
range	_	_
of	_	_
threads/core	_	_
.	_	_

#473
For	_	_
a	_	_
fixed	_	_
Z	_	_
(	_	_
fast	_	_
memory	_	_
size	_	_
)	_	_
,	_	_
the	_	_
maximum	_	_
sub-block	_	_
size	_	_
B	_	_
can	capability-feasibility	_
be	_	_
determined	_	_
.	_	_

#474
Then	_	_
,	_	_
varying	_	_
thread	_	_
counts	_	_
has	_	_
the	_	_
same	_	_
effect	_	_
as	_	_
previously	_	_
illustrated	_	_
in	_	_
Fig	_	_
.	_	_
3	_	_
,	_	_
increasing	_	_
threads/core	_	_
increases	_	_
performance	_	_
until	_	_
the	_	_
PRAM	_	_
range	_	_
is	_	_
reached	_	_
.	_	_

#475
But	_	_
as	_	_
we	_	_
can	feasibility-rhetorical	_
see	_	_
from	_	_
the	_	_
figure	_	_
,	_	_
different	_	_
block	_	_
sizes	_	_
have	_	_
different	_	_
performance	_	_
for	_	_
the	_	_
same	_	_
number	_	_
of	_	_
threads/core	_	_
.	_	_

#476
This	_	_
effect	_	_
is	_	_
predicted	_	_
by	_	_
Eq	_	_
.	_	_
(	_	_
10	_	_
)	_	_
.	_	_

#477
As	_	_
we	_	_
increase	_	_
the	_	_
size	_	_
of	_	_
local	_	_
memory	_	_
,	_	_
the	_	_
performance	_	_
improves	_	_
,	_	_
since	_	_
we	_	_
can	feasibility	_
use	_	_
bigger	_	_
blocks	_	_
.	_	_

#478
In	_	_
order	_	_
to	_	_
isolate	_	_
the	_	_
effect	_	_
of	_	_
block	_	_
size	_	_
from	_	_
the	_	_
effects	_	_
of	_	_
other	_	_
parameters	_	_
,	_	_
we	_	_
also	_	_
plot	_	_
this	_	_
data	_	_
in	_	_
a	_	_
pair	_	_
of	_	_
different	_	_
formats	_	_
in	_	_
Figs	_	_
.	_	_
7	_	_
and	_	_
8	_	_
,	_	_
in	_	_
both	_	_
cases	_	_
limiting	_	_
the	_	_
number	_	_
of	_	_
threads/core	_	_
to	_	_
below	_	_
the	_	_
PRAM	_	_
range	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
range	_	_
where	_	_
speedup	_	_
is	_	_
linear	_	_
in	_	_
threads/core	_	_
)	_	_
.	_	_

#479
The	_	_
first	_	_
curve	_	_
shows	_	_
the	_	_
difference	_	_
between	_	_
the	_	_
speedups	_	_
for	_	_
different	_	_
block	_	_
sizes	_	_
.	_	_

#480
As	_	_
the	_	_
curve	_	_
indicates	_	_
,	_	_
the	_	_
delta	_	_
speedup	_	_
increases	_	_
linearly	_	_
with	_	_
the	_	_
number	_	_
of	_	_
threads/core	_	_
,	_	_
consistent	_	_
with	_	_
the	_	_
model	_	_
prediction	_	_
of	_	_
(	_	_
B1-B2	_	_
)	_	_
T.	_	_
The	_	_
second	_	_
curve	_	_
shows	_	_
the	_	_
ratio	_	_
of	_	_
the	_	_
performance	_	_
of	_	_
block	_	_
size	_	_
64	_	_
to	_	_
block	_	_
size	_	_
32	_	_
,	_	_
indicating	_	_
a	_	_
flat	_	_
line	_	_
,	_	_
since	_	_
the	_	_
thread	_	_
term	_	_
cancels	_	_
out	_	_
.	_	_

#481
Comparison	_	_
between	_	_
the	_	_
dynamic	_	_
programming	_	_
and	_	_
Johnson	_	_
's	_	_
algorithms	_	_
It	_	_
is	_	_
interesting	_	_
to	_	_
compare	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
and	_	_
Johnson	_	_
's	_	_
algorithm	_	_
with	_	_
arrays	_	_
,	_	_
since	_	_
the	_	_
PRAM	_	_
and	_	_
the	_	_
TMM	_	_
model	_	_
differ	_	_
in	_	_
predicting	_	_
the	_	_
relative	_	_
performance	_	_
of	_	_
these	_	_
algorithms	_	_
.	_	_

#482
The	_	_
PRAM	_	_
model	_	_
predicts	_	_
that	_	_
Johnson	_	_
's	_	_
algorithm	_	_
should	inference	_
always	_	_
be	_	_
better	_	_
.	_	_

#483
However	_	_
,	_	_
from	_	_
Section	_	_
5.2	_	_
,	_	_
for	_	_
a	_	_
small	_	_
number	_	_
of	_	_
threads/core	_	_
working	_	_
on	_	_
a	_	_
dense	_	_
graph	_	_
,	_	_
the	_	_
TMM	_	_
model	_	_
predicts	_	_
that	_	_
dynamic	_	_
programming	_	_
may	capability-speculation	_
be	_	_
better	_	_
.	_	_

#484
For	_	_
the	_	_
graphs	_	_
with	_	_
8k	_	_
vertices	_	_
that	_	_
we	_	_
explored	_	_
earlier	_	_
,	_	_
lgn	_	_
<	_	_
Z	_	_
.	_	_

#485
Consequently	_	_
,	_	_
TMM	_	_
predicts	_	_
Johnson	_	_
's	_	_
algorithm	_	_
is	_	_
generally	_	_
faster	_	_
than	_	_
dynamic	_	_
programming	_	_
for	_	_
sparse	_	_
graphs	_	_
,	_	_
but	_	_
slower	_	_
for	_	_
relatively	_	_
dense	_	_
ones	_	_
.	_	_

#486
Fig	_	_
.	_	_
9	_	_
demonstrates	_	_
this	_	_
effect	_	_
concretely	_	_
.	_	_

#487
In	_	_
addition	_	_
,	_	_
for	_	_
the	_	_
dense	_	_
graph	_	_
,	_	_
the	_	_
figure	_	_
also	_	_
shows	_	_
the	_	_
intersection	_	_
between	_	_
the	_	_
runtime	_	_
curves	_	_
of	_	_
the	_	_
two	_	_
algorithms	_	_
.	_	_

#488
At	_	_
that	_	_
point	_	_
(	_	_
32	_	_
threads/core	_	_
)	_	_
,	_	_
dynamic	_	_
programming	_	_
has	_	_
already	_	_
been	_	_
in	_	_
the	_	_
PRAM	_	_
range	_	_
with	_	_
stable	_	_
performance	_	_
since	_	_
16	_	_
threads/core	_	_
,	_	_
while	_	_
Johnson	_	_
's	_	_
has	_	_
not	_	_
.	_	_

#489
Its	_	_
runtime	_	_
is	_	_
still	_	_
benefiting	_	_
by	_	_
increasing	_	_
the	_	_
threads/core	_	_
.	_	_

#490
As	_	_
a	_	_
result	_	_
,	_	_
we	_	_
predict	_	_
that	_	_
Johnson	_	_
's	_	_
runtime	_	_
will	_	_
flip	_	_
to	_	_
be	_	_
the	_	_
better	_	_
one	_	_
if	_	_
given	_	_
sufficient	_	_
threads	_	_
.	_	_

#491
The	_	_
peak	_	_
performance	_	_
of	_	_
Johnson	_	_
's	_	_
being	_	_
better	_	_
than	_	_
that	_	_
of	_	_
dynamic	_	_
programming	_	_
is	_	_
consistent	_	_
with	_	_
what	_	_
the	_	_
PRAM	_	_
model	_	_
predicts	_	_
.	_	_

#492
Conclusions	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
present	_	_
a	_	_
memory	_	_
access	_	_
model	_	_
,	_	_
called	_	_
the	_	_
Threaded	_	_
Many-core	_	_
Memory	_	_
(	_	_
TMM	_	_
)	_	_
model	_	_
,	_	_
that	_	_
is	_	_
well	_	_
suited	_	_
for	_	_
modern	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
systems	_	_
that	_	_
employ	_	_
many	_	_
threads	_	_
and	_	_
fast	_	_
context	_	_
switching	_	_
to	_	_
hide	_	_
memory	_	_
latency	_	_
.	_	_

#493
The	_	_
model	_	_
analyzes	_	_
the	_	_
significant	_	_
factors	_	_
that	_	_
affect	_	_
performance	_	_
on	_	_
many-core	_	_
machines	_	_
.	_	_

#494
In	_	_
particular	_	_
,	_	_
it	_	_
requires	_	_
the	_	_
work	_	_
and	_	_
depth	_	_
(	_	_
like	_	_
PRAM	_	_
algorithms	_	_
)	_	_
,	_	_
but	_	_
also	_	_
requires	_	_
the	_	_
analysis	_	_
of	_	_
the	_	_
number	_	_
of	_	_
memory	_	_
accesses	_	_
.	_	_

#495
Using	_	_
these	_	_
three	_	_
values	_	_
,	_	_
we	_	_
can	feasibility	_
properly	_	_
order	_	_
algorithms	_	_
from	_	_
slow	_	_
to	_	_
fast	_	_
for	_	_
many	_	_
different	_	_
settings	_	_
of	_	_
machine	_	_
parameters	_	_
on	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#496
We	_	_
analyzed	_	_
4	_	_
shortest	_	_
paths	_	_
algorithms	_	_
in	_	_
the	_	_
TMM	_	_
model	_	_
and	_	_
compared	_	_
the	_	_
analysis	_	_
with	_	_
the	_	_
PRAM	_	_
analysis	_	_
.	_	_

#497
We	_	_
find	_	_
that	_	_
algorithms	_	_
with	_	_
the	_	_
same	_	_
PRAM	_	_
performance	_	_
can	options	_
have	_	_
different	_	_
TMM	_	_
performance	_	_
under	_	_
certain	_	_
machine	_	_
parameter	_	_
settings	_	_
.	_	_

#498
In	_	_
addition	_	_
,	_	_
for	_	_
certain	_	_
problem	_	_
sizes	_	_
which	_	_
fit	_	_
in	_	_
local	_	_
memory	_	_
,	_	_
algorithms	_	_
which	_	_
are	_	_
faster	_	_
on	_	_
PRAM	_	_
may	capability-options	_
be	_	_
slower	_	_
under	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#499
Further	_	_
,	_	_
we	_	_
implemented	_	_
a	_	_
pair	_	_
of	_	_
the	_	_
algorithms	_	_
and	_	_
showed	_	_
empirical	_	_
performance	_	_
is	_	_
effectively	_	_
predicted	_	_
by	_	_
the	_	_
TMM	_	_
model	_	_
under	_	_
a	_	_
variety	_	_
of	_	_
circumstances	_	_
.	_	_

#500
Therefore	_	_
,	_	_
TMM	_	_
is	_	_
a	_	_
model	_	_
well-suited	_	_
to	_	_
compare	_	_
algorithms	_	_
and	_	_
decide	_	_
which	_	_
one	_	_
to	_	_
implement	_	_
under	_	_
particular	_	_
environments	_	_
.	_	_

#501
To	_	_
our	_	_
knowledge	_	_
,	_	_
this	_	_
is	_	_
the	_	_
first	_	_
attempt	_	_
to	_	_
formalize	_	_
the	_	_
analysis	_	_
of	_	_
algorithms	_	_
for	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
computers	_	_
using	_	_
a	_	_
formal	_	_
model	_	_
and	_	_
asymptotic	_	_
analysis	_	_
.	_	_

#502
There	_	_
are	_	_
many	_	_
directions	_	_
of	_	_
future	_	_
work	_	_
.	_	_

#503
One	_	_
obvious	_	_
direction	_	_
is	_	_
to	_	_
design	_	_
more	_	_
algorithms	_	_
under	_	_
the	_	_
TMM	_	_
model	_	_
.	_	_

#504
Ideally	_	_
,	_	_
this	_	_
model	_	_
can	capability	_
help	_	_
us	_	_
come	_	_
up	_	_
with	_	_
new	_	_
algorithms	_	_
for	_	_
highly-threaded	_	_
,	_	_
many-core	_	_
machines	_	_
.	_	_

#505
Empirical	_	_
validation	_	_
of	_	_
the	_	_
TMM	_	_
model	_	_
across	_	_
a	_	_
wider	_	_
number	_	_
of	_	_
physical	_	_
machines	_	_
and	_	_
manufacturers	_	_
is	_	_
also	_	_
worth	_	_
doing	_	_
.	_	_

#506
In	_	_
addition	_	_
,	_	_
our	_	_
current	_	_
model	_	_
only	_	_
incorporates	_	_
2	_	_
levels	_	_
of	_	_
memory	_	_
hierarchy	_	_
.	_	_

#507
While	_	_
in	_	_
this	_	_
paper	_	_
we	_	_
assume	_	_
that	_	_
it	_	_
is	_	_
global	_	_
memory	_	_
vs	_	_
.	_	_

#508
memory	_	_
local	_	_
to	_	_
core	_	_
groups	_	_
,	_	_
in	_	_
principle	_	_
,	_	_
it	_	_
can	options	_
be	_	_
any	_	_
two	_	_
levels	_	_
of	_	_
fast	_	_
and	_	_
slow	_	_
memory	_	_
.	_	_

#509
We	_	_
would	_	_
like	_	_
to	_	_
extend	_	_
it	_	_
to	_	_
multi-level	_	_
hierarchies	_	_
which	_	_
are	_	_
becoming	_	_
increasingly	_	_
common	_	_
.	_	_

#510
One	_	_
way	_	_
to	_	_
do	_	_
this	_	_
is	_	_
to	_	_
design	_	_
a	_	_
"	_	_
parameter-oblivious	_	_
"	_	_
model	_	_
where	_	_
algorithms	_	_
do	_	_
not	_	_
know	_	_
the	_	_
machine	_	_
parameters	_	_
.	_	_

#511
Other	_	_
than	_	_
the	_	_
dynamic	_	_
programming	_	_
algorithm	_	_
,	_	_
all	_	_
of	_	_
the	_	_
algorithms	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
are	_	_
,	_	_
in	_	_
fact	_	_
,	_	_
parameter-oblivious	_	_
.	_	_

#512
And	_	_
matrix	_	_
multiplication	_	_
in	_	_
the	_	_
dynamic	_	_
programming	_	_
can	feasibility	_
easily	_	_
be	_	_
made	_	_
parameter-oblivious	_	_
.	_	_

#513
In	_	_
this	_	_
case	_	_
,	_	_
the	_	_
algorithms	_	_
should	inference	_
perform	_	_
well	_	_
under	_	_
all	_	_
settings	_	_
of	_	_
parameters	_	_
,	_	_
allowing	_	_
us	_	_
to	_	_
apply	_	_
the	_	_
model	_	_
at	_	_
any	_	_
two	_	_
levels	_	_
and	_	_
get	_	_
the	_	_
same	_	_
results	_	_
.	_	_

#514
Acknowledgments	_	_
This	_	_
work	_	_
was	_	_
supported	_	_
by	_	_
NSF	_	_
grants	_	_
CNS-0905368	_	_
and	_	_
CNS-0931693	_	_
and	_	_
Exegy	_	_
,	_	_
Inc	_	_
.	_	_