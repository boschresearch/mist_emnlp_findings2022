#0
An	_	_
experimental	_	_
comparison	_	_
of	_	_
classification	_	_
algorithms	_	_
for	_	_
imbalanced	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
Abstract	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
set	_	_
out	_	_
to	_	_
compare	_	_
several	_	_
techniques	_	_
that	_	_
can	capability-feasibility	_
be	_	_
used	_	_
in	_	_
the	_	_
analysis	_	_
of	_	_
imbalanced	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
.	_	_

#1
In	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
,	_	_
imbalanced	_	_
data	_	_
sets	_	_
frequently	_	_
occur	_	_
as	_	_
the	_	_
number	_	_
of	_	_
defaulting	_	_
loans	_	_
in	_	_
a	_	_
portfolio	_	_
is	_	_
usually	_	_
much	_	_
lower	_	_
than	_	_
the	_	_
number	_	_
of	_	_
observations	_	_
that	_	_
do	_	_
not	_	_
default	_	_
.	_	_

#2
As	_	_
well	_	_
as	_	_
using	_	_
traditional	_	_
classification	_	_
techniques	_	_
such	_	_
as	_	_
logistic	_	_
regression	_	_
,	_	_
neural	_	_
networks	_	_
and	_	_
decision	_	_
trees	_	_
,	_	_
this	_	_
paper	_	_
will	_	_
also	_	_
explore	_	_
the	_	_
suitability	_	_
of	_	_
gradient	_	_
boosting	_	_
,	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
and	_	_
random	_	_
forests	_	_
for	_	_
loan	_	_
default	_	_
prediction	_	_
.	_	_

#3
Five	_	_
real-world	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
are	_	_
used	_	_
to	_	_
build	_	_
classifiers	_	_
and	_	_
test	_	_
their	_	_
performance	_	_
.	_	_

#4
In	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
progressively	_	_
increase	_	_
class	_	_
imbalance	_	_
in	_	_
each	_	_
of	_	_
these	_	_
data	_	_
sets	_	_
by	_	_
randomly	_	_
under-sampling	_	_
the	_	_
minority	_	_
class	_	_
of	_	_
defaulters	_	_
,	_	_
so	_	_
as	_	_
to	_	_
identify	_	_
to	_	_
what	_	_
extent	_	_
the	_	_
predictive	_	_
power	_	_
of	_	_
the	_	_
respective	_	_
techniques	_	_
is	_	_
adversely	_	_
affected	_	_
.	_	_

#5
The	_	_
performance	_	_
criterion	_	_
chosen	_	_
to	_	_
measure	_	_
this	_	_
effect	_	_
is	_	_
the	_	_
area	_	_
under	_	_
the	_	_
receiver	_	_
operating	_	_
characteristic	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
;	_	_
Friedman	_	_
's	_	_
statistic	_	_
and	_	_
Nemenyi	_	_
post	_	_
hoc	_	_
tests	_	_
are	_	_
used	_	_
to	_	_
test	_	_
for	_	_
significance	_	_
of	_	_
AUC	_	_
differences	_	_
between	_	_
techniques	_	_
.	_	_

#6
The	_	_
results	_	_
from	_	_
this	_	_
empirical	_	_
study	_	_
indicate	_	_
that	_	_
the	_	_
random	_	_
forest	_	_
and	_	_
gradient	_	_
boosting	_	_
classifiers	_	_
perform	_	_
very	_	_
well	_	_
in	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
and	_	_
are	_	_
able	_	_
to	_	_
cope	_	_
comparatively	_	_
well	_	_
with	_	_
pronounced	_	_
class	_	_
imbalances	_	_
in	_	_
these	_	_
data	_	_
sets	_	_
.	_	_

#7
We	_	_
also	_	_
found	_	_
that	_	_
,	_	_
when	_	_
faced	_	_
with	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
,	_	_
the	_	_
C4.5	_	_
decision	_	_
tree	_	_
algorithm	_	_
,	_	_
quadratic	_	_
discriminant	_	_
analysis	_	_
and	_	_
k-nearest	_	_
neighbours	_	_
perform	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
best	_	_
performing	_	_
classifiers	_	_
.	_	_

#8
Introduction	_	_
The	_	_
aim	_	_
of	_	_
credit	_	_
scoring	_	_
is	_	_
essentially	_	_
to	_	_
classify	_	_
loan	_	_
applicants	_	_
into	_	_
two	_	_
classes	_	_
,	_	_
i.e.	_	_
,	_	_
good	_	_
payers	_	_
(	_	_
i.e.	_	_
,	_	_
those	_	_
who	_	_
are	_	_
likely	_	_
to	_	_
keep	_	_
up	_	_
with	_	_
their	_	_
repayments	_	_
)	_	_
and	_	_
bad	_	_
payers	_	_
(	_	_
i.e.	_	_
,	_	_
those	_	_
who	_	_
are	_	_
likely	_	_
to	_	_
default	_	_
on	_	_
their	_	_
loans	_	_
)	_	_
.	_	_

#9
In	_	_
the	_	_
current	_	_
financial	_	_
climate	_	_
,	_	_
and	_	_
with	_	_
the	_	_
recent	_	_
introduction	_	_
of	_	_
the	_	_
Basel	_	_
II	_	_
Accord	_	_
,	_	_
financial	_	_
institutions	_	_
have	_	_
even	_	_
more	_	_
incentives	_	_
to	_	_
select	_	_
and	_	_
implement	_	_
the	_	_
most	_	_
appropriate	_	_
credit	_	_
scoring	_	_
techniques	_	_
for	_	_
their	_	_
credit	_	_
portfolios	_	_
.	_	_

#10
It	_	_
is	_	_
stated	_	_
in	_	_
Henley	_	_
and	_	_
Hand	_	_
(	_	_
1997	_	_
)	_	_
that	_	_
companies	_	_
could	feasibility-speculation	_
make	_	_
significant	_	_
future	_	_
savings	_	_
if	_	_
an	_	_
improvement	_	_
of	_	_
only	_	_
a	_	_
fraction	_	_
of	_	_
a	_	_
percent	_	_
could	feasibility-speculation	_
be	_	_
made	_	_
in	_	_
the	_	_
accuracy	_	_
of	_	_
the	_	_
credit	_	_
scoring	_	_
techniques	_	_
implemented	_	_
.	_	_

#11
However	_	_
,	_	_
in	_	_
the	_	_
research	_	_
literature	_	_
,	_	_
portfolios	_	_
that	_	_
can	feasibility	_
be	_	_
considered	_	_
as	_	_
very	_	_
low	_	_
risk	_	_
,	_	_
or	_	_
low	_	_
default	_	_
portfolios	_	_
(	_	_
LDPs	_	_
)	_	_
,	_	_
have	_	_
had	_	_
relatively	_	_
little	_	_
attention	_	_
paid	_	_
to	_	_
them	_	_
in	_	_
particular	_	_
with	_	_
regards	_	_
to	_	_
which	_	_
techniques	_	_
are	_	_
most	_	_
appropriate	_	_
for	_	_
scoring	_	_
them	_	_
.	_	_

#12
The	_	_
underlying	_	_
problem	_	_
with	_	_
LDPs	_	_
is	_	_
that	_	_
they	_	_
contain	_	_
a	_	_
much	_	_
smaller	_	_
number	_	_
of	_	_
observations	_	_
in	_	_
the	_	_
class	_	_
of	_	_
defaulters	_	_
than	_	_
in	_	_
that	_	_
of	_	_
the	_	_
good	_	_
payers	_	_
.	_	_

#13
A	_	_
large	_	_
class	_	_
imbalance	_	_
is	_	_
therefore	_	_
present	_	_
which	_	_
some	_	_
techniques	_	_
may	capability-options	negation
not	_	_
be	_	_
able	_	_
to	_	_
successfully	_	_
handle	_	_
.	_	_

#14
Typical	_	_
examples	_	_
of	_	_
low	_	_
default	_	_
portfolios	_	_
include	_	_
high-quality	_	_
corporate	_	_
borrowers	_	_
,	_	_
banks	_	_
,	_	_
sovereigns	_	_
and	_	_
some	_	_
categories	_	_
of	_	_
specialised	_	_
lending	_	_
(	_	_
Van	_	_
Der	_	_
Burgt	_	_
,	_	_
2007	_	_
)	_	_
but	_	_
in	_	_
some	_	_
countries	_	_
even	_	_
certain	_	_
retail	_	_
lending	_	_
portfolios	_	_
could	speculation	_
turn	_	_
out	_	_
to	_	_
have	_	_
very	_	_
low	_	_
numbers	_	_
of	_	_
defaults	_	_
compared	_	_
to	_	_
the	_	_
majority	_	_
class	_	_
.	_	_

#15
In	_	_
a	_	_
recent	_	_
FSA	_	_
publication	_	_
regarding	_	_
conservative	_	_
estimation	_	_
of	_	_
low	_	_
default	_	_
portfolios	_	_
,	_	_
regulatory	_	_
concerns	_	_
were	_	_
raised	_	_
about	_	_
whether	_	_
firms	_	_
can	capability	_
adequately	_	_
asses	_	_
the	_	_
risk	_	_
of	_	_
LDPs	_	_
(	_	_
Benjamin	_	_
,	_	_
Cathcart	_	_
,	_	_
&	_	_
Ryan	_	_
,	_	_
2006	_	_
)	_	_
.	_	_

#16
A	_	_
wide	_	_
range	_	_
of	_	_
classification	_	_
techniques	_	_
have	_	_
already	_	_
been	_	_
proposed	_	_
in	_	_
the	_	_
credit	_	_
scoring	_	_
literature	_	_
,	_	_
including	_	_
statistical	_	_
techniques	_	_
,	_	_
such	_	_
as	_	_
linear	_	_
discriminant	_	_
analysis	_	_
and	_	_
logistic	_	_
regression	_	_
,	_	_
and	_	_
non-parametric	_	_
models	_	_
,	_	_
such	_	_
as	_	_
k-nearest	_	_
neighbour	_	_
and	_	_
decision	_	_
trees	_	_
.	_	_

#17
But	_	_
it	_	_
is	_	_
currently	_	_
unclear	_	_
from	_	_
the	_	_
literature	_	_
which	_	_
technique	_	_
is	_	_
the	_	_
most	_	_
appropriate	_	_
for	_	_
improving	_	_
discrimination	_	_
for	_	_
LDPs	_	_
.	_	_

#18
Table	_	_
1	_	_
provides	_	_
a	_	_
selection	_	_
of	_	_
techniques	_	_
currently	_	_
applied	_	_
in	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
,	_	_
along	_	_
with	_	_
references	_	_
showing	_	_
some	_	_
of	_	_
their	_	_
reported	_	_
applications	_	_
in	_	_
the	_	_
literature	_	_
.	_	_

#19
Hence	_	_
,	_	_
the	_	_
aim	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
to	_	_
conduct	_	_
a	_	_
study	_	_
of	_	_
various	_	_
classification	_	_
techniques	_	_
based	_	_
on	_	_
five	_	_
real-life	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
.	_	_

#20
These	_	_
data	_	_
sets	_	_
will	_	_
then	_	_
have	_	_
the	_	_
size	_	_
of	_	_
their	_	_
minority	_	_
class	_	_
of	_	_
defaulters	_	_
further	_	_
reduced	_	_
by	_	_
decrements	_	_
of	_	_
5	_	_
%	_	_
(	_	_
from	_	_
an	_	_
original	_	_
70/30	_	_
good/bad	_	_
split	_	_
)	_	_
to	_	_
see	_	_
how	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
various	_	_
classification	_	_
techniques	_	_
is	_	_
affected	_	_
by	_	_
increasing	_	_
class	_	_
imbalance	_	_
.	_	_

#21
The	_	_
five	_	_
real-life	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
used	_	_
in	_	_
this	_	_
empirical	_	_
research	_	_
study	_	_
include	_	_
two	_	_
data	_	_
sets	_	_
from	_	_
Benelux	_	_
(	_	_
Belgium	_	_
,	_	_
Netherlands	_	_
and	_	_
Luxembourg	_	_
)	_	_
institutions	_	_
,	_	_
the	_	_
German	_	_
Credit	_	_
and	_	_
Australian	_	_
Credit	_	_
data	_	_
sets	_	_
which	_	_
are	_	_
publicly	_	_
available	_	_
at	_	_
the	_	_
UCI	_	_
repository	_	_
(	_	_
http	_	_
:	_	_
//kdd.ics.uci.edu/	_	_
)	_	_
,	_	_
and	_	_
the	_	_
fifth	_	_
data	_	_
set	_	_
is	_	_
a	_	_
behavioural	_	_
scoring	_	_
data	_	_
set	_	_
,	_	_
which	_	_
was	_	_
also	_	_
obtained	_	_
from	_	_
a	_	_
Benelux	_	_
institution	_	_
.	_	_

#22
The	_	_
techniques	_	_
that	_	_
will	_	_
be	_	_
applied	_	_
in	_	_
this	_	_
paper	_	_
are	_	_
logistic	_	_
regression	_	_
(	_	_
LOG	_	_
)	_	_
,	_	_
linear	_	_
and	_	_
quadratic	_	_
discriminant	_	_
analysis	_	_
(	_	_
LDA	_	_
,	_	_
QDA	_	_
)	_	_
,	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
(	_	_
LS-SVM	_	_
)	_	_
,	_	_
decision	_	_
trees	_	_
(	_	_
C4.5	_	_
)	_	_
,	_	_
neural	_	_
networks	_	_
(	_	_
NN	_	_
)	_	_
,	_	_
nearest-neighbour	_	_
classifiers	_	_
(	_	_
k-NN10	_	_
,	_	_
k-NN100	_	_
)	_	_
,	_	_
a	_	_
gradient	_	_
boosting	_	_
algorithm	_	_
and	_	_
random	_	_
forests	_	_
.	_	_

#23
We	_	_
are	_	_
especially	_	_
interested	_	_
in	_	_
the	_	_
power	_	_
and	_	_
usefulness	_	_
of	_	_
the	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forest	_	_
classifiers	_	_
which	_	_
have	_	_
yet	_	_
to	_	_
be	_	_
thoroughly	_	_
investigated	_	_
in	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
.	_	_

#24
All	_	_
techniques	_	_
will	_	_
be	_	_
evaluated	_	_
in	_	_
terms	_	_
of	_	_
their	_	_
area	_	_
under	_	_
the	_	_
receiver	_	_
operating	_	_
characteristic	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
.	_	_

#25
This	_	_
is	_	_
a	_	_
measure	_	_
of	_	_
the	_	_
discrimination	_	_
power	_	_
of	_	_
a	_	_
classifier	_	_
without	_	_
regard	_	_
to	_	_
class	_	_
distribution	_	_
or	_	_
misclassification	_	_
cost	_	_
(	_	_
Baesens	_	_
et	_	_
al.	_	_
,	_	_
2003	_	_
)	_	_
.	_	_

#26
To	_	_
make	_	_
statistical	_	_
inferences	_	_
from	_	_
the	_	_
observed	_	_
difference	_	_
in	_	_
AUC	_	_
,	_	_
we	_	_
followed	_	_
the	_	_
recommendations	_	_
given	_	_
in	_	_
a	_	_
recent	_	_
article	_	_
(	_	_
Demšar	_	_
,	_	_
2006	_	_
)	_	_
that	_	_
looked	_	_
at	_	_
the	_	_
problem	_	_
of	_	_
benchmarking	_	_
classifiers	_	_
on	_	_
multiple	_	_
data	_	_
sets	_	_
.	_	_

#27
The	_	_
recommendations	_	_
given	_	_
were	_	_
for	_	_
a	_	_
set	_	_
of	_	_
simple	_	_
robust	_	_
non-parametric	_	_
tests	_	_
for	_	_
the	_	_
statistical	_	_
comparison	_	_
of	_	_
the	_	_
classifiers	_	_
(	_	_
Demšar	_	_
,	_	_
2006	_	_
)	_	_
.	_	_

#28
The	_	_
AUC	_	_
measures	_	_
will	_	_
therefore	_	_
be	_	_
compared	_	_
using	_	_
Friedman	_	_
's	_	_
average	_	_
rank	_	_
test	_	_
,	_	_
and	_	_
Nemenyi	_	_
's	_	_
post	_	_
hoc	_	_
test	_	_
will	_	_
be	_	_
employed	_	_
to	_	_
test	_	_
the	_	_
significance	_	_
of	_	_
the	_	_
differences	_	_
in	_	_
rank	_	_
between	_	_
individual	_	_
classifiers	_	_
.	_	_

#29
Finally	_	_
,	_	_
a	_	_
variant	_	_
of	_	_
Demšar	_	_
's	_	_
significance	_	_
diagrams	_	_
will	_	_
be	_	_
plotted	_	_
to	_	_
visualise	_	_
their	_	_
results	_	_
.	_	_

#30
The	_	_
organisation	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
as	_	_
follows	_	_
.	_	_

#31
Section	_	_
2	_	_
will	_	_
begin	_	_
by	_	_
providing	_	_
a	_	_
literature	_	_
review	_	_
of	_	_
the	_	_
work	_	_
that	_	_
has	_	_
been	_	_
conducted	_	_
on	_	_
the	_	_
topic	_	_
of	_	_
classification	_	_
for	_	_
imbalanced	_	_
data	_	_
sets	_	_
.	_	_

#32
A	_	_
brief	_	_
explanation	_	_
will	_	_
then	_	_
be	_	_
given	_	_
for	_	_
the	_	_
ten	_	_
classification	_	_
techniques	_	_
to	_	_
be	_	_
used	_	_
in	_	_
the	_	_
analysis	_	_
of	_	_
the	_	_
data	_	_
sets	_	_
.	_	_

#33
Secondly	_	_
,	_	_
the	_	_
empirical	_	_
set	_	_
up	_	_
and	_	_
criteria	_	_
used	_	_
for	_	_
comparing	_	_
the	_	_
classification	_	_
performance	_	_
will	_	_
be	_	_
described	_	_
.	_	_

#34
Thirdly	_	_
,	_	_
the	_	_
results	_	_
of	_	_
our	_	_
experiments	_	_
are	_	_
presented	_	_
and	_	_
discussed	_	_
.	_	_

#35
Finally	_	_
,	_	_
conclusions	_	_
will	_	_
be	_	_
drawn	_	_
from	_	_
the	_	_
study	_	_
and	_	_
recommendations	_	_
for	_	_
further	_	_
research	_	_
work	_	_
will	_	_
be	_	_
outlined	_	_
.	_	_

#36
Literature	_	_
review	_	_
A	_	_
wide	_	_
range	_	_
of	_	_
different	_	_
classification	_	_
techniques	_	_
for	_	_
scoring	_	_
credit	_	_
data	_	_
sets	_	_
has	_	_
been	_	_
proposed	_	_
in	_	_
the	_	_
literature	_	_
,	_	_
a	_	_
non-exhaustive	_	_
list	_	_
of	_	_
which	_	_
was	_	_
provided	_	_
earlier	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#37
In	_	_
addition	_	_
,	_	_
some	_	_
benchmarking	_	_
studies	_	_
have	_	_
been	_	_
undertaken	_	_
to	_	_
empirically	_	_
compare	_	_
the	_	_
performance	_	_
of	_	_
these	_	_
various	_	_
techniques	_	_
(	_	_
e.g.	_	_
,	_	_
Baesens	_	_
et	_	_
al.	_	_
,	_	_
2003	_	_
)	_	_
,	_	_
but	_	_
they	_	_
did	_	_
not	_	_
focus	_	_
specifically	_	_
on	_	_
how	_	_
these	_	_
techniques	_	_
compare	_	_
on	_	_
heavily	_	_
imbalanced	_	_
samples	_	_
,	_	_
or	_	_
to	_	_
what	_	_
extent	_	_
any	_	_
such	_	_
comparison	_	_
is	_	_
affected	_	_
by	_	_
the	_	_
issue	_	_
of	_	_
class	_	_
imbalance	_	_
.	_	_

#38
For	_	_
example	_	_
,	_	_
in	_	_
Baesens	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2003	_	_
)	_	_
seventeen	_	_
techniques	_	_
including	_	_
both	_	_
well-known	_	_
techniques	_	_
such	_	_
as	_	_
logistic	_	_
regression	_	_
and	_	_
discriminant	_	_
analysis	_	_
and	_	_
more	_	_
advanced	_	_
techniques	_	_
such	_	_
as	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
were	_	_
compared	_	_
on	_	_
eight	_	_
real-life	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
.	_	_

#39
Although	_	_
more	_	_
complicated	_	_
techniques	_	_
such	_	_
as	_	_
radial	_	_
basis	_	_
function	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
(	_	_
RBF	_	_
LS-SVM	_	_
)	_	_
and	_	_
neural	_	_
networks	_	_
(	_	_
NN	_	_
)	_	_
yielded	_	_
good	_	_
performances	_	_
in	_	_
terms	_	_
of	_	_
AUC	_	_
,	_	_
simpler	_	_
linear	_	_
classifiers	_	_
such	_	_
as	_	_
linear	_	_
discriminant	_	_
analysis	_	_
(	_	_
LDA	_	_
)	_	_
and	_	_
logistic	_	_
regression	_	_
(	_	_
LOG	_	_
)	_	_
also	_	_
gave	_	_
very	_	_
good	_	_
performances	_	_
.	_	_

#40
However	_	_
,	_	_
there	_	_
are	_	_
often	_	_
conflicting	_	_
opinions	_	_
when	_	_
comparing	_	_
the	_	_
conclusions	_	_
of	_	_
studies	_	_
promoting	_	_
differing	_	_
techniques	_	_
.	_	_

#41
For	_	_
example	_	_
,	_	_
in	_	_
Yobas	_	_
,	_	_
Crook	_	_
,	_	_
and	_	_
Ross	_	_
(	_	_
2000	_	_
)	_	_
,	_	_
the	_	_
authors	_	_
found	_	_
that	_	_
linear	_	_
discriminant	_	_
analysis	_	_
(	_	_
LDA	_	_
)	_	_
outperformed	_	_
neural	_	_
networks	_	_
in	_	_
the	_	_
prediction	_	_
of	_	_
loan	_	_
default	_	_
,	_	_
whereas	_	_
in	_	_
Desai	_	_
,	_	_
Crook	_	_
,	_	_
and	_	_
Overstreet	_	_
(	_	_
1996	_	_
)	_	_
,	_	_
neural	_	_
networks	_	_
were	_	_
reported	_	_
to	_	_
actually	_	_
perform	_	_
significantly	_	_
better	_	_
than	_	_
LDA	_	_
.	_	_

#42
Furthermore	_	_
,	_	_
many	_	_
empirical	_	_
studies	_	_
only	_	_
evaluate	_	_
a	_	_
small	_	_
number	_	_
of	_	_
classification	_	_
techniques	_	_
on	_	_
a	_	_
single	_	_
credit	_	_
scoring	_	_
data	_	_
set	_	_
.	_	_

#43
The	_	_
data	_	_
sets	_	_
used	_	_
in	_	_
these	_	_
empirical	_	_
studies	_	_
are	_	_
also	_	_
often	_	_
far	_	_
smaller	_	_
and	_	_
less	_	_
imbalanced	_	_
than	_	_
those	_	_
data	_	_
sets	_	_
used	_	_
in	_	_
practice	_	_
.	_	_

#44
Hence	_	_
,	_	_
the	_	_
issue	_	_
of	_	_
which	_	_
classification	_	_
technique	_	_
to	_	_
use	_	_
for	_	_
credit	_	_
scoring	_	_
,	_	_
particularly	_	_
with	_	_
a	_	_
small	_	_
number	_	_
of	_	_
bad	_	_
observations	_	_
,	_	_
remains	_	_
a	_	_
challenging	_	_
problem	_	_
(	_	_
Baesens	_	_
et	_	_
al.	_	_
,	_	_
2003	_	_
)	_	_
.	_	_

#45
The	_	_
topic	_	_
of	_	_
which	_	_
good/bad	_	_
distribution	_	_
is	_	_
the	_	_
most	_	_
appropriate	_	_
in	_	_
classifying	_	_
a	_	_
data	_	_
set	_	_
has	_	_
been	_	_
discussed	_	_
in	_	_
some	_	_
detail	_	_
in	_	_
the	_	_
machine	_	_
learning	_	_
and	_	_
data	_	_
mining	_	_
literature	_	_
.	_	_

#46
In	_	_
Weiss	_	_
and	_	_
Provost	_	_
(	_	_
2003	_	_
)	_	_
it	_	_
was	_	_
found	_	_
that	_	_
the	_	_
naturally	_	_
occurring	_	_
class	_	_
distributions	_	_
in	_	_
the	_	_
25	_	_
data	_	_
sets	_	_
looked	_	_
at	_	_
,	_	_
often	_	_
did	_	_
not	_	_
produce	_	_
the	_	_
best-performing	_	_
classifiers	_	_
.	_	_

#47
More	_	_
specifically	_	_
,	_	_
based	_	_
on	_	_
the	_	_
AUC	_	_
measure	_	_
(	_	_
which	_	_
was	_	_
preferred	_	_
over	_	_
the	_	_
use	_	_
of	_	_
the	_	_
error	_	_
rate	_	_
)	_	_
,	_	_
it	_	_
was	_	_
shown	_	_
that	_	_
the	_	_
optimal	_	_
class	_	_
distribution	_	_
should	deontic	_
contain	_	_
between	_	_
50	_	_
%	_	_
and	_	_
90	_	_
%	_	_
minority	_	_
class	_	_
examples	_	_
within	_	_
the	_	_
training	_	_
set	_	_
.	_	_

#48
Alternatively	_	_
,	_	_
a	_	_
progressive	_	_
adaptive	_	_
sampling	_	_
strategy	_	_
for	_	_
selecting	_	_
the	_	_
optimal	_	_
class	_	_
distribution	_	_
is	_	_
proposed	_	_
in	_	_
Provost	_	_
,	_	_
Jensen	_	_
,	_	_
and	_	_
Oates	_	_
(	_	_
1999	_	_
)	_	_
.	_	_

#49
Whilst	_	_
this	_	_
method	_	_
of	_	_
class	_	_
adjustment	_	_
can	capability-options	_
be	_	_
very	_	_
effective	_	_
for	_	_
large	_	_
data	_	_
sets	_	_
,	_	_
with	_	_
adequate	_	_
observations	_	_
in	_	_
the	_	_
minority	_	_
class	_	_
of	_	_
defaulters	_	_
,	_	_
in	_	_
some	_	_
low	_	_
default	_	_
portfolios	_	_
there	_	_
are	_	_
only	_	_
a	_	_
very	_	_
small	_	_
number	_	_
of	_	_
loan	_	_
defaults	_	_
to	_	_
begin	_	_
with	_	_
.	_	_

#50
Various	_	_
kinds	_	_
of	_	_
techniques	_	_
have	_	_
been	_	_
compared	_	_
in	_	_
the	_	_
literature	_	_
to	_	_
try	_	_
and	_	_
ascertain	_	_
the	_	_
most	_	_
effective	_	_
way	_	_
of	_	_
overcoming	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
.	_	_

#51
Chawla	_	_
,	_	_
Bowyer	_	_
,	_	_
Hall	_	_
,	_	_
and	_	_
Kegelmeyer	_	_
(	_	_
2002	_	_
)	_	_
proposed	_	_
a	_	_
synthetic	_	_
minority	_	_
over-sampling	_	_
technique	_	_
(	_	_
SMOTE	_	_
)	_	_
which	_	_
was	_	_
applied	_	_
to	_	_
example	_	_
data	_	_
sets	_	_
in	_	_
fraud	_	_
,	_	_
telecommunications	_	_
management	_	_
,	_	_
and	_	_
detection	_	_
of	_	_
oil	_	_
spills	_	_
in	_	_
satellite	_	_
images	_	_
.	_	_

#52
In	_	_
Japkowicz	_	_
(	_	_
2000	_	_
)	_	_
,	_	_
over-sampling	_	_
and	_	_
downsizing	_	_
were	_	_
compared	_	_
to	_	_
the	_	_
author	_	_
's	_	_
own	_	_
method	_	_
of	_	_
"	_	_
learning	_	_
by	_	_
recognition	_	_
"	_	_
in	_	_
order	_	_
to	_	_
determine	_	_
the	_	_
most	_	_
effective	_	_
technique	_	_
.	_	_

#53
The	_	_
findings	_	_
,	_	_
however	_	_
,	_	_
were	_	_
inconclusive	_	_
but	_	_
demonstrated	_	_
that	_	_
both	_	_
over-sampling	_	_
the	_	_
minority	_	_
class	_	_
and	_	_
downsizing	_	_
the	_	_
majority	_	_
class	_	_
can	capability-options	_
be	_	_
very	_	_
effective	_	_
.	_	_

#54
Subsequently	_	_
,	_	_
Batista	_	_
(	_	_
2004	_	_
)	_	_
identified	_	_
ten	_	_
alternative	_	_
techniques	_	_
in	_	_
dealing	_	_
with	_	_
class	_	_
imbalances	_	_
and	_	_
trialed	_	_
them	_	_
on	_	_
thirteen	_	_
data	_	_
sets	_	_
.	_	_

#55
The	_	_
techniques	_	_
chosen	_	_
included	_	_
a	_	_
variety	_	_
of	_	_
under-sampling	_	_
and	_	_
over-sampling	_	_
methods	_	_
.	_	_

#56
Findings	_	_
suggested	_	_
that	_	_
generally	_	_
over-sampling	_	_
methods	_	_
provide	_	_
more	_	_
accurate	_	_
results	_	_
than	_	_
under-sampling	_	_
methods	_	_
.	_	_

#57
Also	_	_
,	_	_
a	_	_
combination	_	_
of	_	_
either	_	_
SMOTE	_	_
(	_	_
Chawla	_	_
et	_	_
al.	_	_
,	_	_
2002	_	_
)	_	_
and	_	_
Tomek	_	_
links	_	_
or	_	_
SMOTE	_	_
and	_	_
ENN	_	_
(	_	_
a	_	_
nearest-neighbour	_	_
cleaning	_	_
rule	_	_
)	_	_
,	_	_
were	_	_
proposed	_	_
.	_	_

#58
Overview	_	_
of	_	_
classification	_	_
techniques	_	_
This	_	_
study	_	_
aims	_	_
to	_	_
compare	_	_
the	_	_
performance	_	_
of	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
classification	_	_
techniques	_	_
within	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
,	_	_
thereby	_	_
assessing	_	_
to	_	_
what	_	_
extent	_	_
they	_	_
are	_	_
affected	_	_
by	_	_
increasing	_	_
class	_	_
imbalance	_	_
.	_	_

#59
For	_	_
the	_	_
purpose	_	_
of	_	_
this	_	_
study	_	_
,	_	_
ten	_	_
classifiers	_	_
have	_	_
been	_	_
selected	_	_
which	_	_
provide	_	_
a	_	_
balance	_	_
between	_	_
well-established	_	_
credit	_	_
scoring	_	_
techniques	_	_
such	_	_
as	_	_
logistic	_	_
regression	_	_
,	_	_
decision	_	_
trees	_	_
and	_	_
neural	_	_
networks	_	_
,	_	_
and	_	_
newly	_	_
developed	_	_
machine	_	_
learning	_	_
techniques	_	_
such	_	_
as	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
,	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forests	_	_
.	_	_

#60
A	_	_
brief	_	_
explanation	_	_
of	_	_
each	_	_
of	_	_
the	_	_
techniques	_	_
applied	_	_
in	_	_
this	_	_
paper	_	_
is	_	_
presented	_	_
below	_	_
.	_	_

#61
Logistic	_	_
regression	_	_
For	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
will	_	_
be	_	_
focusing	_	_
on	_	_
the	_	_
binary	_	_
response	_	_
of	_	_
whether	_	_
a	_	_
creditor	_	_
turns	_	_
out	_	_
to	_	_
be	_	_
a	_	_
good	_	_
or	_	_
bad	_	_
payer	_	_
(	_	_
i.e.	_	_
,	_	_
non-defaulter	_	_
vs	_	_
.	_	_

#62
defaulter	_	_
)	_	_
.	_	_

#63
For	_	_
this	_	_
binary	_	_
response	_	_
model	_	_
,	_	_
the	_	_
response	_	_
variable	_	_
,	_	_
y	_	_
,	_	_
can	options	_
take	_	_
on	_	_
one	_	_
of	_	_
two	_	_
possible	_	_
values	_	_
;	_	_
i.e.	_	_
,	_	_
y=0	_	_
if	_	_
the	_	_
customer	_	_
is	_	_
a	_	_
bad	_	_
payer	_	_
,	_	_
y=1	_	_
if	_	_
he/she	_	_
is	_	_
a	_	_
good	_	_
payer	_	_
.	_	_

#64
Let	_	_
us	_	_
assume	_	_
x	_	_
is	_	_
a	_	_
column	_	_
vector	_	_
of	_	_
M	_	_
explanatory	_	_
variables	_	_
and	_	_
π=Pr	_	_
(	_	_
y=1|x	_	_
)	_	_
is	_	_
the	_	_
response	_	_
probability	_	_
to	_	_
be	_	_
modelled	_	_
.	_	_

#65
The	_	_
number	_	_
of	_	_
observations	_	_
is	_	_
denoted	_	_
by	_	_
N.	_	_
The	_	_
logistic	_	_
regression	_	_
model	_	_
then	_	_
takes	_	_
the	_	_
form	_	_
:	_	_
(	_	_
1	_	_
)	_	_
logit	_	_
(	_	_
π	_	_
)	_	_
≡logπ1-π=α+βTx	_	_
,	_	_
where	_	_
α	_	_
is	_	_
the	_	_
intercept	_	_
parameter	_	_
and	_	_
βT	_	_
contains	_	_
the	_	_
variable	_	_
coefficients	_	_
(	_	_
Hosmer	_	_
&	_	_
Stanley	_	_
,	_	_
2000	_	_
)	_	_
.	_	_

#66
Linear	_	_
and	_	_
quadratic	_	_
discriminant	_	_
analysis	_	_
Discriminant	_	_
analysis	_	_
assigns	_	_
an	_	_
observation	_	_
to	_	_
the	_	_
response	_	_
,	_	_
y	_	_
(	_	_
y∈	_	_
{	_	_
0,1	_	_
}	_	_
)	_	_
,	_	_
with	_	_
the	_	_
largest	_	_
posterior	_	_
probability	_	_
;	_	_
i.e.	_	_
,	_	_
classify	_	_
into	_	_
class	_	_
0	_	_
if	_	_
p	_	_
(	_	_
0|x	_	_
)	_	_
>	_	_
p	_	_
(	_	_
1|x	_	_
)	_	_
,	_	_
or	_	_
class	_	_
1	_	_
if	_	_
the	_	_
reverse	_	_
is	_	_
true	_	_
.	_	_

#67
According	_	_
to	_	_
Bayes	_	_
'	_	_
theorem	_	_
,	_	_
these	_	_
posterior	_	_
probabilities	_	_
are	_	_
given	_	_
by	_	_
(	_	_
2	_	_
)	_	_
p	_	_
(	_	_
y|x	_	_
)	_	_
=p	_	_
(	_	_
x|y	_	_
)	_	_
p	_	_
(	_	_
y	_	_
)	_	_
p	_	_
(	_	_
x	_	_
)	_	_
.Assuming	_	_
now	_	_
that	_	_
the	_	_
class-conditional	_	_
distributions	_	_
p	_	_
(	_	_
x|y=0	_	_
)	_	_
,	_	_
p	_	_
(	_	_
x|y=1	_	_
)	_	_
are	_	_
multivariate	_	_
normal	_	_
distributions	_	_
with	_	_
mean	_	_
vector	_	_
μ0	_	_
,	_	_
μ1	_	_
,	_	_
and	_	_
covariance	_	_
matrix	_	_
Σ0	_	_
,	_	_
Σ1	_	_
,	_	_
respectively	_	_
,	_	_
the	_	_
classification	_	_
rule	_	_
becomes	_	_
:	_	_
classify	_	_
as	_	_
y=0	_	_
if	_	_
the	_	_
following	_	_
is	_	_
satisfied	_	_
:	_	_
(	_	_
3	_	_
)	_	_
x-μ0T∑0-1x-μ0-x-μ1T∑1-1x-μ1	_	_
<	_	_
2logP	_	_
(	_	_
y=0	_	_
)	_	_
-log	_	_
(	_	_
P	_	_
(	_	_
y=1	_	_
)	_	_
)	_	_
+log|Σ1|-log|Σ0|Linear	_	_
discriminant	_	_
analysis	_	_
is	_	_
then	_	_
obtained	_	_
if	_	_
the	_	_
simplifying	_	_
assumption	_	_
is	_	_
made	_	_
that	_	_
both	_	_
covariance	_	_
matrices	_	_
are	_	_
equal	_	_
,	_	_
i.e.	_	_
,	_	_
Σ0=Σ1=Σ	_	_
,	_	_
which	_	_
has	_	_
the	_	_
effect	_	_
of	_	_
cancelling	_	_
out	_	_
the	_	_
quadratic	_	_
terms	_	_
in	_	_
the	_	_
expression	_	_
above	_	_
.	_	_

#68
Neural	_	_
networks	_	_
(	_	_
Multi-layer	_	_
perceptron	_	_
)	_	_
Neural	_	_
networks	_	_
(	_	_
NN	_	_
)	_	_
are	_	_
mathematical	_	_
representations	_	_
modelled	_	_
on	_	_
the	_	_
functionality	_	_
of	_	_
the	_	_
human	_	_
brain	_	_
(	_	_
Bishop	_	_
,	_	_
1995	_	_
)	_	_
.	_	_

#69
The	_	_
added	_	_
benefit	_	_
of	_	_
a	_	_
NN	_	_
is	_	_
its	_	_
flexibility	_	_
in	_	_
modelling	_	_
virtually	_	_
any	_	_
non-linear	_	_
association	_	_
between	_	_
input	_	_
variables	_	_
and	_	_
target	_	_
variable	_	_
.	_	_

#70
Although	_	_
various	_	_
architectures	_	_
have	_	_
been	_	_
proposed	_	_
,	_	_
our	_	_
study	_	_
focuses	_	_
on	_	_
probably	_	_
the	_	_
most	_	_
widely	_	_
used	_	_
type	_	_
of	_	_
NN	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
multilayer	_	_
perceptron	_	_
(	_	_
MLP	_	_
)	_	_
.	_	_

#71
A	_	_
MLP	_	_
is	_	_
typically	_	_
composed	_	_
of	_	_
an	_	_
input	_	_
layer	_	_
(	_	_
consisting	_	_
of	_	_
neurons	_	_
for	_	_
all	_	_
input	_	_
variables	_	_
)	_	_
,	_	_
a	_	_
hidden	_	_
layer	_	_
(	_	_
consisting	_	_
of	_	_
any	_	_
number	_	_
of	_	_
hidden	_	_
neurons	_	_
)	_	_
,	_	_
and	_	_
an	_	_
output	_	_
layer	_	_
(	_	_
in	_	_
our	_	_
case	_	_
,	_	_
one	_	_
neuron	_	_
)	_	_
.	_	_

#72
Each	_	_
neuron	_	_
processes	_	_
its	_	_
inputs	_	_
and	_	_
transmits	_	_
its	_	_
output	_	_
value	_	_
to	_	_
the	_	_
neurons	_	_
in	_	_
the	_	_
subsequent	_	_
layer	_	_
.	_	_

#73
Each	_	_
such	_	_
connection	_	_
between	_	_
neurons	_	_
is	_	_
assigned	_	_
a	_	_
weight	_	_
during	_	_
training	_	_
.	_	_

#74
The	_	_
output	_	_
of	_	_
hidden	_	_
neuron	_	_
i	_	_
is	_	_
computed	_	_
by	_	_
applying	_	_
an	_	_
activation	_	_
function	_	_
f	_	_
(	_	_
1	_	_
)	_	_
(	_	_
for	_	_
example	_	_
the	_	_
logistic	_	_
function	_	_
)	_	_
to	_	_
the	_	_
weighted	_	_
inputs	_	_
and	_	_
its	_	_
bias	_	_
term	_	_
bi	_	_
(	_	_
1	_	_
)	_	_
:	_	_
(	_	_
4	_	_
)	_	_
hi=f	_	_
(	_	_
1	_	_
)	_	_
bi	_	_
(	_	_
1	_	_
)	_	_
+∑j=1MWijxj	_	_
,	_	_
where	_	_
W	_	_
represents	_	_
a	_	_
weight	_	_
matrix	_	_
in	_	_
which	_	_
Wij	_	_
denotes	_	_
the	_	_
weight	_	_
connecting	_	_
input	_	_
j	_	_
to	_	_
hidden	_	_
neuron	_	_
i.	_	_
For	_	_
the	_	_
analysis	_	_
conducted	_	_
in	_	_
this	_	_
paper	_	_
,	_	_
a	_	_
binary	_	_
prediction	_	_
will	_	_
be	_	_
made	_	_
;	_	_
hence	_	_
,	_	_
for	_	_
the	_	_
activation	_	_
function	_	_
in	_	_
the	_	_
output	_	_
layer	_	_
,	_	_
we	_	_
will	_	_
be	_	_
using	_	_
the	_	_
logistic	_	_
(	_	_
sigmoid	_	_
)	_	_
activation	_	_
function	_	_
,	_	_
f	_	_
(	_	_
2	_	_
)	_	_
(	_	_
x	_	_
)	_	_
=11+e-x	_	_
to	_	_
obtain	_	_
a	_	_
response	_	_
probability	_	_
:	_	_
(	_	_
5	_	_
)	_	_
π=f	_	_
(	_	_
2	_	_
)	_	_
b	_	_
(	_	_
2	_	_
)	_	_
+∑j=1nhvjhj	_	_
,	_	_
with	_	_
nh	_	_
the	_	_
number	_	_
of	_	_
hidden	_	_
neurons	_	_
and	_	_
v	_	_
the	_	_
weight	_	_
vector	_	_
where	_	_
vj	_	_
represents	_	_
the	_	_
weight	_	_
connecting	_	_
hidden	_	_
neuron	_	_
j	_	_
to	_	_
the	_	_
output	_	_
neuron	_	_
.	_	_

#75
During	_	_
model	_	_
estimation	_	_
,	_	_
the	_	_
weights	_	_
of	_	_
the	_	_
network	_	_
are	_	_
first	_	_
randomly	_	_
initialised	_	_
and	_	_
then	_	_
iteratively	_	_
adjusted	_	_
so	_	_
as	_	_
to	_	_
minimise	_	_
an	_	_
objective	_	_
function	_	_
,	_	_
e.g.	_	_
,	_	_
the	_	_
sum	_	_
of	_	_
squared	_	_
errors	_	_
(	_	_
possibly	_	_
accompanied	_	_
by	_	_
a	_	_
regularisation	_	_
term	_	_
to	_	_
prevent	_	_
over-fitting	_	_
)	_	_
.	_	_

#76
This	_	_
iterative	_	_
procedure	_	_
can	options	_
be	_	_
based	_	_
on	_	_
simple	_	_
gradient	_	_
descent	_	_
learning	_	_
or	_	_
more	_	_
sophisticated	_	_
optimisation	_	_
methods	_	_
such	_	_
as	_	_
Levenberg-Marquardt	_	_
or	_	_
Quasi-Newton	_	_
.	_	_

#77
The	_	_
number	_	_
of	_	_
hidden	_	_
neurons	_	_
can	feasibility	_
be	_	_
determined	_	_
through	_	_
a	_	_
grid	_	_
search	_	_
based	_	_
on	_	_
validation	_	_
set	_	_
performance	_	_
.	_	_

#78
Least	_	_
square	_	_
support	_	_
vector	_	_
machines	_	_
(	_	_
LS-SVMs	_	_
)	_	_
Support	_	_
vector	_	_
machines	_	_
(	_	_
SVMs	_	_
)	_	_
are	_	_
a	_	_
set	_	_
of	_	_
powerful	_	_
supervised	_	_
learning	_	_
techniques	_	_
used	_	_
for	_	_
classification	_	_
and	_	_
regression	_	_
.	_	_

#79
Their	_	_
basic	_	_
principle	_	_
is	_	_
to	_	_
construct	_	_
a	_	_
maximum-margin	_	_
separating	_	_
hyperplane	_	_
in	_	_
some	_	_
transformed	_	_
feature	_	_
space	_	_
.	_	_

#80
Rather	_	_
than	_	_
requiring	_	_
one	_	_
to	_	_
specify	_	_
the	_	_
exact	_	_
transformation	_	_
though	_	_
,	_	_
they	_	_
use	_	_
the	_	_
principle	_	_
of	_	_
kernel	_	_
substitution	_	_
to	_	_
turn	_	_
them	_	_
into	_	_
a	_	_
general	_	_
(	_	_
non-linear	_	_
)	_	_
model	_	_
.	_	_

#81
The	_	_
least	_	_
square	_	_
support	_	_
vector	_	_
machine	_	_
(	_	_
LS-SVM	_	_
)	_	_
proposed	_	_
by	_	_
Suykens	_	_
,	_	_
Van	_	_
Gestel	_	_
,	_	_
De	_	_
Brabanter	_	_
,	_	_
De	_	_
Moor	_	_
,	_	_
and	_	_
Vandewalle	_	_
(	_	_
2002	_	_
)	_	_
is	_	_
a	_	_
further	_	_
adaptation	_	_
of	_	_
Vapnik	_	_
's	_	_
original	_	_
SVM	_	_
formulation	_	_
which	_	_
leads	_	_
to	_	_
solving	_	_
linear	_	_
KKT	_	_
(	_	_
Karush-Kuhn-Tucker	_	_
)	_	_
systems	_	_
(	_	_
rather	_	_
than	_	_
a	_	_
more	_	_
complex	_	_
quadratic	_	_
programing	_	_
problem	_	_
)	_	_
.	_	_

#82
The	_	_
optimisation	_	_
problem	_	_
for	_	_
the	_	_
LS-SVM	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
(	_	_
6	_	_
)	_	_
minw	_	_
,	_	_
b	_	_
,	_	_
eJ	_	_
(	_	_
w	_	_
,	_	_
b	_	_
,	_	_
e	_	_
)	_	_
=12wTw+γ12∑i=1Nei2	_	_
,	_	_
subject	_	_
to	_	_
the	_	_
following	_	_
equality	_	_
constraints	_	_
:	_	_
(	_	_
7	_	_
)	_	_
yiwTφ	_	_
(	_	_
xi	_	_
)	_	_
+b=1-ei	_	_
,	_	_
i=1	_	_
,	_	_
…	_	_
,	_	_
N	_	_
,	_	_
Where	_	_
w	_	_
is	_	_
the	_	_
weight	_	_
vector	_	_
in	_	_
primal	_	_
space	_	_
,	_	_
γ	_	_
is	_	_
the	_	_
regularisation	_	_
parameter	_	_
,	_	_
and	_	_
yi=+1	_	_
or	_	_
-1	_	_
for	_	_
good	_	_
(	_	_
bad	_	_
)	_	_
payers	_	_
,	_	_
respectively	_	_
(	_	_
Suykens	_	_
et	_	_
al.	_	_
,	_	_
2002	_	_
)	_	_
.	_	_

#83
A	_	_
solution	_	_
can	feasibility-rhetorical	_
then	_	_
be	_	_
obtained	_	_
after	_	_
constructing	_	_
the	_	_
Lagrangian	_	_
,	_	_
and	_	_
choosing	_	_
a	_	_
particular	_	_
kernel	_	_
function	_	_
K	_	_
(	_	_
x	_	_
,	_	_
xi	_	_
)	_	_
that	_	_
computes	_	_
inner	_	_
products	_	_
in	_	_
the	_	_
transformed	_	_
space	_	_
,	_	_
based	_	_
on	_	_
which	_	_
a	_	_
classifier	_	_
of	_	_
the	_	_
following	_	_
form	_	_
is	_	_
obtained	_	_
:	_	_
y	_	_
(	_	_
x	_	_
)	_	_
=sign∑i=1NαiyiK	_	_
(	_	_
x	_	_
,	_	_
xi	_	_
)	_	_
+b	_	_
,	_	_
where	_	_
by	_	_
K	_	_
(	_	_
x	_	_
,	_	_
xi	_	_
)	_	_
=φ	_	_
(	_	_
x	_	_
)	_	_
Tφ	_	_
(	_	_
xi	_	_
)	_	_
is	_	_
taken	_	_
to	_	_
be	_	_
a	_	_
positive	_	_
definite	_	_
kernel	_	_
satisfying	_	_
the	_	_
Mercer	_	_
theorem.The	_	_
hyper	_	_
parameter	_	_
γ	_	_
for	_	_
the	_	_
LS-SVM	_	_
classification	_	_
technique	_	_
is	_	_
tuned	_	_
using	_	_
10-fold	_	_
cross	_	_
validation	_	_
.	_	_

#84
C4.5	_	_
.	_	_

#85
decision	_	_
trees	_	_
A	_	_
decision	_	_
tree	_	_
consists	_	_
of	_	_
internal	_	_
nodes	_	_
that	_	_
specify	_	_
tests	_	_
on	_	_
individual	_	_
input	_	_
variables	_	_
or	_	_
attributes	_	_
that	_	_
split	_	_
the	_	_
data	_	_
into	_	_
smaller	_	_
subsets	_	_
,	_	_
and	_	_
a	_	_
series	_	_
of	_	_
leaf	_	_
nodes	_	_
assigning	_	_
a	_	_
class	_	_
to	_	_
each	_	_
of	_	_
the	_	_
observations	_	_
in	_	_
the	_	_
resulting	_	_
segments	_	_
.	_	_

#86
For	_	_
our	_	_
study	_	_
,	_	_
we	_	_
chose	_	_
the	_	_
popular	_	_
decision	_	_
tree	_	_
classifier	_	_
C4.5	_	_
,	_	_
which	_	_
builds	_	_
decision	_	_
trees	_	_
using	_	_
the	_	_
concept	_	_
of	_	_
information	_	_
entropy	_	_
(	_	_
Quinlan	_	_
,	_	_
1993	_	_
)	_	_
.	_	_

#87
The	_	_
entropy	_	_
of	_	_
a	_	_
sample	_	_
S	_	_
of	_	_
classified	_	_
observations	_	_
is	_	_
given	_	_
by	_	_
(	_	_
8	_	_
)	_	_
Entropy	_	_
(	_	_
S	_	_
)	_	_
=-p1log2	_	_
(	_	_
p1	_	_
)	_	_
-p0log2	_	_
(	_	_
p0	_	_
)	_	_
,	_	_
where	_	_
p1	_	_
(	_	_
p0	_	_
)	_	_
are	_	_
the	_	_
proportions	_	_
of	_	_
the	_	_
class	_	_
values	_	_
1	_	_
(	_	_
0	_	_
)	_	_
in	_	_
the	_	_
sample	_	_
S	_	_
,	_	_
respectively	_	_
.	_	_

#88
C4.5	_	_
examines	_	_
the	_	_
normalised	_	_
information	_	_
gain	_	_
(	_	_
entropy	_	_
difference	_	_
)	_	_
that	_	_
results	_	_
from	_	_
choosing	_	_
an	_	_
attribute	_	_
for	_	_
splitting	_	_
the	_	_
data	_	_
.	_	_

#89
The	_	_
attribute	_	_
with	_	_
the	_	_
highest	_	_
normalised	_	_
information	_	_
gain	_	_
is	_	_
the	_	_
one	_	_
used	_	_
to	_	_
make	_	_
the	_	_
decision	_	_
.	_	_

#90
The	_	_
algorithm	_	_
then	_	_
recurs	_	_
on	_	_
the	_	_
smaller	_	_
subsets	_	_
.	_	_

#91
k-NN	_	_
(	_	_
memory	_	_
based	_	_
reasoning	_	_
)	_	_
The	_	_
k-nearest	_	_
neighbours	_	_
algorithm	_	_
(	_	_
k-NN	_	_
)	_	_
classifies	_	_
a	_	_
data	_	_
point	_	_
by	_	_
taking	_	_
a	_	_
majority	_	_
vote	_	_
of	_	_
its	_	_
k	_	_
most	_	_
similar	_	_
data	_	_
points	_	_
(	_	_
Hastie	_	_
,	_	_
Tibshirani	_	_
,	_	_
&	_	_
Friedman	_	_
,	_	_
2001	_	_
)	_	_
.	_	_

#92
The	_	_
similarity	_	_
measure	_	_
used	_	_
in	_	_
this	_	_
paper	_	_
is	_	_
the	_	_
Euclidean	_	_
distance	_	_
between	_	_
the	_	_
two	_	_
points	_	_
:	_	_
(	_	_
9	_	_
)	_	_
dxi	_	_
,	_	_
xj=‖xi-xj‖=xi-xjTxi-xj1/2	_	_
.	_	_

#93
Random	_	_
forests	_	_
Random	_	_
forests	_	_
are	_	_
defined	_	_
as	_	_
a	_	_
group	_	_
of	_	_
un-pruned	_	_
classification	_	_
or	_	_
regression	_	_
trees	_	_
,	_	_
trained	_	_
on	_	_
bootstrap	_	_
samples	_	_
of	_	_
the	_	_
training	_	_
data	_	_
using	_	_
random	_	_
feature	_	_
selection	_	_
in	_	_
the	_	_
process	_	_
of	_	_
tree	_	_
generation	_	_
.	_	_

#94
After	_	_
a	_	_
large	_	_
number	_	_
of	_	_
trees	_	_
have	_	_
been	_	_
generated	_	_
,	_	_
each	_	_
tree	_	_
votes	_	_
for	_	_
the	_	_
most	_	_
popular	_	_
class	_	_
.	_	_

#95
These	_	_
tree	_	_
voting	_	_
procedures	_	_
are	_	_
collectively	_	_
defined	_	_
as	_	_
random	_	_
forests	_	_
.	_	_

#96
A	_	_
more	_	_
detailed	_	_
explanation	_	_
of	_	_
how	_	_
to	_	_
train	_	_
a	_	_
random	_	_
forest	_	_
can	feasibility-rhetorical	_
be	_	_
found	_	_
in	_	_
Breiman	_	_
(	_	_
2001	_	_
)	_	_
.	_	_

#97
For	_	_
the	_	_
Random	_	_
Forests	_	_
classification	_	_
technique	_	_
two	_	_
parameters	_	_
require	_	_
tuning	_	_
.	_	_

#98
These	_	_
are	_	_
the	_	_
number	_	_
of	_	_
trees	_	_
and	_	_
the	_	_
number	_	_
of	_	_
attributes	_	_
used	_	_
to	_	_
grow	_	_
each	_	_
tree	_	_
.	_	_

#99
Gradient	_	_
boosting	_	_
Gradient	_	_
boosting	_	_
(	_	_
Friedman	_	_
,	_	_
2001	_	_
,	_	_
2002	_	_
)	_	_
is	_	_
an	_	_
ensemble	_	_
algorithm	_	_
that	_	_
improves	_	_
the	_	_
accuracy	_	_
of	_	_
a	_	_
predictive	_	_
function	_	_
through	_	_
incremental	_	_
minimisation	_	_
of	_	_
the	_	_
error	_	_
term	_	_
.	_	_

#100
After	_	_
the	_	_
initial	_	_
base	_	_
learner	_	_
(	_	_
most	_	_
commonly	_	_
a	_	_
tree	_	_
)	_	_
is	_	_
grown	_	_
,	_	_
each	_	_
tree	_	_
in	_	_
the	_	_
series	_	_
is	_	_
fit	_	_
to	_	_
the	_	_
so-called	_	_
"	_	_
pseudo	_	_
residuals	_	_
"	_	_
of	_	_
the	_	_
prediction	_	_
from	_	_
the	_	_
earlier	_	_
trees	_	_
with	_	_
the	_	_
purpose	_	_
of	_	_
reducing	_	_
the	_	_
error	_	_
.	_	_

#101
This	_	_
leads	_	_
to	_	_
the	_	_
following	_	_
model	_	_
:	_	_
(	_	_
10	_	_
)	_	_
F	_	_
(	_	_
x	_	_
)	_	_
=G0+β1T1	_	_
(	_	_
x	_	_
)	_	_
+β2T2	_	_
(	_	_
x	_	_
)	_	_
+⋯+βnTn	_	_
(	_	_
x	_	_
)	_	_
,	_	_
where	_	_
G0	_	_
equals	_	_
the	_	_
first	_	_
value	_	_
for	_	_
the	_	_
series	_	_
,	_	_
T1	_	_
,	_	_
…	_	_
,	_	_
Tn	_	_
are	_	_
the	_	_
trees	_	_
fitted	_	_
to	_	_
the	_	_
pseudo-residuals	_	_
,	_	_
and	_	_
βi	_	_
are	_	_
coefficients	_	_
for	_	_
the	_	_
respective	_	_
tree	_	_
nodes	_	_
computed	_	_
by	_	_
the	_	_
gradient	_	_
boosting	_	_
algorithm	_	_
.	_	_

#102
A	_	_
more	_	_
detailed	_	_
explanation	_	_
of	_	_
gradient	_	_
boosting	_	_
can	feasibility-rhetorical	_
be	_	_
found	_	_
in	_	_
Friedman	_	_
(	_	_
2001	_	_
,	_	_
2002	_	_
)	_	_
.	_	_

#103
The	_	_
gradient	_	_
boosting	_	_
classifier	_	_
requires	_	_
tuning	_	_
of	_	_
the	_	_
number	_	_
of	_	_
iterations	_	_
and	_	_
the	_	_
maximum	_	_
branch	_	_
size	_	_
used	_	_
in	_	_
the	_	_
splitting	_	_
rule	_	_
.	_	_

#104
Experimental	_	_
set-up	_	_
and	_	_
data	_	_
sets	_	_
Data	_	_
set	_	_
characteristics	_	_
The	_	_
characteristics	_	_
of	_	_
the	_	_
data	_	_
sets	_	_
used	_	_
in	_	_
evaluating	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
aforementioned	_	_
classification	_	_
techniques	_	_
are	_	_
given	_	_
below	_	_
in	_	_
Table	_	_
2	_	_
.	_	_

#105
The	_	_
Bene1	_	_
and	_	_
Bene2	_	_
data	_	_
sets	_	_
were	_	_
obtained	_	_
from	_	_
two	_	_
major	_	_
financial	_	_
institutions	_	_
in	_	_
the	_	_
Benelux	_	_
region	_	_
.	_	_

#106
For	_	_
these	_	_
two	_	_
data	_	_
sets	_	_
,	_	_
a	_	_
bad	_	_
customer	_	_
was	_	_
defined	_	_
as	_	_
someone	_	_
who	_	_
had	_	_
missed	_	_
three	_	_
consecutive	_	_
months	_	_
of	_	_
payments	_	_
.	_	_

#107
The	_	_
German	_	_
credit	_	_
data	_	_
set	_	_
and	_	_
the	_	_
Australian	_	_
Credit	_	_
data	_	_
set	_	_
are	_	_
publicly	_	_
available	_	_
at	_	_
the	_	_
UCI	_	_
repository	_	_
(	_	_
http	_	_
:	_	_
//www.kdd.ics.uci.edu/	_	_
)	_	_
.	_	_

#108
The	_	_
Behav	_	_
data	_	_
set	_	_
was	_	_
also	_	_
acquired	_	_
from	_	_
a	_	_
Benelux	_	_
institution	_	_
.	_	_

#109
As	_	_
all	_	_
the	_	_
data	_	_
sets	_	_
used	_	_
have	_	_
a	_	_
reasonable	_	_
number	_	_
of	_	_
observations	_	_
they	_	_
will	_	_
each	_	_
be	_	_
split	_	_
into	_	_
a	_	_
training	_	_
(	_	_
two	_	_
thirds	_	_
)	_	_
and	_	_
a	_	_
test	_	_
set	_	_
(	_	_
one	_	_
third	_	_
)	_	_
.	_	_

#110
This	_	_
test	_	_
set	_	_
will	_	_
remain	_	_
unchanged	_	_
throughout	_	_
the	_	_
analysis	_	_
of	_	_
the	_	_
techniques	_	_
.	_	_

#111
Re-sampling	_	_
setup	_	_
and	_	_
performance	_	_
metrics	_	_
In	_	_
order	_	_
for	_	_
the	_	_
percentage	_	_
reduction	_	_
in	_	_
the	_	_
bad	_	_
observations	_	_
,	_	_
in	_	_
each	_	_
data	_	_
set	_	_
,	_	_
to	_	_
be	_	_
relatively	_	_
compared	_	_
,	_	_
the	_	_
Bene1	_	_
set	_	_
,	_	_
Australian	_	_
credit	_	_
and	_	_
the	_	_
Behavioural	_	_
Scoring	_	_
set	_	_
have	_	_
first	_	_
been	_	_
altered	_	_
to	_	_
give	_	_
a	_	_
70/30	_	_
class	_	_
distribution	_	_
.	_	_

#112
This	_	_
was	_	_
done	_	_
by	_	_
either	_	_
under-sampling	_	_
the	_	_
bad	_	_
observations	_	_
(	_	_
from	_	_
a	_	_
total	_	_
of	_	_
1041	_	_
bad	_	_
observations	_	_
in	_	_
the	_	_
Bene1	_	_
data	_	_
set	_	_
,	_	_
only	_	_
892	_	_
observations	_	_
have	_	_
been	_	_
used	_	_
;	_	_
and	_	_
from	_	_
a	_	_
total	_	_
of	_	_
307	_	_
bad	_	_
observations	_	_
in	_	_
the	_	_
Australian	_	_
credit	_	_
data	_	_
set	_	_
,	_	_
only	_	_
164	_	_
observations	_	_
have	_	_
been	_	_
used	_	_
)	_	_
or	_	_
under-sampling	_	_
the	_	_
good	_	_
observations	_	_
in	_	_
the	_	_
behavioural	_	_
scoring	_	_
data	_	_
set	_	_
,	_	_
(	_	_
from	_	_
a	_	_
total	_	_
of	_	_
1436	_	_
good	_	_
observations	_	_
,	_	_
only	_	_
838	_	_
observations	_	_
have	_	_
been	_	_
used	_	_
)	_	_
.	_	_

#113
For	_	_
this	_	_
empirical	_	_
study	_	_
,	_	_
the	_	_
class	_	_
of	_	_
defaulters	_	_
in	_	_
each	_	_
of	_	_
the	_	_
training	_	_
data	_	_
sets	_	_
was	_	_
artificially	_	_
reduced	_	_
,	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
5	_	_
%	_	_
up	_	_
to	_	_
95	_	_
%	_	_
then	_	_
by	_	_
2.5	_	_
%	_	_
and	_	_
1	_	_
%	_	_
,	_	_
so	_	_
as	_	_
to	_	_
create	_	_
a	_	_
larger	_	_
difference	_	_
in	_	_
class	_	_
distribution	_	_
.	_	_

#114
As	_	_
a	_	_
result	_	_
of	_	_
this	_	_
reduction	_	_
,	_	_
eight	_	_
data	_	_
sets	_	_
were	_	_
created	_	_
for	_	_
each	_	_
of	_	_
the	_	_
five	_	_
original	_	_
data	_	_
sets	_	_
.	_	_

#115
The	_	_
percentage	_	_
splits	_	_
created	_	_
were	_	_
75	_	_
%	_	_
,	_	_
80	_	_
%	_	_
,	_	_
85	_	_
%	_	_
,	_	_
90	_	_
%	_	_
,	_	_
95	_	_
%	_	_
,	_	_
97.5	_	_
%	_	_
,	_	_
99	_	_
%	_	_
good	_	_
observations	_	_
.	_	_

#116
For	_	_
this	_	_
empirical	_	_
study	_	_
our	_	_
focus	_	_
is	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
classification	_	_
techniques	_	_
on	_	_
data	_	_
sets	_	_
with	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
.	_	_

#117
Therefore	_	_
detailed	_	_
results	_	_
will	_	_
only	_	_
be	_	_
presented	_	_
for	_	_
the	_	_
data	_	_
set	_	_
with	_	_
the	_	_
original	_	_
70/30	_	_
split	_	_
,	_	_
as	_	_
a	_	_
benchmark	_	_
,	_	_
and	_	_
data	_	_
sets	_	_
with	_	_
85	_	_
%	_	_
,	_	_
90	_	_
%	_	_
and	_	_
99	_	_
%	_	_
splits	_	_
.	_	_

#118
By	_	_
doing	_	_
so	_	_
,	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
identify	_	_
whether	_	_
techniques	_	_
are	_	_
adversely	_	_
affected	_	_
in	_	_
the	_	_
prediction	_	_
of	_	_
the	_	_
target	_	_
variable	_	_
when	_	_
there	_	_
is	_	_
a	_	_
substantially	_	_
lower	_	_
number	_	_
of	_	_
observations	_	_
in	_	_
one	_	_
of	_	_
the	_	_
classes	_	_
.	_	_

#119
The	_	_
performance	_	_
criterion	_	_
chosen	_	_
to	_	_
measure	_	_
this	_	_
effect	_	_
is	_	_
the	_	_
area	_	_
under	_	_
the	_	_
receiver	_	_
operator	_	_
characteristic	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
statistic	_	_
as	_	_
proposed	_	_
by	_	_
Baesens	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2003	_	_
)	_	_
.	_	_

#120
The	_	_
receiver	_	_
operating	_	_
characteristic	_	_
curve	_	_
(	_	_
ROC	_	_
)	_	_
is	_	_
a	_	_
two-dimensional	_	_
graphical	_	_
illustration	_	_
of	_	_
the	_	_
trade-off	_	_
between	_	_
the	_	_
true	_	_
positive	_	_
rate	_	_
(	_	_
sensitivity	_	_
)	_	_
and	_	_
false	_	_
positive	_	_
rate	_	_
(	_	_
1-specificity	_	_
)	_	_
.	_	_

#121
The	_	_
ROC	_	_
curve	_	_
illustrates	_	_
the	_	_
behaviour	_	_
of	_	_
a	_	_
classifier	_	_
without	_	_
having	_	_
to	_	_
take	_	_
into	_	_
consideration	_	_
the	_	_
class	_	_
distribution	_	_
or	_	_
misclassification	_	_
cost	_	_
.	_	_

#122
In	_	_
order	_	_
to	_	_
compare	_	_
the	_	_
ROC	_	_
curves	_	_
of	_	_
different	_	_
classifiers	_	_
,	_	_
the	_	_
area	_	_
under	_	_
the	_	_
receiver	_	_
operating	_	_
characteristic	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
must	deontic	_
be	_	_
computed	_	_
.	_	_

#123
The	_	_
AUC	_	_
statistic	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
Gini	_	_
coefficient	_	_
which	_	_
is	_	_
equal	_	_
to	_	_
2×	_	_
(	_	_
AUC-0.5	_	_
)	_	_
.	_	_

#124
An	_	_
example	_	_
of	_	_
an	_	_
ROC	_	_
curve	_	_
is	_	_
depicted	_	_
in	_	_
Fig	_	_
.	_	_
1	_	_
:	_	_
The	_	_
diagonal	_	_
line	_	_
represents	_	_
the	_	_
trade-off	_	_
between	_	_
the	_	_
sensitivity	_	_
and	_	_
(	_	_
1-specificity	_	_
)	_	_
for	_	_
a	_	_
random	_	_
model	_	_
,	_	_
and	_	_
has	_	_
an	_	_
AUC	_	_
of	_	_
0.5	_	_
.	_	_

#125
For	_	_
a	_	_
well	_	_
performing	_	_
classifier	_	_
the	_	_
ROC	_	_
curve	_	_
needs	_	_
to	_	_
be	_	_
as	_	_
far	_	_
to	_	_
the	_	_
top	_	_
left-hand	_	_
corner	_	_
as	_	_
possible	_	_
.	_	_

#126
In	_	_
the	_	_
example	_	_
shown	_	_
in	_	_
Fig	_	_
.	_	_
1	_	_
,	_	_
the	_	_
classifier	_	_
that	_	_
performs	_	_
the	_	_
best	_	_
is	_	_
the	_	_
ROC1	_	_
curve	_	_
.	_	_

#127
Parameter	_	_
tuning	_	_
and	_	_
input	_	_
selection	_	_
The	_	_
linear	_	_
discriminant	_	_
analysis	_	_
(	_	_
LDA	_	_
)	_	_
,	_	_
quadratic	_	_
discriminant	_	_
analysis	_	_
(	_	_
QDA	_	_
)	_	_
and	_	_
logistic	_	_
regression	_	_
(	_	_
LOG	_	_
)	_	_
classification	_	_
techniques	_	_
require	_	_
no	_	_
parameter	_	_
tuning	_	_
.	_	_

#128
The	_	_
LOG	_	_
model	_	_
was	_	_
built	_	_
in	_	_
SAS	_	_
using	_	_
proc	_	_
logistic	_	_
and	_	_
using	_	_
a	_	_
stepwise	_	_
variable	_	_
selection	_	_
method	_	_
.	_	_

#129
Both	_	_
the	_	_
LDA	_	_
and	_	_
QDA	_	_
techniques	_	_
were	_	_
run	_	_
in	_	_
SAS	_	_
using	_	_
proc	_	_
discrim	_	_
.	_	_

#130
Before	_	_
all	_	_
the	_	_
techniques	_	_
were	_	_
run	_	_
,	_	_
dummy	_	_
variables	_	_
were	_	_
created	_	_
for	_	_
the	_	_
categorical	_	_
variables	_	_
.	_	_

#131
The	_	_
AUC	_	_
statistic	_	_
was	_	_
computed	_	_
using	_	_
the	_	_
ROC	_	_
macro	_	_
by	_	_
DeLong	_	_
,	_	_
DeLong	_	_
,	_	_
and	_	_
Clarke-Pearson	_	_
(	_	_
1988	_	_
)	_	_
,	_	_
which	_	_
is	_	_
available	_	_
from	_	_
the	_	_
SAS	_	_
website	_	_
(	_	_
http	_	_
:	_	_
//.support.sas.com/kb/25/017.html	_	_
)	_	_
.	_	_

#132
For	_	_
the	_	_
LS-SVM	_	_
classifier	_	_
,	_	_
a	_	_
linear	_	_
kernel	_	_
was	_	_
chosen	_	_
and	_	_
a	_	_
grid	_	_
search	_	_
mechanism	_	_
was	_	_
used	_	_
to	_	_
tune	_	_
the	_	_
hyper-parameters	_	_
.	_	_

#133
For	_	_
the	_	_
LS-SVM	_	_
,	_	_
the	_	_
LS-SVMlab	_	_
Matlab	_	_
toolbox	_	_
developed	_	_
by	_	_
Suykens	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2002	_	_
)	_	_
was	_	_
used	_	_
.	_	_

#134
The	_	_
NN	_	_
classifiers	_	_
were	_	_
trained	_	_
after	_	_
selecting	_	_
the	_	_
best	_	_
performing	_	_
number	_	_
of	_	_
hidden	_	_
neurons	_	_
based	_	_
on	_	_
a	_	_
validation	_	_
set	_	_
.	_	_

#135
The	_	_
neural	_	_
networks	_	_
were	_	_
trained	_	_
in	_	_
SAS	_	_
Enterprise	_	_
Miner	_	_
using	_	_
a	_	_
logistic	_	_
hidden	_	_
and	_	_
target	_	_
layer	_	_
activation	_	_
function	_	_
.	_	_

#136
The	_	_
confidence	_	_
level	_	_
for	_	_
the	_	_
pruning	_	_
strategy	_	_
of	_	_
C4.5	_	_
was	_	_
varied	_	_
from	_	_
0.01	_	_
to	_	_
0.5	_	_
,	_	_
and	_	_
the	_	_
most	_	_
appropriate	_	_
value	_	_
was	_	_
selected	_	_
for	_	_
each	_	_
data	_	_
set	_	_
based	_	_
on	_	_
validation	_	_
set	_	_
performance	_	_
.	_	_

#137
The	_	_
tree	_	_
was	_	_
built	_	_
using	_	_
the	_	_
Weka	_	_
(	_	_
Witten	_	_
&	_	_
Frank	_	_
,	_	_
2005	_	_
)	_	_
package	_	_
.	_	_

#138
Two	_	_
parameters	_	_
have	_	_
to	_	_
be	_	_
set	_	_
for	_	_
the	_	_
Random	_	_
Forests	_	_
technique	_	_
:	_	_
these	_	_
are	_	_
the	_	_
number	_	_
of	_	_
trees	_	_
and	_	_
the	_	_
number	_	_
of	_	_
attributes	_	_
used	_	_
to	_	_
grow	_	_
each	_	_
tree	_	_
.	_	_

#139
A	_	_
range	_	_
of	_	_
[	_	_
10,50,100,250,500,1000	_	_
]	_	_
trees	_	_
has	_	_
been	_	_
assessed	_	_
,	_	_
as	_	_
well	_	_
as	_	_
three	_	_
different	_	_
settings	_	_
for	_	_
the	_	_
number	_	_
of	_	_
randomly	_	_
selected	_	_
attributes	_	_
per	_	_
tree	_	_
(	_	_
[	_	_
0.5,1,2	_	_
]	_	_
.M	_	_
)	_	_
,	_	_
whereby	_	_
M	_	_
denotes	_	_
the	_	_
number	_	_
of	_	_
attributes	_	_
within	_	_
the	_	_
respective	_	_
data	_	_
set	_	_
(	_	_
Breiman	_	_
,	_	_
2001	_	_
)	_	_
.	_	_

#140
As	_	_
with	_	_
the	_	_
C4.5	_	_
algorithm	_	_
,	_	_
Random	_	_
Forests	_	_
were	_	_
also	_	_
trained	_	_
in	_	_
Weka	_	_
(	_	_
Witten	_	_
&	_	_
Frank	_	_
,	_	_
2005	_	_
)	_	_
,	_	_
using	_	_
10-fold	_	_
cross-validation	_	_
for	_	_
tuning	_	_
the	_	_
parameters	_	_
.	_	_

#141
The	_	_
k-Nearest	_	_
Neighbours	_	_
technique	_	_
was	_	_
applied	_	_
for	_	_
both	_	_
k=10	_	_
and	_	_
k=100	_	_
,	_	_
using	_	_
the	_	_
Weka	_	_
(	_	_
Witten	_	_
&	_	_
Frank	_	_
,	_	_
2005	_	_
)	_	_
IBk	_	_
classifier	_	_
.	_	_

#142
For	_	_
the	_	_
gradient	_	_
boosting	_	_
classifier	_	_
a	_	_
partitioning	_	_
algorithm	_	_
was	_	_
used	_	_
as	_	_
proposed	_	_
by	_	_
Friedman	_	_
(	_	_
2001	_	_
)	_	_
.	_	_

#143
The	_	_
number	_	_
of	_	_
iterations	_	_
was	_	_
varied	_	_
in	_	_
the	_	_
range	_	_
[	_	_
10,50,100,250,500,1000	_	_
]	_	_
,	_	_
with	_	_
a	_	_
maximum	_	_
branch	_	_
size	_	_
of	_	_
two	_	_
selected	_	_
for	_	_
the	_	_
splitting	_	_
rule	_	_
(	_	_
Friedman	_	_
,	_	_
2001	_	_
)	_	_
.	_	_

#144
The	_	_
gradient	_	_
boosting	_	_
node	_	_
in	_	_
SAS	_	_
Enterprise	_	_
Miner	_	_
was	_	_
used	_	_
to	_	_
run	_	_
this	_	_
technique	_	_
.	_	_

#145
Statistical	_	_
comparison	_	_
of	_	_
classifiers	_	_
We	_	_
used	_	_
Friedman	_	_
's	_	_
test	_	_
(	_	_
Friedman	_	_
,	_	_
1940	_	_
)	_	_
to	_	_
compare	_	_
the	_	_
AUCs	_	_
of	_	_
the	_	_
different	_	_
classifiers	_	_
.	_	_

#146
The	_	_
Friedman	_	_
test	_	_
statistic	_	_
is	_	_
based	_	_
on	_	_
the	_	_
average	_	_
ranked	_	_
(	_	_
AR	_	_
)	_	_
performances	_	_
of	_	_
the	_	_
classification	_	_
techniques	_	_
on	_	_
each	_	_
data	_	_
set	_	_
,	_	_
and	_	_
is	_	_
calculated	_	_
as	_	_
follows	_	_
:	_	_
(	_	_
11	_	_
)	_	_
χF2=12DK	_	_
(	_	_
K+1	_	_
)	_	_
∑j=1KARj2-K	_	_
(	_	_
K+1	_	_
)	_	_
24	_	_
,	_	_
whereARj=1D∑i=1Drij.In	_	_
(	_	_
13	_	_
)	_	_
,	_	_
D	_	_
denotes	_	_
the	_	_
number	_	_
of	_	_
data	_	_
sets	_	_
used	_	_
in	_	_
the	_	_
study	_	_
,	_	_
K	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
classifiers	_	_
and	_	_
rij	_	_
is	_	_
the	_	_
rank	_	_
of	_	_
classifier	_	_
j	_	_
on	_	_
data	_	_
set	_	_
i.	_	_
χF2	_	_
is	_	_
distributed	_	_
according	_	_
to	_	_
the	_	_
Chi-square	_	_
distribution	_	_
with	_	_
K-1	_	_
degrees	_	_
of	_	_
freedom	_	_
.	_	_

#147
If	_	_
the	_	_
value	_	_
of	_	_
χF2	_	_
is	_	_
large	_	_
enough	_	_
,	_	_
then	_	_
the	_	_
null	_	_
hypothesis	_	_
that	_	_
there	_	_
is	_	_
no	_	_
difference	_	_
between	_	_
the	_	_
techniques	_	_
can	feasibility	_
be	_	_
rejected	_	_
.	_	_

#148
The	_	_
Friedman	_	_
statistic	_	_
is	_	_
well	_	_
suited	_	_
for	_	_
this	_	_
type	_	_
of	_	_
data	_	_
analysis	_	_
as	_	_
it	_	_
is	_	_
less	_	_
susceptible	_	_
to	_	_
outliers	_	_
(	_	_
Friedman	_	_
,	_	_
1940	_	_
)	_	_
.	_	_

#149
The	_	_
post	_	_
hoc	_	_
Nemenyi	_	_
test	_	_
(	_	_
Nemenyi	_	_
,	_	_
1963	_	_
)	_	_
is	_	_
applied	_	_
to	_	_
report	_	_
any	_	_
significant	_	_
differences	_	_
between	_	_
individual	_	_
classifiers	_	_
.	_	_

#150
The	_	_
Nemenyi	_	_
post	_	_
hoc	_	_
test	_	_
states	_	_
that	_	_
the	_	_
performances	_	_
of	_	_
two	_	_
or	_	_
more	_	_
classifiers	_	_
are	_	_
significantly	_	_
different	_	_
if	_	_
their	_	_
average	_	_
ranks	_	_
differ	_	_
by	_	_
at	_	_
least	_	_
the	_	_
critical	_	_
difference	_	_
(	_	_
CD	_	_
)	_	_
,	_	_
given	_	_
by	_	_
(	_	_
12	_	_
)	_	_
CD=qα	_	_
,	_	_
∞	_	_
,	_	_
KK	_	_
(	_	_
K+1	_	_
)	_	_
12D.In	_	_
this	_	_
formula	_	_
,	_	_
the	_	_
value	_	_
qα	_	_
,	_	_
∞	_	_
,	_	_
K	_	_
is	_	_
based	_	_
on	_	_
the	_	_
studentised	_	_
range	_	_
statistic	_	_
(	_	_
Nemenyi	_	_
,	_	_
1963	_	_
)	_	_
.	_	_

#151
Finally	_	_
,	_	_
the	_	_
results	_	_
from	_	_
Friedman	_	_
's	_	_
statistic	_	_
and	_	_
the	_	_
Nemenyi	_	_
post	_	_
hoc	_	_
tests	_	_
are	_	_
displayed	_	_
using	_	_
a	_	_
modified	_	_
version	_	_
of	_	_
Demšar	_	_
(	_	_
2006	_	_
)	_	_
significance	_	_
diagrams	_	_
(	_	_
Lessmann	_	_
,	_	_
Baesens	_	_
,	_	_
Mues	_	_
,	_	_
&	_	_
Pietsch	_	_
,	_	_
2008	_	_
)	_	_
.	_	_

#152
These	_	_
diagrams	_	_
display	_	_
the	_	_
ranked	_	_
performances	_	_
of	_	_
the	_	_
classification	_	_
techniques	_	_
along	_	_
with	_	_
the	_	_
critical	_	_
difference	_	_
to	_	_
clearly	_	_
show	_	_
any	_	_
techniques	_	_
which	_	_
are	_	_
significantly	_	_
different	_	_
to	_	_
the	_	_
best	_	_
performing	_	_
classifiers	_	_
.	_	_

#153
Results	_	_
and	_	_
discussion	_	_
The	_	_
table	_	_
on	_	_
the	_	_
following	_	_
page	_	_
(	_	_
Table	_	_
3	_	_
)	_	_
reports	_	_
the	_	_
AUCs	_	_
of	_	_
all	_	_
ten	_	_
classifiers	_	_
on	_	_
the	_	_
five	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
at	_	_
varying	_	_
degrees	_	_
of	_	_
class	_	_
imbalance	_	_
.	_	_

#154
For	_	_
each	_	_
level	_	_
of	_	_
imbalance	_	_
,	_	_
the	_	_
Friedman	_	_
test	_	_
statistic	_	_
and	_	_
corresponding	_	_
p-value	_	_
is	_	_
shown	_	_
.	_	_

#155
As	_	_
these	_	_
were	_	_
all	_	_
significant	_	_
(	_	_
p	_	_
<	_	_
0.005	_	_
)	_	_
a	_	_
post	_	_
hoc	_	_
Nemenyi	_	_
test	_	_
was	_	_
then	_	_
applied	_	_
to	_	_
each	_	_
class	_	_
distribution	_	_
.	_	_

#156
The	_	_
technique	_	_
achieving	_	_
the	_	_
highest	_	_
AUC	_	_
on	_	_
each	_	_
data	_	_
set	_	_
is	_	_
underlined	_	_
as	_	_
well	_	_
as	_	_
the	_	_
overall	_	_
highest	_	_
ranked	_	_
technique	_	_
.	_	_

#157
Table	_	_
3	_	_
shows	_	_
that	_	_
the	_	_
gradient	_	_
boosting	_	_
algorithm	_	_
has	_	_
the	_	_
highest	_	_
Friedman	_	_
score	_	_
(	_	_
average	_	_
rank	_	_
(	_	_
AR	_	_
)	_	_
)	_	_
on	_	_
two	_	_
of	_	_
the	_	_
five	_	_
different	_	_
percentage	_	_
class	_	_
splits	_	_
.	_	_

#158
However	_	_
at	_	_
the	_	_
extreme	_	_
class	_	_
split	_	_
(	_	_
99	_	_
%	_	_
good	_	_
,	_	_
1	_	_
%	_	_
bad	_	_
)	_	_
Random	_	_
Forests	_	_
provides	_	_
the	_	_
best	_	_
average	_	_
ranking	_	_
across	_	_
the	_	_
five	_	_
data	_	_
sets	_	_
(	_	_
Random	_	_
Forests	_	_
also	_	_
ranks	_	_
first	_	_
on	_	_
the	_	_
10	_	_
%	_	_
data	_	_
set	_	_
)	_	_
.	_	_

#159
In	_	_
the	_	_
majority	_	_
of	_	_
the	_	_
class	_	_
splits	_	_
,	_	_
the	_	_
AR	_	_
of	_	_
the	_	_
QDA	_	_
and	_	_
Lin	_	_
LS-SVM	_	_
classifiers	_	_
are	_	_
statistically	_	_
worse	_	_
than	_	_
the	_	_
AR	_	_
of	_	_
the	_	_
Random	_	_
Forests	_	_
classifier	_	_
at	_	_
the	_	_
5	_	_
%	_	_
critical	_	_
difference	_	_
level	_	_
(	_	_
α=0.05	_	_
)	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
the	_	_
significance	_	_
diagrams	_	_
included	_	_
next	_	_
.	_	_

#160
Note	_	_
that	_	_
,	_	_
even	_	_
though	_	_
the	_	_
differences	_	_
between	_	_
the	_	_
classifiers	_	_
are	_	_
small	_	_
,	_	_
it	_	_
is	_	_
important	_	_
to	_	_
note	_	_
that	_	_
in	_	_
a	_	_
credit	_	_
scoring	_	_
context	_	_
,	_	_
an	_	_
increase	_	_
in	_	_
the	_	_
discrimination	_	_
ability	_	_
of	_	_
even	_	_
a	_	_
fraction	_	_
of	_	_
a	_	_
percent	_	_
may	options	_
translate	_	_
into	_	_
significant	_	_
future	_	_
savings	_	_
(	_	_
Henley	_	_
&	_	_
Hand	_	_
,	_	_
1997	_	_
)	_	_
.	_	_

#161
The	_	_
following	_	_
significance	_	_
diagrams	_	_
display	_	_
the	_	_
AUC	_	_
performance	_	_
ranks	_	_
of	_	_
the	_	_
classifiers	_	_
,	_	_
along	_	_
with	_	_
Nemenyi	_	_
's	_	_
critical	_	_
difference	_	_
(	_	_
CD	_	_
)	_	_
tail	_	_
.	_	_

#162
The	_	_
CD	_	_
value	_	_
for	_	_
all	_	_
the	_	_
following	_	_
diagrams	_	_
is	_	_
equal	_	_
to	_	_
6.06	_	_
.	_	_

#163
Each	_	_
diagram	_	_
shows	_	_
the	_	_
classification	_	_
techniques	_	_
listed	_	_
in	_	_
ascending	_	_
order	_	_
of	_	_
ranked	_	_
performance	_	_
on	_	_
the	_	_
y-axis	_	_
,	_	_
and	_	_
the	_	_
classifier	_	_
's	_	_
mean	_	_
rank	_	_
across	_	_
all	_	_
five	_	_
data	_	_
sets	_	_
displayed	_	_
on	_	_
the	_	_
x-axis	_	_
.	_	_

#164
Two	_	_
vertical	_	_
dashed	_	_
lines	_	_
have	_	_
been	_	_
inserted	_	_
to	_	_
clearly	_	_
identify	_	_
the	_	_
end	_	_
of	_	_
the	_	_
best	_	_
performing	_	_
classifier	_	_
's	_	_
tail	_	_
and	_	_
the	_	_
start	_	_
of	_	_
the	_	_
next	_	_
significantly	_	_
different	_	_
classifier	_	_
.	_	_

#165
The	_	_
first	_	_
significance	_	_
diagram	_	_
(	_	_
see	_	_
Fig	_	_
.	_	_
2	_	_
)	_	_
displays	_	_
the	_	_
average	_	_
rank	_	_
of	_	_
the	_	_
classifiers	_	_
at	_	_
the	_	_
original	_	_
class	_	_
distribution	_	_
of	_	_
a	_	_
70	_	_
%	_	_
good	_	_
,	_	_
30	_	_
%	_	_
bad	_	_
split	_	_
:	_	_
At	_	_
this	_	_
original	_	_
70/30	_	_
%	_	_
split	_	_
,	_	_
the	_	_
linear	_	_
LS-SVM	_	_
is	_	_
the	_	_
best	_	_
performing	_	_
classification	_	_
technique	_	_
with	_	_
an	_	_
AR	_	_
value	_	_
of	_	_
1.2	_	_
.	_	_

#166
This	_	_
diagram	_	_
clearly	_	_
shows	_	_
that	_	_
the	_	_
k-NN10	_	_
,	_	_
QDA	_	_
and	_	_
C4.5	_	_
techniques	_	_
perform	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
best	_	_
performing	_	_
classifier	_	_
with	_	_
values	_	_
of	_	_
7.7	_	_
,	_	_
8.5	_	_
and	_	_
9.1	_	_
respectively	_	_
.	_	_

#167
The	_	_
following	_	_
significance	_	_
diagram	_	_
displays	_	_
the	_	_
average	_	_
rank	_	_
of	_	_
the	_	_
classifiers	_	_
at	_	_
an	_	_
85	_	_
%	_	_
good	_	_
,	_	_
15	_	_
%	_	_
bad	_	_
class	_	_
split	_	_
:	_	_
At	_	_
the	_	_
level	_	_
where	_	_
only	_	_
15	_	_
%	_	_
of	_	_
the	_	_
data	_	_
sets	_	_
are	_	_
bad	_	_
observations	_	_
,	_	_
it	_	_
is	_	_
shown	_	_
in	_	_
the	_	_
significance	_	_
diagram	_	_
that	_	_
gradient	_	_
boosting	_	_
becomes	_	_
the	_	_
best	_	_
performing	_	_
classifier	_	_
(	_	_
see	_	_
Fig	_	_
.	_	_
3	_	_
)	_	_
.	_	_

#168
The	_	_
gradient	_	_
boosting	_	_
classifier	_	_
performs	_	_
significantly	_	_
better	_	_
than	_	_
the	_	_
quadratic	_	_
discriminant	_	_
analysis	_	_
(	_	_
QDA	_	_
)	_	_
classifier	_	_
.	_	_

#169
From	_	_
these	_	_
findings	_	_
we	_	_
can	feasibility	_
make	_	_
a	_	_
preliminary	_	_
assumption	_	_
that	_	_
when	_	_
a	_	_
larger	_	_
class	_	_
imbalance	_	_
is	_	_
present	_	_
,	_	_
the	_	_
QDA	_	_
classifier	_	_
remains	_	_
significantly	_	_
different	_	_
to	_	_
the	_	_
gradient	_	_
boosting	_	_
classifier	_	_
.	_	_

#170
All	_	_
the	_	_
other	_	_
techniques	_	_
used	_	_
are	_	_
not	_	_
significantly	_	_
different	_	_
.	_	_

#171
At	_	_
a	_	_
90	_	_
%	_	_
good	_	_
,	_	_
10	_	_
%	_	_
bad	_	_
class	_	_
split	_	_
the	_	_
significance	_	_
diagram	_	_
shown	_	_
in	_	_
Fig	_	_
.	_	_
4	_	_
indicates	_	_
that	_	_
the	_	_
C4.5	_	_
and	_	_
QDA	_	_
algorithms	_	_
are	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
random	_	_
forests	_	_
classifier	_	_
.	_	_

#172
It	_	_
can	feasibility-rhetorical	_
be	_	_
noted	_	_
that	_	_
the	_	_
Linear	_	_
LS-SVM	_	_
classifier	_	_
however	_	_
is	_	_
progressively	_	_
becoming	_	_
less	_	_
powerful	_	_
as	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
is	_	_
present	_	_
(	_	_
see	_	_
Fig	_	_
.	_	_
5	_	_
)	_	_
.	_	_

#173
The	_	_
final	_	_
split	_	_
,	_	_
displaying	_	_
a	_	_
99	_	_
%	_	_
good	_	_
,	_	_
1	_	_
%	_	_
bad	_	_
class	_	_
split	_	_
,	_	_
indicates	_	_
that	_	_
,	_	_
at	_	_
the	_	_
most	_	_
extreme	_	_
class	_	_
distribution	_	_
analysed	_	_
,	_	_
two	_	_
classification	_	_
techniques	_	_
are	_	_
significantly	_	_
worse	_	_
(	_	_
Lin	_	_
LS-SVM	_	_
and	_	_
QDA	_	_
)	_	_
.	_	_

#174
This	_	_
displays	_	_
an	_	_
interesting	_	_
finding	_	_
that	_	_
at	_	_
the	_	_
extreme	_	_
split	_	_
,	_	_
LOG	_	_
is	_	_
now	_	_
close	_	_
to	_	_
being	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
Random	_	_
Forests	_	_
algorithm	_	_
.	_	_

#175
The	_	_
logistic	_	_
regression	_	_
technique	_	_
therefore	_	_
shows	_	_
limited	_	_
power	_	_
in	_	_
correctly	_	_
classifying	_	_
observations	_	_
where	_	_
only	_	_
a	_	_
small	_	_
number	_	_
of	_	_
bad	_	_
observations	_	_
exist	_	_
.	_	_

#176
It	_	_
can	feasibility-rhetorical	_
also	_	_
be	_	_
concluded	_	_
that	_	_
the	_	_
random	_	_
forests	_	_
classifier	_	_
performs	_	_
surprisingly	_	_
well	_	_
given	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
.	_	_

#177
In	_	_
summary	_	_
,	_	_
when	_	_
considering	_	_
the	_	_
AUC	_	_
performance	_	_
measures	_	_
,	_	_
it	_	_
can	feasibility-rhetorical	_
be	_	_
concluded	_	_
that	_	_
the	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forest	_	_
classifiers	_	_
yield	_	_
a	_	_
very	_	_
good	_	_
performance	_	_
at	_	_
extreme	_	_
levels	_	_
of	_	_
class	_	_
imbalance	_	_
,	_	_
whereas	_	_
the	_	_
Lin	_	_
LS-SVM	_	_
sees	_	_
a	_	_
reduction	_	_
in	_	_
performance	_	_
as	_	_
a	_	_
larger	_	_
class	_	_
imbalance	_	_
is	_	_
introduced	_	_
.	_	_

#178
However	_	_
,	_	_
the	_	_
simpler	_	_
,	_	_
linear	_	_
classification	_	_
techniques	_	_
such	_	_
as	_	_
LDA	_	_
and	_	_
LOG	_	_
also	_	_
give	_	_
a	_	_
relatively	_	_
good	_	_
performance	_	_
,	_	_
which	_	_
is	_	_
not	_	_
significantly	_	_
different	_	_
from	_	_
that	_	_
of	_	_
the	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forest	_	_
classifiers	_	_
.	_	_

#179
This	_	_
finding	_	_
seems	_	_
to	_	_
confirm	_	_
the	_	_
suggestion	_	_
made	_	_
in	_	_
Baesens	_	_
et	_	_
al	_	_
.	_	_
(	_	_
2003	_	_
)	_	_
that	_	_
most	_	_
credit	_	_
scoring	_	_
data	_	_
sets	_	_
are	_	_
only	_	_
weakly	_	_
non-linear	_	_
.	_	_

#180
However	_	_
,	_	_
techniques	_	_
such	_	_
as	_	_
QDA	_	_
,	_	_
C4.5	_	_
and	_	_
k-NN10	_	_
perform	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
best	_	_
performing	_	_
classifiers	_	_
at	_	_
each	_	_
percentage	_	_
reduction	_	_
.	_	_

#181
The	_	_
majority	_	_
of	_	_
classification	_	_
techniques	_	_
yielded	_	_
classification	_	_
performances	_	_
that	_	_
are	_	_
quite	_	_
competitive	_	_
with	_	_
each	_	_
other	_	_
.	_	_

#182
Conclusions	_	_
and	_	_
recommendations	_	_
for	_	_
further	_	_
work	_	_
In	_	_
this	_	_
comparative	_	_
study	_	_
we	_	_
have	_	_
looked	_	_
at	_	_
a	_	_
number	_	_
of	_	_
credit	_	_
scoring	_	_
techniques	_	_
,	_	_
and	_	_
studied	_	_
their	_	_
performance	_	_
over	_	_
various	_	_
class	_	_
distributions	_	_
in	_	_
five	_	_
real-life	_	_
credit	_	_
data	_	_
sets	_	_
.	_	_

#183
Two	_	_
techniques	_	_
that	_	_
have	_	_
yet	_	_
to	_	_
be	_	_
fully	_	_
researched	_	_
in	_	_
the	_	_
context	_	_
of	_	_
credit	_	_
scoring	_	_
,	_	_
i.e.	_	_
,	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forests	_	_
,	_	_
were	_	_
also	_	_
chosen	_	_
to	_	_
give	_	_
a	_	_
broader	_	_
review	_	_
of	_	_
the	_	_
techniques	_	_
available	_	_
.	_	_

#184
The	_	_
classification	_	_
power	_	_
of	_	_
these	_	_
techniques	_	_
was	_	_
assessed	_	_
based	_	_
on	_	_
the	_	_
area	_	_
under	_	_
the	_	_
receiver	_	_
operating	_	_
characteristic	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
.	_	_

#185
Friedman	_	_
's	_	_
test	_	_
and	_	_
Nemenyi	_	_
's	_	_
post	_	_
hoc	_	_
tests	_	_
were	_	_
then	_	_
applied	_	_
to	_	_
determine	_	_
whether	_	_
the	_	_
differences	_	_
between	_	_
the	_	_
average	_	_
ranked	_	_
performances	_	_
of	_	_
the	_	_
AUCs	_	_
were	_	_
statistically	_	_
significant	_	_
.	_	_

#186
Finally	_	_
,	_	_
these	_	_
significance	_	_
results	_	_
were	_	_
visualised	_	_
using	_	_
significance	_	_
diagrams	_	_
for	_	_
each	_	_
of	_	_
the	_	_
various	_	_
class	_	_
distributions	_	_
analysed	_	_
.	_	_

#187
The	_	_
results	_	_
of	_	_
these	_	_
experiments	_	_
show	_	_
that	_	_
the	_	_
gradient	_	_
boosting	_	_
and	_	_
random	_	_
forest	_	_
classifiers	_	_
performed	_	_
well	_	_
in	_	_
dealing	_	_
with	_	_
samples	_	_
where	_	_
a	_	_
large	_	_
class	_	_
imbalance	_	_
was	_	_
present	_	_
.	_	_

#188
It	_	_
does	_	_
appear	_	_
that	_	_
in	_	_
extreme	_	_
cases	_	_
the	_	_
ability	_	_
of	_	_
random	_	_
forests	_	_
and	_	_
gradient	_	_
boosting	_	_
to	_	_
concentrate	_	_
on	_	_
'local	_	_
'	_	_
features	_	_
in	_	_
the	_	_
imbalanced	_	_
data	_	_
is	_	_
useful	_	_
.	_	_

#189
The	_	_
most	_	_
commonly	_	_
used	_	_
credit	_	_
scoring	_	_
techniques	_	_
,	_	_
linear	_	_
discriminant	_	_
analysis	_	_
(	_	_
LDA	_	_
)	_	_
and	_	_
logistic	_	_
regression	_	_
(	_	_
LOG	_	_
)	_	_
,	_	_
gave	_	_
results	_	_
that	_	_
were	_	_
reasonably	_	_
competitive	_	_
with	_	_
the	_	_
more	_	_
complex	_	_
techniques	_	_
and	_	_
this	_	_
competitive	_	_
performance	_	_
continued	_	_
even	_	_
when	_	_
the	_	_
samples	_	_
became	_	_
much	_	_
more	_	_
imbalanced	_	_
.	_	_

#190
This	_	_
would	_	_
suggest	_	_
that	_	_
the	_	_
currently	_	_
most	_	_
popular	_	_
approaches	_	_
are	_	_
fairly	_	_
robust	_	_
to	_	_
imbalanced	_	_
class	_	_
sizes	_	_
.	_	_

#191
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
techniques	_	_
such	_	_
as	_	_
QDA	_	_
and	_	_
C4.5	_	_
were	_	_
significantly	_	_
worse	_	_
than	_	_
the	_	_
best	_	_
performing	_	_
classifiers	_	_
.	_	_

#192
It	_	_
can	feasibility-rhetorical	_
also	_	_
be	_	_
concluded	_	_
that	_	_
the	_	_
use	_	_
of	_	_
a	_	_
linear	_	_
kernel	_	_
LS-SVM	_	_
would	_	_
not	_	_
be	_	_
beneficial	_	_
in	_	_
the	_	_
scoring	_	_
of	_	_
data	_	_
sets	_	_
where	_	_
a	_	_
very	_	_
large	_	_
class	_	_
imbalance	_	_
exists	_	_
.	_	_

#193
Further	_	_
work	_	_
that	_	_
could	feasibility-speculation	_
be	_	_
conducted	_	_
,	_	_
as	_	_
a	_	_
result	_	_
of	_	_
these	_	_
findings	_	_
,	_	_
would	_	_
be	_	_
to	_	_
firstly	_	_
consider	_	_
a	_	_
stacking	_	_
approach	_	_
to	_	_
classification	_	_
through	_	_
the	_	_
combination	_	_
of	_	_
multiple	_	_
techniques	_	_
.	_	_

#194
Such	_	_
an	_	_
approach	_	_
would	_	_
allow	_	_
a	_	_
meta-learner	_	_
to	_	_
pick	_	_
the	_	_
best	_	_
model	_	_
to	_	_
classify	_	_
an	_	_
observation	_	_
.	_	_

#195
Secondly	_	_
,	_	_
another	_	_
interesting	_	_
extension	_	_
to	_	_
the	_	_
research	_	_
would	_	_
be	_	_
to	_	_
apply	_	_
these	_	_
techniques	_	_
on	_	_
much	_	_
larger	_	_
data	_	_
sets	_	_
which	_	_
display	_	_
a	_	_
wider	_	_
variety	_	_
of	_	_
class	_	_
distributions	_	_
.	_	_

#196
It	_	_
would	_	_
also	_	_
be	_	_
of	_	_
interest	_	_
to	_	_
look	_	_
into	_	_
the	_	_
effect	_	_
of	_	_
not	_	_
only	_	_
the	_	_
percentage	_	_
class	_	_
distribution	_	_
but	_	_
also	_	_
the	_	_
effect	_	_
of	_	_
the	_	_
actual	_	_
number	_	_
of	_	_
observations	_	_
in	_	_
a	_	_
data	_	_
set	_	_
.	_	_

#197
Finally	_	_
,	_	_
as	_	_
stated	_	_
in	_	_
the	_	_
literature	_	_
review	_	_
section	_	_
of	_	_
this	_	_
paper	_	_
,	_	_
there	_	_
have	_	_
been	_	_
several	_	_
approaches	_	_
already	_	_
researched	_	_
in	_	_
the	_	_
area	_	_
of	_	_
over-sampling	_	_
techniques	_	_
to	_	_
deal	_	_
with	_	_
large	_	_
class	_	_
imbalances	_	_
.	_	_

#198
Further	_	_
research	_	_
into	_	_
this	_	_
and	_	_
their	_	_
effect	_	_
on	_	_
credit	_	_
scoring	_	_
model	_	_
performance	_	_
would	_	_
be	_	_
beneficial	_	_
.	_	_

#199
Acknowledgements	_	_
The	_	_
authors	_	_
of	_	_
this	_	_
paper	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
the	_	_
EPSRC	_	_
and	_	_
SAS	_	_
UK	_	_
for	_	_
their	_	_
financial	_	_
support	_	_
to	_	_
Iain	_	_
Brown	_	_
.	_	_