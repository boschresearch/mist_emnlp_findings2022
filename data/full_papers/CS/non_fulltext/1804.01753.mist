#0
Bringing	_	_
Cartoons	_	_
to	_	_
Life	_	_
:	_	_
Towards	_	_
Improved	_	_
Cartoon	_	_
Face	_	_
Detection	_	_
and	_	_
Recognition	_	_
Systems	_	_
Saurav	_	_
Jha	_	_
mail	_	_
@	_	_
sauravjha.com.np	_	_
MNNIT	_	_
Allahabad	_	_
,	_	_
India	_	_
Nikhil	_	_
Agarwal	_	_
nikil.agar	_	_
@	_	_
gmail.com	_	_
MNNIT	_	_
Allahabad	_	_
,	_	_
India	_	_
Suneeta	_	_
Agarwal	_	_
suneeta	_	_
@	_	_
mnnit.ac.in	_	_
MNNIT	_	_
Allahabad	_	_
,	_	_
India	_	_
ABSTRACT	_	_
Given	_	_
the	_	_
recent	_	_
deep	_	_
learning	_	_
advancements	_	_
in	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
techniques	_	_
for	_	_
human	_	_
faces	_	_
,	_	_
this	_	_
paper	_	_
answers	_	_
the	_	_
question	_	_
”how	_	_
well	_	_
would	_	_
they	_	_
work	_	_
for	_	_
cartoons’	_	_
?	_	_
”	_	_
-	_	_
a	_	_
domain	_	_
that	_	_
remains	_	_
largely	_	_
unexplored	_	_
until	_	_
recently	_	_
,	_	_
mainly	_	_
due	_	_
to	_	_
the	_	_
unavailability	_	_
of	_	_
large	_	_
scale	_	_
datasets	_	_
and	_	_
the	_	_
failure	_	_
of	_	_
traditional	_	_
methods	_	_
on	_	_
these	_	_
.	_	_

#1
Our	_	_
work	_	_
studies	_	_
and	_	_
extends	_	_
multiple	_	_
frameworks	_	_
for	_	_
the	_	_
aforementioned	_	_
tasks	_	_
.	_	_

#2
For	_	_
face	_	_
detection	_	_
,	_	_
we	_	_
incorporate	_	_
the	_	_
Multi-task	_	_
Cascaded	_	_
Convolutional	_	_
Network	_	_
(	_	_
MTCNN	_	_
)	_	_
architecture	_	_
[	_	_
1	_	_
]	_	_
and	_	_
contrast	_	_
it	_	_
with	_	_
conventional	_	_
methods	_	_
.	_	_

#3
For	_	_
face	_	_
recognition	_	_
,	_	_
our	_	_
two-fold	_	_
contributions	_	_
include	_	_
:	_	_
(	_	_
i	_	_
)	_	_
an	_	_
inductive	_	_
transfer	_	_
learning	_	_
approach	_	_
combining	_	_
the	_	_
feature	_	_
learning	_	_
capability	_	_
of	_	_
the	_	_
Inception	_	_
v3	_	_
network	_	_
[	_	_
2	_	_
]	_	_
and	_	_
the	_	_
feature	_	_
recognizing	_	_
capability	_	_
of	_	_
Support	_	_
Vector	_	_
Machines	_	_
(	_	_
SVMs	_	_
)	_	_
,	_	_
(	_	_
ii	_	_
)	_	_
a	_	_
proposed	_	_
Hybrid	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
(	_	_
HCNN	_	_
)	_	_
framework	_	_
trained	_	_
over	_	_
a	_	_
fusion	_	_
of	_	_
pixel	_	_
values	_	_
and	_	_
15	_	_
manually	_	_
located	_	_
facial	_	_
keypoints	_	_
.	_	_

#4
All	_	_
the	_	_
methods	_	_
are	_	_
evaluated	_	_
on	_	_
the	_	_
Cartoon	_	_
Faces	_	_
in	_	_
the	_	_
Wild	_	_
(	_	_
IIIT-CFW	_	_
)	_	_
database	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#5
We	_	_
demonstrate	_	_
that	_	_
the	_	_
HCNN	_	_
model	_	_
offers	_	_
stability	_	_
superior	_	_
to	_	_
that	_	_
of	_	_
Inception+SVM	_	_
over	_	_
larger	_	_
input	_	_
variations	_	_
,	_	_
and	_	_
explore	_	_
the	_	_
plausible	_	_
architectural	_	_
principles	_	_
.	_	_

#6
We	_	_
show	_	_
that	_	_
the	_	_
Inception+SVM	_	_
model	_	_
establishes	_	_
a	_	_
state-of-the-art	_	_
(	_	_
SOTA	_	_
)	_	_
F1	_	_
score	_	_
on	_	_
the	_	_
task	_	_
of	_	_
gender	_	_
recognition	_	_
of	_	_
cartoon	_	_
faces	_	_
.	_	_

#7
Further	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
small	_	_
database	_	_
hosting	_	_
location	_	_
coordinates	_	_
of	_	_
15	_	_
points	_	_
on	_	_
the	_	_
cartoon	_	_
faces	_	_
belonging	_	_
to	_	_
50	_	_
public	_	_
figures	_	_
of	_	_
the	_	_
IIIT-CFW	_	_
database	_	_
.	_	_

#8
Index	_	_
Terms	_	_
:	_	_
Cartoon	_	_
face	_	_
detection	_	_
and	_	_
recognition—MTCNN—	_	_
Inception	_	_
v3—Hybrid	_	_
CNN	_	_

#9
1	_	_
INTRODUCTION	_	_

#10
The	_	_
exponential	_	_
rise	_	_
in	_	_
digital	_	_
media	_	_
over	_	_
the	_	_
recent	_	_
years	_	_
has	_	_
in	_	_
turn	_	_
elevated	_	_
the	_	_
trend	_	_
of	_	_
cartoonifying	_	_
subjects	_	_
,	_	_
as	_	_
they	_	_
serve	_	_
to	_	_
inhabit	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
life	_	_
aspects	_	_
such	_	_
as	_	_
providing	_	_
home	_	_
schools	_	_
for	_	_
children	_	_
[	_	_
4	_	_
]	_	_
,	_	_
enhancing	_	_
the	_	_
teaching	_	_
process	_	_
and	_	_
academic	_	_
achievement	_	_
[	_	_
5	_	_
]	_	_
as	_	_
well	_	_
as	_	_
depicting	_	_
one’s	_	_
opinion	_	_
on	_	_
the	_	_
practices	_	_
of	_	_
society	_	_
(	_	_
through	_	_
political	_	_
cartoons	_	_
and	_	_
comics	_	_
journalism	_	_
)	_	_
.	_	_

#11
In	_	_
contrast	_	_
to	_	_
the	_	_
standard	_	_
drawings	_	_
,	_	_
the	_	_
subjects	_	_
appearing	_	_
in	_	_
cartoons1	_	_
possess	_	_
features	_	_
exaggerated	_	_
in	_	_
ways	_	_
that	_	_
often	_	_
lead	_	_
to	_	_
the	_	_
deviation	_	_
of	_	_
such	_	_
faces	_	_
from	_	_
the	_	_
implicit	_	_
humanly	_	_
attributes	_	_
(	_	_
e.g.	_	_
facial	_	_
symmetry	_	_
violation	_	_
,	_	_
unnatural	_	_
skin	_	_
tone	_	_
,	_	_
anomalous	_	_
facial	_	_
outline	_	_
,	_	_
etc	_	_
.	_	_
)	_	_

#12
presumed	_	_
by	_	_
most	_	_
of	_	_
the	_	_
benchmark	_	_
detection	_	_
[	_	_
6–8	_	_
]	_	_
and	_	_
recognition	_	_
techniques	_	_
[	_	_
9–11	_	_
]	_	_
.	_	_

#13
While	_	_
such	_	_
techniques	_	_
have	_	_
found	_	_
their	_	_
wide	_	_
usage	_	_
for	_	_
humans	_	_
in	_	_
day-to-day	_	_
appliances	_	_
such	_	_
as	_	_
biometric	_	_
scanners	_	_
and	_	_
healthcare	_	_
equipment	_	_
,	_	_
the	_	_
spectacular	_	_
rise	_	_
in	_	_
the	_	_
cartoon	_	_
industry	_	_
has	_	_
inflated	_	_
the	_	_
need	_	_
for	_	_
similar	_	_
techniques	_	_
for	_	_
cartoon	_	_
faces	_	_
with	_	_
some	_	_
prominent	_	_
applications	_	_
including	_	_
:	_	_
(	_	_
i	_	_
)	_	_
Incorporation	_	_
in	_	_
image	_	_
search	_	_
engines	_	_
for	_	_
searching	_	_
the	_	_
web	_	_
for	_	_
similar	_	_
cartoons	_	_
.	_	_

#14
(	_	_
ii	_	_
)	_	_
1By	_	_
cartoons	_	_
,	_	_
we	_	_
refer	_	_
to	_	_
cartoons	_	_
,	_	_
caricatures	_	_
,	_	_
and	_	_
comics	_	_
.	_	_

#15
Integration	_	_
with	_	_
screen	_	_
readers	_	_
to	_	_
assist	_	_
visually	_	_
impaired	_	_
people	_	_
understand	_	_
cartoon	_	_
movies	_	_
.	_	_

#16
(	_	_
iii	_	_
)	_	_
Help	_	_
content-control	_	_
softwares	_	_
censor	_	_
inappropriate	_	_
cartoon	_	_
images	_	_
on	_	_
social	_	_
media	_	_
.	_	_

#17
Our	_	_
work	_	_
targets	_	_
meeting	_	_
the	_	_
above	_	_
mentioned	_	_
goal	_	_
by	_	_
leveraging	_	_
deep	_	_
learning	_	_
systems	_	_
that	_	_
are	_	_
capable	_	_
of	_	_
more	_	_
accurately	_	_
detecting	_	_
and	_	_
recognizing	_	_
the	_	_
cartoon	_	_
faces	_	_
along	_	_
with	_	_
providing	_	_
stable	_	_
results	_	_
over	_	_
greater	_	_
artistic	_	_
variations	_	_
of	_	_
the	_	_
facial	_	_
features	_	_
.	_	_

#18
With	_	_
the	_	_
advent	_	_
of	_	_
large	_	_
scale	_	_
cartoon	_	_
databases	_	_
like	_	_
the	_	_
IIIT-CFW	_	_
database	_	_
[	_	_
3	_	_
]	_	_
(	_	_
Section	_	_
4.1	_	_
)	_	_
,	_	_
we	_	_
depict	_	_
that	_	_
such	_	_
systems	_	_
can	_	_
be	_	_
effectively	_	_
trained	_	_
and	_	_
evaluated	_	_
over	_	_
reasonably	_	_
adequate	_	_
samples	_	_
.	_	_

#19
To	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
these	_	_
are	_	_
the	_	_
first	_	_
ever	_	_
deep	_	_
neural	_	_
detection	_	_
and	_	_
recognition	_	_
systems	_	_
to	_	_
be	_	_
built	_	_
specifically	_	_
for	_	_
cartoon	_	_
faces	_	_
.	_	_

#20
The	_	_
contributions	_	_
of	_	_
our	_	_
work	_	_
are	_	_
four-fold	_	_
:	_	_
•	_	_
We	_	_
explore	_	_
,	_	_
incorporate	_	_
and	_	_
extend	_	_
some	_	_
of	_	_
the	_	_
current	_	_
research	_	_
on	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
into	_	_
our	_	_
framework	_	_
for	_	_
cartoon	_	_
faces	_	_
.	_	_

#21
•	_	_
For	_	_
face	_	_
detection	_	_
(	_	_
3.1	_	_
)	_	_
,	_	_
we	_	_
exploit	_	_
the	_	_
MTCNN	_	_
[	_	_
1	_	_
]	_	_
framework	_	_
,	_	_
and	_	_
contrast	_	_
its	_	_
performance	_	_
with	_	_
that	_	_
of	_	_
the	_	_
Histogram	_	_
of	_	_
Oriented	_	_
Gradients	_	_
(	_	_
HOG	_	_
)	_	_
features	_	_
[	_	_
12	_	_
]	_	_
and	_	_
Haar	_	_
features	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#22
Our	_	_
study	_	_
further	_	_
yields	_	_
substantial	_	_
improvements	_	_
over	_	_
the	_	_
SOTA	_	_
Jaw	_	_
contour	_	_
and	_	_
symmetry	_	_
based	_	_
cartoon	_	_
face	_	_
detector	_	_
[	_	_
14	_	_
]	_	_
,	_	_
as	_	_
described	_	_
in	_	_
Section	_	_
2	_	_
.	_	_

#23
•	_	_
For	_	_
face	_	_
recognition	_	_
(	_	_
Section	_	_
4.5	_	_
)	_	_
,	_	_
we	_	_
study	_	_
two	_	_
deep	_	_
neural	_	_
network	_	_
frameworks	_	_
:	_	_
the	_	_
former	_	_
architecture	_	_
employs	_	_
a	_	_
pre-trained	_	_
GoogleNet	_	_
Inception	_	_
v3	_	_
based	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
(	_	_
CNN	_	_
)	_	_
architecture	_	_
[	_	_
2	_	_
]	_	_
as	_	_
feature	_	_
extractor	_	_
assisted	_	_
by	_	_
SVM	_	_
[	_	_
15	_	_
]	_	_
and	_	_
Gradient	_	_
Boosting	_	_
(	_	_
GB	_	_
)	_	_
[	_	_
16	_	_
]	_	_
classifiers	_	_
as	_	_
feature	_	_
recognizer	_	_
,	_	_
whilst	_	_
the	_	_
latter	_	_
is	_	_
a	_	_
proposed	_	_
HCNN	_	_
framework	_	_
that	_	_
leverages	_	_
the	_	_
pixel	_	_
values	_	_
of	_	_
images	_	_
along	_	_
with	_	_
the	_	_
location	_	_
coordinates	_	_
of	_	_
15	_	_
facial	_	_
keypoints	_	_
in	_	_
an	_	_
end-to-end	_	_
fashion	_	_
.	_	_

#24
We	_	_
demonstrate	_	_
that	_	_
both	_	_
of	_	_
these	_	_
frameworks	_	_
outperform	_	_
the	_	_
SOTA	_	_
[	_	_
14	_	_
]	_	_
in	_	_
terms	_	_
of	_	_
F-measure	_	_
(	_	_
Section	_	_
4.5	_	_
)	_	_
.	_	_

#25
•	_	_
We	_	_
investigate	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
recognition	_	_
models	_	_
under	_	_
various	_	_
input	_	_
constraints	_	_
governed	_	_
by	_	_
a	_	_
number	_	_
of	_	_
metrics	_	_
for	_	_
classifying	_	_
characters	_	_
and	_	_
gender	_	_
of	_	_
the	_	_
cartoon	_	_
faces	_	_
.	_	_

#26
For	_	_
both	_	_
the	_	_
systems	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
effectiveness	_	_
of	_	_
the	_	_
key-points	_	_
extraction	_	_
mechanism	_	_
(	_	_
Section	_	_
4.4.2	_	_
)	_	_
and	_	_
empirically	_	_
show	_	_
that	_	_
the	_	_
inclusion	_	_
of	_	_
the	_	_
facial	_	_
keypoints	_	_
location	_	_
results	_	_
in	_	_
a	_	_
5.87	_	_
%	_	_
gain	_	_
on	_	_
the	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
the	_	_
HCNN	_	_
model	_	_
(	_	_
Section	_	_
4.5	_	_
)	_	_
.	_	_

#27
Our	_	_
further	_	_
analysis	_	_
demonstrate	_	_
that	_	_
while	_	_
the	_	_
HCNN	_	_
model	_	_
offers	_	_
enhanced	_	_
stability	_	_
over	_	_
the	_	_
Inception	_	_
v3+SVM	_	_
model	_	_
as	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
increase	_	_
,	_	_
the	_	_
Inception+SVM	_	_
model	_	_
thus	_	_
employed	_	_
on	_	_
the	_	_
gender	_	_
recognition	_	_
task	_	_
establishes	_	_
a	_	_
SOTA	_	_
F-measure	_	_
of	_	_
0.910	_	_
(	_	_
Section	_	_
4.6	_	_
)	_	_
.	_	_

#28
2	_	_
RELATED	_	_
WORKS	_	_

#29
A	_	_
greater	_	_
amount	_	_
of	_	_
the	_	_
previous	_	_
researches	_	_
on	_	_
the	_	_
subject	_	_
of	_	_
cartoon	_	_
and	_	_
comic	_	_
characters	_	_
recognition	_	_
revolve	_	_
around	_	_
the	_	_
task	_	_
of	_	_
classifying	_	_
image-level	_	_
video	_	_
into	_	_
cartoon/non-cartoon	_	_
genre	_	_
,	_	_
dating	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
4	_	_
.	_	_

#30
3v	_	_
2	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
6	_	_
J	_	_
ul	_	_
2	_	_
Figure	_	_
1	_	_
:	_	_
Inductive	_	_
transfer	_	_
framework	_	_
for	_	_
face	_	_
recognition	_	_
:	_	_
the	_	_
2048D	_	_
bottleneck	_	_
features	_	_
extracted	_	_
by	_	_
the	_	_
Inception	_	_
v3	_	_
model	_	_
are	_	_
fed	_	_
into	_	_
SVM	_	_
and	_	_
GB	_	_
classifiers	_	_
back	_	_
to	_	_
the	_	_
work	_	_
of	_	_
Glasberg	_	_
et	_	_
al.	_	_
[	_	_
17	_	_
]	_	_
classifying	_	_
100	_	_
MPEG-2	_	_
video	_	_
sequences	_	_
by	_	_
combining	_	_
the	_	_
visual	_	_
features	_	_
through	_	_
a	_	_
multilayered	_	_
perceptron	_	_
followed	_	_
by	_	_
a	_	_
fusion	_	_
of	_	_
the	_	_
concerned	_	_
audio	_	_
features	_	_
extracted	_	_
from	_	_
consecutive	_	_
frames	_	_
.	_	_

#31
Following	_	_
them	_	_
,	_	_
Glasberg	_	_
et	_	_
al.	_	_
[	_	_
18	_	_
]	_	_
used	_	_
a	_	_
probability-based	_	_
approach	_	_
employing	_	_
two	_	_
Hidden	_	_
Markov	_	_
Models	_	_
and	_	_
five	_	_
visual	_	_
descriptors	_	_
(	_	_
features	_	_
)	_	_
to	_	_
achieve	_	_
an	_	_
improved	_	_
performance	_	_
.	_	_

#32
Ionescu	_	_
et	_	_
al.	_	_
[	_	_
19	_	_
]	_	_
used	_	_
temporal	_	_
and	_	_
color	_	_
based	_	_
content	_	_
descriptors	_	_
to	_	_
achieve	_	_
image-level	_	_
classification	_	_
of	_	_
animated	_	_
contents	_	_
over	_	_
159	_	_
hours	_	_
of	_	_
video	_	_
footage	_	_
.	_	_

#33
However	_	_
,	_	_
the	_	_
task	_	_
of	_	_
cartoon	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
accounts	_	_
for	_	_
more	_	_
diverse	_	_
feature	_	_
contemplation	_	_
than	_	_
the	_	_
aforementioned	_	_
tasks	_	_
as	_	_
the	_	_
individual	_	_
cartoon	_	_
images	_	_
possess	_	_
greater	_	_
artistic	_	_
variations	_	_
among	_	_
each	_	_
other	_	_
compared	_	_
to	_	_
the	_	_
artistically	_	_
similar	_	_
characters	_	_
appearing	_	_
in	_	_
the	_	_
consecutive	_	_
video	_	_
frames	_	_
.	_	_

#34
Takayama	_	_
et	_	_
al.	_	_
[	_	_
14	_	_
]	_	_
presented	_	_
the	_	_
first	_	_
relevant	_	_
work	_	_
concerning	_	_
cartoon	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
.	_	_

#35
For	_	_
detection	_	_
,	_	_
they	_	_
used	_	_
jaw	_	_
contour	_	_
and	_	_
symmetry	_	_
as	_	_
two	_	_
criteria	_	_
to	_	_
evaluate	_	_
whether	_	_
a	_	_
segmented	_	_
region	_	_
based	_	_
upon	_	_
skin	_	_
color	_	_
and	_	_
edges	_	_
is	_	_
a	_	_
face	_	_
or	_	_
not	_	_
while	_	_
for	_	_
recognition	_	_
,	_	_
they	_	_
extracted	_	_
three	_	_
features	_	_
(	_	_
skin	_	_
color	_	_
,	_	_
hair	_	_
color	_	_
and	_	_
hair	_	_
quantity	_	_
)	_	_
from	_	_
each	_	_
image	_	_
and	_	_
distinguished	_	_
the	_	_
correct	_	_
input	_	_
image	_	_
class	_	_
based	_	_
on	_	_
the	_	_
similarity	_	_
of	_	_
feature	_	_
vectors	_	_
.	_	_

#36
Their	_	_
method	_	_
is	_	_
nonetheless	_	_
limited	_	_
to	_	_
color	_	_
images	_	_
with	_	_
skin	_	_
color	_	_
near	_	_
to	_	_
real	_	_
people	_	_
and	_	_
targets	_	_
mainly	_	_
frontal	_	_
posture	_	_
.	_	_

#37
Prior	_	_
to	_	_
this	_	_
,	_	_
packages	_	_
such	_	_
as	_	_
the	_	_
AnimeFace	_	_
20092	_	_
would	_	_
use	_	_
simple	_	_
perceptron	_	_
architectures	_	_
trained	_	_
over	_	_
millions	_	_
of	_	_
image	_	_
data	_	_
to	_	_
judge	_	_
whether	_	_
the	_	_
face	_	_
region	_	_
candidates	_	_
of	_	_
anime	_	_
faces	_	_
are	_	_
actually	_	_
faces	_	_
or	_	_
not	_	_
.	_	_

#38
Deep	_	_
learning	_	_
based	_	_
approaches	_	_
have	_	_
only	_	_
come	_	_
up	_	_
recently	_	_
.	_	_

#39
Nguyen	_	_
et	_	_
al.	_	_
[	_	_
20	_	_
]	_	_
performed	_	_
comic	_	_
characters	_	_
detection	_	_
by	_	_
applying	_	_
the	_	_
YOLOv2	_	_
model	_	_
[	_	_
21	_	_
]	_	_
to	_	_
predict	_	_
the	_	_
location	_	_
coordinates	_	_
of	_	_
the	_	_
bounding	_	_
boxes	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
location	_	_
of	_	_
the	_	_
SxS	_	_
cells	_	_
grid	_	_
formed	_	_
over	_	_
the	_	_
image	_	_
.	_	_

#40
The	_	_
Manga	_	_
FaceNet	_	_
proposed	_	_
by	_	_
Chu	_	_
and	_	_
Li	_	_
[	_	_
22	_	_
]	_	_
offers	_	_
a	_	_
CNN	_	_
based	_	_
architecture	_	_
for	_	_
detecting	_	_
Manga	_	_
faces	_	_
(	_	_
i.e.	_	_
,	_	_
Japanese	_	_
cartoons	_	_
and	_	_
comics	_	_
)	_	_
.	_	_

#41
Although	_	_
these	_	_
frameworks	_	_
present	_	_
good	_	_
baseline	_	_
on	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
of	_	_
cartoon	_	_
and	_	_
comic	_	_
characters	_	_
,	_	_
a	_	_
lot	_	_
of	_	_
the	_	_
state-of-the-art	_	_
methods	_	_
available	_	_
today	_	_
remain	_	_
unexplored	_	_
on	_	_
the	_	_
tasks	_	_
.	_	_

#42
3	_	_
METHODOLOGY	_	_

#43
Section	_	_
3.1	_	_
and	_	_
3.2	_	_
describe	_	_
the	_	_
detection	_	_
and	_	_
recognition	_	_
models	_	_
.	_	_

#44
The	_	_
cartoon	_	_
database	_	_
used	_	_
is	_	_
described	_	_
in	_	_
Section	_	_
4.1	_	_
.	_	_

#45
The	_	_
normalized	_	_
images	_	_
refer	_	_
to	_	_
96x96	_	_
resized	_	_
images	_	_
preserving	_	_
the	_	_
original	_	_
aspect	_	_
ratio	_	_
.	_	_

#46
3.1	_	_
Face	_	_
Detection	_	_

#47
The	_	_
MTCNN	_	_
[	_	_
1	_	_
]	_	_
architecture	_	_
offers	_	_
a	_	_
deep	_	_
cascaded	_	_
multi-task	_	_
framework	_	_
with	_	_
three	_	_
sequential	_	_
deep	_	_
CNNs	_	_
:	_	_
the	_	_
Proposal	_	_
Net	_	_
,	_	_
the	_	_
Residual	_	_
Net	_	_
and	_	_
the	_	_
Output	_	_
Net	_	_
.	_	_

#48
The	_	_
input	_	_
to	_	_
the	_	_
proposal	_	_
2https	_	_
:	_	_
//github.com/nagadomi/animeface-2009	_	_
net	_	_
is	_	_
an	_	_
image	_	_
pyramid	_	_
formed	_	_
by	_	_
resizing	_	_
the	_	_
input	_	_
image	_	_
to	_	_
different	_	_
scales	_	_
.	_	_

#49
Each	_	_
subsequent	_	_
layer	_	_
then	_	_
performs	_	_
candidate	_	_
window	_	_
calibration	_	_
using	_	_
the	_	_
estimated	_	_
bounding	_	_
box	_	_
regression	_	_
vectors	_	_
,	_	_
merges	_	_
the	_	_
highly	_	_
overlapped	_	_
candidates	_	_
,	_	_
thereby	_	_
outputting	_	_
a	_	_
final	_	_
face	_	_
bounding	_	_
box	_	_
with	_	_
five	_	_
facial	_	_
landmarks’	_	_
positions	_	_
.	_	_

#50
Compared	_	_
to	_	_
its	_	_
precursor	_	_
[	_	_
23	_	_
]	_	_
,	_	_
MTCNN	_	_
avails	_	_
three	_	_
major	_	_
tweaks	_	_
:	_	_
reduced	_	_
number	_	_
of	_	_
filters	_	_
and	_	_
smaller	_	_
filter	_	_
sizes	_	_
(	_	_
3x3	_	_
)	_	_
lessen	_	_
the	_	_
computational	_	_
burden	_	_
while	_	_
increased	_	_
depth	_	_
of	_	_
network	_	_
improves	_	_
the	_	_
performance	_	_
.	_	_

#51
We	_	_
further	_	_
employ	_	_
the	_	_
Haar	_	_
features	_	_
[	_	_
13	_	_
]	_	_
and	_	_
the	_	_
HOG	_	_
features	_	_
[	_	_
12	_	_
]	_	_
based	_	_
detectors	_	_
for	_	_
securing	_	_
baseline	_	_
results	_	_
in	_	_
order	_	_
to	_	_
gain	_	_
deeper	_	_
insights	_	_
into	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
MTCNN	_	_
framework	_	_
.	_	_

#52
3.2	_	_
Face	_	_
Recognition	_	_

#53
We	_	_
experiment	_	_
on	_	_
two	_	_
different	_	_
face	_	_
recognition	_	_
techniques	_	_
:	_	_

#54
3.2.1	_	_
Inductive	_	_
transfer	_	_
using	_	_
Inception	_	_
v3	_	_
+	_	_
SVM/GB	_	_

#55
We	_	_
employ	_	_
the	_	_
GoogleNet	_	_
Inception	_	_
v3	_	_
architecture	_	_
[	_	_
2	_	_
]	_	_
pre-trained	_	_
on	_	_
the	_	_
ImageNet	_	_
Large	_	_
Scale	_	_
Visual	_	_
Recognition	_	_
Challenge’s	_	_
2012	_	_
(	_	_
ILSVRC2012	_	_
)	_	_
database	_	_
for	_	_
extracting	_	_
2048	_	_
dimensional	_	_
features	_	_
from	_	_
each	_	_
normalized	_	_
image	_	_
.	_	_

#56
The	_	_
Inception	_	_
architectures	_	_
are	_	_
well-known	_	_
for	_	_
availing	_	_
auxiliary	_	_
classifiers	_	_
(	_	_
a.k.a	_	_
.	_	_

#57
side-heads	_	_
)	_	_
in	_	_
addition	_	_
to	_	_
the	_	_
main	_	_
classifier	_	_
for	_	_
achieving	_	_
more	_	_
stable	_	_
learning	_	_
in	_	_
the	_	_
latter	_	_
training	_	_
stages	_	_
.	_	_

#58
The	_	_
major	_	_
tweak	_	_
in	_	_
the	_	_
Inception	_	_
v3	_	_
network	_	_
(	_	_
compared	_	_
to	_	_
its	_	_
antecedent	_	_
[	_	_
24	_	_
]	_	_
)	_	_
lies	_	_
in	_	_
factoring	_	_
the	_	_
first	_	_
7x7	_	_
convolutional	_	_
layer	_	_
into	_	_
a	_	_
sequence	_	_
of	_	_
3x3	_	_
convolutional	_	_
layers	_	_
followed	_	_
by	_	_
a	_	_
batch-normalization	_	_
(	_	_
BN	_	_
)	_	_
of	_	_
the	_	_
fully	_	_
connected	_	_
layer	_	_
of	_	_
auxiliary	_	_
classifier	_	_
.	_	_

#59
Such	_	_
factorization	_	_
offers	_	_
significant	_	_
computational	_	_
cost	_	_
savings	_	_
through	_	_
sharing	_	_
of	_	_
weights	_	_
between	_	_
the	_	_
sub-layers	_	_
while	_	_
the	_	_
BN	_	_
serves	_	_
as	_	_
a	_	_
good	_	_
regularizer	_	_
.	_	_

#60
The	_	_
potency	_	_
of	_	_
these	_	_
adaptions	_	_
can	_	_
be	_	_
evaluated	_	_
by	_	_
the	_	_
fact	_	_
that	_	_
these	_	_
architectures	_	_
have	_	_
achieved	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
3.46	_	_
%	_	_
on	_	_
the	_	_
ILSVRC2012’s	_	_
validation	_	_
data	_	_
beating	_	_
the	_	_
human-level3	_	_
error	_	_
rate	_	_
of	_	_
5.1	_	_
%	_	_
.	_	_

#61
The	_	_
bottleneck	_	_
features	_	_
retrieved	_	_
from	_	_
the	_	_
antepenultimate	_	_
layer	_	_
of	_	_
the	_	_
Inception	_	_
v3	_	_
architecture	_	_
are	_	_
used	_	_
to	_	_
train	_	_
a	_	_
SVM	_	_
and	_	_
a	_	_
GB	_	_
classifier	_	_
as	_	_
final	_	_
recognizers	_	_
(	_	_
Figure	_	_
1	_	_
)	_	_
.	_	_

#62
Our	_	_
idea	_	_
of	_	_
replacement	_	_
of	_	_
the	_	_
softmax	_	_
layer	_	_
as	_	_
the	_	_
final	_	_
classifier	_	_
is	_	_
derived	_	_
from	_	_
several	_	_
image	_	_
classification	_	_
works	_	_
[	_	_
25	_	_
,	_	_
26	_	_
]	_	_
wherein	_	_
,	_	_
the	_	_
use	_	_
of	_	_
third-party	_	_
classifiers	_	_
have	_	_
been	_	_
known	_	_
to	_	_
advance	_	_
the	_	_
generalization	_	_
capacity	_	_
of	_	_
the	_	_
original	_	_
models	_	_
.	_	_

#63
3.2.2	_	_
Proposed	_	_
Method	_	_

#64
Our	_	_
method	_	_
for	_	_
cartoon	_	_
face	_	_
recognition	_	_
can	_	_
be	_	_
described	_	_
in	_	_
two	_	_
major	_	_
phases	_	_
,	_	_
as	_	_
depicted	_	_
in	_	_
Figure	_	_
2a	_	_
.	_	_

#65
Phase	_	_
I	_	_
deals	_	_
with	_	_
annotation	_	_
of	_	_
the	_	_
coordinates	_	_
of	_	_
15	_	_
facial	_	_
keypoints	_	_
over	_	_
following	_	_
steps	_	_
:	_	_
3https	_	_
:	_	_
//karpathy.github.io/2014/09/02/whatilearnedfromcompetingagainstaconvnetonimagenet/	_	_
(	_	_
a	_	_
)	_	_
Recognition	_	_
framework	_	_
:	_	_
P	_	_
(	_	_
Ci	_	_
)	_	_
indicates	_	_
the	_	_
probability	_	_
predicted	_	_
for	_	_
the	_	_
ith	_	_
class	_	_
(	_	_
b	_	_
)	_	_
Proposed	_	_
HCNN	_	_
classifier	_	_
Figure	_	_
2	_	_
:	_	_
Schematic	_	_
diagram	_	_
of	_	_
the	_	_
proposed	_	_
framework	_	_
built	_	_
upon	_	_
the	_	_
landmark	_	_
extraction	_	_
concept	_	_
and	_	_
the	_	_
HCNN	_	_
classifier	_	_
employed	_	_
within	_	_
.	_	_

#66
1	_	_
.	_	_

#67
Pre-processing	_	_
:	_	_
The	_	_
cartoon	_	_
images	_	_
are	_	_
first	_	_
grayscaled	_	_
and	_	_
normalized	_	_
.	_	_

#68
2	_	_
.	_	_

#69
Landmark	_	_
extraction	_	_
:	_	_
The	_	_
location	_	_
coordinates	_	_
of	_	_
15	_	_
facial	_	_
landmarks	_	_
of	_	_
750	_	_
normalized	_	_
images	_	_
belonging	_	_
to	_	_
50	_	_
cartoon	_	_
classes	_	_
(	_	_
15	_	_
for	_	_
each	_	_
)	_	_
were	_	_
manually	_	_
annotated	_	_
by	_	_
three	_	_
undergraduates	_	_
using	_	_
a	_	_
Java	_	_
swing	_	_
application	_	_
.	_	_

#70
The	_	_
images	_	_
were	_	_
selected	_	_
irrespective	_	_
of	_	_
their	_	_
facial	_	_
posture	_	_
.	_	_

#71
Any	_	_
absent	_	_
or	_	_
unlocatable	_	_
keypoint	_	_
was	_	_
assigned	_	_
a	_	_
null	_	_
value	_	_
.	_	_

#72
The	_	_
landmarks	_	_
follow	_	_
the	_	_
ordering	_	_
listed	_	_
in	_	_
the	_	_
Kaggle	_	_
Facial	_	_
Keypoints	_	_
Detection	_	_
Challenge4	_	_
database	_	_
:	_	_
left	_	_
eye	_	_
center	_	_
,	_	_
right	_	_
eye	_	_
center	_	_
,	_	_
left	_	_
eye	_	_
inner	_	_
corner	_	_
,	_	_
left	_	_
eye	_	_
outer	_	_
corner	_	_
,	_	_
right	_	_
eye	_	_
inner	_	_
corner	_	_
,	_	_
right	_	_
eye	_	_
outer	_	_
corner	_	_
,	_	_
left	_	_
eyebrow	_	_
inner	_	_
end	_	_
,	_	_
left	_	_
eyebrow	_	_
outer	_	_
end	_	_
,	_	_
right	_	_
eyebrow	_	_
inner	_	_
end	_	_
,	_	_
right	_	_
eyebrow	_	_
outer	_	_
end	_	_
,	_	_
nose	_	_
tip	_	_
,	_	_
mouth	_	_
left	_	_
corner	_	_
,	_	_
mouth	_	_
right	_	_
corner	_	_
,	_	_
mouth	_	_
center	_	_
top	_	_
lip	_	_
,	_	_
mouth	_	_
center	_	_
bottom	_	_
lip	_	_
For	_	_
verification	_	_
of	_	_
the	_	_
annotations	_	_
,	_	_
each	_	_
of	_	_
the	_	_
coordinates	_	_
were	_	_
cross-checked	_	_
to	_	_
be	_	_
within	_	_
the	_	_
valid	_	_
range5	_	_
.	_	_

#73
(	_	_
Database	_	_
in	_	_
supplementary	_	_
materials	_	_
.	_	_
)	_	_

#74
3	_	_
.	_	_

#75
Landmark	_	_
detection	_	_
:	_	_
For	_	_
predicting	_	_
the	_	_
landmarks	_	_
of	_	_
rest	_	_
of	_	_
the	_	_
cartoon	_	_
images	_	_
in	_	_
the	_	_
database	_	_
,	_	_
we	_	_
employ	_	_
the	_	_
5-layer	_	_
LeNet	_	_
architecture	_	_
with	_	_
dropout	_	_
,	_	_
as	_	_
described	_	_
in	_	_
Longpre	_	_
and	_	_
Sohmshetty	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#76
Since	_	_
,	_	_
the	_	_
labelled	_	_
landmark	_	_
data	_	_
for	_	_
cartoon	_	_
faces	_	_
is	_	_
too	_	_
small	_	_
for	_	_
training	_	_
the	_	_
architecture	_	_
,	_	_
we	_	_
further	_	_
merge	_	_
it	_	_
with	_	_
2000	_	_
real	_	_
human	_	_
instances	_	_
of	_	_
the	_	_
Kaggle’s	_	_
database	_	_
.	_	_

#77
4https	_	_
:	_	_
//www.kaggle.com/c/facial-keypoints-detection	_	_
5Valid	_	_
range	_	_
for	_	_
each	_	_
feature	_	_
was	_	_
decided	_	_
using	_	_
the	_	_
mean	_	_
value	_	_
for	_	_
that	_	_
column	_	_
.	_	_

#78
Given	_	_
the	_	_
landmark	_	_
positions	_	_
of	_	_
each	_	_
image	_	_
,	_	_
phase	_	_
II	_	_
then	_	_
leverages	_	_
these	_	_
for	_	_
face	_	_
recognition	_	_
using	_	_
a	_	_
proposed	_	_
CNN	_	_
model	_	_
,	_	_
which	_	_
we	_	_
refer	_	_
to	_	_
as	_	_
the	_	_
hybrid	_	_
CNN	_	_
(	_	_
HCNN	_	_
)	_	_
model	_	_
.	_	_

#79
The	_	_
HCNN	_	_
model	_	_
,	_	_
as	_	_
depicted	_	_
in	_	_
Figure	_	_
2b	_	_
,	_	_
adapts	_	_
the	_	_
abstract	_	_
stack	_	_
structure	_	_
of	_	_
LeNet	_	_
architectures	_	_
.	_	_

#80
The	_	_
model	_	_
architecture	_	_
was	_	_
chosen	_	_
through	_	_
rigorous	_	_
experiments	_	_
.	_	_

#81
The	_	_
inputs	_	_
comprise	_	_
of	_	_
pixel	_	_
values	_	_
of	_	_
the	_	_
normalized	_	_
image	_	_
and	_	_
the	_	_
30	_	_
features	_	_
forming	_	_
the	_	_
location	_	_
coordinates	_	_
.	_	_

#82
As	_	_
practised	_	_
in	_	_
[	_	_
27	_	_
]	_	_
,	_	_
we	_	_
employ	_	_
four	_	_
stacks	_	_
of	_	_
alternating	_	_
convolution	_	_
and	_	_
max	_	_
pooling	_	_
layers	_	_
.	_	_

#83
The	_	_
filter	_	_
shapes	_	_
of	_	_
the	_	_
convolutional	_	_
layers	_	_
descend	_	_
from	_	_
Conv2D1	_	_
with	_	_
(	_	_
4,4	_	_
)	_	_
to	_	_
Conv2D4	_	_
with	_	_
(	_	_
1,1	_	_
)	_	_
while	_	_
all	_	_
the	_	_
maxpooling	_	_
layers	_	_
have	_	_
a	_	_
pool	_	_
shape	_	_
of	_	_
(	_	_
2,2	_	_
)	_	_
with	_	_
non-overlapping	_	_
strides	_	_
and	_	_
without	_	_
zero	_	_
padding	_	_
.	_	_

#84
The	_	_
output	_	_
of	_	_
the	_	_
Conv2D	_	_
stack	_	_
is	_	_
fed	_	_
to	_	_
two	_	_
subsequent	_	_
feed	_	_
forward	_	_
layers	_	_
following	_	_
which	_	_
,	_	_
an	_	_
additional	_	_
such	_	_
layer	_	_
with	_	_
softmax	_	_
activation	_	_
serves	_	_
as	_	_
the	_	_
auxiliary	_	_
classifier	_	_
with	_	_
a	_	_
discounted	_	_
loss	_	_
of	_	_
0.60	_	_
added	_	_
to	_	_
the	_	_
total	_	_
training	_	_
loss	_	_
whereas	_	_
,	_	_
three	_	_
more	_	_
such	_	_
dense	_	_
layers	_	_
process	_	_
the	_	_
concatenated	_	_
inputs	_	_
to	_	_
form	_	_
the	_	_
main	_	_
classifier	_	_
.	_	_

#85
All	_	_
the	_	_
Conv2D	_	_
layers	_	_
use	_	_
Leaky	_	_
Rectified	_	_
Linear	_	_
Unit	_	_
(	_	_
ReLu	_	_
)	_	_
activations	_	_
[	_	_
28	_	_
]	_	_
to	_	_
resolve	_	_
the	_	_
problem	_	_
of	_	_
dead	_	_
ReLus	_	_
seen	_	_
during	_	_
initial	_	_
test	_	_
runs	_	_
.	_	_

#86
All	_	_
but	_	_
the	_	_
final	_	_
dense	_	_
layers	_	_
employ	_	_
linear	_	_
activations	_	_
so	_	_
that	_	_
the	_	_
non-linear	_	_
outputs	_	_
of	_	_
the	_	_
previous	_	_
layers	_	_
do	_	_
not	_	_
remain	_	_
restricted	_	_
to	_	_
the	_	_
positive	_	_
domain	_	_
.	_	_

#87
We	_	_
apply	_	_
BN	_	_
[	_	_
29	_	_
]	_	_
in	_	_
between	_	_
the	_	_
network	_	_
layers	_	_
and	_	_
their	_	_
activations	_	_
.	_	_

#88
Backed	_	_
by	_	_
successive	_	_
experiments	_	_
,	_	_
we	_	_
drop	_	_
the	_	_
dropout	_	_
layers	_	_
throughout	_	_
the	_	_
inner	_	_
layers	_	_
of	_	_
the	_	_
network	_	_
as	_	_
their	_	_
combination	_	_
with	_	_
BN	_	_
further	_	_
degraded	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
model	_	_
(	_	_
Section	_	_
4.5.1	_	_
)	_	_
.	_	_

#89
Dropouts	_	_
are	_	_
applied	_	_
only	_	_
to	_	_
the	_	_
feed	_	_
forward	_	_
layers	_	_
right	_	_
before	_	_
the	_	_
final	_	_
softmax	_	_
activations	_	_
considering	_	_
that	_	_
there	_	_
are	_	_
no	_	_
subsequent	_	_
layers	_	_
incorporating	_	_
BN	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#90
A	_	_
shortcut	_	_
connection	_	_
[	_	_
31	_	_
]	_	_
concatenates	_	_
the	_	_
outputs	_	_
of	_	_
the	_	_
final	_	_
convolutional	_	_
layer	_	_
with	_	_
that	_	_
of	_	_
the	_	_
landmark	_	_
inputs	_	_
.	_	_

#91
Each	_	_
convolutional	_	_
layer	_	_
is	_	_
assigned	_	_
a	_	_
randomly	_	_
initialized	_	_
weight	_	_
.	_	_

#92
The	_	_
weights	_	_
of	_	_
all	_	_
the	_	_
dense	_	_
layers	_	_
are	_	_
initialized	_	_
using	_	_
the	_	_
Glorot	_	_
uniform	_	_
initialization	_	_
[	_	_
32	_	_
]	_	_
.	_	_

#93
The	_	_
main	_	_
classifier	_	_
uses	_	_
the	_	_
Adam	_	_
optimization	_	_
[	_	_
33	_	_
]	_	_
technique	_	_
with	_	_
a	_	_
learning	_	_
rate	_	_
of	_	_
0.001	_	_
,	_	_
beta1	_	_
of	_	_
0.9	_	_
,	_	_
beta2	_	_
of	_	_
0.999	_	_
,	_	_
epsilon	_	_
of	_	_
10-8	_	_
while	_	_
the	_	_
auxiliary	_	_
classifier	_	_
employs	_	_
the	_	_
Stochastic	_	_
Gradient	_	_
Descent	_	_
optimization	_	_
[	_	_
34	_	_
]	_	_
with	_	_
a	_	_
Nesterov	_	_
momentum	_	_
of	_	_
0.9	_	_
,	_	_
a	_	_
weight	_	_
decay	_	_
of	_	_
0.0001	_	_
,	_	_
and	_	_
a	_	_
learning	_	_
rate	_	_
starting	_	_
from	_	_
0.2	_	_
and	_	_
being	_	_
divided	_	_
by	_	_
10	_	_
as	_	_
the	_	_
error	_	_
plateaus	_	_
over	_	_
1,000	_	_
epochs	_	_
.	_	_

#94
Both	_	_
the	_	_
classifiers	_	_
minimize	_	_
the	_	_
categorical	_	_
cross-entropy	_	_
loss	_	_
throughout	_	_
training	_	_
.	_	_

#95
4	_	_
EXPERIMENTAL	_	_
SETTINGS	_	_

#96
Experiments	_	_
were	_	_
carried	_	_
out	_	_
from	_	_
two	_	_
points	_	_
of	_	_
view	_	_
to	_	_
investigate	_	_
the	_	_
performance	_	_
of	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
models	_	_
independently	_	_
.	_	_

#97
Our	_	_
system	_	_
consists	_	_
of	_	_
x86	_	_
64	_	_
GNU/Linux	_	_
with	_	_
8G	_	_
memory	_	_
using	_	_
one	_	_
NVIDIA	_	_
GeForce	_	_
840M	_	_
with	_	_
CUDA	_	_
V8.0.61	_	_
,	_	_
Tensorflow-1.4.1	_	_
,	_	_
Keras-2.0.6	_	_
,	_	_
and	_	_
OpenCV	_	_
V3.4.0	_	_
.	_	_

#98
The	_	_
2048-D	_	_
feature	_	_
vectors	_	_
extracted	_	_
from	_	_
the	_	_
Inception	_	_
v3	_	_
model	_	_
are	_	_
normalized	_	_
into	_	_
[	_	_
0,1	_	_
]	_	_
range	_	_
using	_	_
min-max	_	_
normalization	_	_
.	_	_

#99
Table	_	_
1	_	_
describes	_	_
the	_	_
best	_	_
model	_	_
parameters	_	_
of	_	_
the	_	_
SVM	_	_
and	_	_
the	_	_
GB	_	_
classifiers	_	_
determined	_	_
using	_	_
the	_	_
Grid	_	_
Search	_	_
algorithm	_	_
[	_	_
35	_	_
]	_	_
over	_	_
10-fold	_	_
cross	_	_
validation	_	_
on	_	_
100	_	_
input	_	_
classes	_	_
.	_	_

#100
Table	_	_
1	_	_
:	_	_
Settings	_	_
for	_	_
SVM	_	_
and	_	_
GB	_	_
assisted	_	_
model	_	_
parameters	_	_
Classifier	_	_
Parameter	_	_
Setting	_	_
SVM	_	_
Penalty	_	_
parameter	_	_
=	_	_
50	_	_
,	_	_
Kernel	_	_
=	_	_
Radial	_	_
basis	_	_
function	_	_
(	_	_
RBF	_	_
)	_	_
,	_	_
Kernel	_	_
coefficient	_	_
=	_	_
10-3	_	_
,	_	_
Probability	_	_
estimates	_	_
=	_	_
enabled	_	_
GB	_	_
Loss	_	_
function	_	_
=	_	_
’deviance’	_	_
for	_	_
probabilistic	_	_
outputs	_	_
,	_	_
Shrinkage	_	_
contribution	_	_
of	_	_
each	_	_
tree	_	_
=	_	_
0.08	_	_
,	_	_
Maximum	_	_
depth	_	_
of	_	_
the	_	_
individual	_	_
regression	_	_
estimators	_	_
=	_	_
3	_	_
,	_	_
No	_	_
.	_	_

#101
of	_	_
boosting	_	_
stages	_	_
=	_	_
100	_	_

#102
4.1	_	_
Datasets	_	_

#103
We	_	_
use	_	_
the	_	_
benchmark	_	_
IIIT-CFW	_	_
(	_	_
Cartoon	_	_
Faces	_	_
in	_	_
the	_	_
Wild	_	_
)	_	_
database	_	_
containing	_	_
8,928	_	_
annotated	_	_
images	_	_
of	_	_
cartoon	_	_
faces	_	_
belonging	_	_
to	_	_
100	_	_
global	_	_
public	_	_
figu	_	_
res	_	_
.	_	_

#104
The	_	_
annotations	_	_
consist	_	_
of	_	_
face	_	_
bounding	_	_
boxes	_	_
estimated	_	_
manually	_	_
along	_	_
with	_	_
some	_	_
additional	_	_
attributes	_	_
such	_	_
as	_	_
age	_	_
group	_	_
,	_	_
view	_	_
,	_	_
expression	_	_
,	_	_
pose	_	_
etc	_	_
.	_	_

#105
for	_	_
each	_	_
image	_	_
.	_	_

#106
For	_	_
the	_	_
character	_	_
and	_	_
gender	_	_
recognition	_	_
tasks	_	_
,	_	_
we	_	_
carry	_	_
out	_	_
80:20	_	_
train	_	_
:	_	_
test	_	_
splits	_	_
based	_	_
on	_	_
class-wise	_	_
and	_	_
gender-wise	_	_
manners	_	_
respectively	_	_
.	_	_

#107
A	_	_
validation	_	_
split	_	_
of	_	_
0.1	_	_
is	_	_
further	_	_
made	_	_
on	_	_
the	_	_
train	_	_
sets	_	_
.	_	_

#108
Considering	_	_
the	_	_
insufficient	_	_
cartoon	_	_
face	_	_
instances	_	_
in	_	_
IIIT-CFW	_	_
database	_	_
,	_	_
we	_	_
use	_	_
the	_	_
CASIA	_	_
WebFace	_	_
Database6	_	_
for	_	_
training	_	_
the	_	_
MTCNN	_	_
model	_	_
.	_	_

#109
The	_	_
Haar	_	_
feature-based	_	_
Cascade	_	_
Classifier	_	_
was	_	_
trained	_	_
using	_	_
3,000	_	_
positive	_	_
and	_	_
negative	_	_
normalized	_	_
images	_	_
for	_	_
50	_	_
stages	_	_
.	_	_

#110
Positive	_	_
samples	_	_
comprised	_	_
entirely	_	_
of	_	_
the	_	_
cartoon	_	_
images	_	_
while	_	_
the	_	_
negative	_	_
samples	_	_
were	_	_
a	_	_
blend	_	_
of	_	_
750	_	_
images	_	_
of	_	_
fish	_	_
,	_	_
flower	_	_
,	_	_
utensils	_	_
and	_	_
beverages	_	_
each	_	_
,	_	_
extracted	_	_
from	_	_
the	_	_
ImageNet	_	_
URL	_	_
links	_	_
.	_	_

#111
4.2	_	_
Data	_	_
Augmentation	_	_

#112
The	_	_
unequal	_	_
instances	_	_
of	_	_
cartoon	_	_
characters	_	_
provided	_	_
in	_	_
the	_	_
IIIT-CFW	_	_
database	_	_
introduces	_	_
class-imbalance	_	_
with	_	_
the	_	_
number	_	_
of	_	_
images	_	_
varying	_	_
from	_	_
as	_	_
many	_	_
as	_	_
299	_	_
to	_	_
as	_	_
few	_	_
as	_	_
11	_	_
for	_	_
different	_	_
celebrities	_	_
.	_	_

#113
A	_	_
similar	_	_
problem	_	_
persists	_	_
in	_	_
the	_	_
case	_	_
of	_	_
gender-wise	_	_
6http	_	_
:	_	_
//www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html	_	_
split	_	_
whereby	_	_
,	_	_
the	_	_
number	_	_
of	_	_
male	_	_
and	_	_
female	_	_
faces	_	_
in	_	_
the	_	_
train	_	_
set	_	_
amount	_	_
to	_	_
5,242	_	_
and	_	_
1,896	_	_
respectively	_	_
.	_	_

#114
It	_	_
is	_	_
worth	_	_
noting	_	_
that	_	_
the	_	_
train	_	_
sets	_	_
integrate	_	_
the	_	_
real	_	_
human	_	_
faces	_	_
of	_	_
celebrities	_	_
included	_	_
within	_	_
the	_	_
database	_	_
.	_	_

#115
While	_	_
we	_	_
leave	_	_
the	_	_
test	_	_
sets	_	_
unchanged	_	_
,	_	_
we	_	_
take	_	_
the	_	_
following	_	_
measures	_	_
to	_	_
attenuate	_	_
the	_	_
effect	_	_
of	_	_
biased	_	_
training	_	_
of	_	_
the	_	_
recognition	_	_
models	_	_
due	_	_
to	_	_
the	_	_
imbalance	_	_
:	_	_
1	_	_
.	_	_

#116
For	_	_
the	_	_
character	_	_
recognition	_	_
task	_	_
,	_	_
each	_	_
class	_	_
is	_	_
allowed	_	_
to	_	_
have	_	_
a	_	_
maximum	_	_
of	_	_
800	_	_
and	_	_
a	_	_
minimum	_	_
of	_	_
600	_	_
instances	_	_
prior	_	_
to	_	_
training	_	_
.	_	_

#117
The	_	_
images	_	_
are	_	_
over-sampled	_	_
using	_	_
a	_	_
blend	_	_
of	_	_
three	_	_
augmentation	_	_
steps	_	_
:	_	_
horizontal	_	_
and/or	_	_
vertical	_	_
flip	_	_
of	_	_
the	_	_
images	_	_
followed	_	_
by	_	_
horizontal	_	_
and/or	_	_
vertical	_	_
shifts	_	_
,	_	_
and	_	_
rotation	_	_
of	_	_
images	_	_
.	_	_

#118
The	_	_
shifts	_	_
are	_	_
performed	_	_
in	_	_
a	_	_
range	_	_
of	_	_
30	_	_
%	_	_
of	_	_
the	_	_
original	_	_
width	_	_
and	_	_
height	_	_
of	_	_
the	_	_
images	_	_
while	_	_
the	_	_
rotation	_	_
range	_	_
is	_	_
within	_	_
30	_	_
degrees	_	_
.	_	_

#119
For	_	_
training	_	_
the	_	_
HCNN	_	_
model	_	_
,	_	_
the	_	_
coordinate	_	_
locations	_	_
of	_	_
the	_	_
landmark	_	_
points	_	_
are	_	_
adjusted	_	_
accordingly	_	_
.	_	_

#120
2	_	_
.	_	_

#121
For	_	_
the	_	_
gender	_	_
recognition	_	_
task	_	_
,	_	_
the	_	_
same	_	_
oversampling	_	_
steps	_	_
are	_	_
applied	_	_
to	_	_
the	_	_
female	_	_
faces	_	_
until	_	_
the	_	_
ratio	_	_
of	_	_
male	_	_
:	_	_
female	_	_
instances	_	_
become	_	_
1:1	_	_
.	_	_

#122
4.3	_	_
Face	_	_
Detection	_	_
Results	_	_

#123
We	_	_
use	_	_
the	_	_
bounding	_	_
box	_	_
information	_	_
provided	_	_
in	_	_
the	_	_
annotations	_	_
as	_	_
ground	_	_
truth	_	_
values	_	_
.	_	_

#124
The	_	_
implementation	_	_
of	_	_
Haar	_	_
features-based	_	_
detector	_	_
relies	_	_
upon	_	_
OpenCV	_	_
while	_	_
that	_	_
of	_	_
HOG	_	_
features-based	_	_
detector	_	_
is	_	_
based	_	_
upon	_	_
the	_	_
dlib	_	_
library	_	_
.	_	_

#125
4.3.1	_	_
Evaluation	_	_
Measures	_	_

#126
We	_	_
count	_	_
on	_	_
three	_	_
evaluation	_	_
measures	_	_
for	_	_
the	_	_
detection	_	_
systems	_	_
:	_	_
True	_	_
Positive	_	_
Rate	_	_
(	_	_
TPR	_	_
)	_	_
relates	_	_
having	_	_
detected	_	_
a	_	_
right	_	_
facial	_	_
region	_	_
,	_	_
False	_	_
Positive	_	_
Rate	_	_
(	_	_
FPR	_	_
)	_	_
relates	_	_
having	_	_
detected	_	_
wrong	_	_
regions	_	_
in	_	_
addition	_	_
to	_	_
a	_	_
right	_	_
face	_	_
region	_	_
,	_	_
and	_	_
False	_	_
Negative	_	_
Rate	_	_
(	_	_
FNR	_	_
)	_	_
relates	_	_
not	_	_
having	_	_
detected	_	_
a	_	_
right	_	_
face	_	_
region	_	_
.	_	_

#127
Table	_	_
2	_	_
shows	_	_
a	_	_
comparison	_	_
of	_	_
the	_	_
results	_	_
obtained	_	_
by	_	_
the	_	_
employed	_	_
models	_	_
.	_	_

#128
For	_	_
a	_	_
fair	_	_
comparison	_	_
with	_	_
the	_	_
state-of-the-art	_	_
(	_	_
Takayama	_	_
et	_	_
al	_	_
.	_	_
)	_	_

#129
[	_	_
14	_	_
]	_	_
,	_	_
we	_	_
compute	_	_
the	_	_
rates	_	_
by	_	_
averaging	_	_
over	_	_
an	_	_
input	_	_
of	_	_
500	_	_
color	_	_
images	_	_
(	_	_
5	_	_
images	_	_
for	_	_
each	_	_
class	_	_
)	_	_
selected	_	_
from	_	_
the	_	_
database	_	_
.	_	_

#130
The	_	_
jaw	_	_
contour	_	_
and	_	_
facial	_	_
symmetry	_	_
based	_	_
method	_	_
of	_	_
Takayama	_	_
et	_	_
al.	_	_
reports	_	_
a	_	_
TPR	_	_
of	_	_
74.2	_	_
%	_	_
which	_	_
is	_	_
the	_	_
highest	_	_
reported	_	_
yet	_	_
to	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
.	_	_

#131
Along	_	_
with	_	_
outperforming	_	_
it	_	_
in	_	_
terms	_	_
of	_	_
TPR	_	_
,	_	_
the	_	_
MTCNN	_	_
based	_	_
method	_	_
also	_	_
delivers	_	_
lesser	_	_
FPR	_	_
and	_	_
FNR	_	_
score	_	_
.	_	_

#132
While	_	_
the	_	_
Haar-features	_	_
result	_	_
the	_	_
poorest	_	_
in	_	_
correctly	_	_
detecting	_	_
the	_	_
right	_	_
facial	_	_
regions	_	_
,	_	_
it	_	_
outperforms	_	_
HOG	_	_
in	_	_
terms	_	_
of	_	_
being	_	_
confused	_	_
for	_	_
detecting	_	_
the	_	_
non-facial	_	_
regions	_	_
.	_	_

#133
Also	_	_
,	_	_
it	_	_
is	_	_
worth	_	_
noting	_	_
that	_	_
the	_	_
results	_	_
of	_	_
the	_	_
OpenCV	_	_
classifier	_	_
mentioned	_	_
in	_	_
Takayama	_	_
et	_	_
al.	_	_
vary	_	_
greatly	_	_
from	_	_
ours	_	_
.	_	_

#134
While	_	_
they	_	_
do	_	_
not	_	_
mention	_	_
the	_	_
training	_	_
strategy	_	_
,	_	_
a	_	_
plausible	_	_
explanation	_	_
could	speculation	_
be	_	_
the	_	_
classifier	_	_
being	_	_
pre-trained	_	_
on	_	_
real	_	_
human	_	_
faces	_	_
instead	_	_
of	_	_
cartoons	_	_
.	_	_

#135
As	_	_
depicted	_	_
in	_	_
Figure	_	_
3	_	_
,	_	_
each	_	_
of	_	_
the	_	_
classifiers	_	_
possess	_	_
their	_	_
own	_	_
failure	_	_
cases	_	_
.	_	_

#136
Moreover	_	_
,	_	_
there	_	_
are	_	_
multiple	_	_
such	_	_
instances	_	_
where	_	_
the	_	_
classifiers	_	_
detect	_	_
no	_	_
facial	_	_
regions	_	_
at	_	_
all	_	_
.	_	_

#137
These	_	_
act	_	_
as	_	_
bottlenecks	_	_
in	_	_
using	_	_
the	_	_
detected	_	_
regions	_	_
for	_	_
the	_	_
face	_	_
recognition	_	_
experiments	_	_
.	_	_

#138
Hence	_	_
,	_	_
we	_	_
use	_	_
the	_	_
original	_	_
bounding	_	_
box	_	_
annotations	_	_
for	_	_
extracting	_	_
the	_	_
facial	_	_
regions	_	_
prior	_	_
to	_	_
face	_	_
recognition	_	_
.	_	_

#139
4.4	_	_
Face	_	_
Recognition	_	_
Results	_	_

#140
We	_	_
demonstrate	_	_
the	_	_
performance	_	_
of	_	_
both	_	_
the	_	_
recognition	_	_
models	_	_
on	_	_
the	_	_
task	_	_
of	_	_
personality	_	_
and	_	_
gender	_	_
recognition	_	_
of	_	_
cartoon	_	_
faces	_	_
alongside	_	_
evaluating	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
5-layer	_	_
LeNet	_	_
architecture	_	_
[	_	_
27	_	_
]	_	_
for	_	_
landmark	_	_
extraction	_	_
.	_	_

#141
The	_	_
inputs	_	_
are	_	_
normalized	_	_
after	_	_
extracting	_	_
the	_	_
face	_	_
bounding	_	_
box	_	_
.	_	_

#142
(	_	_
a	_	_
)	_	_
MTCNN	_	_
based	_	_
detector	_	_
(	_	_
b	_	_
)	_	_
HOG	_	_
based	_	_
detector	_	_
(	_	_
c	_	_
)	_	_
Haar-features	_	_
based	_	_
detector	_	_
Figure	_	_
3	_	_
:	_	_
Figure	_	_
showing	_	_
outputs	_	_
of	_	_
different	_	_
face	_	_
detection	_	_
methods	_	_
.	_	_

#143
Table	_	_
2	_	_
:	_	_
Face	_	_
detection	_	_
scores	_	_
of	_	_
various	_	_
methods	_	_
Models	_	_
Randomly	_	_
chosen	_	_
Frontal	_	_
TPR	_	_
FPR	_	_
FNR	_	_
TPR	_	_
FPR	_	_
FNR	_	_
MTCNN	_	_
78.17	_	_
%	_	_
12.81	_	_
%	_	_
9.02	_	_
%	_	_
83.67	_	_
%	_	_
4.90	_	_
%	_	_
11.43	_	_
%	_	_
HOG	_	_
features	_	_
70.51	_	_
%	_	_
17.33	_	_
%	_	_
12.16	_	_
%	_	_
77.32	_	_
%	_	_
11.80	_	_
%	_	_
10.88	_	_
%	_	_
Haar-features	_	_
57.24	_	_
%	_	_
8.39	_	_
%	_	_
34.37	_	_
%	_	_
69.44	_	_
%	_	_
3.05	_	_
%	_	_
27.51	_	_
%	_	_
Takayama	_	_
et	_	_
al.	_	_
-	_	_
-	_	_
-	_	_
74.2	_	_
%	_	_
14.0	_	_
%	_	_
11.8	_	_
%	_	_

#144
4.4.1	_	_
Evaluation	_	_
Measures	_	_

#145
Precision	_	_
,	_	_
recall	_	_
and	_	_
F-measure	_	_
averaged	_	_
over	_	_
all	_	_
the	_	_
classes	_	_
are	_	_
used	_	_
as	_	_
primary	_	_
quality	_	_
assessment	_	_
metrics	_	_
.	_	_

#146
Additionally	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
accuracy	_	_
and	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
each	_	_
model	_	_
.	_	_

#147
The	_	_
performance	_	_
of	_	_
the	_	_
landmark	_	_
extraction	_	_
system	_	_
is	_	_
evaluated	_	_
in	_	_
terms	_	_
of	_	_
the	_	_
Root	_	_
Mean	_	_
Squared	_	_
Error	_	_
(	_	_
RMSE	_	_
)	_	_
measured	_	_
in	_	_
between	_	_
the	_	_
actual	_	_
and	_	_
the	_	_
predicted	_	_
landmarks	_	_
.	_	_

#148
4.4.2	_	_
Landmark	_	_
Extraction	_	_
system	_	_

#149
We	_	_
perform	_	_
a	_	_
80:20	_	_
split	_	_
of	_	_
the	_	_
merged	_	_
database	_	_
(	_	_
Section	_	_
3.2.2	_	_
)	_	_
to	_	_
obtain	_	_
the	_	_
train	_	_
and	_	_
validation	_	_
sets	_	_
.	_	_

#150
Table	_	_
3	_	_
shows	_	_
a	_	_
comparison	_	_
of	_	_
validation	_	_
RMSEs	_	_
of	_	_
the	_	_
LeNet	_	_
based	_	_
model	_	_
[	_	_
27	_	_
]	_	_
on	_	_
three	_	_
different	_	_
train	_	_
sets	_	_
:	_	_
(	_	_
i	_	_
)	_	_
the	_	_
standard	_	_
train	_	_
set	_	_
consisting	_	_
a	_	_
blend	_	_
of	_	_
cartoon	_	_
and	_	_
human	_	_
faces	_	_
(	_	_
ii	_	_
)	_	_
only	_	_
the	_	_
750	_	_
cartoon	_	_
faces	_	_
(	_	_
iii	_	_
)	_	_
only	_	_
the	_	_
human	_	_
faces	_	_
(	_	_
as	_	_
presented	_	_
in	_	_
[	_	_
27	_	_
]	_	_
)	_	_
.	_	_

#151
The	_	_
validation	_	_
RMSE	_	_
degrades	_	_
by	_	_
∼3.4	_	_
times	_	_
on	_	_
the	_	_
blended	_	_
train	_	_
set	_	_
,	_	_
and	_	_
worsens	_	_
further	_	_
when	_	_
(	_	_
ii	_	_
)	_	_
is	_	_
used	_	_
for	_	_
training	_	_
.	_	_

#152
While	_	_
the	_	_
mapping	_	_
of	_	_
landmark	_	_
features	_	_
on	_	_
cartoons	_	_
could	capability-speculation	_
require	_	_
greater	_	_
degrees	_	_
of	_	_
non-linear	_	_
approximations	_	_
than	_	_
humanly	_	_
faces	_	_
[	_	_
36	_	_
]	_	_
,	_	_
the	_	_
degradation	_	_
can	_	_
be	_	_
attributed	_	_
to	_	_
the	_	_
fewer	_	_
size	_	_
of	_	_
annotated	_	_
cartoon	_	_
samples	_	_
as	_	_
well	_	_
.	_	_

#153
Further	_	_
increase	_	_
of	_	_
human	_	_
instances	_	_
in	_	_
the	_	_
train	_	_
set	_	_
worsen	_	_
the	_	_
scores	_	_
as	_	_
the	_	_
predictions	_	_
gain	_	_
more	_	_
of	_	_
human-like	_	_
patterns	_	_
.	_	_

#154
Figure	_	_
4	_	_
depicts	_	_
such	_	_
instances	_	_
where	_	_
the	_	_
model	_	_
fails	_	_
miserably	_	_
in	_	_
locating	_	_
the	_	_
keypoint	_	_
co-ordinates	_	_
using	_	_
(	_	_
i	_	_
)	_	_
.	_	_

#155
More	_	_
closer	_	_
analysis	_	_
imply	_	_
that	_	_
the	_	_
images	_	_
with	_	_
low	_	_
intensity	_	_
boundaries	_	_
,	_	_
anomalous	_	_
organ	_	_
shapes	_	_
,	_	_
missing	_	_
facial	_	_
regions	_	_
and	_	_
extremely	_	_
exaggerated	_	_
features	_	_
in	_	_
the	_	_
database	_	_
serve	_	_
majorly	_	_
towards	_	_
the	_	_
performance	_	_
degradations	_	_
.	_	_

#156
Images	_	_
with	_	_
lower	_	_
resolutions	_	_
,	_	_
by	_	_
contrast	_	_
,	_	_
do	_	_
not	_	_
result	_	_
in	_	_
any	_	_
significant	_	_
decline	_	_
unless	_	_
the	_	_
aforesaid	_	_
characteristics	_	_
are	_	_
present	_	_
.	_	_

#157
Figure	_	_
4	_	_
:	_	_
Erroneously	_	_
predicted	_	_
landmark	_	_
locations	_	_
on	_	_
various	_	_
faces	_	_

#158
4.5	_	_
Character	_	_
Recognition	_	_
Results	_	_

#159
Table	_	_
4	_	_
presents	_	_
the	_	_
accuracy	_	_
and	_	_
top-5	_	_
error	_	_
rates	_	_
of	_	_
SVM	_	_
and	_	_
GB	_	_
classifiers	_	_
on	_	_
the	_	_
Inception	_	_
v3	_	_
features	_	_
over	_	_
20	_	_
,	_	_
50	_	_
and	_	_
100	_	_
classes	_	_
,	_	_
and	_	_
contrasts	_	_
them	_	_
with	_	_
the	_	_
results	_	_
of	_	_
using	_	_
the	_	_
original	_	_
(	_	_
a	_	_
)	_	_
Accuracy	_	_
comparison	_	_
(	_	_
b	_	_
)	_	_
Top-5	_	_
error	_	_
rate	_	_
comparison	_	_
Figure	_	_
5	_	_
:	_	_
Accuracy	_	_
and	_	_
Top-5	_	_
error	_	_
rate	_	_
comparison	_	_
between	_	_
the	_	_
Inception+SVM	_	_
model	_	_
and	_	_
the	_	_
proposed	_	_
model	_	_
over	_	_
20	_	_
,	_	_
30	_	_
,	_	_
40	_	_
,	_	_
50	_	_
and	_	_
100	_	_
cartoon	_	_
classes	_	_
Table	_	_
4	_	_
:	_	_
Accuracy	_	_
and	_	_
Top-5	_	_
error	_	_
rate	_	_
comparison	_	_
of	_	_
Inception-based	_	_
frameworks	_	_
Models	_	_
Accuracy	_	_
Top-5	_	_
error	_	_
20	_	_
50	_	_
100	_	_
20	_	_
50	_	_
100	_	_
Inception	_	_
v3	_	_
79.43	_	_
%	_	_
63.98	_	_
%	_	_
56.61	_	_
%	_	_
5.114	_	_
%	_	_
22.307	_	_
%	_	_
34.659	_	_
%	_	_
Inception	_	_
v3+SVM	_	_
84.81	_	_
%	_	_
67.49	_	_
%	_	_
60.94	_	_
%	_	_
2.051	_	_
%	_	_
16.944	_	_
%	_	_
27.750	_	_
%	_	_
Inception	_	_
v3+GB	_	_
66.44	_	_
%	_	_
48.49	_	_
%	_	_
34.51	_	_
%	_	_
12.528	_	_
%	_	_
37.871	_	_
%	_	_
63.798	_	_
%	_	_
softmax	_	_
classifier	_	_
instead	_	_
.	_	_

#160
While	_	_
the	_	_
use	_	_
of	_	_
SVM	_	_
improves	_	_
the	_	_
top-5	_	_
error	_	_
by	_	_
6.909	_	_
%	_	_
over	_	_
the	_	_
softmax	_	_
layer	_	_
,	_	_
the	_	_
GB	_	_
assisted	_	_
ensemble	_	_
classifier	_	_
consistently	_	_
lags	_	_
in	_	_
terms	_	_
of	_	_
both	_	_
metrics	_	_
.	_	_

#161
Further	_	_
,	_	_
the	_	_
lag	_	_
in	_	_
performance	_	_
of	_	_
the	_	_
GB	_	_
classifier	_	_
with	_	_
respect	_	_
to	_	_
SVM	_	_
increases	_	_
as	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
increase	_	_
.	_	_

#162
This	_	_
lag	_	_
is	_	_
in	_	_
the	_	_
favor	_	_
of	_	_
the	_	_
conjecture	_	_
that	_	_
boosting	_	_
methods	_	_
,	_	_
because	_	_
of	_	_
their	_	_
high	_	_
correlation	_	_
with	_	_
the	_	_
noise	_	_
present	_	_
in	_	_
the	_	_
data	_	_
set	_	_
,	_	_
can	_	_
show	_	_
a	_	_
zero	_	_
gain	_	_
or	_	_
even	_	_
a	_	_
decrease	_	_
in	_	_
performance	_	_
from	_	_
a	_	_
single	_	_
classifier	_	_
[	_	_
37	_	_
]	_	_
.	_	_

#163
We	_	_
further	_	_
observe	_	_
such	_	_
decrement	_	_
becoming	_	_
more	_	_
evident	_	_
as	_	_
the	_	_
diversity	_	_
of	_	_
the	_	_
training	_	_
instances	_	_
flourish	_	_
.	_	_

#164
We	_	_
do	_	_
not	_	_
emphasize	_	_
the	_	_
results	_	_
of	_	_
incorporating	_	_
the	_	_
landmark	_	_
features	_	_
into	_	_
the	_	_
SVM	_	_
classifier	_	_
as	_	_
doing	_	_
so	_	_
,	_	_
resulted	_	_
in	_	_
the	_	_
drop	_	_
of	_	_
error	_	_
rate	_	_
to	_	_
approx	_	_
.	_	_

#165
43	_	_
%	_	_
on	_	_
100	_	_
classes	_	_
,	_	_
which	_	_
is	_	_
worse	_	_
than	_	_
the	_	_
original	_	_
softmax	_	_
classifier	_	_
.	_	_

#166
Table	_	_
5	_	_
compares	_	_
the	_	_
performance	_	_
of	_	_
both	_	_
of	_	_
our	_	_
recognition	_	_
models	_	_
with	_	_
the	_	_
current	_	_
state-of-the-art	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#167
In	_	_
contrast	_	_
to	_	_
[	_	_
14	_	_
]	_	_
reporting	_	_
their	_	_
results	_	_
on	_	_
300	_	_
cartoon	_	_
images	_	_
belonging	_	_
to	_	_
150	_	_
different	_	_
characters	_	_
,	_	_
our	_	_
evaluations	_	_
leverage	_	_
1786	_	_
images	_	_
belonging	_	_
to	_	_
100	_	_
characters	_	_
.	_	_

#168
Our	_	_
systems	_	_
nonetheless	_	_
outperform	_	_
theirs	_	_
(	_	_
Section	_	_
2	_	_
)	_	_
with	_	_
a	_	_
high	_	_
margin	_	_
.	_	_

#169
It	_	_
is	_	_
discernible	_	_
that	_	_
although	_	_
the	_	_
predictions	_	_
retrieved	_	_
by	_	_
Inception+SVM	_	_
model	_	_
contain	_	_
higher	_	_
fraction	_	_
of	_	_
relevant	_	_
cartoon	_	_
classes	_	_
(	_	_
high	_	_
precision	_	_
)	_	_
,	_	_
the	_	_
greater	_	_
recall	_	_
score	_	_
of	_	_
the	_	_
HCNN	_	_
model	_	_
suggests	_	_
its	_	_
higher	_	_
resistance	_	_
to	_	_
mistaking	_	_
the	_	_
relevant	_	_
cartoon	_	_
classes	_	_
for	_	_
irrelevant	_	_
ones	_	_
.	_	_

#170
Table	_	_
5	_	_
:	_	_
Performance	_	_
comparison	_	_
of	_	_
recognition	_	_
models	_	_
Model	_	_
Precision	_	_
Recall	_	_
F-Measure	_	_
Proposed	_	_
Model	_	_
0.622	_	_
0.680	_	_
0.649	_	_
Inception	_	_
v3+SVM	_	_
0.682	_	_
0.659	_	_
0.670	_	_
Takayama	_	_
et	_	_
al.	_	_
0.476	_	_
0.563	_	_
0.516	_	_
Table	_	_
6	_	_
shows	_	_
the	_	_
performance	_	_
comparison	_	_
of	_	_
the	_	_
main	_	_
and	_	_
auxiliary	_	_
classifiers	_	_
of	_	_
the	_	_
HCNN	_	_
model	_	_
over	_	_
100	_	_
classes	_	_
.	_	_

#171
The	_	_
inclusion	_	_
of	_	_
the	_	_
keypoint	_	_
features	_	_
impart	_	_
respective	_	_
gains	_	_
of	_	_
2.66	_	_
%	_	_
and	_	_
5.87	_	_
%	_	_
to	_	_
the	_	_
accuracy	_	_
and	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
the	_	_
main	_	_
classifier	_	_
over	_	_
the	_	_
auxiliary	_	_
classifier	_	_
.	_	_

#172
Our	_	_
experiments	_	_
show	_	_
that	_	_
on	_	_
dropping	_	_
of	_	_
the	_	_
BN	_	_
and	_	_
skip	_	_
connections	_	_
,	_	_
the	_	_
gain	_	_
in	_	_
accuracy	_	_
decreases	_	_
to	_	_
2.51	_	_
%	_	_
,	_	_
while	_	_
the	_	_
error	_	_
rate	_	_
witnesses	_	_
an	_	_
increment	_	_
to	_	_
6.08	_	_
%	_	_
.	_	_

#173
Overall	_	_
,	_	_
we	_	_
notice	_	_
that	_	_
the	_	_
presence	_	_
of	_	_
auxiliary	_	_
classifier	_	_
imparts	_	_
greater	_	_
stability	_	_
to	_	_
the	_	_
model	_	_
by	_	_
increasing	_	_
the	_	_
number	_	_
of	_	_
training	_	_
epochs	_	_
before	_	_
convergence	_	_
,	_	_
and	_	_
keeping	_	_
the	_	_
validation	_	_
error	_	_
from	_	_
larger	_	_
fluctuations	_	_
afterwards	_	_
.	_	_

#174
Table	_	_
6	_	_
:	_	_
Results	_	_
of	_	_
the	_	_
main	_	_
and	_	_
auxiliary	_	_
classifiers	_	_
of	_	_
the	_	_
HCNN	_	_
model	_	_
Classifier	_	_
type	_	_
Accuracy	_	_
Top-5	_	_
error	_	_
Main	_	_
59.11	_	_
%	_	_
25.10	_	_
%	_	_
Auxiliary	_	_
56.45	_	_
%	_	_
30.97	_	_
%	_	_
Figure	_	_
5	_	_
juxtaposes	_	_
the	_	_
accuracy	_	_
and	_	_
the	_	_
top-5	_	_
error	_	_
rates	_	_
of	_	_
the	_	_
models	_	_
on	_	_
varying	_	_
number	_	_
of	_	_
classes	_	_
.	_	_

#175
The	_	_
graph	_	_
in	_	_
fig	_	_
.	_	_

#176
5a	_	_
suggests	_	_
a	_	_
rather	_	_
striking	_	_
trend	_	_
:	_	_
though	_	_
the	_	_
accuracy	_	_
of	_	_
the	_	_
HCNN	_	_
model	_	_
remains	_	_
significantly	_	_
lower	_	_
than	_	_
that	_	_
of	_	_
Inception	_	_
v3+SVM	_	_
for	_	_
lesser	_	_
number	_	_
of	_	_
classes	_	_
(	_	_
i.e.	_	_
,	_	_
5.4	_	_
%	_	_
for	_	_
20	_	_
classes	_	_
)	_	_
,	_	_
their	_	_
differences	_	_
diminish	_	_
to	_	_
as	_	_
low	_	_
as	_	_
1.83	_	_
%	_	_
,	_	_
as	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
approach	_	_
to	_	_
100	_	_
.	_	_

#177
A	_	_
similar	_	_
trend	_	_
can	_	_
be	_	_
observed	_	_
from	_	_
fig	_	_
.	_	_

#178
5b	_	_
wherein	_	_
,	_	_
the	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
HCNN	_	_
lags	_	_
noticeably	_	_
beyond	_	_
Inception	_	_
v3+SVM	_	_
for	_	_
lesser	_	_
number	_	_
of	_	_
classes	_	_
(	_	_
20-40	_	_
)	_	_
,	_	_
becomes	_	_
almost	_	_
identical	_	_
for	_	_
50	_	_
classes	_	_
and	_	_
eventually	_	_
outperforms	_	_
the	_	_
latter	_	_
for	_	_
100	_	_
classes	_	_
.	_	_

#179
These	_	_
trends	_	_
,	_	_
apart	_	_
from	_	_
being	_	_
successively	_	_
procurable	_	_
from	_	_
one	_	_
another	_	_
,	_	_
show	_	_
that	_	_
the	_	_
Inception	_	_
v3+SVM	_	_
offers	_	_
lesser	_	_
stability	_	_
as	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
vary	_	_
and	_	_
as	_	_
the	_	_
diversity	_	_
of	_	_
the	_	_
input	_	_
instances	_	_
increase	_	_
.	_	_

#180
We	_	_
suggest	_	_
two	_	_
tenable	_	_
explanations	_	_
for	_	_
such	_	_
poor	_	_
stability	_	_
of	_	_
the	_	_
model	_	_
.	_	_

#181
Firstly	_	_
,	_	_
the	_	_
RBF	_	_
kernel	_	_
employed	_	_
in	_	_
the	_	_
SVM	_	_
assumes	_	_
that	_	_
the	_	_
optimal	_	_
decision	_	_
boundary	_	_
remains	_	_
smooth	_	_
in	_	_
all	_	_
the	_	_
instances	_	_
.	_	_

#182
However	_	_
,	_	_
with	_	_
greater	_	_
number	_	_
of	_	_
classes	_	_
,	_	_
the	_	_
violations	_	_
of	_	_
this	_	_
assumption	_	_
elevate	_	_
leading	_	_
to	_	_
rise	_	_
in	_	_
the	_	_
entropy	_	_
captured	_	_
by	_	_
the	_	_
model’s	_	_
hyperparameters	_	_
due	_	_
to	_	_
the	_	_
diverse	_	_
dispersion	_	_
of	_	_
outliers	_	_
than	_	_
earlier	_	_
.	_	_

#183
Secondly	_	_
,	_	_
for	_	_
lesser	_	_
number	_	_
of	_	_
classes	_	_
,	_	_
the	_	_
HCNN	_	_
model	_	_
with	_	_
parameters	_	_
much	_	_
greater	_	_
than	_	_
SVM	_	_
might	speculation	_
have	_	_
overfitted	_	_
due	_	_
to	_	_
smaller	_	_
train	_	_
set	_	_
size	_	_
.	_	_

#184
4.5.1	_	_
Effects	_	_
of	_	_
skip	_	_
connection	_	_
and	_	_
BN	_	_
on	_	_
HCNN	_	_
:	_	_

#185
We	_	_
experiment	_	_
with	_	_
different	_	_
combinations	_	_
of	_	_
skip	_	_
connections	_	_
applied	_	_
to	_	_
the	_	_
outputs	_	_
of	_	_
each	_	_
Conv2D	_	_
and	_	_
BN	_	_
layer	_	_
,	_	_
one	_	_
at	_	_
a	_	_
time	_	_
while	_	_
preserving	_	_
the	_	_
original	_	_
destination	_	_
layer	_	_
.	_	_

#186
We	_	_
notice	_	_
that	_	_
the	_	_
validation	_	_
loss	_	_
increases	_	_
as	_	_
the	_	_
connection	_	_
is	_	_
placed	_	_
right	_	_
after	_	_
the	_	_
BN	_	_
layers	_	_
,	_	_
than	_	_
when	_	_
they	_	_
are	_	_
applied	_	_
after	_	_
the	_	_
Conv2D	_	_
layers	_	_
.	_	_

#187
The	_	_
validation	_	_
loss	_	_
and	_	_
the	_	_
training	_	_
epochs	_	_
before	_	_
convergence	_	_
increase	_	_
as	_	_
the	_	_
connections	_	_
are	_	_
made	_	_
in	_	_
presence	_	_
of	_	_
BN	_	_
along	_	_
with	_	_
dropouts	_	_
.	_	_

#188
The	_	_
skip	_	_
connection	_	_
plays	_	_
a	_	_
rather	_	_
important	_	_
role	_	_
in	_	_
stabilizing	_	_
the	_	_
training	_	_
epochs	_	_
for	_	_
convergence	_	_
,	_	_
as	_	_
its	_	_
elimination	_	_
increases	_	_
the	_	_
number	_	_
of	_	_
epochs	_	_
to	_	_
103	_	_
(	_	_
and	_	_
the	_	_
top-5	_	_
error	_	_
rate	_	_
to	_	_
36	_	_
%	_	_
)	_	_
while	_	_
on	_	_
its	_	_
presence	_	_
,	_	_
these	_	_
reduce	_	_
drastically	_	_
to	_	_
47	_	_
.	_	_

#189
4.6	_	_
Gender	_	_
Recognition	_	_
Results	_	_

#190
Table	_	_
7	_	_
depicts	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
recognition	_	_
models	_	_
on	_	_
the	_	_
task	_	_
of	_	_
gender	_	_
classification	_	_
of	_	_
cartoon	_	_
faces	_	_
.	_	_

#191
The	_	_
Inception	_	_
v3+SVM	_	_
model	_	_
clearly	_	_
outperforms	_	_
the	_	_
HCNN	_	_
on	_	_
all	_	_
the	_	_
three	_	_
metrics	_	_
.	_	_

#192
The	_	_
lower	_	_
recall	_	_
scores	_	_
for	_	_
both	_	_
the	_	_
models	_	_
suggest	_	_
that	_	_
the	_	_
mistaken	_	_
instances	_	_
of	_	_
genders	_	_
for	_	_
their	_	_
counterparts	_	_
are	_	_
comparatively	_	_
higher	_	_
.	_	_

#193
The	_	_
scores	_	_
of	_	_
the	_	_
Inception	_	_
v3+SVM	_	_
model	_	_
on	_	_
the	_	_
binary	_	_
classification	_	_
task	_	_
further	_	_
strengthens	_	_
the	_	_
argument	_	_
mentioned	_	_
in	_	_
Section	_	_
4.5	_	_
explaining	_	_
for	_	_
its	_	_
poor	_	_
stability	_	_
as	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
grow	_	_
in	_	_
the	_	_
character	_	_
recognition	_	_
task	_	_
.	_	_

#194
At	_	_
the	_	_
time	_	_
of	_	_
writing	_	_
,	_	_
no	_	_
previous	_	_
work	_	_
in	_	_
the	_	_
literature	_	_
talks	_	_
of	_	_
such	_	_
experiment	_	_
on	_	_
cartoon	_	_
faces	_	_
and	_	_
thus	_	_
,	_	_
we	_	_
believe	_	_
that	_	_
the	_	_
scores	_	_
hold	_	_
a	_	_
state-of-the-art	_	_
.	_	_

#195
Table	_	_
7	_	_
:	_	_
Performance	_	_
comparison	_	_
of	_	_
models	_	_
for	_	_
gender	_	_
recognition	_	_
Model	_	_
Precision	_	_
Recall	_	_
F-Measure	_	_
Proposed	_	_
Model	_	_
0.904	_	_
0.827	_	_
0.864	_	_
Inception+SVM	_	_
0.927	_	_
0.894	_	_
0.910	_	_

#196
5	_	_
CONCLUSION	_	_

#197
Towards	_	_
the	_	_
end	_	_
goal	_	_
of	_	_
improving	_	_
cartoon	_	_
face	_	_
detection	_	_
and	_	_
recognition	_	_
systems	_	_
with	_	_
the	_	_
latest	_	_
advancements	_	_
in	_	_
deep	_	_
learning	_	_
frameworks	_	_
,	_	_
we	_	_
present	_	_
the	_	_
following	_	_
contributions	_	_
:	_	_
•	_	_
For	_	_
the	_	_
face	_	_
detection	_	_
task	_	_
,	_	_
we	_	_
show	_	_
that	_	_
the	_	_
MTCNN	_	_
framework	_	_
outperforms	_	_
the	_	_
state-of-the-art	_	_
[	_	_
14	_	_
]	_	_
in	_	_
terms	_	_
of	_	_
TPR	_	_
,	_	_
FPR	_	_
and	_	_
FNR	_	_
.	_	_

#198
We	_	_
further	_	_
confirm	_	_
that	_	_
the	_	_
model	_	_
performs	_	_
the	_	_
best	_	_
when	_	_
presented	_	_
with	_	_
frontal	_	_
faces	_	_
.	_	_

#199
•	_	_
For	_	_
the	_	_
face	_	_
recognition	_	_
task	_	_
,	_	_
firstly	_	_
,	_	_
we	_	_
show	_	_
that	_	_
a	_	_
combination	_	_
of	_	_
Inception	_	_
v3	_	_
as	_	_
feature	_	_
extractor	_	_
followed	_	_
by	_	_
SVM	_	_
as	_	_
feature	_	_
recognizer	_	_
achieves	_	_
a	_	_
benchmark	_	_
F-score	_	_
of	_	_
0.670	_	_
.	_	_

#200
Secondly	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
LeNet	_	_
inspired	_	_
CNN	_	_
framework	_	_
that	_	_
helps	_	_
us	_	_
achieve	_	_
a	_	_
more	_	_
stable	_	_
top-5	_	_
error	_	_
rate	_	_
than	_	_
the	_	_
former	_	_
as	_	_
the	_	_
number	_	_
of	_	_
cartoon	_	_
classes	_	_
elevate	_	_
.	_	_

#201
We	_	_
further	_	_
suggest	_	_
intuitions	_	_
behind	_	_
the	_	_
pitfalls	_	_
of	_	_
the	_	_
SVM	_	_
and	_	_
GB	_	_
based	_	_
classifiers	_	_
,	_	_
and	_	_
depict	_	_
the	_	_
differences	_	_
of	_	_
employing	_	_
the	_	_
LeNet	_	_
architecture	_	_
for	_	_
extracting	_	_
the	_	_
landmarks	_	_
of	_	_
cartoon	_	_
faces	_	_
and	_	_
real	_	_
human	_	_
faces	_	_
.	_	_

#202
Our	_	_
experiments	_	_
demonstrate	_	_
that	_	_
the	_	_
inclusion	_	_
of	_	_
the	_	_
facial	_	_
keypoint	_	_
locations	_	_
can	_	_
help	_	_
improve	_	_
the	_	_
top-5	_	_
error	_	_
rate	_	_
of	_	_
the	_	_
proposed	_	_
recognition	_	_
model	_	_
by	_	_
5.87	_	_
%	_	_
.	_	_

#203
The	_	_
annotated	_	_
facial	_	_
keypoints	_	_
are	_	_
thus	_	_
made	_	_
publically	_	_
available	_	_
in	_	_
hope	_	_
to	_	_
aid	_	_
further	_	_
researches	_	_
related	_	_
to	_	_
the	_	_
field	_	_
.	_	_

#204
Lastly	_	_
,	_	_
we	_	_
show	_	_
that	_	_
the	_	_
Inception+SVM	_	_
establishes	_	_
a	_	_
state-of-the-art	_	_
F-measure	_	_
of	_	_
0.927	_	_
when	_	_
employed	_	_
to	_	_
the	_	_
task	_	_
of	_	_
gender	_	_
recognition	_	_
of	_	_
cartoon	_	_
faces	_	_
.	_	_