#0
Aggregated	_	_
Sparse	_	_
Attention	_	_
for	_	_
Steering	_	_
Angle	_	_
Prediction	_	_
Sen	_	_
He	_	_
,	_	_
Dmitry	_	_
Kangin	_	_
,	_	_
Yang	_	_
Mi	_	_
and	_	_
Nicolas	_	_
Pugeault	_	_
Department	_	_
of	_	_
Computer	_	_
Sciences	_	_
,	_	_
University	_	_
of	_	_
Exeter	_	_
,	_	_
Exeter	_	_
,	_	_
EX4	_	_
4QF	_	_
Email	_	_
:	_	_
{	_	_
sh752	_	_
,	_	_
D.Kangin	_	_
,	_	_
ym310	_	_
,	_	_
N.Pugeault	_	_
}	_	_
@	_	_
exeter.ac.uk	_	_
Abstract—In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
apply	_	_
the	_	_
attention	_	_
mechanism	_	_
to	_	_
autonomous	_	_
driving	_	_
for	_	_
steering	_	_
angle	_	_
prediction	_	_
.	_	_

#1
We	_	_
propose	_	_
the	_	_
first	_	_
model	_	_
,	_	_
applying	_	_
the	_	_
recently	_	_
introduced	_	_
sparse	_	_
attention	_	_
mechanism	_	_
to	_	_
visual	_	_
domain	_	_
,	_	_
as	_	_
well	_	_
as	_	_
the	_	_
aggregated	_	_
extension	_	_
for	_	_
this	_	_
model	_	_
.	_	_

#2
We	_	_
show	_	_
the	_	_
improvement	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
,	_	_
comparing	_	_
to	_	_
no	_	_
attention	_	_
as	_	_
well	_	_
as	_	_
to	_	_
different	_	_
types	_	_
of	_	_
attention	_	_
.	_	_

#3
I	_	_
.	_	_

#4
INTRODUCTION	_	_
Consider	_	_
a	_	_
human	_	_
driving	_	_
a	_	_
car	_	_
on	_	_
a	_	_
countryside	_	_
road	_	_
.	_	_

#5
The	_	_
driver’s	_	_
brain	_	_
is	_	_
subjected	_	_
to	_	_
a	_	_
continuous	_	_
flow	_	_
of	_	_
large	_	_
quantities	_	_
of	_	_
visual	_	_
information	_	_
,	_	_
interpreting	_	_
it	_	_
in	_	_
real	_	_
time	_	_
to	_	_
provide	_	_
fast	_	_
,	_	_
precise	_	_
and	_	_
reliable	_	_
control	_	_
of	_	_
the	_	_
vehicle	_	_
.	_	_

#6
An	_	_
essential	_	_
mechanism	_	_
that	_	_
allows	_	_
such	_	_
an	_	_
efficient	_	_
and	_	_
fast	_	_
processing	_	_
of	_	_
information	_	_
is	_	_
visual	_	_
attention	_	_
,	_	_
which	_	_
has	_	_
been	_	_
extensively	_	_
studied	_	_
by	_	_
psychologists	_	_
.	_	_

#7
Early	_	_
computational	_	_
models	_	_
of	_	_
attention	_	_
,	_	_
inspired	_	_
by	_	_
the	_	_
seminal	_	_
work	_	_
of	_	_
Itti	_	_
&	_	_
Koch	_	_
[	_	_
1	_	_
]	_	_
,	_	_
focused	_	_
on	_	_
the	_	_
top-down	_	_
mechanism	_	_
that	_	_
elicit	_	_
eye	_	_
movements	_	_
when	_	_
subjects	_	_
perform	_	_
a	_	_
visual	_	_
search	_	_
of	_	_
objects	_	_
on	_	_
images	_	_
.	_	_

#8
The	_	_
aim	_	_
of	_	_
such	_	_
models	_	_
is	_	_
to	_	_
estimate	_	_
from	_	_
an	_	_
image	_	_
a	_	_
so-called	_	_
saliency	_	_
map	_	_
:	_	_
an	_	_
estimate	_	_
of	_	_
how	_	_
likely	_	_
are	_	_
the	_	_
subject’s	_	_
eyes	_	_
to	_	_
look	_	_
at	_	_
image	_	_
locations	_	_
given	_	_
the	_	_
patterns	_	_
it	_	_
contains	_	_
(	_	_
see	_	_
,	_	_
eg	_	_
,	_	_
[	_	_
2	_	_
]	_	_
,	_	_
[	_	_
3	_	_
]	_	_
)	_	_
.	_	_

#9
Saliency	_	_
models	_	_
can	_	_
either	_	_
be	_	_
engineered	_	_
based	_	_
on	_	_
properties	_	_
of	_	_
images	_	_
,	_	_
or	_	_
learnt	_	_
from	_	_
eye	_	_
tracking	_	_
records	_	_
of	_	_
human	_	_
subjects	_	_
.	_	_

#10
In	_	_
both	_	_
case	_	_
,	_	_
the	_	_
quality	_	_
of	_	_
saliency	_	_
models	_	_
is	_	_
estimated	_	_
by	_	_
comparing	_	_
their	_	_
prediction	_	_
with	_	_
actual	_	_
eye	_	_
fixations	_	_
on	_	_
dataset	_	_
of	_	_
images	_	_
.	_	_

#11
Although	_	_
such	_	_
approaches	_	_
can	_	_
predict	_	_
fairly	_	_
well	_	_
the	_	_
eye	_	_
fixations	_	_
of	_	_
human	_	_
subjects	_	_
when	_	_
asked	_	_
to	_	_
perform	_	_
a	_	_
visual	_	_
search	_	_
task	_	_
,	_	_
their	_	_
predictiveness	_	_
is	_	_
much	_	_
worse	_	_
when	_	_
the	_	_
subjects	_	_
are	_	_
performing	_	_
an	_	_
active	_	_
task	_	_
,	_	_
such	_	_
as	_	_
playing	_	_
video	_	_
games	_	_
or	_	_
driving	_	_
[	_	_
4	_	_
]	_	_
,	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#12
More	_	_
recently	_	_
,	_	_
several	_	_
groups	_	_
have	_	_
proposed	_	_
to	_	_
learn	_	_
attention	_	_
not	_	_
by	_	_
mimicking	_	_
the	_	_
gaze	_	_
of	_	_
human	_	_
subjects	_	_
,	_	_
but	_	_
by	_	_
optimising	_	_
a	_	_
system’s	_	_
performance	_	_
at	_	_
a	_	_
specific	_	_
task	_	_
[	_	_
6	_	_
]	_	_
,	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#13
In	_	_
contrast	_	_
to	_	_
saliency	_	_
,	_	_
that	_	_
is	_	_
purely	_	_
bottom-up	_	_
,	_	_
such	_	_
models	_	_
are	_	_
explicitly	_	_
task	_	_
dependent	_	_
.	_	_

#14
This	_	_
article	_	_
proposes	_	_
a	_	_
novel	_	_
attention	_	_
mechanism	_	_
for	_	_
convolutional	_	_
neural	_	_
networks	_	_
that	_	_
is	_	_
based	_	_
on	_	_
learning	_	_
a	_	_
task-specific	_	_
sparse	_	_
attention	_	_
mechanism	_	_
.	_	_

#15
In	_	_
particular	_	_
,	_	_
we	_	_
focus	_	_
on	_	_
the	_	_
challenging	_	_
task	_	_
of	_	_
predicting	_	_
steering	_	_
angle	_	_
from	_	_
visual	_	_
input	_	_
only	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#16
We	_	_
demonstrate	_	_
that	_	_
such	_	_
a	_	_
sparse	_	_
attention	_	_
focusing	_	_
leads	_	_
to	_	_
better	_	_
performance	_	_
.	_	_

#17
Moreover	_	_
,	_	_
we	_	_
provide	_	_
experimental	_	_
evidence	_	_
that	_	_
such	_	_
an	_	_
attention	_	_
model	_	_
is	_	_
very	_	_
sensitive	_	_
to	_	_
initial	_	_
conditions	_	_
and	_	_
demonstrate	_	_
that	_	_
an	_	_
ensemble	_	_
of	_	_
sparse	_	_
attentional	_	_
models	_	_
can	_	_
significantly	_	_
improve	_	_
not	_	_
Fig.	_	_
1	_	_
:	_	_
Architecture	_	_
of	_	_
the	_	_
proposed	_	_
aggregated	_	_
attention	_	_
model	_	_
.	_	_

#18
only	_	_
the	_	_
robustness	_	_
of	_	_
the	_	_
learning	_	_
process	_	_
,	_	_
but	_	_
also	_	_
overall	_	_
performance	_	_
.	_	_

#19
The	_	_
rest	_	_
of	_	_
this	_	_
article	_	_
is	_	_
organised	_	_
as	_	_
follows	_	_
:	_	_
Section	_	_
II	_	_
reviews	_	_
the	_	_
use	_	_
of	_	_
attention	_	_
mechanism	_	_
in	_	_
computer	_	_
vision	_	_
as	_	_
well	_	_
as	_	_
steering	_	_
angle	_	_
prediction	_	_
;	_	_
Section	_	_
III	_	_
provides	_	_
the	_	_
detailed	_	_
methodology	_	_
used	_	_
in	_	_
this	_	_
work	_	_
;	_	_
Experimental	_	_
results	_	_
,	_	_
together	_	_
with	_	_
comparison	_	_
are	_	_
presented	_	_
in	_	_
Section	_	_
IV	_	_
;	_	_
and	_	_
conclusions	_	_
are	_	_
drawn	_	_
in	_	_
Section	_	_
V.	_	_
II	_	_
.	_	_

#20
RELATED	_	_
WORK	_	_
A	_	_
broad	_	_
range	_	_
of	_	_
attention	_	_
models	_	_
have	_	_
been	_	_
proposed	_	_
over	_	_
the	_	_
years	_	_
in	_	_
the	_	_
literature	_	_
.	_	_

#21
This	_	_
article	_	_
is	_	_
concerned	_	_
in	_	_
particular	_	_
with	_	_
the	_	_
problem	_	_
of	_	_
task-dependent	_	_
attention	_	_
,	_	_
where	_	_
the	_	_
focusing	_	_
of	_	_
attention	_	_
is	_	_
optimised	_	_
to	_	_
improve	_	_
a	_	_
system’s	_	_
performance	_	_
at	_	_
a	_	_
given	_	_
task	_	_
.	_	_

#22
This	_	_
is	_	_
in	_	_
contrast	_	_
to	_	_
saliency	_	_
models	_	_
which	_	_
are	_	_
designed	_	_
to	_	_
mimic	_	_
human	_	_
subjects’	_	_
gaze	_	_
patterns	_	_
irrespective	_	_
of	_	_
the	_	_
tasks	_	_
demands	_	_
.	_	_

#23
Existing	_	_
models	_	_
of	_	_
task-dependent	_	_
attention	_	_
for	_	_
neural	_	_
networks	_	_
can	_	_
be	_	_
classified	_	_
in	_	_
two	_	_
groups	_	_
:	_	_
soft	_	_
attention	_	_
and	_	_
hard	_	_
attention	_	_
.	_	_

#24
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#25
5v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
5	_	_
M	_	_
ar	_	_
2	_	_
In	_	_
soft	_	_
attention	_	_
,	_	_
the	_	_
visual	_	_
input	_	_
is	_	_
processed	_	_
by	_	_
a	_	_
pre-trained	_	_
convolutional	_	_
neural	_	_
network	_	_
,	_	_
and	_	_
the	_	_
output	_	_
of	_	_
the	_	_
top	_	_
convolutional	_	_
layer	_	_
is	_	_
encoded	_	_
by	_	_
a	_	_
feature	_	_
tensor	_	_
.	_	_

#26
Soft	_	_
attention	_	_
consists	_	_
in	_	_
weighing	_	_
the	_	_
feature	_	_
tensor	_	_
with	_	_
an	_	_
attention	_	_
matrix	_	_
that	_	_
encodes	_	_
the	_	_
relative	_	_
importance	_	_
of	_	_
all	_	_
locations	_	_
in	_	_
the	_	_
feature	_	_
tensor	_	_
.	_	_

#27
This	_	_
weighted	_	_
tensor	_	_
is	_	_
fed	_	_
to	_	_
another	_	_
network	_	_
(	_	_
ie	_	_
,	_	_
a	_	_
fully	_	_
connected	_	_
or	_	_
a	_	_
recurrent	_	_
neural	_	_
network	_	_
)	_	_
to	_	_
optimise	_	_
the	_	_
desired	_	_
task	_	_
.	_	_

#28
The	_	_
attention	_	_
matrix	_	_
is	_	_
therefore	_	_
learnt	_	_
from	_	_
the	_	_
task	_	_
and	_	_
normalised	_	_
using	_	_
a	_	_
softmax	_	_
function	_	_
.	_	_

#29
Li	_	_
et	_	_
al	_	_
[	_	_
6	_	_
]	_	_
used	_	_
soft	_	_
attention	_	_
to	_	_
develop	_	_
a	_	_
multilevel	_	_
attention	_	_
model	_	_
for	_	_
video	_	_
captioning	_	_
.	_	_

#30
Their	_	_
model	_	_
uses	_	_
two	_	_
attention	_	_
layers	_	_
.	_	_

#31
The	_	_
first	_	_
layer	_	_
models	_	_
region-level	_	_
attention	_	_
,	_	_
which	_	_
encodes	_	_
the	_	_
importance	_	_
of	_	_
each	_	_
region	_	_
in	_	_
a	_	_
frame	_	_
.	_	_

#32
The	_	_
second	_	_
attention	_	_
layer	_	_
models	_	_
frame-level	_	_
attention	_	_
,	_	_
which	_	_
encodes	_	_
the	_	_
importance	_	_
of	_	_
each	_	_
frame	_	_
in	_	_
a	_	_
short	_	_
video	_	_
.	_	_

#33
Sharma	_	_
et	_	_
al	_	_
[	_	_
7	_	_
]	_	_
proposed	_	_
a	_	_
soft	_	_
attention	_	_
model	_	_
for	_	_
action	_	_
recognition	_	_
.	_	_

#34
In	_	_
their	_	_
model	_	_
,	_	_
the	_	_
output	_	_
of	_	_
a	_	_
pre-trained	_	_
deep	_	_
convolutional	_	_
neural	_	_
network	_	_
is	_	_
fed	_	_
to	_	_
a	_	_
long	_	_
short-term	_	_
memory	_	_
(	_	_
LSTM	_	_
)	_	_
network	_	_
to	_	_
output	_	_
the	_	_
action	_	_
as	_	_
well	_	_
as	_	_
the	_	_
attention	_	_
matrix	_	_
.	_	_

#35
Xu	_	_
et	_	_
al	_	_
[	_	_
8	_	_
]	_	_
used	_	_
soft	_	_
attention	_	_
for	_	_
image	_	_
captioning	_	_
.	_	_

#36
Their	_	_
model	_	_
also	_	_
uses	_	_
a	_	_
LSTM	_	_
network	_	_
that	_	_
generates	_	_
an	_	_
attention	_	_
matrix	_	_
at	_	_
each	_	_
time	_	_
step	_	_
and	_	_
generating	_	_
sentences	_	_
to	_	_
describe	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#37
One	_	_
limitation	_	_
of	_	_
soft	_	_
attention	_	_
is	_	_
that	_	_
it	_	_
only	_	_
reweighs	_	_
the	_	_
convolutional	_	_
features	_	_
and	_	_
therefore	_	_
everything	_	_
is	_	_
always	_	_
attended	_	_
to	_	_
,	_	_
although	_	_
not	_	_
with	_	_
the	_	_
same	_	_
relative	_	_
importance	_	_
(	_	_
hence	_	_
soft	_	_
attention	_	_
)	_	_
.	_	_

#38
In	_	_
contrast	_	_
,	_	_
hard	_	_
attention	_	_
models	_	_
only	_	_
process	_	_
part	_	_
of	_	_
the	_	_
input	_	_
,	_	_
which	_	_
is	_	_
assumed	_	_
to	_	_
be	_	_
the	_	_
most	_	_
important	_	_
region	_	_
.	_	_

#39
Because	_	_
hard	_	_
attention	_	_
is	_	_
not	_	_
differentiable	_	_
,	_	_
it	_	_
is	_	_
more	_	_
challenging	_	_
to	_	_
optimise	_	_
.	_	_

#40
Mnih	_	_
et	_	_
al	_	_
[	_	_
9	_	_
]	_	_
proposes	_	_
to	_	_
learn	_	_
hard	_	_
attention	_	_
from	_	_
reinforcement	_	_
learning	_	_
.	_	_

#41
There	_	_
are	_	_
two	_	_
crucial	_	_
component	_	_
in	_	_
their	_	_
network	_	_
:	_	_
The	_	_
first	_	_
one	_	_
is	_	_
a	_	_
glimpse	_	_
sensor	_	_
,	_	_
which	_	_
can	_	_
be	_	_
used	_	_
to	_	_
extract	_	_
a	_	_
retina-like	_	_
representation	_	_
centred	_	_
at	_	_
a	_	_
given	_	_
location	_	_
in	_	_
the	_	_
input	_	_
;	_	_
The	_	_
second	_	_
component	_	_
is	_	_
a	_	_
glimpse	_	_
network	_	_
,	_	_
it	_	_
is	_	_
used	_	_
to	_	_
process	_	_
the	_	_
retina-like	_	_
representation	_	_
extracted	_	_
from	_	_
the	_	_
glimpse	_	_
sensor	_	_
,	_	_
and	_	_
the	_	_
processed	_	_
information	_	_
is	_	_
then	_	_
fed	_	_
into	_	_
a	_	_
recurrent	_	_
neural	_	_
network	_	_
(	_	_
RNN	_	_
)	_	_
which	_	_
estimates	_	_
the	_	_
attention	_	_
focus	_	_
for	_	_
the	_	_
glimpse	_	_
sensor	_	_
at	_	_
the	_	_
next	_	_
iteration	_	_
.	_	_

#42
This	_	_
article	_	_
is	_	_
especially	_	_
concerned	_	_
with	_	_
active	_	_
tasks	_	_
,	_	_
and	_	_
in	_	_
particular	_	_
the	_	_
problem	_	_
of	_	_
estimating	_	_
steering	_	_
from	_	_
vision	_	_
.	_	_

#43
Pugeault	_	_
&	_	_
Bowden	_	_
[	_	_
5	_	_
]	_	_
developed	_	_
a	_	_
pre-attentive	_	_
model	_	_
using	_	_
gist	_	_
[	_	_
10	_	_
]	_	_
and	_	_
random	_	_
forests	_	_
,	_	_
while	_	_
the	_	_
deep	_	_
network	_	_
models	_	_
are	_	_
CNN	_	_
or	_	_
CNN+	_	_
LSTM	_	_
based	_	_
,	_	_
Bojarski	_	_
et	_	_
al	_	_
[	_	_
11	_	_
]	_	_
used	_	_
a	_	_
convolutional	_	_
neural	_	_
network	_	_
to	_	_
map	_	_
images	_	_
to	_	_
steering	_	_
command	_	_
.	_	_

#44
Du	_	_
et	_	_
al	_	_
[	_	_
12	_	_
]	_	_
explore	_	_
two	_	_
different	_	_
models	_	_
for	_	_
steering	_	_
angle	_	_
prediction	_	_
.	_	_

#45
The	_	_
first	_	_
is	_	_
a	_	_
3D	_	_
convolutional	_	_
model	_	_
with	_	_
residual	_	_
connections	_	_
and	_	_
LSTM	_	_
cell	_	_
.	_	_

#46
The	_	_
second	_	_
is	_	_
uses	_	_
transfer	_	_
learning	_	_
to	_	_
fine-tune	_	_
a	_	_
pre-trained	_	_
CNN	_	_
and	_	_
predict	_	_
steering	_	_
angles	_	_
for	_	_
individual	_	_
images	_	_
.	_	_

#47
Much	_	_
less	_	_
work	_	_
has	_	_
been	_	_
done	_	_
on	_	_
the	_	_
application	_	_
of	_	_
attention	_	_
mechanism	_	_
in	_	_
autonomous	_	_
driving	_	_
.	_	_

#48
In	_	_
this	_	_
article	_	_
:	_	_
i	_	_
)	_	_
we	_	_
propose	_	_
a	_	_
new	_	_
sparse	_	_
attention	_	_
model	_	_
,	_	_
based	_	_
on	_	_
the	_	_
sparsemax	_	_
function	_	_
[	_	_
13	_	_
]	_	_
,	_	_
yields	_	_
better	_	_
performance	_	_
;	_	_
ii	_	_
)	_	_
we	_	_
demonstrate	_	_
that	_	_
bagging	_	_
multiple	_	_
sparse	_	_
attention	_	_
models	_	_
can	_	_
provide	_	_
a	_	_
significant	_	_
performance	_	_
improvement	_	_
over	_	_
single	_	_
models	_	_
;	_	_
iii	_	_
)	_	_
we	_	_
show	_	_
that	_	_
the	_	_
proposed	_	_
architecture	_	_
performs	_	_
better	_	_
than	_	_
the	_	_
state-of-the-art	_	_
soft	_	_
attention	_	_
model	_	_
,	_	_
CNN	_	_
,	_	_
CNN+LSTM	_	_
for	_	_
steering	_	_
angle	_	_
prediction	_	_
.	_	_

#49
III	_	_
.	_	_

#50
REGRESSION	_	_
OF	_	_
STEERING	_	_
ANGLES	_	_
WITH	_	_
ATTENTION	_	_
This	_	_
section	_	_
describes	_	_
the	_	_
proposed	_	_
sparse	_	_
attention	_	_
model	_	_
:	_	_
First	_	_
,	_	_
we	_	_
present	_	_
the	_	_
overall	_	_
architecture	_	_
in	_	_
Figure	_	_
1	_	_
;	_	_
we	_	_
then	_	_
describe	_	_
the	_	_
LSTM	_	_
model	_	_
and	_	_
sparse	_	_
attention	_	_
formulation	_	_
for	_	_
steering	_	_
regression	_	_
;	_	_
and	_	_
finally	_	_
,	_	_
the	_	_
proposed	_	_
model	_	_
aggregation	_	_
approach	_	_
.	_	_

#51
A	_	_
.	_	_

#52
Feature	_	_
Extraction	_	_
The	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
have	_	_
achieved	_	_
great	_	_
success	_	_
in	_	_
computer	_	_
vision	_	_
due	_	_
to	_	_
its	_	_
ability	_	_
to	_	_
learn	_	_
hierarchical	_	_
features	_	_
.	_	_

#53
In	_	_
our	_	_
model	_	_
,	_	_
we	_	_
extract	_	_
the	_	_
feature	_	_
for	_	_
each	_	_
frame	_	_
in	_	_
a	_	_
driving	_	_
video	_	_
using	_	_
the	_	_
convolutional	_	_
part	_	_
of	_	_
VGG16	_	_
[	_	_
14	_	_
]	_	_
,	_	_
which	_	_
was	_	_
trained	_	_
for	_	_
image	_	_
recognition	_	_
.	_	_

#54
After	_	_
feature	_	_
extraction	_	_
,	_	_
each	_	_
frame	_	_
was	_	_
represented	_	_
by	_	_
a	_	_
tensor	_	_
of	_	_
shape	_	_
M	_	_
×	_	_
N	_	_
×	_	_
K	_	_
determined	_	_
by	_	_
the	_	_
input	_	_
size	_	_
.	_	_

#55
We	_	_
refer	_	_
to	_	_
feature	_	_
tensor	_	_
as	_	_
a	_	_
feature	_	_
cube	_	_
with	_	_
M	_	_
×	_	_
N	_	_
locations	_	_
,	_	_
and	_	_
each	_	_
location	_	_
was	_	_
represented	_	_
by	_	_
a	_	_
feature	_	_
vector	_	_
of	_	_
K	_	_
elements	_	_
:	_	_
X	_	_
=	_	_
[	_	_
X1	_	_
,	_	_
X2	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
XM×N	_	_
]	_	_
(	_	_
1	_	_
)	_	_
The	_	_
feature	_	_
extraction	_	_
part	_	_
is	_	_
fixed	_	_
during	_	_
the	_	_
experiment	_	_
and	_	_
not	_	_
fine-tuned	_	_
.	_	_

#56
B.	_	_
LSTM	_	_
In	_	_
order	_	_
to	_	_
take	_	_
into	_	_
account	_	_
the	_	_
previous	_	_
context	_	_
to	_	_
predict	_	_
steering	_	_
angle	_	_
,	_	_
the	_	_
recurrent	_	_
neural	_	_
network	_	_
was	_	_
used	_	_
.	_	_

#57
Recurrent	_	_
neural	_	_
network	_	_
(	_	_
RNN	_	_
)	_	_
can	_	_
process	_	_
the	_	_
time	_	_
sequence	_	_
by	_	_
remembering	_	_
the	_	_
needed	_	_
information	_	_
and	_	_
forgetting	_	_
the	_	_
redundant	_	_
.	_	_

#58
Long	_	_
Short	_	_
Term	_	_
Memory	_	_
(	_	_
LSTM	_	_
)	_	_
networks	_	_
[	_	_
15	_	_
]	_	_
are	_	_
a	_	_
kind	_	_
of	_	_
gated	_	_
RNN	_	_
,	_	_
which	_	_
can	_	_
avoid	_	_
the	_	_
gradient	_	_
vanishing	_	_
or	_	_
exploding	_	_
problems	_	_
encountered	_	_
by	_	_
standard	_	_
RNNs	_	_
.	_	_

#59
C.	_	_
Sparse	_	_
Attention	_	_
A	_	_
fundamental	_	_
limitation	_	_
of	_	_
soft	_	_
attention	_	_
is	_	_
that	_	_
all	_	_
image	_	_
regions	_	_
are	_	_
in	_	_
effect	_	_
attended	_	_
to	_	_
at	_	_
all	_	_
times	_	_
:	_	_
Their	_	_
importance	_	_
is	_	_
merely	_	_
reweighed	_	_
by	_	_
the	_	_
attention	_	_
model	_	_
.	_	_

#60
This	_	_
is	_	_
contrary	_	_
to	_	_
the	_	_
very	_	_
intent	_	_
of	_	_
attention	_	_
learning	_	_
.	_	_

#61
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
mitigate	_	_
this	_	_
limitation	_	_
by	_	_
implementing	_	_
a	_	_
sparse	_	_
attention	_	_
mechanism	_	_
based	_	_
on	_	_
[	_	_
13	_	_
]	_	_
,	_	_
but	_	_
extending	_	_
it	_	_
to	_	_
visual	_	_
inputs	_	_
.	_	_

#62
The	_	_
output	_	_
of	_	_
the	_	_
attention	_	_
transformation	_	_
is	_	_
defined	_	_
as	_	_
Xweighted	_	_
=	_	_
X	_	_
·A	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
the	_	_
elements	_	_
of	_	_
sparse	_	_
attention	_	_
matrix	_	_
A	_	_
sums	_	_
to	_	_
1	_	_
,	_	_
and	_	_
it	_	_
is	_	_
determined	_	_
by	_	_
the	_	_
feature	_	_
cube	_	_
of	_	_
the	_	_
current	_	_
input	_	_
frame	_	_
and	_	_
the	_	_
model	_	_
hidden	_	_
state	_	_
in	_	_
the	_	_
last	_	_
time	_	_
step	_	_
and	_	_
normalised	_	_
by	_	_
the	_	_
sparsemax	_	_
function	_	_
:	_	_
A	_	_
=	_	_
sparsemax	_	_
(	_	_
tanh	_	_
(	_	_
WfX	_	_
+WhH	_	_
+	_	_
b	_	_
)	_	_
)	_	_
(	_	_
3	_	_
)	_	_
where	_	_
the	_	_
Wf	_	_
is	_	_
weight	_	_
for	_	_
the	_	_
current	_	_
input	_	_
frame’s	_	_
feature	_	_
,	_	_
Wh	_	_
is	_	_
the	_	_
weight	_	_
for	_	_
the	_	_
model	_	_
hidden	_	_
state	_	_
,	_	_
H	_	_
is	_	_
the	_	_
hidden	_	_
state	_	_
of	_	_
the	_	_
model	_	_
,	_	_
both	_	_
of	_	_
the	_	_
weights	_	_
are	_	_
learned	_	_
during	_	_
training	_	_
to	_	_
form	_	_
the	_	_
attention	_	_
matrix	_	_
,	_	_
the	_	_
sparsemax	_	_
function	_	_
which	_	_
is	_	_
defined	_	_
by	_	_
[	_	_
13	_	_
]	_	_
in	_	_
Algorithm	_	_
1	_	_
.	_	_

#63
Algorithm	_	_
1	_	_
sparsemax	_	_
Input	_	_
:	_	_
z	_	_
Sort	_	_
z	_	_
(	_	_
1	_	_
)	_	_
≥	_	_
·	_	_
·	_	_
·	_	_
≥	_	_
z	_	_
(	_	_
M×N	_	_
)	_	_
Find	_	_
k	_	_
(	_	_
z	_	_
)	_	_
:	_	_
=	_	_
max	_	_
{	_	_
k	_	_
∈	_	_
[	_	_
M	_	_
×N	_	_
]	_	_
|1	_	_
+	_	_
kz	_	_
(	_	_
k	_	_
)	_	_
>	_	_
∑	_	_
j≤k	_	_
z	_	_
(	_	_
j	_	_
)	_	_
}	_	_
Define	_	_
τ	_	_
(	_	_
z	_	_
)	_	_
=	_	_
(	_	_
∑	_	_
j≤k	_	_
(	_	_
z	_	_
)	_	_
z	_	_
(	_	_
j	_	_
)	_	_
)	_	_
−1	_	_
k	_	_
(	_	_
z	_	_
)	_	_
Output	_	_
:	_	_
p	_	_
s.t	_	_
.	_	_

#64
pi	_	_
=	_	_
[	_	_
zi	_	_
−	_	_
τ	_	_
(	_	_
z	_	_
)	_	_
]	_	_
+	_	_
One	_	_
can	_	_
see	_	_
that	_	_
the	_	_
sparsemax	_	_
function	_	_
is	_	_
not	_	_
continuous	_	_
.	_	_

#65
More	_	_
importantly	_	_
,	_	_
compared	_	_
to	_	_
the	_	_
softmax	_	_
function	_	_
,	_	_
it	_	_
has	_	_
the	_	_
ability	_	_
to	_	_
inhibit	_	_
the	_	_
unimportant	_	_
but	_	_
enhance	_	_
the	_	_
significant	_	_
elements	_	_
of	_	_
the	_	_
input	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#66
The	_	_
final	_	_
prediction	_	_
is	_	_
generated	_	_
by	_	_
a	_	_
two	_	_
layer	_	_
fully	_	_
connected	_	_
layers	_	_
(	_	_
FCN	_	_
)	_	_
:	_	_
S	_	_
(	_	_
t	_	_
)	_	_
=Wfcn2	_	_
(	_	_
Wfcn1Ht	_	_
+	_	_
bfcn1	_	_
)	_	_
+	_	_
bfcn2	_	_
(	_	_
4	_	_
)	_	_
where	_	_
,	_	_
S	_	_
(	_	_
t	_	_
)	_	_
is	_	_
the	_	_
predicted	_	_
steering	_	_
angle	_	_
,	_	_
Wfcn1	_	_
and	_	_
Wfcn2	_	_
are	_	_
the	_	_
weights	_	_
of	_	_
each	_	_
fully	_	_
connected	_	_
layer	_	_
,	_	_
bfcn1	_	_
and	_	_
bfcn2	_	_
are	_	_
the	_	_
bias	_	_
of	_	_
each	_	_
layer	_	_
,	_	_
and	_	_
Ht	_	_
is	_	_
the	_	_
output	_	_
of	_	_
LSTM	_	_
.	_	_

#67
D.	_	_
Model	_	_
Aggregation	_	_
Due	_	_
to	_	_
the	_	_
non-continuity	_	_
of	_	_
the	_	_
sparsemax	_	_
function	_	_
,	_	_
we	_	_
suggest	_	_
that	_	_
the	_	_
result	_	_
of	_	_
training	_	_
a	_	_
sparse	_	_
attention	_	_
model	_	_
is	_	_
highly	_	_
dependent	_	_
on	_	_
(	_	_
random	_	_
)	_	_
initialisation	_	_
.	_	_

#68
This	_	_
means	_	_
that	_	_
the	_	_
resulting	_	_
attention	_	_
models	_	_
after	_	_
training	_	_
,	_	_
although	_	_
converging	_	_
to	_	_
similar	_	_
performance	_	_
levels	_	_
,	_	_
correspond	_	_
to	_	_
very	_	_
different	_	_
local	_	_
minima	_	_
depending	_	_
on	_	_
the	_	_
random	_	_
initialisation	_	_
.	_	_

#69
In	_	_
other	_	_
words	_	_
,	_	_
the	_	_
same	_	_
task	_	_
can	_	_
afford	_	_
multiple	_	_
attention	_	_
models	_	_
of	_	_
similar	_	_
quality	_	_
.	_	_

#70
If	_	_
those	_	_
models	_	_
all	_	_
capture	_	_
different	_	_
aspects	_	_
of	_	_
the	_	_
task	_	_
,	_	_
a	_	_
combination	_	_
of	_	_
those	_	_
models	_	_
could	speculation	_
lead	_	_
to	_	_
better	_	_
performance	_	_
.	_	_

#71
Therefore	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
train	_	_
a	_	_
collection	_	_
of	_	_
N	_	_
(	_	_
we	_	_
choose	_	_
N	_	_
=	_	_
3	_	_
in	_	_
the	_	_
experiment	_	_
)	_	_
randomised	_	_
attention	_	_
models	_	_
.	_	_

#72
Because	_	_
model	_	_
variance	_	_
can	_	_
be	_	_
ensured	_	_
from	_	_
the	_	_
random	_	_
initialisation	_	_
,	_	_
we	_	_
can	_	_
train	_	_
them	_	_
all	_	_
using	_	_
the	_	_
same	_	_
dataset	_	_
(	_	_
experiments	_	_
confirmed	_	_
that	_	_
training	_	_
each	_	_
model	_	_
on	_	_
a	_	_
separate	_	_
bootstrap	_	_
samples	_	_
did	_	_
not	_	_
alter	_	_
the	_	_
results	_	_
significantly	_	_
)	_	_
.	_	_

#73
At	_	_
inference	_	_
time	_	_
we	_	_
propose	_	_
to	_	_
combine	_	_
these	_	_
attention	_	_
models	_	_
and	_	_
average	_	_
their	_	_
predictions	_	_
,	_	_
similarly	_	_
to	_	_
model	_	_
bagging	_	_
.	_	_

#74
IV	_	_
.	_	_

#75
EXPERIMENTAL	_	_
RESULTS	_	_
The	_	_
advantages	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
are	_	_
shown	_	_
using	_	_
DIPLECS	_	_
dataset	_	_
[	_	_
5	_	_
]	_	_
,	_	_
containing	_	_
indoor	_	_
and	_	_
outdoor	_	_
scenarios	_	_
,	_	_
and	_	_
Comma.ai	_	_
dataset	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#76
The	_	_
proposed	_	_
method	_	_
is	_	_
compared	_	_
to	_	_
soft	_	_
attention	_	_
and	_	_
aggregated	_	_
soft	_	_
attention	_	_
(	_	_
ASA	_	_
)	_	_
.	_	_

#77
Also	_	_
the	_	_
method	_	_
is	_	_
compared	_	_
to	_	_
the	_	_
gist-based	_	_
approach	_	_
[	_	_
5	_	_
]	_	_
,	_	_
method	_	_
with	_	_
no	_	_
attention	_	_
and	_	_
no	_	_
LSTM	_	_
(	_	_
CNN	_	_
)	_	_
,	_	_
as	_	_
well	_	_
as	_	_
LSTM	_	_
without	_	_
attention	_	_
(	_	_
CNN+LSTM	_	_
)	_	_
.	_	_

#78
To	_	_
measure	_	_
the	_	_
prediction	_	_
quality	_	_
,	_	_
we	_	_
use	_	_
the	_	_
mean	_	_
absolute	_	_
error	_	_
.	_	_

#79
A.	_	_
Dataset	_	_
description	_	_
The	_	_
indoor	_	_
part	_	_
of	_	_
the	_	_
DIPLECS	_	_
dataset	_	_
[	_	_
5	_	_
]	_	_
is	_	_
collected	_	_
using	_	_
a	_	_
radio	_	_
controlled	_	_
car	_	_
(	_	_
see	_	_
Figure	_	_
2	_	_
,	_	_
left	_	_
)	_	_
.	_	_

#80
There	_	_
are	_	_
two	_	_
tracks	_	_
,	_	_
P-shaped	_	_
and	_	_
O-shaped	_	_
,	_	_
eight	_	_
recordings	_	_
for	_	_
each	_	_
of	_	_
them	_	_
from	_	_
different	_	_
starting	_	_
point	_	_
.	_	_

#81
For	_	_
each	_	_
of	_	_
the	_	_
tracks	_	_
,	_	_
three	_	_
recordings	_	_
are	_	_
used	_	_
for	_	_
training	_	_
,	_	_
one	_	_
for	_	_
validation	_	_
,	_	_
and	_	_
the	_	_
rest	_	_
for	_	_
testing	_	_
.	_	_

#82
The	_	_
outdoor	_	_
part	_	_
of	_	_
the	_	_
DIPLECS	_	_
dataset	_	_
[	_	_
5	_	_
]	_	_
contains	_	_
real	_	_
world	_	_
driving	_	_
scenarios	_	_
(	_	_
see	_	_
Figure	_	_
2	_	_
,	_	_
middle	_	_
)	_	_
,	_	_
totalling	_	_
about	_	_
47	_	_
minutes	_	_
of	_	_
driving	_	_
,	_	_
or	_	_
84	_	_
,	_	_
690	_	_
frames	_	_
.	_	_

#83
This	_	_
dataset	_	_
has	_	_
been	_	_
divided	_	_
into	_	_
eight	_	_
subsequences	_	_
of	_	_
the	_	_
same	_	_
length	_	_
,	_	_
with	_	_
junctions	_	_
removed	_	_
as	_	_
causing	_	_
ambiguity	_	_
which	_	_
can	_	_
not	_	_
be	_	_
resolved	_	_
using	_	_
vision-based	_	_
information	_	_
.	_	_

#84
Six	_	_
subsequences	_	_
were	_	_
used	_	_
to	_	_
train	_	_
the	_	_
model	_	_
,	_	_
another	_	_
two	_	_
were	_	_
used	_	_
for	_	_
validation	_	_
and	_	_
testing	_	_
.	_	_

#85
In	_	_
order	_	_
to	_	_
factor	_	_
out	_	_
focusing	_	_
attention	_	_
on	_	_
the	_	_
steering	_	_
wheel	_	_
and	_	_
the	_	_
mirror	_	_
,	_	_
these	_	_
regions	_	_
were	_	_
cropped	_	_
.	_	_

#86
Comma.ai	_	_
dataset	_	_
consists	_	_
of	_	_
10	_	_
day-	_	_
and	_	_
night-time	_	_
highway	_	_
driving	_	_
video	_	_
clips	_	_
of	_	_
variable	_	_
size	_	_
,	_	_
in	_	_
total	_	_
7.5	_	_
hours	_	_
(	_	_
see	_	_
Figure	_	_
2	_	_
,	_	_
right	_	_
)	_	_
.	_	_

#87
We	_	_
extract	_	_
8	_	_
sequences	_	_
from	_	_
the	_	_
dataset	_	_
,	_	_
each	_	_
of	_	_
them	_	_
contains	_	_
4000	_	_
frames	_	_
,	_	_
and	_	_
use	_	_
6	_	_
of	_	_
them	_	_
to	_	_
train	_	_
and	_	_
the	_	_
rest	_	_
for	_	_
validating	_	_
and	_	_
testing	_	_
.	_	_

#88
B	_	_
.	_	_

#89
Training	_	_
parameters	_	_
Our	_	_
models	_	_
are	_	_
trained	_	_
using	_	_
Tensorflow	_	_
[	_	_
17	_	_
]	_	_
with	_	_
the	_	_
L1	_	_
norm	_	_
loss	_	_
function	_	_
,	_	_
we	_	_
set	_	_
the	_	_
learning	_	_
rate	_	_
as	_	_
10−4	_	_
and	_	_
use	_	_
Adam	_	_
[	_	_
18	_	_
]	_	_
optimisation	_	_
method	_	_
to	_	_
train	_	_
the	_	_
model	_	_
,	_	_
all	_	_
weights	_	_
to	_	_
be	_	_
trained	_	_
in	_	_
the	_	_
model	_	_
are	_	_
initialised	_	_
using	_	_
Xavier	_	_
[	_	_
19	_	_
]	_	_
initialisation	_	_
method	_	_
.	_	_

#90
C.	_	_
Indoor	_	_
Dataset	_	_
Results	_	_
In	_	_
the	_	_
indoor	_	_
dataset	_	_
,	_	_
the	_	_
remote	_	_
control	_	_
steering	_	_
angle	_	_
signal	_	_
was	_	_
normalised	_	_
to	_	_
[	_	_
−1	_	_
,	_	_
1	_	_
]	_	_
(	_	_
−1	_	_
corresponds	_	_
to	_	_
the	_	_
leftmost	_	_
and	_	_
1	_	_
to	_	_
the	_	_
rightmost	_	_
angle	_	_
)	_	_
.	_	_

#91
In	_	_
Figures	_	_
4	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
steering	_	_
regression	_	_
improves	_	_
with	_	_
the	_	_
addition	_	_
of	_	_
any	_	_
attention	_	_
mechanism	_	_
.	_	_

#92
Also	_	_
,	_	_
sparse	_	_
attention	_	_
performs	_	_
better	_	_
than	_	_
soft	_	_
attention	_	_
,	_	_
and	_	_
the	_	_
proposed	_	_
aggregated	_	_
sparse	_	_
attention	_	_
model	_	_
performs	_	_
best	_	_
among	_	_
those	_	_
models	_	_
.	_	_

#93
Importantly	_	_
,	_	_
we	_	_
note	_	_
that	_	_
the	_	_
models	_	_
with	_	_
attention	_	_
provide	_	_
a	_	_
steering	_	_
control	_	_
that	_	_
is	_	_
not	_	_
only	_	_
more	_	_
accurate	_	_
but	_	_
also	_	_
smoother	_	_
,	_	_
which	_	_
may	_	_
be	_	_
favourable	_	_
for	_	_
control	_	_
applications	_	_
.	_	_

#94
This	_	_
particularly	_	_
true	_	_
for	_	_
the	_	_
proposed	_	_
model	_	_
.	_	_

#95
Figure	_	_
5	_	_
show	_	_
the	_	_
attention	_	_
maps	_	_
for	_	_
soft	_	_
and	_	_
sparse	_	_
attention	_	_
respectively	_	_
.	_	_

#96
Note	_	_
that	_	_
the	_	_
attended	_	_
regions	_	_
for	_	_
most	_	_
models	_	_
appear	_	_
to	_	_
be	_	_
focused	_	_
on	_	_
the	_	_
road	_	_
markings	_	_
,	_	_
some	_	_
of	_	_
them	_	_
on	_	_
the	_	_
boundary	_	_
marking	_	_
or	_	_
on	_	_
the	_	_
central	_	_
one	_	_
and	_	_
some	_	_
of	_	_
them	_	_
on	_	_
both	_	_
.	_	_

#97
We	_	_
note	_	_
also	_	_
that	_	_
the	_	_
attention	_	_
map	_	_
for	_	_
sparse	_	_
attention	_	_
is	_	_
sparser	_	_
than	_	_
soft	_	_
attention	_	_
,	_	_
which	_	_
was	_	_
the	_	_
purpose	_	_
of	_	_
using	_	_
the	_	_
sparsemax	_	_
function	_	_
.	_	_

#98
D.	_	_
Outdoor	_	_
Dataset	_	_
Results	_	_
In	_	_
practice	_	_
,	_	_
a	_	_
driver’s	_	_
actions	_	_
are	_	_
not	_	_
instantaneous	_	_
:	_	_
due	_	_
to	_	_
reaction	_	_
time	_	_
,	_	_
the	_	_
driver’s	_	_
actions	_	_
at	_	_
any	_	_
instant	_	_
t	_	_
are	_	_
based	_	_
on	_	_
the	_	_
visual	_	_
input	_	_
received	_	_
some	_	_
time	_	_
before	_	_
.	_	_

#99
According	_	_
to	_	_
the	_	_
studies	_	_
in	_	_
[	_	_
20	_	_
]	_	_
,	_	_
[	_	_
21	_	_
]	_	_
,	_	_
a	_	_
driver’s	_	_
reaction	_	_
time	_	_
can	_	_
varies	_	_
from	_	_
a	_	_
few	_	_
hundred	_	_
milliseconds	_	_
to	_	_
several	_	_
seconds	_	_
.	_	_

#100
Before	_	_
this	_	_
section	_	_
,	_	_
we	_	_
were	_	_
predicting	_	_
just	_	_
the	_	_
steering	_	_
angle	_	_
for	_	_
the	_	_
current	_	_
frame	_	_
(	_	_
s	_	_
(	_	_
t	_	_
)	_	_
=	_	_
f	_	_
(	_	_
i	_	_
(	_	_
t	_	_
)	_	_
)	_	_
)	_	_
.	_	_

#101
Here	_	_
we	_	_
use	_	_
the	_	_
current	_	_
frame	_	_
to	_	_
predict	_	_
the	_	_
steering	_	_
angle	_	_
for	_	_
different	_	_
time	_	_
delays	_	_
(	_	_
s	_	_
(	_	_
t	_	_
+	_	_
d	_	_
)	_	_
=	_	_
f	_	_
(	_	_
i	_	_
(	_	_
t	_	_
)	_	_
)	_	_
.	_	_

#102
We	_	_
choose	_	_
the	_	_
delays	_	_
corresponding	_	_
to	_	_
Fig.	_	_
2	_	_
:	_	_
The	_	_
datasets	_	_
used	_	_
in	_	_
this	_	_
article	_	_
:	_	_
left	_	_
,	_	_
the	_	_
DIPLECS	_	_
indoor	_	_
dataset	_	_
(	_	_
figure	_	_
reproduced	_	_
from	_	_
[	_	_
5	_	_
]	_	_
)	_	_
;	_	_
middle	_	_
,	_	_
the	_	_
DIPLECS	_	_
indoor	_	_
dataset	_	_
(	_	_
figure	_	_
reproduced	_	_
from	_	_
[	_	_
5	_	_
]	_	_
;	_	_
right	_	_
,	_	_
the	_	_
Comma.ai	_	_
dataset	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#103
(	_	_
a	_	_
)	_	_
CNN	_	_
(	_	_
b	_	_
)	_	_
CNN+LSTM	_	_
(	_	_
c	_	_
)	_	_
Soft	_	_
attention	_	_
(	_	_
d	_	_
)	_	_
ASA	_	_
(	_	_
e	_	_
)	_	_
Proposed	_	_
method	_	_
Fig.	_	_
3	_	_
:	_	_
Steering	_	_
angle	_	_
prediction	_	_
of	_	_
different	_	_
method	_	_
in	_	_
one	_	_
recordings	_	_
of	_	_
the	_	_
indoor	_	_
dataset	_	_
.	_	_

#104
Fig.	_	_
4	_	_
:	_	_
The	_	_
mean	_	_
error	_	_
of	_	_
different	_	_
methods	_	_
in	_	_
the	_	_
DIPLECS	_	_
indoor	_	_
dataset	_	_
(	_	_
left	_	_
)	_	_
,	_	_
and	_	_
the	_	_
mean	_	_
error	_	_
of	_	_
each	_	_
single	_	_
predictor	_	_
and	_	_
aggregated	_	_
predictor	_	_
for	_	_
sparse	_	_
and	_	_
soft	_	_
attention	_	_
in	_	_
DIPLECS	_	_
indoor	_	_
dataset	_	_
(	_	_
right	_	_
)	_	_
.	_	_

#105
Fig.	_	_
5	_	_
:	_	_
The	_	_
attention	_	_
map	_	_
for	_	_
soft	_	_
attention	_	_
(	_	_
left	_	_
)	_	_
and	_	_
sparse	_	_
attention	_	_
(	_	_
right	_	_
)	_	_
in	_	_
DIPLECTS	_	_
indoor	_	_
dataset	_	_
0s	_	_
,	_	_
0.25s	_	_
,	_	_
0.5s	_	_
,	_	_
0.75s	_	_
,	_	_
1s	_	_
,	_	_
and	_	_
compare	_	_
the	_	_
results	_	_
for	_	_
different	_	_
attention	_	_
models	_	_
.	_	_

#106
One	_	_
can	_	_
see	_	_
from	_	_
Figure	_	_
7	_	_
that	_	_
the	_	_
proposed	_	_
aggregated	_	_
sparse	_	_
attention	_	_
model	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
among	_	_
all	_	_
methods	_	_
with	_	_
0.5s	_	_
time	_	_
delay	_	_
,	_	_
which	_	_
is	_	_
also	_	_
the	_	_
minimum	_	_
mean	_	_
error	_	_
for	_	_
all	_	_
time	_	_
delay	_	_
among	_	_
all	_	_
methods	_	_
.	_	_

#107
All	_	_
models	_	_
,	_	_
which	_	_
have	_	_
an	_	_
attention	_	_
mechanism	_	_
,	_	_
perform	_	_
much	_	_
better	_	_
than	_	_
those	_	_
without	_	_
attention	_	_
,	_	_
and	_	_
Figure	_	_
6	_	_
shows	_	_
that	_	_
the	_	_
model	_	_
with	_	_
attention	_	_
mechanism	_	_
is	_	_
more	_	_
stable	_	_
,	_	_
with	_	_
less	_	_
perturbation	_	_
for	_	_
steering	_	_
angle	_	_
prediction	_	_
,	_	_
than	_	_
the	_	_
model	_	_
without	_	_
attention	_	_
.	_	_

#108
Figures	_	_
8	_	_
and	_	_
9	_	_
,	_	_
show	_	_
the	_	_
area	_	_
of	_	_
the	_	_
visual	_	_
field	_	_
where	_	_
attention	_	_
is	_	_
focused	_	_
,	_	_
for	_	_
selected	_	_
frames	_	_
.	_	_

#109
Each	_	_
single	_	_
attention	_	_
map	_	_
only	_	_
focus	_	_
on	_	_
a	_	_
few	_	_
different	_	_
areas	_	_
(	_	_
the	_	_
bright	_	_
parts	_	_
)	_	_
.	_	_

#110
Initially	_	_
,	_	_
attended	_	_
locations	_	_
are	_	_
mostly	_	_
at	_	_
the	_	_
bottom	_	_
of	_	_
the	_	_
screen	_	_
,	_	_
but	_	_
when	_	_
time	_	_
delay	_	_
increases	_	_
,	_	_
we	_	_
start	_	_
seeing	_	_
locations	_	_
higher	_	_
in	_	_
the	_	_
image	_	_
being	_	_
attended—this	_	_
is	_	_
especially	_	_
visible	_	_
for	_	_
prediction	_	_
time	_	_
delay	_	_
of	_	_
0.25s	_	_
and	_	_
0.5s	_	_
.	_	_

#111
It	_	_
is	_	_
also	_	_
remarkable	_	_
that	_	_
in	_	_
Figures	_	_
10	_	_
,	_	_
even	_	_
though	_	_
every	_	_
single	_	_
predictor	_	_
within	_	_
the	_	_
aggregated	_	_
sparse	_	_
attention	_	_
is	_	_
trained	_	_
using	_	_
the	_	_
same	_	_
dataset	_	_
,	_	_
the	_	_
aggregated	_	_
model	_	_
performs	_	_
better	_	_
than	_	_
any	_	_
single	_	_
sparse	_	_
attention	_	_
predictor	_	_
.	_	_

#112
For	_	_
the	_	_
aggregated	_	_
soft	_	_
attention	_	_
model	_	_
,	_	_
two	_	_
varieties	_	_
of	_	_
the	_	_
model	_	_
were	_	_
compared	_	_
:	_	_
each	_	_
single	_	_
model	_	_
within	_	_
the	_	_
aggregation	_	_
has	_	_
been	_	_
trained	_	_
on	_	_
the	_	_
same	_	_
training	_	_
set	_	_
(	_	_
ASA	_	_
)	_	_
or	_	_
on	_	_
different	_	_
random	_	_
subsets	_	_
(	_	_
ASAR	_	_
)	_	_
.	_	_

#113
After	_	_
model	_	_
aggregation	_	_
,	_	_
the	_	_
aggregated	_	_
soft	_	_
attention	_	_
model	_	_
performed	_	_
worse	_	_
than	_	_
the	_	_
single	_	_
soft	_	_
attention	_	_
model	_	_
for	_	_
time	_	_
delays	_	_
0.25	_	_
s,0.5	_	_
s.	_	_
We	_	_
suggest	_	_
this	_	_
is	_	_
due	_	_
to	_	_
the	_	_
cross-correlation	_	_
between	_	_
the	_	_
attention	_	_
maps	_	_
of	_	_
each	_	_
single	_	_
soft	_	_
attention	_	_
predictor	_	_
.	_	_

#114
In	_	_
Figure	_	_
11	_	_
,	_	_
one	_	_
can	_	_
see	_	_
that	_	_
there	_	_
is	_	_
a	_	_
high	_	_
correlation	_	_
between	_	_
the	_	_
attention	_	_
maps	_	_
;	_	_
even	_	_
the	_	_
smallest	_	_
correlation	_	_
coefficient	_	_
of	_	_
the	_	_
soft	_	_
attention	_	_
map	_	_
pairs	_	_
is	_	_
larger	_	_
than	_	_
the	_	_
largest	_	_
correlation	_	_
coefficient	_	_
for	_	_
the	_	_
sparse	_	_
attention	_	_
map	_	_
,	_	_
which	_	_
suggests	_	_
those	_	_
soft	_	_
attention	_	_
model	_	_
tend	_	_
to	_	_
focus	_	_
on	_	_
more	_	_
or	_	_
less	_	_
the	_	_
same	_	_
region	_	_
.	_	_

#115
In	_	_
this	_	_
case	_	_
,	_	_
if	_	_
a	_	_
single	_	_
predictor	_	_
model	_	_
over-	_	_
or	_	_
underestimates	_	_
the	_	_
steering	_	_
angle	_	_
at	_	_
some	_	_
time	_	_
,	_	_
the	_	_
other	_	_
correlated	_	_
predictors	_	_
would	_	_
also	_	_
(	_	_
a	_	_
)	_	_
CNN+LSTM	_	_
(	_	_
b	_	_
)	_	_
Soft	_	_
attention	_	_
(	_	_
c	_	_
)	_	_
ASA	_	_
(	_	_
d	_	_
)	_	_
ASAR	_	_
(	_	_
e	_	_
)	_	_
Proposed	_	_
method	_	_
Fig.	_	_
6	_	_
:	_	_
Predicting	_	_
the	_	_
steering	_	_
angle	_	_
0.5	_	_
seconds	_	_
later	_	_
,	_	_
by	_	_
different	_	_
methods	_	_
for	_	_
a	_	_
subsequence	_	_
of	_	_
the	_	_
DIPLECS	_	_
outdoor	_	_
testing	_	_
dataset	_	_
,	_	_
the	_	_
blue	_	_
curve	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
and	_	_
the	_	_
red	_	_
one	_	_
is	_	_
the	_	_
predicted	_	_
steering	_	_
angle	_	_
.	_	_

#116
Fig.	_	_
7	_	_
:	_	_
The	_	_
mean	_	_
regression	_	_
error	_	_
of	_	_
different	_	_
attention	_	_
models	_	_
on	_	_
the	_	_
DIPLECS	_	_
outdoor	_	_
testing	_	_
dataset	_	_
,	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
.	_	_

#117
Delay	_	_
Attention	_	_
map1	_	_
Attention	_	_
map2	_	_
Attention	_	_
map3	_	_
0.25	_	_
s	_	_
0.5	_	_
s	_	_
Fig.	_	_
8	_	_
:	_	_
Attention	_	_
maps	_	_
for	_	_
0.25s	_	_
and	_	_
0.5s	_	_
delays	_	_
of	_	_
each	_	_
single	_	_
sparse	_	_
predictor	_	_
have	_	_
the	_	_
same	_	_
trend	_	_
in	_	_
steering	_	_
angle	_	_
estimation	_	_
,	_	_
and	_	_
after	_	_
model	_	_
aggregation	_	_
it	_	_
would	_	_
negatively	_	_
impact	_	_
the	_	_
final	_	_
error	_	_
.	_	_

#118
This	_	_
result	_	_
also	_	_
confirms	_	_
our	_	_
suggestion	_	_
about	_	_
diversity	_	_
of	_	_
individual	_	_
sparse	_	_
attention	_	_
maps	_	_
,	_	_
made	_	_
in	_	_
section	_	_
III-D.	_	_
E.	_	_
Comma.ai	_	_
Dataset	_	_
The	_	_
testing	_	_
procedure	_	_
for	_	_
Comma.ai	_	_
dataset	_	_
is	_	_
the	_	_
same	_	_
as	_	_
for	_	_
DIPLECS	_	_
outdoor	_	_
dataset	_	_
.	_	_

#119
One	_	_
can	_	_
see	_	_
from	_	_
Figure	_	_
12	_	_
,	_	_
13	_	_
that	_	_
the	_	_
proposed	_	_
aggregated	_	_
sparse	_	_
attention	_	_
model	_	_
still	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
among	_	_
all	_	_
methods	_	_
,	_	_
but	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
(	_	_
1s	_	_
time	_	_
delay	_	_
)	_	_
,	_	_
we	_	_
suggest	_	_
that	_	_
this	_	_
is	_	_
due	_	_
to	_	_
the	_	_
driving	_	_
environment	_	_
being	_	_
a	_	_
highway	_	_
with	_	_
a	_	_
broad	_	_
view	_	_
far	_	_
ahead	_	_
of	_	_
the	_	_
car	_	_
,	_	_
and	_	_
therefore	_	_
possibly	_	_
requiring	_	_
(	_	_
a	_	_
)	_	_
0s	_	_
delay	_	_
(	_	_
b	_	_
)	_	_
0.25s	_	_
delay	_	_
(	_	_
c	_	_
)	_	_
0.5s	_	_
delay	_	_
Fig.	_	_
9	_	_
:	_	_
The	_	_
input	_	_
frame	_	_
overlapped	_	_
by	_	_
the	_	_
attention	_	_
map	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
.	_	_

#120
Fig.	_	_
10	_	_
:	_	_
The	_	_
mean	_	_
error	_	_
of	_	_
each	_	_
single	_	_
predictor	_	_
as	_	_
well	_	_
as	_	_
aggregated	_	_
predictor	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
of	_	_
sparse	_	_
(	_	_
left	_	_
)	_	_
and	_	_
soft	_	_
(	_	_
right	_	_
)	_	_
attention	_	_
in	_	_
DIPLECS	_	_
outdoor	_	_
dataset	_	_
.	_	_

#121
(	_	_
a	_	_
)	_	_
Soft	_	_
attention	_	_
(	_	_
b	_	_
)	_	_
Sparse	_	_
attention	_	_
Fig.	_	_
11	_	_
:	_	_
The	_	_
cross	_	_
correlation	_	_
between	_	_
the	_	_
attention	_	_
map	_	_
of	_	_
each	_	_
single	_	_
attention	_	_
model	_	_
for	_	_
aggregated	_	_
sparse	_	_
attention	_	_
model	_	_
and	_	_
aggregated	_	_
soft	_	_
attention	_	_
model	_	_
with	_	_
0.5s	_	_
time	_	_
delay	_	_
.	_	_

#122
less	_	_
attention	_	_
from	_	_
the	_	_
driver	_	_
than	_	_
the	_	_
countryside	_	_
road	_	_
in	_	_
the	_	_
DIPLECS	_	_
outdoor	_	_
dataset	_	_
.	_	_

#123
On	_	_
this	_	_
dataset	_	_
as	_	_
previously	_	_
,	_	_
all	_	_
models	_	_
with	_	_
attention	_	_
mechanism	_	_
perform	_	_
much	_	_
better	_	_
than	_	_
models	_	_
without	_	_
attention	_	_
.	_	_

#124
Also	_	_
,	_	_
Figure	_	_
14	_	_
(	_	_
left	_	_
)	_	_
confirms	_	_
,	_	_
as	_	_
was	_	_
the	_	_
case	_	_
with	_	_
the	_	_
DIPLECS	_	_
outdoor	_	_
dataset	_	_
,	_	_
that	_	_
even	_	_
though	_	_
every	_	_
single	_	_
predictor	_	_
within	_	_
the	_	_
aggregated	_	_
sparse	_	_
attention	_	_
is	_	_
trained	_	_
using	_	_
the	_	_
same	_	_
dataset	_	_
,	_	_
the	_	_
aggregated	_	_
model	_	_
performs	_	_
significantly	_	_
better	_	_
than	_	_
any	_	_
single	_	_
sparse	_	_
attention	_	_
predictor	_	_
.	_	_

#125
The	_	_
performance	_	_
improvement	_	_
from	_	_
attention	_	_
model	_	_
aggregation	_	_
is	_	_
less	_	_
evident	_	_
when	_	_
considering	_	_
soft	_	_
attention	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
14	_	_
(	_	_
right	_	_
)	_	_
.	_	_

#126
V.	_	_
CONCLUSION	_	_
Attention	_	_
plays	_	_
an	_	_
essential	_	_
role	_	_
in	_	_
human	_	_
driving	_	_
.	_	_

#127
This	_	_
article	_	_
experiments	_	_
with	_	_
existing	_	_
neural	_	_
network	_	_
models	_	_
for	_	_
task-directed	_	_
attention	_	_
,	_	_
and	_	_
proposed	_	_
improved	_	_
models	_	_
the	_	_
task	_	_
of	_	_
steering	_	_
a	_	_
car	_	_
autonomously	_	_
.	_	_

#128
Our	_	_
experiments	_	_
show	_	_
that	_	_
:	_	_
i	_	_
)	_	_
(	_	_
a	_	_
)	_	_
CNN+LSTM	_	_
(	_	_
b	_	_
)	_	_
Soft	_	_
attention	_	_
(	_	_
c	_	_
)	_	_
ASA	_	_
(	_	_
d	_	_
)	_	_
ASAR	_	_
(	_	_
e	_	_
)	_	_
Proposed	_	_
method	_	_
Fig.	_	_
12	_	_
:	_	_
Predicting	_	_
the	_	_
steering	_	_
angle	_	_
1	_	_
seconds	_	_
later	_	_
,	_	_
by	_	_
different	_	_
methods	_	_
for	_	_
a	_	_
subsequence	_	_
of	_	_
the	_	_
Comma.ai	_	_
testing	_	_
dataset	_	_
,	_	_
the	_	_
blue	_	_
curve	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
and	_	_
the	_	_
red	_	_
one	_	_
is	_	_
the	_	_
predicted	_	_
steering	_	_
angle	_	_
.	_	_

#129
Fig.	_	_
13	_	_
:	_	_
The	_	_
mean	_	_
error	_	_
of	_	_
different	_	_
methods	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
in	_	_
Comma.ai	_	_
dataset	_	_
.	_	_

#130
Fig.	_	_
14	_	_
:	_	_
The	_	_
mean	_	_
error	_	_
of	_	_
each	_	_
single	_	_
predictor	_	_
as	_	_
well	_	_
as	_	_
aggregated	_	_
predictor	_	_
for	_	_
different	_	_
time	_	_
delay	_	_
of	_	_
sparse	_	_
(	_	_
left	_	_
)	_	_
and	_	_
soft	_	_
(	_	_
right	_	_
)	_	_
attention	_	_
in	_	_
Comma.ai	_	_
dataset	_	_
.	_	_

#131
all	_	_
attention	_	_
models	_	_
improve	_	_
steering	_	_
prediction	_	_
significantly	_	_
;	_	_
ii	_	_
)	_	_
a	_	_
sparse	_	_
attention	_	_
model	_	_
yields	_	_
better	_	_
performance	_	_
than	_	_
classical	_	_
soft	_	_
attention	_	_
;	_	_
and	_	_
iii	_	_
)	_	_
an	_	_
aggregated	_	_
ensemble	_	_
based	_	_
on	_	_
randomised	_	_
attention	_	_
models	_	_
can	_	_
achieve	_	_
significantly	_	_
better	_	_
performances	_	_
than	_	_
a	_	_
single	_	_
attention	_	_
model	_	_
,	_	_
even	_	_
when	_	_
trained	_	_
on	_	_
the	_	_
same	_	_
data	_	_
.	_	_

#132
The	_	_
method	_	_
has	_	_
been	_	_
assessed	_	_
in	_	_
a	_	_
variety	_	_
of	_	_
scenarios	_	_
on	_	_
three	_	_
datasets	_	_
,	_	_
and	_	_
achieves	_	_
better	_	_
performance	_	_
than	_	_
state-of-the-art	_	_
.	_	_

#133
Additionally	_	_
,	_	_
as	_	_
was	_	_
done	_	_
in	_	_
previous	_	_
published	_	_
works	_	_
the	_	_
problem	_	_
of	_	_
steering	_	_
angle	_	_
prediction	_	_
with	_	_
a	_	_
perception-action	_	_
delay	_	_
has	_	_
been	_	_
considered	_	_
,	_	_
demonstrating	_	_
that	_	_
the	_	_
model	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
for	_	_
0.5s	_	_
delay	_	_
for	_	_
countryside	_	_
road	_	_
and	_	_
1s	_	_
delay	_	_
for	_	_
a	_	_
highway	_	_
.	_	_