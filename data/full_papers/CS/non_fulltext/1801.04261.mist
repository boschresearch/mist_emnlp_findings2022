#0
Deep	_	_
saliency	_	_
:	_	_
What	_	_
is	_	_
learnt	_	_
by	_	_
a	_	_
deep	_	_
network	_	_
about	_	_
saliency	_	_
?	_	_

#1
Sen	_	_
He	_	_
1	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#2
Abstract	_	_

#3
Deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
have	_	_
achieved	_	_
impressive	_	_
performance	_	_
on	_	_
a	_	_
broad	_	_
range	_	_
of	_	_
problems	_	_
,	_	_
beating	_	_
prior	_	_
art	_	_
on	_	_
established	_	_
benchmarks	_	_
,	_	_
but	_	_
it	_	_
often	_	_
remains	_	_
unclear	_	_
what	_	_
are	_	_
the	_	_
representations	_	_
learnt	_	_
by	_	_
those	_	_
systems	_	_
and	_	_
how	_	_
they	_	_
achieve	_	_
such	_	_
performance	_	_
.	_	_

#4
This	_	_
article	_	_
examines	_	_
the	_	_
specific	_	_
problem	_	_
of	_	_
saliency	_	_
detection	_	_
,	_	_
where	_	_
benchmarks	_	_
are	_	_
currently	_	_
dominated	_	_
by	_	_
CNN-based	_	_
approaches	_	_
,	_	_
and	_	_
investigates	_	_
the	_	_
properties	_	_
of	_	_
the	_	_
learnt	_	_
representation	_	_
by	_	_
visualizing	_	_
the	_	_
artificial	_	_
neurons’	_	_
receptive	_	_
fields	_	_
.	_	_

#5
We	_	_
demonstrate	_	_
that	_	_
fine	_	_
tuning	_	_
a	_	_
pre-trained	_	_
network	_	_
on	_	_
the	_	_
saliency	_	_
detection	_	_
task	_	_
lead	_	_
to	_	_
a	_	_
profound	_	_
transformation	_	_
of	_	_
the	_	_
network’s	_	_
deeper	_	_
layers	_	_
.	_	_

#6
Moreover	_	_
we	_	_
argue	_	_
that	_	_
this	_	_
transformation	_	_
leads	_	_
to	_	_
the	_	_
emergence	_	_
of	_	_
receptive	_	_
fields	_	_
conceptually	_	_
similar	_	_
to	_	_
the	_	_
centre-surround	_	_
filters	_	_
hypothesized	_	_
by	_	_
early	_	_
research	_	_
on	_	_
visual	_	_
saliency	_	_
.	_	_

#7
1.	_	_
Introduction	_	_

#8
Deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
have	_	_
achieved	_	_
great	_	_
success	_	_
in	_	_
dealing	_	_
with	_	_
computer	_	_
vision	_	_
problems	_	_
,	_	_
such	_	_
as	_	_
image	_	_
classification	_	_
(	_	_
Krizhevsky	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
,	_	_
object	_	_
detection	_	_
(	_	_
Girshick	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
and	_	_
semantic	_	_
segmentation	_	_
(	_	_
Girshick	_	_
et	_	_
al.	_	_
,	_	_
2014	_	_
)	_	_
,	_	_
etc	_	_
.	_	_

#9
However	_	_
,	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
are	_	_
complex	_	_
non-linear	_	_
system	_	_
,	_	_
and	_	_
what	_	_
is	_	_
learnt	_	_
by	_	_
intermediate	_	_
layers	_	_
remain	_	_
in	_	_
most	_	_
case	_	_
mysterious	_	_
.	_	_

#10
In	_	_
addition	_	_
,	_	_
despite	_	_
high	_	_
performance	_	_
on	_	_
many	_	_
benchmarks	_	_
,	_	_
recent	_	_
published	_	_
research	_	_
has	_	_
demonstrated	_	_
that	_	_
despite	_	_
high	_	_
performance	_	_
on	_	_
benchmark	_	_
measures	_	_
,	_	_
deep	_	_
networks	_	_
could	feasibility	_
be	_	_
easily	_	_
fooled	_	_
by	_	_
small	_	_
perturbations	_	_
of	_	_
the	_	_
original	_	_
signal	_	_
(	_	_
Moosavi-Dezfooli	_	_
et	_	_
al.	_	_
,	_	_
2016	_	_
)	_	_
,	_	_
begging	_	_
the	_	_
question	_	_
what	_	_
are	_	_
the	_	_
representations	_	_
learnt	_	_
by	_	_
the	_	_
networks	_	_
and	_	_
how	_	_
they	_	_
are	_	_
used	_	_
to	_	_
answer	_	_
the	_	_
chosen	_	_
task	_	_
(	_	_
ie	_	_
,	_	_
do	_	_
we	_	_
recognise	_	_
a	_	_
bunny	_	_
by	_	_
its	_	_
ears	_	_
or	_	_
by	_	_
the	_	_
texture	_	_
of	_	_
its	_	_
fur	_	_
?	_	_
)	_	_
.	_	_

#11
For	_	_
this	_	_
reason	_	_
,	_	_
it	_	_
becomes	_	_
increasingly	_	_
important	_	_
for	_	_
scientists	_	_
to	_	_
investigate	_	_
what	_	_
is	_	_
learnt	_	_
by	_	_
such	_	_
networks	_	_
and	_	_
what	_	_
features	_	_
deep	_	_
artificial	_	_
neurons	_	_
are	_	_
attuned	_	_
to	_	_
,	_	_
in	_	_
a	_	_
way	_	_
not	_	_
dissimilar	_	_
to	_	_
what	_	_
neuroscientists	_	_
did	_	_
for	_	_
the	_	_
human	_	_
visual	_	_
cortex	_	_
.	_	_

#12
In	_	_
this	_	_
article	_	_
,	_	_
we	_	_
are	_	_
concerned	_	_
with	_	_
the	_	_
task	_	_
of	_	_
predicting	_	_
image	_	_
saliency	_	_
.	_	_

#13
Saliency	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
how	_	_
likely	_	_
a	_	_
visual	_	_
pattern	_	_
is	_	_
to	_	_
attract	_	_
a	_	_
human	_	_
viewer’s	_	_
gaze	_	_
when	_	_
observing	_	_
the	_	_
image	_	_
.	_	_

#14
Visual	_	_
saliency	_	_
has	_	_
been	_	_
the	_	_
subject	_	_
of	_	_
intense	_	_
study	_	_
over	_	_
the	_	_
last	_	_
decades	_	_
,	_	_
both	_	_
in	_	_
psychology	_	_
and	_	_
computer	_	_
vision	_	_
(	_	_
Borji	_	_
&	_	_
Itti	_	_
,	_	_
2013	_	_
)	_	_
,	_	_
and	_	_
recent	_	_
publications	_	_
have	_	_
demonstrated	_	_
that	_	_
deep	_	_
neural	_	_
networks	_	_
can	_	_
achieve	_	_
very	_	_
high	_	_
performance	_	_
on	_	_
this	_	_
task	_	_
(	_	_
Bylinskii	_	_
et	_	_
al.	_	_
,	_	_
2016	_	_
)	_	_
.	_	_

#15
We	_	_
will	_	_
use	_	_
a	_	_
recently	_	_
developed	_	_
architecture	_	_
(	_	_
see	_	_
Figure	_	_
1.1	_	_
)	_	_
for	_	_
saliency	_	_
detection	_	_
based	_	_
on	_	_
a	_	_
standard	_	_
CNN	_	_
encoding	_	_
(	_	_
so-called	_	_
VGG19	_	_
(	_	_
Simonyan	_	_
&	_	_
Zisserman	_	_
,	_	_
2014	_	_
)	_	_
)	_	_
,	_	_
and	_	_
visualise	_	_
the	_	_
receptive	_	_
fields	_	_
of	_	_
the	_	_
artificial	_	_
neurons	_	_
before	_	_
and	_	_
after	_	_
fine-tuning	_	_
(	_	_
the	_	_
CNN	_	_
encoder	_	_
is	_	_
pre-trained	_	_
on	_	_
a	_	_
standard	_	_
ImageNet	_	_
classification	_	_
task	_	_
)	_	_
.	_	_

#16
We	_	_
demonstrate	_	_
that	_	_
after	_	_
fine-tuning	_	_
the	_	_
network	_	_
for	_	_
the	_	_
task	_	_
,	_	_
the	_	_
deeper	_	_
neurons	_	_
have	_	_
evolved	_	_
vastly	_	_
different	_	_
receptive	_	_
fields	_	_
to	_	_
the	_	_
pre-trained	_	_
neurons	_	_
,	_	_
and	_	_
display	_	_
characteristic	_	_
patterns	_	_
that	_	_
evoke	_	_
the	_	_
centre-surround	_	_
difference	_	_
paradigm	_	_
hypothesized	_	_
by	_	_
early	_	_
psychophysical	_	_
research	_	_
(	_	_
Treisman	_	_
&	_	_
Gelade	_	_
,	_	_
1980	_	_
)	_	_
.	_	_

#17
2	_	_
.	_	_

#18
Related	_	_
Work	_	_
With	_	_
the	_	_
rise	_	_
in	_	_
popularity	_	_
of	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
several	_	_
groups	_	_
have	_	_
recently	_	_
attempted	_	_
to	_	_
visualise	_	_
which	_	_
features	_	_
a	_	_
network	_	_
has	_	_
learned	_	_
.	_	_

#19
Zeiler	_	_
&	_	_
Fergus	_	_
(	_	_
2014	_	_
)	_	_
proposed	_	_
to	_	_
back-propagate	_	_
feature	_	_
maps	_	_
obtained	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
1	_	_
.	_	_

#20
1v	_	_
2	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
2	_	_
M	_	_
ar	_	_
2	_	_
Submission	_	_
and	_	_
Formatting	_	_
Instructions	_	_
for	_	_
ICML	_	_
2017	_	_
Figure	_	_
1.2	_	_
.	_	_

#21
The	_	_
model	_	_
used	_	_
in	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
try	_	_
to	_	_
visualise	_	_
the	_	_
general	_	_
patterns	_	_
of	_	_
the	_	_
encoder	_	_
part	_	_
in	_	_
Figure	_	_
1.1	_	_
that	_	_
would	_	_
activate	_	_
the	_	_
red	_	_
pooling	_	_
layers	_	_
by	_	_
processing	_	_
a	_	_
specific	_	_
image	_	_
through	_	_
the	_	_
network	_	_
,	_	_
in	_	_
order	_	_
to	_	_
visualise	_	_
the	_	_
image	_	_
content	_	_
that	_	_
activated	_	_
the	_	_
feature	_	_
maps	_	_
.	_	_

#22
Yosinski	_	_
et	_	_
al.	_	_
(	_	_
2015	_	_
)	_	_
follow	_	_
a	_	_
similar	_	_
approach	_	_
to	_	_
develop	_	_
a	_	_
tool	_	_
for	_	_
deep	_	_
visualisation	_	_
,	_	_
and	_	_
additionally	_	_
proposed	_	_
an	_	_
approach	_	_
to	_	_
visualise	_	_
features	_	_
of	_	_
each	_	_
layer	_	_
via	_	_
regularised	_	_
optimisation	_	_
in	_	_
image	_	_
space	_	_
.	_	_

#23
Nguyen	_	_
et	_	_
al.	_	_
(	_	_
2015	_	_
)	_	_
,	_	_
they	_	_
found	_	_
that	_	_
the	_	_
deep	_	_
neural	_	_
networks	_	_
are	_	_
easily	_	_
fooled	_	_
,	_	_
and	_	_
use	_	_
evolutionary	_	_
algorithm	_	_
and	_	_
gradient	_	_
ascent	_	_
method	_	_
to	_	_
derive	_	_
a	_	_
pattern	_	_
that	_	_
the	_	_
network	_	_
has	_	_
a	_	_
high	_	_
confidence	_	_
to	_	_
determine	_	_
the	_	_
derived	_	_
pattern	_	_
is	_	_
belong	_	_
to	_	_
a	_	_
specific	_	_
class	_	_
.	_	_

#24
In	_	_
contrast	_	_
,	_	_
we	_	_
manually	_	_
clamp	_	_
the	_	_
value	_	_
of	_	_
single	_	_
neurons	_	_
selected	_	_
from	_	_
intermediate	_	_
layers	_	_
in	_	_
the	_	_
network	_	_
,	_	_
and	_	_
back-propagate	_	_
the	_	_
activation	_	_
to	_	_
the	_	_
image	_	_
space	_	_
,	_	_
thus	_	_
deriving	_	_
the	_	_
optimal	_	_
activation	_	_
pattern	_	_
for	_	_
individual	_	_
selected	_	_
neurons	_	_
.	_	_

#25
Hence	_	_
,	_	_
this	_	_
visualisation	_	_
provides	_	_
us	_	_
with	_	_
an	_	_
understanding	_	_
of	_	_
what	_	_
patterns	_	_
the	_	_
deep	_	_
representations	_	_
have	_	_
become	_	_
attuned	_	_
to	_	_
.	_	_

#26
3	_	_
.	_	_

#27
Methods	_	_
In	_	_
this	_	_
work	_	_
we	_	_
will	_	_
be	_	_
concerned	_	_
with	_	_
visualising	_	_
the	_	_
input	_	_
patterns	_	_
most	_	_
strongly	_	_
related	_	_
to	_	_
individual	_	_
neurons	_	_
in	_	_
the	_	_
network	_	_
.	_	_

#28
In	_	_
the	_	_
following	_	_
we	_	_
will	_	_
call	_	_
these	_	_
patterns	_	_
as	_	_
the	_	_
neurons’	_	_
receptive	_	_
fields	_	_
,	_	_
in	_	_
analogy	_	_
to	_	_
biological	_	_
neurons	_	_
.	_	_

#29
In	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
the	_	_
forward	_	_
pass	_	_
typically	_	_
consists	_	_
of	_	_
three	_	_
main	_	_
processes	_	_
:	_	_
convolution	_	_
,	_	_
non-linearity	_	_
(	_	_
usually	_	_
a	_	_
ReLU	_	_
function	_	_
)	_	_
and	_	_
pooling	_	_
.	_	_

#30
Similarly	_	_
,	_	_
the	_	_
visualisation	_	_
of	_	_
neural	_	_
patterns	_	_
are	_	_
produced	_	_
by	_	_
three	_	_
similar	_	_
processes	_	_
,	_	_
in	_	_
reverse	_	_
order	_	_
:	_	_
that	_	_
is	_	_
,	_	_
upsampling	_	_
,	_	_
deconvolution	_	_
,	_	_
and	_	_
non-linearity	_	_
(	_	_
again	_	_
,	_	_
ReLU	_	_
)	_	_
.	_	_

#31
We	_	_
will	_	_
describe	_	_
the	_	_
three	_	_
processes	_	_
in	_	_
turn	_	_
.	_	_

#32
3.1.	_	_
upsampling	_	_
The	_	_
purpose	_	_
of	_	_
upsampling	_	_
is	_	_
to	_	_
recover	_	_
the	_	_
gradually	_	_
reduced	_	_
resolution	_	_
caused	_	_
by	_	_
pooling	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
.	_	_

#33
The	_	_
classic	_	_
upsampling	_	_
method	_	_
in	_	_
feature	_	_
visualisation	_	_
is	_	_
unpooling	_	_
,	_	_
using	_	_
the	_	_
pooling	_	_
indices	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
to	_	_
do	_	_
unpooling—see	_	_
Figure	_	_
3.1	_	_
.	_	_

#34
Figure	_	_
3.1.	_	_
classic	_	_
upsampling	_	_
method	_	_
Because	_	_
pooling	_	_
indices	_	_
only	_	_
exist	_	_
when	_	_
processing	_	_
an	_	_
actual	_	_
image	_	_
through	_	_
the	_	_
network	_	_
,	_	_
these	_	_
indices	_	_
are	_	_
not	_	_
available	_	_
when	_	_
visualizing	_	_
a	_	_
neuron’s	_	_
receptive	_	_
field	_	_
in	_	_
abstraction	_	_
from	_	_
any	_	_
input	_	_
.	_	_

#35
Hence	_	_
,	_	_
in	_	_
order	_	_
to	_	_
visualize	_	_
individual	_	_
neuron’s	_	_
receptive	_	_
fields	_	_
,	_	_
we	_	_
set	_	_
the	_	_
pooled	_	_
feature	_	_
map	_	_
as	_	_
a	_	_
sparse	_	_
matrix	_	_
(	_	_
with	_	_
only	_	_
one	_	_
non-zero	_	_
value	_	_
)	_	_
and	_	_
do	_	_
up-sampling	_	_
by	_	_
repeating	_	_
this	_	_
sparse	_	_
matrix—see	_	_
Figure	_	_
3.2	_	_
.	_	_

#36
Figure	_	_
3.2.	_	_
upsampling	_	_
by	_	_
repeating	_	_
the	_	_
sparse	_	_
feature	_	_
map	_	_
,	_	_
c	_	_
is	_	_
a	_	_
constant	_	_
3.2	_	_
.	_	_

#37
Deconvolution	_	_
Convolution	_	_
is	_	_
the	_	_
key	_	_
process	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
of	_	_
the	_	_
convolutional	_	_
neural	_	_
network	_	_
,	_	_
as	_	_
it	_	_
is	_	_
the	_	_
one	_	_
that	_	_
is	_	_
tuned	_	_
by	_	_
back-propagation	_	_
during	_	_
training	_	_
.	_	_

#38
It	_	_
can	_	_
be	_	_
formulated	_	_
Submission	_	_
and	_	_
Formatting	_	_
Instructions	_	_
for	_	_
ICML	_	_
2017	_	_
as	_	_
:	_	_
O	_	_
=	_	_
I	_	_
∗	_	_
F	_	_
(	_	_
1	_	_
)	_	_
where	_	_
O	_	_
is	_	_
the	_	_
extracted	_	_
feature	_	_
map	_	_
,	_	_
I	_	_
is	_	_
the	_	_
input	_	_
and	_	_
F	_	_
is	_	_
the	_	_
learnt	_	_
filter	_	_
.	_	_

#39
Reconstructing	_	_
the	_	_
input	_	_
pattern	_	_
I	_	_
which	_	_
activated	_	_
an	_	_
extracted	_	_
feature	_	_
map	_	_
O	_	_
,	_	_
can	_	_
be	_	_
formulated	_	_
as	_	_
follows	_	_
:	_	_
A	_	_
=	_	_
O	_	_
∗	_	_
FT	_	_
(	_	_
2	_	_
)	_	_
where	_	_
O	_	_
is	_	_
the	_	_
extracted	_	_
feature	_	_
map	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
,	_	_
FT	_	_
is	_	_
the	_	_
transpose	_	_
of	_	_
the	_	_
learned	_	_
filter	_	_
and	_	_
A	_	_
is	_	_
the	_	_
content	_	_
in	_	_
the	_	_
input	_	_
I	_	_
which	_	_
activated	_	_
O	_	_
.	_	_

#40
3.3	_	_
.	_	_

#41
Relu	_	_
the	_	_
ReLU	_	_
function	_	_
in	_	_
feature	_	_
visualisation	_	_
is	_	_
the	_	_
same	_	_
as	_	_
that	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
of	_	_
deep	_	_
convolutional	_	_
network	_	_
,	_	_
which	_	_
only	_	_
leave	_	_
the	_	_
positive	_	_
components	_	_
of	_	_
the	_	_
input	_	_
,	_	_
and	_	_
can	_	_
be	_	_
formulated	_	_
as	_	_
:	_	_
f	_	_
(	_	_
x	_	_
)	_	_
=	_	_
max	_	_
(	_	_
0	_	_
,	_	_
x	_	_
)	_	_
(	_	_
3	_	_
)	_	_
4.	_	_
pattern	_	_
visualisation	_	_
In	_	_
this	_	_
part	_	_
,	_	_
we	_	_
show	_	_
the	_	_
general	_	_
patterns	_	_
learnt	_	_
by	_	_
fine-tuning	_	_
the	_	_
network	_	_
on	_	_
a	_	_
saliency	_	_
prediction	_	_
task	_	_
,	_	_
as	_	_
well	_	_
as	_	_
the	_	_
patterns	_	_
for	_	_
the	_	_
original	_	_
VGG-19	_	_
network	_	_
,	_	_
pre-trained	_	_
on	_	_
classification	_	_
on	_	_
ImageNet	_	_
.	_	_

#42
Additionally	_	_
,	_	_
we	_	_
also	_	_
visualise	_	_
individual	_	_
neurons’	_	_
receptive	_	_
field	_	_
by	_	_
clamping	_	_
them	_	_
and	_	_
back-projecting	_	_
to	_	_
the	_	_
input	_	_
domain	_	_
as	_	_
described	_	_
above	_	_
.	_	_

#43
4.1	_	_
.	_	_

#44
VGG	_	_
pattern	_	_
and	_	_
salient	_	_
pattern	_	_
Figures	_	_
4.1	_	_
to	_	_
4.5	_	_
contrast	_	_
the	_	_
receptive	_	_
fields	_	_
learnt	_	_
by	_	_
neurons	_	_
in	_	_
various	_	_
layers	_	_
of	_	_
the	_	_
network	_	_
,	_	_
both	_	_
before	_	_
and	_	_
after	_	_
fine-tuning	_	_
on	_	_
the	_	_
saliency	_	_
detection	_	_
task	_	_
.	_	_

#45
Figure	_	_
4.1.	_	_
the	_	_
64	_	_
general	_	_
salient	_	_
(	_	_
left	_	_
)	_	_
and	_	_
vgg	_	_
(	_	_
right	_	_
)	_	_
patterns	_	_
for	_	_
the	_	_
first	_	_
pooling	_	_
layer	_	_
Figure	_	_
4.2.	_	_
the	_	_
first	_	_
64	_	_
(	_	_
128	_	_
in	_	_
total	_	_
)	_	_
general	_	_
salient	_	_
(	_	_
left	_	_
)	_	_
and	_	_
vgg	_	_
(	_	_
right	_	_
)	_	_
patterns	_	_
for	_	_
the	_	_
second	_	_
pooling	_	_
layer	_	_
Figure	_	_
4.3.	_	_
the	_	_
first	_	_
64	_	_
(	_	_
256	_	_
in	_	_
total	_	_
)	_	_
general	_	_
salient	_	_
(	_	_
left	_	_
)	_	_
and	_	_
vgg	_	_
(	_	_
right	_	_
)	_	_
patterns	_	_
for	_	_
the	_	_
third	_	_
pooling	_	_
layer	_	_
Figure	_	_
4.4.	_	_
the	_	_
first	_	_
64	_	_
(	_	_
512	_	_
in	_	_
total	_	_
)	_	_
general	_	_
salient	_	_
(	_	_
left	_	_
)	_	_
and	_	_
vgg	_	_
(	_	_
right	_	_
)	_	_
patterns	_	_
for	_	_
the	_	_
fourth	_	_
pooling	_	_
layer	_	_
Figure	_	_
4.5.	_	_
the	_	_
first	_	_
64	_	_
(	_	_
512	_	_
in	_	_
total	_	_
)	_	_
general	_	_
salient	_	_
(	_	_
left	_	_
)	_	_
and	_	_
vgg	_	_
(	_	_
right	_	_
)	_	_
patterns	_	_
for	_	_
the	_	_
fifth	_	_
pooling	_	_
layer	_	_
In	_	_
those	_	_
figures	_	_
,	_	_
we	_	_
can	_	_
see	_	_
little	_	_
differences	_	_
between	_	_
the	_	_
neurons’	_	_
receptive	_	_
fields	_	_
in	_	_
the	_	_
first	_	_
three	_	_
pooling	_	_
layers	_	_
afSubmission	_	_
and	_	_
Formatting	_	_
Instructions	_	_
for	_	_
ICML	_	_
2017	_	_
ter	_	_
fine-tuning	_	_
.	_	_

#46
Some	_	_
patterns	_	_
are	_	_
the	_	_
same	_	_
as	_	_
the	_	_
edge	_	_
pattern	_	_
.	_	_

#47
However	_	_
,	_	_
when	_	_
considering	_	_
deeper	_	_
layers	_	_
,	_	_
fundamentally	_	_
different	_	_
patterns	_	_
arise	_	_
in	_	_
the	_	_
neurons’	_	_
receptive	_	_
field	_	_
after	_	_
fine-tuning	_	_
for	_	_
the	_	_
saliency	_	_
task	_	_
:	_	_
after	_	_
fine	_	_
-tuning	_	_
the	_	_
deep	_	_
neurons	_	_
all	_	_
appear	_	_
to	_	_
have	_	_
attuned	_	_
to	_	_
variations	_	_
of	_	_
central-surround	_	_
patterns	_	_
.	_	_

#48
Interestingly	_	_
,	_	_
such	_	_
patterns	_	_
emerge	_	_
solely	_	_
through	_	_
the	_	_
process	_	_
of	_	_
fine-tuning	_	_
the	_	_
network	_	_
,	_	_
starting	_	_
from	_	_
vastly	_	_
different	_	_
receptive	_	_
fields	_	_
,	_	_
and	_	_
they	_	_
appear	_	_
to	_	_
be	_	_
consistent	_	_
with	_	_
theoretical	_	_
and	_	_
experimental	_	_
research	_	_
on	_	_
saliency	_	_
by	_	_
psychologists	_	_
.	_	_

#49
4.2	_	_
.	_	_

#50
Pattern	_	_
Propagation	_	_
In	_	_
a	_	_
second	_	_
experiment	_	_
,	_	_
we	_	_
illustrate	_	_
how	_	_
the	_	_
different	_	_
patterns	_	_
yield	_	_
different	_	_
levels	_	_
of	_	_
activation	_	_
to	_	_
specific	_	_
deep	_	_
neurons	_	_
.	_	_

#51
This	_	_
is	_	_
achieved	_	_
by	_	_
clamping	_	_
a	_	_
neuron-specific	_	_
output	_	_
to	_	_
a	_	_
constant	_	_
value	_	_
from	_	_
0.5	_	_
to	_	_
3	_	_
.	_	_

#52
These	_	_
results	_	_
are	_	_
recorded	_	_
in	_	_
Figure	_	_
4.6	_	_
.	_	_

#53
From	_	_
the	_	_
figure	_	_
above	_	_
,	_	_
when	_	_
inFigure	_	_
4.6.	_	_
the	_	_
pattern	_	_
propagation	_	_
by	_	_
increasing	_	_
the	_	_
constant	_	_
value	_	_
(	_	_
from	_	_
left	_	_
to	_	_
right	_	_
,	_	_
up	_	_
to	_	_
down	_	_
)	_	_
creasing	_	_
the	_	_
neuron’s	_	_
output	_	_
value	_	_
its	_	_
receptive	_	_
field	_	_
shows	_	_
patterns	_	_
that	_	_
propagates	_	_
like	_	_
the	_	_
water	_	_
wave	_	_
propagation	_	_
.	_	_

#54
We	_	_
argue	_	_
that	_	_
the	_	_
constant	_	_
in	_	_
the	_	_
sparse	_	_
matrix	_	_
is	_	_
the	_	_
energy	_	_
of	_	_
the	_	_
pattern	_	_
,	_	_
the	_	_
higher	_	_
energy	_	_
of	_	_
a	_	_
pattern	_	_
,	_	_
the	_	_
wider	_	_
it	_	_
will	_	_
propagate	_	_
.	_	_

#55
4.3	_	_
.	_	_

#56
Pattern	_	_
Validating	_	_
In	_	_
the	_	_
previous	_	_
sections	_	_
,	_	_
we	_	_
have	_	_
shown	_	_
that	_	_
the	_	_
proposed	_	_
visualisation	_	_
strategy	_	_
can	_	_
be	_	_
used	_	_
to	_	_
illustrate	_	_
the	_	_
patterns	_	_
learned	_	_
by	_	_
deep	_	_
neurons	_	_
in	_	_
a	_	_
network	_	_
.	_	_

#57
In	_	_
this	_	_
section	_	_
we	_	_
verify	_	_
that	_	_
those	_	_
back-propagated	_	_
patterns	_	_
actually	_	_
activate	_	_
the	_	_
selected	_	_
neuron	_	_
.	_	_

#58
An	_	_
additional	_	_
question	_	_
is	_	_
how	_	_
such	_	_
patterns	_	_
affect	_	_
other	_	_
neurons	_	_
in	_	_
the	_	_
same	_	_
layer	_	_
.	_	_

#59
This	_	_
is	_	_
tested	_	_
in	_	_
a	_	_
straightforward	_	_
manner	_	_
by	_	_
feeding	_	_
the	_	_
back-propagated	_	_
pattern	_	_
as	_	_
an	_	_
input	_	_
to	_	_
the	_	_
network	_	_
.	_	_

#60
Note	_	_
that	_	_
due	_	_
to	_	_
the	_	_
pooling	_	_
process	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
,	_	_
the	_	_
resulted	_	_
pooling	_	_
feature	_	_
map	_	_
may	options	_
not	_	_
the	_	_
same	_	_
as	_	_
the	_	_
clamped	_	_
sparse	_	_
matrix	_	_
used	_	_
to	_	_
generate	_	_
the	_	_
pattern	_	_
.	_	_

#61
We	_	_
check	_	_
the	_	_
activation	_	_
by	_	_
the	_	_
summation	_	_
of	_	_
the	_	_
pooled	_	_
feature	_	_
map	_	_
—	_	_
see	_	_
Figure	_	_
4.7	_	_
.	_	_

#62
From	_	_
this	_	_
figure	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
first	_	_
pattern	_	_
indeed	_	_
activate	_	_
the	_	_
first	_	_
neuron	_	_
,	_	_
as	_	_
expected	_	_
.	_	_

#63
Furthermore	_	_
,	_	_
some	_	_
other	_	_
neurons	_	_
have	_	_
higher	_	_
activation	_	_
,	_	_
demonstrating	_	_
that	_	_
the	_	_
network	_	_
has	_	_
developed	_	_
some	_	_
redundancy	_	_
in	_	_
its	_	_
coding	_	_
,	_	_
whereas	_	_
other	_	_
neurons	_	_
are	_	_
inhibited	_	_
.	_	_

#64
We	_	_
argue	_	_
that	_	_
this	_	_
is	_	_
because	_	_
most	_	_
of	_	_
the	_	_
learned	_	_
general	_	_
patterns	_	_
are	_	_
similar	_	_
(	_	_
central	_	_
surround	_	_
difference	_	_
)	_	_
,	_	_
some	_	_
neurons	_	_
are	_	_
more	_	_
sensitive	_	_
to	_	_
the	_	_
central	_	_
surround	_	_
difference	_	_
pattern	_	_
;	_	_
and	_	_
for	_	_
those	_	_
neuron	_	_
that	_	_
are	_	_
inhibited	_	_
is	_	_
due	_	_
to	_	_
the	_	_
lateral	_	_
inhibition	_	_
(	_	_
Ratliff	_	_
et	_	_
al.	_	_
,	_	_
1967	_	_
)	_	_
which	_	_
is	_	_
also	_	_
used	_	_
in	_	_
local	_	_
response	_	_
normalisation	_	_
(	_	_
Krizhevsky	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
)	_	_
when	_	_
training	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
.	_	_

#65
5	_	_
.	_	_

#66
Conclusion	_	_
In	_	_
this	_	_
article	_	_
we	_	_
proposed	_	_
a	_	_
novel	_	_
approach	_	_
for	_	_
visualising	_	_
the	_	_
representations	_	_
learnt	_	_
by	_	_
deep	_	_
neural	_	_
networks	_	_
,	_	_
and	_	_
specifically	_	_
to	_	_
visualise	_	_
the	_	_
receptive	_	_
fields	_	_
of	_	_
individual	_	_
deep	_	_
neurons	_	_
.	_	_

#67
We	_	_
have	_	_
demonstrated	_	_
this	_	_
approach	_	_
to	_	_
a	_	_
VGG-19	_	_
network	_	_
pre-trained	_	_
on	_	_
ImageNet	_	_
classification	_	_
and	_	_
fine-tuned	_	_
for	_	_
the	_	_
task	_	_
of	_	_
saliency	_	_
detection	_	_
.	_	_

#68
Importantly	_	_
,	_	_
we	_	_
demonstrate	_	_
that	_	_
this	_	_
approach	_	_
can	_	_
reveal	_	_
important	_	_
insights	_	_
in	_	_
what	_	_
is	_	_
learnt	_	_
by	_	_
the	_	_
network	_	_
to	_	_
achieve	_	_
its	_	_
high	_	_
performance	_	_
:	_	_
receptive	_	_
fields	_	_
are	_	_
shown	_	_
to	_	_
change	_	_
drastically	_	_
from	_	_
the	_	_
original	_	_
VGG-19	_	_
representation	_	_
to	_	_
characteristic	_	_
centre-surround	_	_
patterns	_	_
.	_	_

#69
Interestingly	_	_
,	_	_
these	_	_
emergent	_	_
patterns	_	_
are	_	_
consistent	_	_
with	_	_
the	_	_
psychological	_	_
theories	_	_
of	_	_
saliency	_	_
.	_	_

#70
This	_	_
demonstrates	_	_
that	_	_
such	_	_
a	_	_
visualisation	_	_
offers	_	_
an	_	_
important	_	_
tool	_	_
for	_	_
interpreting	_	_
the	_	_
workings	_	_
of	_	_
deep	_	_
neural	_	_
networks	_	_
.	_	_

#71
To	_	_
sum	_	_
up	_	_
,	_	_
by	_	_
manually	_	_
set	_	_
the	_	_
feature	_	_
map	_	_
as	_	_
a	_	_
sparse	_	_
maSubmission	_	_
and	_	_
Formatting	_	_
Instructions	_	_
for	_	_
ICML	_	_
2017	_	_
trix	_	_
,	_	_
we	_	_
derive	_	_
a	_	_
set	_	_
of	_	_
general	_	_
patterns	_	_
for	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
.	_	_

#72
We	_	_
also	_	_
double	_	_
check	_	_
the	_	_
resulted	_	_
general	_	_
patterns	_	_
by	_	_
forwarding	_	_
it	_	_
into	_	_
the	_	_
network	_	_
,	_	_
which	_	_
show	_	_
those	_	_
general	_	_
patterns	_	_
are	_	_
not	_	_
illogical	_	_
and	_	_
follow	_	_
the	_	_
evidence	_	_
in	_	_
neurobiology	_	_
.	_	_

#73
6	_	_
.	_	_

#74
Acknowledgements	_	_
This	_	_
work	_	_
was	_	_
supported	_	_
by	_	_
the	_	_
EPSRC	_	_
project	_	_
DEVA	_	_
EP/N035399/1	_	_
.	_	_