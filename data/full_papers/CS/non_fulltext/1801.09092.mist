#0
Interactive	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
for	_	_
Facial	_	_
Expression	_	_
Generation	_	_
in	_	_
Dyadic	_	_
Interactions	_	_
Behnaz	_	_
Nojavanasghari	_	_
University	_	_
of	_	_
Central	_	_
Florida	_	_
behnaz	_	_
@	_	_
eecs.ucf.edu	_	_
Yuchi	_	_
Huang	_	_
Educational	_	_
Testing	_	_
Service	_	_
yhuang001	_	_
@	_	_
ets.org	_	_
Saad	_	_
Khan	_	_
Educational	_	_
Testing	_	_
Service	_	_
skhan002	_	_
@	_	_
ets.org	_	_

#1
Abstract	_	_

#2
A	_	_
social	_	_
interaction	_	_
is	_	_
a	_	_
social	_	_
exchange	_	_
between	_	_
two	_	_
or	_	_
more	_	_
individuals	_	_
,	_	_
where	_	_
individuals	_	_
modify	_	_
and	_	_
adjust	_	_
their	_	_
behaviors	_	_
in	_	_
response	_	_
to	_	_
their	_	_
interaction	_	_
partners	_	_
.	_	_

#3
Our	_	_
social	_	_
interactions	_	_
are	_	_
one	_	_
of	_	_
most	_	_
fundamental	_	_
aspects	_	_
of	_	_
our	_	_
lives	_	_
and	_	_
can	_	_
profoundly	_	_
affect	_	_
our	_	_
mood	_	_
,	_	_
both	_	_
positively	_	_
and	_	_
negatively	_	_
.	_	_

#4
With	_	_
growing	_	_
interest	_	_
in	_	_
virtual	_	_
reality	_	_
and	_	_
avatar-mediated	_	_
interactions	_	_
,	_	_
it	_	_
is	_	_
desirable	_	_
to	_	_
make	_	_
these	_	_
interactions	_	_
natural	_	_
and	_	_
human	_	_
like	_	_
to	_	_
promote	_	_
positive	_	_
effect	_	_
in	_	_
the	_	_
interactions	_	_
and	_	_
applications	_	_
such	_	_
as	_	_
intelligent	_	_
tutoring	_	_
systems	_	_
,	_	_
automated	_	_
interview	_	_
systems	_	_
and	_	_
e-learning	_	_
.	_	_

#5
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
method	_	_
to	_	_
generate	_	_
facial	_	_
behaviors	_	_
for	_	_
an	_	_
agent	_	_
.	_	_

#6
These	_	_
behaviors	_	_
include	_	_
facial	_	_
expressions	_	_
and	_	_
head	_	_
pose	_	_
and	_	_
they	_	_
are	_	_
generated	_	_
considering	_	_
the	_	_
users	_	_
affective	_	_
state	_	_
.	_	_

#7
Our	_	_
models	_	_
learn	_	_
semantically	_	_
meaningful	_	_
representations	_	_
of	_	_
the	_	_
face	_	_
and	_	_
generate	_	_
appropriate	_	_
and	_	_
temporally	_	_
smooth	_	_
facial	_	_
behaviors	_	_
in	_	_
dyadic	_	_
interactions	_	_
.	_	_

#8
1	_	_
.	_	_

#9
Introduction	_	_
Designing	_	_
interactive	_	_
virtual	_	_
agents	_	_
and	_	_
robots	_	_
have	_	_
gained	_	_
a	_	_
lot	_	_
of	_	_
attention	_	_
in	_	_
recent	_	_
years	_	_
[	_	_
10	_	_
,	_	_
21	_	_
,	_	_
43	_	_
]	_	_
.	_	_

#10
The	_	_
popularity	_	_
and	_	_
growing	_	_
interest	_	_
in	_	_
these	_	_
agents	_	_
is	_	_
partially	_	_
because	_	_
of	_	_
their	_	_
wide	_	_
applications	_	_
in	_	_
real	_	_
world	_	_
scenarios	_	_
.	_	_

#11
They	_	_
can	_	_
be	_	_
used	_	_
for	_	_
variety	_	_
of	_	_
applications	_	_
from	_	_
education	_	_
[	_	_
30	_	_
,	_	_
31	_	_
]	_	_
,	_	_
training	_	_
[	_	_
36	_	_
]	_	_
and	_	_
therapy	_	_
[	_	_
7	_	_
,	_	_
34	_	_
]	_	_
to	_	_
elderly	_	_
care	_	_
[	_	_
5	_	_
]	_	_
and	_	_
companionship	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#12
One	_	_
of	_	_
the	_	_
most	_	_
important	_	_
aspects	_	_
in	_	_
human	_	_
communication	_	_
is	_	_
social	_	_
intelligence	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#13
A	_	_
part	_	_
of	_	_
social	_	_
intelligence	_	_
depends	_	_
on	_	_
understanding	_	_
other	_	_
people’s	_	_
affective	_	_
states	_	_
and	_	_
being	_	_
able	_	_
to	_	_
respond	_	_
appropriately	_	_
.	_	_

#14
To	_	_
have	_	_
a	_	_
natural	_	_
human-	_	_
machine	_	_
interaction	_	_
,	_	_
it	_	_
is	_	_
critical	_	_
to	_	_
enable	_	_
machines	_	_
with	_	_
social	_	_
intelligence	_	_
as	_	_
well	_	_
.	_	_

#15
The	_	_
first	_	_
step	_	_
towards	_	_
this	_	_
capability	_	_
is	_	_
observing	_	_
human	_	_
communication	_	_
dynamics	_	_
and	_	_
learning	_	_
from	_	_
the	_	_
occurring	_	_
behavioral	_	_
patterns	_	_
[	_	_
33	_	_
,	_	_
32	_	_
]	_	_
.	_	_

#16
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
focus	_	_
on	_	_
modeling	_	_
dyadic	_	_
interactions	_	_
between	_	_
two	_	_
partners	_	_
.	_	_

#17
The	_	_
problem	_	_
we	_	_
are	_	_
solving	_	_
is	_	_
given	_	_
the	_	_
affective	_	_
Figure	_	_
1	_	_
.	_	_

#18
The	_	_
problem	_	_
we	_	_
are	_	_
solving	_	_
in	_	_
this	_	_
paper	_	_
is	_	_
,	_	_
given	_	_
affective	_	_
state	_	_
of	_	_
one	_	_
person	_	_
in	_	_
a	_	_
dyadic	_	_
interaction	_	_
,	_	_
we	_	_
generate	_	_
the	_	_
facial	_	_
behaviors	_	_
of	_	_
the	_	_
other	_	_
person	_	_
.	_	_

#19
By	_	_
facial	_	_
behaviors	_	_
,	_	_
we	_	_
refer	_	_
to	_	_
facial	_	_
expressions	_	_
,	_	_
movements	_	_
of	_	_
facial	_	_
landmarks	_	_
and	_	_
head	_	_
pose	_	_
.	_	_

#20
state	_	_
of	_	_
one	_	_
partner	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
generating	_	_
non-verbal	_	_
facial	_	_
behaviors	_	_
of	_	_
the	_	_
other	_	_
partner	_	_
.	_	_

#21
Figure	_	_
1	_	_
shows	_	_
the	_	_
overview	_	_
of	_	_
our	_	_
problem	_	_
.	_	_

#22
The	_	_
affective	_	_
states	_	_
that	_	_
we	_	_
consider	_	_
in	_	_
generating	_	_
facial	_	_
expressions	_	_
include	_	_
joy	_	_
,	_	_
anger	_	_
,	_	_
surprise	_	_
,	_	_
fear	_	_
,	_	_
contempt	_	_
,	_	_
disgust	_	_
,	_	_
sadness	_	_
and	_	_
neutral	_	_
.	_	_

#23
The	_	_
non-verbal	_	_
cues	_	_
that	_	_
we	_	_
are	_	_
generating	_	_
in	_	_
this	_	_
work	_	_
,	_	_
are	_	_
facial	_	_
expressions	_	_
and	_	_
head	_	_
pose	_	_
.	_	_

#24
These	_	_
cues	_	_
help	_	_
us	_	_
generate	_	_
head	_	_
nods	_	_
,	_	_
head	_	_
shakes	_	_
,	_	_
tilted	_	_
head	_	_
and	_	_
various	_	_
facial	_	_
expressions	_	_
such	_	_
as	_	_
smile	_	_
which	_	_
are	_	_
behaviors	_	_
that	_	_
happen	_	_
in	_	_
our	_	_
daily	_	_
face	_	_
to	_	_
face	_	_
communications	_	_
.	_	_

#25
A	_	_
successful	_	_
model	_	_
should	deontic	_
learn	_	_
meaningful	_	_
factors	_	_
of	_	_
the	_	_
face	_	_
for	_	_
generating	_	_
of	_	_
most	_	_
appropriate	_	_
responses	_	_
for	_	_
the	_	_
agent	_	_
.	_	_

#26
We	_	_
have	_	_
designed	_	_
our	_	_
methodology	_	_
based	_	_
on	_	_
the	_	_
following	_	_
factors	_	_
:	_	_
1	_	_
)	_	_
humans	_	_
react	_	_
to	_	_
each	_	_
others’	_	_
affective	_	_
states	_	_
from	_	_
an	_	_
early	_	_
age	_	_
and	_	_
adjust	_	_
their	_	_
behavior	_	_
accordingly	_	_
[	_	_
42	_	_
]	_	_
.	_	_

#27
2	_	_
)	_	_
Each	_	_
affective	_	_
state	_	_
is	_	_
characterized	_	_
by	_	_
particular	_	_
facial	_	_
expressions	_	_
and	_	_
head	_	_
pose	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#28
3	_	_
)	_	_
Each	_	_
affective	_	_
state	_	_
can	_	_
have	_	_
different	_	_
intensities	_	_
,	_	_
such	_	_
as	_	_
happy	_	_
vs	_	_
very	_	_
happy	_	_
.	_	_

#29
These	_	_
factors	_	_
(	_	_
e.g.	_	_
affective	_	_
state	_	_
and	_	_
their	_	_
intensities	_	_
)	_	_
effect	_	_
how	_	_
fast	_	_
or	_	_
slow	_	_
or	_	_
facial	_	_
expressions	_	_
and	_	_
head	_	_
pose	_	_
changes	_	_
over	_	_
time	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#30
There	_	_
has	_	_
been	_	_
great	_	_
deal	_	_
of	_	_
research	_	_
on	_	_
facial	_	_
expression	_	_
analysis	_	_
where	_	_
the	_	_
effort	_	_
is	_	_
towards	_	_
describing	_	_
the	_	_
facial	_	_
expressions	_	_
[	_	_
40	_	_
]	_	_
.	_	_

#31
Facial	_	_
expressions	_	_
are	_	_
a	_	_
result	_	_
of	_	_
movements	_	_
of	_	_
facial	_	_
landmarks	_	_
such	_	_
as	_	_
raising	_	_
eyebrows	_	_
,	_	_
smiling	_	_
and	_	_
wrinkling	_	_
nose	_	_
.	_	_

#32
A	_	_
successful	_	_
model	_	_
for	_	_
our	_	_
problem	_	_
,	_	_
should	deontic	_
learn	_	_
the	_	_
movements	_	_
of	_	_
these	_	_
points	_	_
in	_	_
response	_	_
to	_	_
different	_	_
affective	_	_
states	_	_
.	_	_

#33
We	_	_
are	_	_
using	_	_
FACET	_	_
SDK	_	_
for	_	_
extracting	_	_
8-dimensional	_	_
affective	_	_
state	_	_
of	_	_
one	_	_
partner	_	_
in	_	_
the	_	_
interaction	_	_
.	_	_

#34
We	_	_
then	_	_
use	_	_
it	_	_
as	_	_
a	_	_
condition	_	_
in	_	_
generation	_	_
of	_	_
non-verbal	_	_
facial	_	_
behaviors	_	_
of	_	_
the	_	_
other	_	_
person	_	_
.	_	_

#35
Our	_	_
generation	_	_
process	_	_
relies	_	_
on	_	_
a	_	_
conditional	_	_
generative	_	_
adversarial	_	_
networks	_	_
[	_	_
29	_	_
]	_	_
,	_	_
where	_	_
the	_	_
conditioning	_	_
vector	_	_
is	_	_
affective	_	_
state	_	_
of	_	_
one	_	_
party	_	_
in	_	_
the	_	_
interaction	_	_
.	_	_

#36
Our	_	_
proposed	_	_
methodology	_	_
is	_	_
able	_	_
to	_	_
extract	_	_
meaningful	_	_
information	_	_
about	_	_
the	_	_
face	_	_
and	_	_
head	_	_
pose	_	_
and	_	_
integrate	_	_
the	_	_
encoded	_	_
information	_	_
with	_	_
our	_	_
network	_	_
to	_	_
generate	_	_
expressive	_	_
and	_	_
temporally	_	_
smooth	_	_
face	_	_
images	_	_
for	_	_
the	_	_
agent	_	_
.	_	_

#37
2	_	_
.	_	_

#38
Related	_	_
work	_	_
Humans	_	_
communicate	_	_
using	_	_
not	_	_
only	_	_
words	_	_
but	_	_
also	_	_
non-verbal	_	_
behaviors	_	_
such	_	_
as	_	_
facial	_	_
expressions	_	_
,	_	_
body	_	_
posture	_	_
and	_	_
head	_	_
gestures	_	_
.	_	_

#39
In	_	_
our	_	_
daily	_	_
interactions	_	_
,	_	_
we	_	_
are	_	_
constantly	_	_
adjusting	_	_
our	_	_
non-verbal	_	_
behavior	_	_
based	_	_
on	_	_
other	_	_
peoples	_	_
behavior	_	_
.	_	_

#40
In	_	_
collaborative	_	_
activities	_	_
such	_	_
as	_	_
interviews	_	_
and	_	_
negotiations	_	_
,	_	_
people	_	_
tend	_	_
to	_	_
unconsciously	_	_
mimic	_	_
other	_	_
people’s	_	_
behavior	_	_
[	_	_
4	_	_
,	_	_
26	_	_
]	_	_
.	_	_

#41
With	_	_
increasing	_	_
interest	_	_
in	_	_
social	_	_
robotics	_	_
and	_	_
virtual	_	_
reality	_	_
and	_	_
their	_	_
wide	_	_
applications	_	_
in	_	_
social	_	_
situations	_	_
such	_	_
as	_	_
automated	_	_
interview	_	_
systems	_	_
[	_	_
17	_	_
]	_	_
,	_	_
companionship	_	_
for	_	_
elder	_	_
care	_	_
[	_	_
5	_	_
]	_	_
and	_	_
therapy	_	_
for	_	_
autism	_	_
[	_	_
7	_	_
]	_	_
,	_	_
there	_	_
is	_	_
a	_	_
growing	_	_
need	_	_
to	_	_
enable	_	_
computer	_	_
systems	_	_
to	_	_
understand	_	_
,	_	_
interpret	_	_
and	_	_
respond	_	_
to	_	_
people’s	_	_
affective	_	_
states	_	_
appropriately	_	_
.	_	_

#42
As	_	_
a	_	_
person’s	_	_
face	_	_
discloses	_	_
important	_	_
information	_	_
about	_	_
their	_	_
affective	_	_
state	_	_
,	_	_
it	_	_
is	_	_
very	_	_
important	_	_
to	_	_
generate	_	_
appropriate	_	_
facial	_	_
expressions	_	_
in	_	_
interactive	_	_
systems	_	_
as	_	_
well	_	_
.	_	_

#43
There	_	_
has	_	_
been	_	_
a	_	_
number	_	_
of	_	_
studies	_	_
focusing	_	_
on	_	_
automatic	_	_
generation	_	_
of	_	_
facial	_	_
expressions	_	_
.	_	_

#44
These	_	_
efforts	_	_
can	_	_
be	_	_
categorized	_	_
into	_	_
three	_	_
main	_	_
groups	_	_
:	_	_
1	_	_
)	_	_
Rule	_	_
based	_	_
approaches	_	_
where	_	_
affective	_	_
states	_	_
are	_	_
mapped	_	_
into	_	_
a	_	_
pre-defined	_	_
2D	_	_
or	_	_
3D	_	_
face	_	_
model	_	_
[	_	_
15	_	_
]	_	_
.	_	_

#45
2	_	_
)	_	_
Statistical	_	_
approaches	_	_
where	_	_
face	_	_
shape	_	_
is	_	_
modeled	_	_
as	_	_
linear	_	_
combination	_	_
of	_	_
prototypical	_	_
expression	_	_
basis	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#46
3	_	_
)	_	_
Deep	_	_
belief	_	_
networks	_	_
where	_	_
the	_	_
models	_	_
learn	_	_
the	_	_
variation	_	_
of	_	_
facial	_	_
expressions	_	_
in	_	_
presence	_	_
of	_	_
various	_	_
affective	_	_
states	_	_
and	_	_
produce	_	_
convincing	_	_
samples	_	_
[	_	_
39	_	_
]	_	_
.	_	_

#47
While	_	_
generating	_	_
facial	_	_
expressions	_	_
using	_	_
previous	_	_
approaches	_	_
can	_	_
result	_	_
in	_	_
promising	_	_
results	_	_
,	_	_
it	_	_
is	_	_
important	_	_
to	_	_
consider	_	_
other	_	_
people’s	_	_
affective	_	_
states	_	_
while	_	_
generating	_	_
facial	_	_
expressions	_	_
in	_	_
dyadic	_	_
interactions	_	_
.	_	_

#48
There	_	_
has	_	_
been	_	_
previous	_	_
work	_	_
in	_	_
this	_	_
area	_	_
,	_	_
[	_	_
18	_	_
]	_	_
where	_	_
researchers	_	_
use	_	_
conditional	_	_
generative	_	_
adversarial	_	_
networks	_	_
for	_	_
generating	_	_
facial	_	_
expressions	_	_
in	_	_
dyadic	_	_
interactions	_	_
.	_	_

#49
While	_	_
the	_	_
proposed	_	_
approach	_	_
results	_	_
in	_	_
appropriate	_	_
facial	_	_
expression	_	_
for	_	_
one	_	_
frame	_	_
,	_	_
it	_	_
does	_	_
not	_	_
consider	_	_
the	_	_
temporal	_	_
consistency	_	_
in	_	_
a	_	_
sequence	_	_
,	_	_
which	_	_
results	_	_
in	_	_
generating	_	_
non-smooth	_	_
facial	_	_
expressions	_	_
over	_	_
time	_	_
.	_	_

#50
Also	_	_
,	_	_
some	_	_
of	_	_
the	_	_
attributes	_	_
such	_	_
as	_	_
head	_	_
position	_	_
and	_	_
orientation	_	_
that	_	_
are	_	_
important	_	_
cues	_	_
for	_	_
recognizing	_	_
tilted	_	_
head	_	_
,	_	_
head	_	_
nods	_	_
,	_	_
head	_	_
shakes	_	_
are	_	_
not	_	_
captured	_	_
by	_	_
the	_	_
model	_	_
.	_	_

#51
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
address	_	_
the	_	_
problem	_	_
of	_	_
generating	_	_
facial	_	_
expressions	_	_
in	_	_
dyadic	_	_
interactions	_	_
while	_	_
considering	_	_
temporal	_	_
constraints	_	_
and	_	_
additional	_	_
facial	_	_
behaviors	_	_
such	_	_
as	_	_
head	_	_
pose	_	_
.	_	_

#52
Our	_	_
proposed	_	_
model	_	_
extracts	_	_
semantically	_	_
meaningful	_	_
encodings	_	_
of	_	_
the	_	_
face	_	_
regarding	_	_
movements	_	_
of	_	_
facial	_	_
landmarks	_	_
and	_	_
head	_	_
pose	_	_
and	_	_
integrate	_	_
it	_	_
with	_	_
the	_	_
network	_	_
which	_	_
results	_	_
in	_	_
generation	_	_
of	_	_
appropriate	_	_
and	_	_
smooth	_	_
facial	_	_
expressions	_	_
.	_	_

#53
3	_	_
.	_	_

#54
Methodology	_	_
Generative	_	_
adversarial	_	_
networks	_	_
(	_	_
GANs	_	_
)	_	_
[	_	_
13	_	_
]	_	_
have	_	_
been	_	_
widely	_	_
used	_	_
in	_	_
the	_	_
field	_	_
of	_	_
computer	_	_
vision	_	_
and	_	_
machine	_	_
learning	_	_
for	_	_
various	_	_
applications	_	_
such	_	_
as	_	_
image	_	_
to	_	_
image	_	_
translation	_	_
[	_	_
20	_	_
]	_	_
,	_	_
face	_	_
generation	_	_
[	_	_
12	_	_
]	_	_
,	_	_
semantic	_	_
segmentation	_	_
[	_	_
28	_	_
]	_	_
and	_	_
etc	_	_
.	_	_

#55
GANs	_	_
are	_	_
a	_	_
type	_	_
of	_	_
generative	_	_
model	_	_
which	_	_
learn	_	_
to	_	_
generate	_	_
based	_	_
on	_	_
generative	_	_
and	_	_
discriminative	_	_
networks	_	_
that	_	_
are	_	_
trained	_	_
and	_	_
updated	_	_
at	_	_
the	_	_
same	_	_
time	_	_
.	_	_

#56
The	_	_
generative	_	_
network	_	_
tries	_	_
to	_	_
find	_	_
the	_	_
true	_	_
distribution	_	_
of	_	_
the	_	_
data	_	_
and	_	_
generate	_	_
realistic	_	_
samples	_	_
and	_	_
the	_	_
discriminator	_	_
network	_	_
tries	_	_
to	_	_
discriminate	_	_
between	_	_
the	_	_
samples	_	_
that	_	_
are	_	_
generated	_	_
by	_	_
generated	_	_
and	_	_
samples	_	_
from	_	_
real	_	_
data	_	_
.	_	_

#57
The	_	_
general	_	_
formulation	_	_
of	_	_
conditional	_	_
GANs	_	_
are	_	_
as	_	_
follows	_	_
:	_	_
min	_	_
G	_	_
max	_	_
D	_	_
L	_	_
(	_	_
D	_	_
,	_	_
G	_	_
)	_	_
=	_	_
Ex	_	_
,	_	_
y∼pdata	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
[	_	_
logD	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
]	_	_
+	_	_
Ex∼pdata	_	_
(	_	_
x	_	_
)	_	_
,	_	_
z∼pz	_	_
(	_	_
z	_	_
)	_	_
[	_	_
log	_	_
(	_	_
1−D	_	_
(	_	_
x	_	_
,	_	_
G	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
)	_	_
)	_	_
]	_	_
.	_	_

#58
(	_	_
1	_	_
)	_	_
Conditional	_	_
GANs	_	_
are	_	_
generative	_	_
models	_	_
that	_	_
learn	_	_
a	_	_
mapping	_	_
from	_	_
random	_	_
noise	_	_
vector	_	_
z	_	_
to	_	_
output	_	_
image	_	_
y	_	_
conditioned	_	_
on	_	_
auxiliary	_	_
information	_	_
x	_	_
:	_	_
G	_	_
:	_	_
{	_	_
x	_	_
,	_	_
z	_	_
}	_	_
⇒	_	_
y	_	_
.	_	_

#59
A	_	_
conditional	_	_
GAN	_	_
consists	_	_
of	_	_
a	_	_
generator	_	_
G	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
and	_	_
a	_	_
discriminator	_	_
D	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
that	_	_
compete	_	_
in	_	_
a	_	_
two-player	_	_
minimax	_	_
game	_	_
:	_	_
the	_	_
discriminator	_	_
tries	_	_
to	_	_
distinguish	_	_
real	_	_
training	_	_
data	_	_
from	_	_
generated	_	_
images	_	_
,	_	_
and	_	_
the	_	_
generator	_	_
tries	_	_
to	_	_
fail	_	_
the	_	_
discriminator	_	_
.	_	_

#60
That	_	_
is	_	_
,	_	_
D	_	_
and	_	_
G	_	_
play	_	_
the	_	_
following	_	_
game	_	_
on	_	_
V	_	_
(	_	_
D	_	_
,	_	_
G	_	_
)	_	_
.	_	_

#61
As	_	_
you	_	_
can	_	_
see	_	_
the	_	_
goal	_	_
of	_	_
these	_	_
networks	_	_
is	_	_
to	_	_
increase	_	_
the	_	_
probability	_	_
of	_	_
samples	_	_
generated	_	_
by	_	_
generative	_	_
networks	_	_
to	_	_
resemble	_	_
the	_	_
real	_	_
data	_	_
so	_	_
that	_	_
the	_	_
discriminator	_	_
network	_	_
Figure	_	_
2	_	_
.	_	_

#62
Overview	_	_
of	_	_
our	_	_
affect-sketch	_	_
network	_	_
.	_	_

#63
In	_	_
first	_	_
stage	_	_
of	_	_
our	_	_
two-stage	_	_
facial	_	_
expression	_	_
generation	_	_
,	_	_
we	_	_
generate	_	_
face	_	_
sketches	_	_
conditioned	_	_
on	_	_
the	_	_
affective	_	_
state	_	_
of	_	_
the	_	_
interviewee	_	_
and	_	_
using	_	_
a	_	_
z	_	_
vector	_	_
that	_	_
carries	_	_
contains	_	_
semantically	_	_
meaningful	_	_
information	_	_
about	_	_
facial	_	_
behaviors	_	_
.	_	_

#64
This	_	_
vector	_	_
is	_	_
sampled	_	_
from	_	_
generated	_	_
distributions	_	_
of	_	_
rigid	_	_
and	_	_
non-rigid	_	_
face	_	_
shape	_	_
parameters	_	_
using	_	_
our	_	_
proposed	_	_
methods	_	_
.	_	_

#65
Figure	_	_
3	_	_
.	_	_

#66
Overview	_	_
of	_	_
our	_	_
sketch-image	_	_
network	_	_
.	_	_

#67
In	_	_
the	_	_
second	_	_
stage	_	_
of	_	_
our	_	_
two-stage	_	_
facial	_	_
expression	_	_
generation	_	_
,	_	_
we	_	_
generate	_	_
face	_	_
images	_	_
conditioned	_	_
on	_	_
the	_	_
generator	_	_
sketch	_	_
of	_	_
the	_	_
interviewer	_	_
which	_	_
is	_	_
generated	_	_
in	_	_
the	_	_
first	_	_
stage	_	_
of	_	_
our	_	_
network	_	_
.	_	_

#68
fail	_	_
to	_	_
differentiate	_	_
between	_	_
the	_	_
generated	_	_
samples	_	_
and	_	_
the	_	_
real	_	_
data	_	_
.	_	_

#69
Conditional	_	_
generative	_	_
adversarial	_	_
networks	_	_
(	_	_
CGANs	_	_
)	_	_
are	_	_
a	_	_
type	_	_
of	_	_
GAN	_	_
that	_	_
generate	_	_
samples	_	_
considering	_	_
a	_	_
conditioning	_	_
factor	_	_
.	_	_

#70
As	_	_
our	_	_
problem	_	_
is	_	_
generation	_	_
of	_	_
facial	_	_
expressions	_	_
conditioned	_	_
on	_	_
affective	_	_
states	_	_
,	_	_
we	_	_
found	_	_
CGANs	_	_
as	_	_
perfect	_	_
candidate	_	_
for	_	_
addressing	_	_
our	_	_
problem	_	_
.	_	_

#71
We	_	_
chose	_	_
to	_	_
use	_	_
CGANs	_	_
to	_	_
generate	_	_
facial	_	_
expressions	_	_
of	_	_
one	_	_
party	_	_
,	_	_
conditioned	_	_
on	_	_
the	_	_
affective	_	_
state	_	_
of	_	_
the	_	_
other	_	_
partner	_	_
in	_	_
a	_	_
dyadic	_	_
interaction.The	_	_
GAN	_	_
formulation	_	_
uses	_	_
a	_	_
continuous	_	_
input	_	_
noise	_	_
vector	_	_
z	_	_
,	_	_
that	_	_
is	_	_
an	_	_
input	_	_
to	_	_
the	_	_
generator	_	_
.	_	_

#72
However	_	_
,	_	_
this	_	_
vector	_	_
is	_	_
a	_	_
source	_	_
of	_	_
randomness	_	_
to	_	_
the	_	_
model	_	_
and	_	_
does	_	_
not	_	_
capture	_	_
semantically	_	_
meaningful	_	_
information	_	_
.	_	_

#73
However	_	_
,	_	_
using	_	_
a	_	_
meaningful	_	_
distribution	_	_
which	_	_
corresponds	_	_
to	_	_
our	_	_
objective	_	_
and	_	_
desired	_	_
facial	_	_
behaviors	_	_
,	_	_
can	_	_
help	_	_
the	_	_
model	_	_
integrate	_	_
meaningful	_	_
information	_	_
with	_	_
the	_	_
network	_	_
and	_	_
improve	_	_
the	_	_
results	_	_
of	_	_
generation	_	_
.	_	_

#74
For	_	_
example	_	_
,	_	_
in	_	_
case	_	_
of	_	_
generating	_	_
facial	_	_
behaviors	_	_
,	_	_
the	_	_
important	_	_
factors	_	_
are	_	_
the	_	_
variation	_	_
in	_	_
locations	_	_
of	_	_
facial	_	_
landmarks	_	_
such	_	_
as	_	_
smiling	_	_
and	_	_
head	_	_
pose	_	_
such	_	_
as	_	_
tilted	_	_
head	_	_
.	_	_

#75
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
present	_	_
a	_	_
modification	_	_
to	_	_
the	_	_
sampling	_	_
process	_	_
of	_	_
the	_	_
latent	_	_
vector	_	_
z	_	_
which	_	_
is	_	_
an	_	_
input	_	_
to	_	_
the	_	_
CGAN	_	_
network	_	_
.	_	_

#76
We	_	_
sample	_	_
z	_	_
from	_	_
the	_	_
meaningful	_	_
distributions	_	_
of	_	_
facial	_	_
behaviors	_	_
that	_	_
are	_	_
generated	_	_
by	_	_
two	_	_
different	_	_
approaches	_	_
1	_	_
)	_	_
Affect-shape	_	_
dictionary	_	_
and	_	_
2	_	_
)	_	_
Conditional	_	_
LSTM	_	_
.	_	_

#77
This	_	_
step	_	_
helps	_	_
the	_	_
network	_	_
to	_	_
learn	_	_
interpretable	_	_
Figure	_	_
4	_	_
.	_	_

#78
This	_	_
figure	_	_
shows	_	_
68	_	_
landmarks	_	_
of	_	_
the	_	_
face	_	_
that	_	_
we	_	_
intend	_	_
to	_	_
generate	_	_
using	_	_
our	_	_
model	_	_
.	_	_

#79
In	_	_
the	_	_
bottom	_	_
row	_	_
you	_	_
see	_	_
a	_	_
visualization	_	_
of	_	_
non-rigid	_	_
shape	_	_
parameters	_	_
which	_	_
correspond	_	_
to	_	_
movements	_	_
of	_	_
facial	_	_
landmarks	_	_
such	_	_
as	_	_
opening	_	_
the	_	_
mouth	_	_
.	_	_

#80
Our	_	_
model	_	_
aims	_	_
to	_	_
learn	_	_
the	_	_
distribution	_	_
of	_	_
these	_	_
parameters	_	_
and	_	_
use	_	_
them	_	_
in	_	_
generating	_	_
temporally	_	_
smooth	_	_
sequences	_	_
.	_	_

#81
and	_	_
meaningful	_	_
representation	_	_
of	_	_
facial	_	_
behaviors	_	_
.	_	_

#82
In	_	_
the	_	_
next	_	_
sections	_	_
we	_	_
will	_	_
first	_	_
explain	_	_
the	_	_
two	_	_
stage	_	_
architecture	_	_
of	_	_
our	_	_
network	_	_
.	_	_

#83
Then	_	_
we	_	_
will	_	_
explain	_	_
preprocessing	_	_
of	_	_
the	_	_
frames	_	_
by	_	_
going	_	_
over	_	_
the	_	_
intuition	_	_
behind	_	_
our	_	_
choice	_	_
of	_	_
face	_	_
representation	_	_
.	_	_

#84
Finally	_	_
,	_	_
we	_	_
provide	_	_
detailed	_	_
description	_	_
of	_	_
our	_	_
methods	_	_
for	_	_
generating	_	_
distributions	_	_
for	_	_
vector	_	_
z	_	_
which	_	_
is	_	_
the	_	_
input	_	_
to	_	_
our	_	_
CGAN	_	_
networks	_	_
.	_	_

#85
3.1	_	_
.	_	_

#86
Two-Stage	_	_
Conditional	_	_
Generative	_	_
Adversarial	_	_
Network	_	_
To	_	_
generate	_	_
facial	_	_
behaviors	_	_
for	_	_
agent	_	_
,	_	_
we	_	_
use	_	_
a	_	_
two	_	_
stage	_	_
architecture	_	_
,	_	_
both	_	_
of	_	_
which	_	_
are	_	_
based	_	_
on	_	_
conditional	_	_
generative	_	_
adversarial	_	_
networks	_	_
.	_	_

#87
Figure	_	_
2	_	_
shows	_	_
an	_	_
overview	_	_
of	_	_
our	_	_
first	_	_
network	_	_
.	_	_

#88
In	_	_
the	_	_
first	_	_
stage	_	_
we	_	_
use	_	_
a	_	_
conditional	_	_
generative	_	_
adversarial	_	_
network	_	_
which	_	_
takes	_	_
face	_	_
shape	_	_
parameter	_	_
as	_	_
input	_	_
for	_	_
vector	_	_
z	_	_
as	_	_
well	_	_
as	_	_
the	_	_
conditioning	_	_
vector	_	_
which	_	_
is	_	_
the	_	_
8	_	_
affective	_	_
states	_	_
of	_	_
one	_	_
partner	_	_
in	_	_
the	_	_
interaction	_	_
,	_	_
and	_	_
it	_	_
generates	_	_
sketches	_	_
of	_	_
faces	_	_
for	_	_
the	_	_
other	_	_
partner	_	_
in	_	_
the	_	_
interaction	_	_
.	_	_

#89
Note	_	_
that	_	_
the	_	_
z	_	_
vector	_	_
will	_	_
be	_	_
sampled	_	_
using	_	_
one	_	_
of	_	_
our	_	_
proposed	_	_
strategies	_	_
,	_	_
which	_	_
are	_	_
explained	_	_
in	_	_
the	_	_
following	_	_
sections	_	_
.	_	_

#90
In	_	_
the	_	_
second	_	_
stage	_	_
,	_	_
we	_	_
input	_	_
the	_	_
generated	_	_
sketches	_	_
in	_	_
the	_	_
first	_	_
stage	_	_
as	_	_
input	_	_
to	_	_
GAN	_	_
and	_	_
generate	_	_
the	_	_
final	_	_
facial	_	_
expressions	_	_
.	_	_

#91
Figure	_	_
3	_	_
shows	_	_
an	_	_
overview	_	_
of	_	_
our	_	_
second	_	_
network	_	_
.	_	_

#92
3.2	_	_
.	_	_

#93
Data	_	_
Preprocessing	_	_
and	_	_
Face	_	_
Representation	_	_
Facial	_	_
landmarks	_	_
are	_	_
the	_	_
salient	_	_
points	_	_
on	_	_
face	_	_
located	_	_
at	_	_
the	_	_
corners	_	_
,	_	_
tips	_	_
or	_	_
midpoints	_	_
of	_	_
facial	_	_
components	_	_
such	_	_
as	_	_
eyes	_	_
,	_	_
nose	_	_
,	_	_
mouth	_	_
,	_	_
etc	_	_
[	_	_
41	_	_
]	_	_
.	_	_

#94
Movements	_	_
of	_	_
these	_	_
points	_	_
form	_	_
various	_	_
facial	_	_
expressions	_	_
that	_	_
can	_	_
convey	_	_
different	_	_
affective	_	_
states	_	_
.	_	_

#95
For	_	_
example	_	_
,	_	_
widening	_	_
the	_	_
eyes	_	_
can	_	_
communicate	_	_
surprise	_	_
and	_	_
a	_	_
smile	_	_
can	_	_
be	_	_
an	_	_
indication	_	_
of	_	_
happiness	_	_
.	_	_

#96
Since	_	_
the	_	_
locations	_	_
of	_	_
facial	_	_
landmarks	_	_
alone	_	_
are	_	_
not	_	_
particularly	_	_
meaningful	_	_
in	_	_
communicating	_	_
affect	_	_
but	_	_
rather	_	_
their	_	_
movements	_	_
and	_	_
change	_	_
over	_	_
time	_	_
which	_	_
plays	_	_
a	_	_
rule	_	_
in	_	_
affect	_	_
recognition	_	_
[	_	_
38	_	_
]	_	_
,	_	_
we	_	_
decided	_	_
to	_	_
measure	_	_
their	_	_
movements	_	_
and	_	_
change	_	_
over	_	_
time	_	_
.	_	_

#97
To	_	_
do	_	_
so	_	_
,	_	_
we	_	_
are	_	_
projecting	_	_
a	_	_
3D	_	_
point	_	_
distribution	_	_
model	_	_
on	_	_
each	_	_
image	_	_
.	_	_

#98
This	_	_
model	_	_
has	_	_
been	_	_
trained	_	_
on	_	_
in	_	_
the	_	_
wild	_	_
data	_	_
[	_	_
25	_	_
]	_	_
.	_	_

#99
The	_	_
following	_	_
equation	_	_
is	_	_
used	_	_
to	_	_
place	_	_
a	_	_
single	_	_
feature	_	_
point	_	_
of	_	_
the	_	_
3D	_	_
PDM	_	_
in	_	_
a	_	_
given	_	_
input	_	_
image	_	_
:	_	_
x̄i	_	_
=	_	_
s.R2D	_	_
.	_	_

#100
(	_	_
X̄i	_	_
+φiq	_	_
)	_	_
+	_	_
t	_	_
In	_	_
this	_	_
equation	_	_
,	_	_
s	_	_
shows	_	_
scale	_	_
,	_	_
R	_	_
shows	_	_
the	_	_
head	_	_
rotation	_	_
and	_	_
t	_	_
is	_	_
the	_	_
head	_	_
translation	_	_
which	_	_
are	_	_
known	_	_
as	_	_
rigid-	_	_
shape	_	_
parameters	_	_
.	_	_

#101
X̄i	_	_
=	_	_
[	_	_
x̄i	_	_
,	_	_
ȳi	_	_
,	_	_
z̄i	_	_
]	_	_
is	_	_
the	_	_
mean	_	_
value	_	_
of	_	_
i	_	_
th	_	_
feature	_	_
,	_	_
φi	_	_
is	_	_
principle	_	_
component	_	_
matrix	_	_
and	_	_
q	_	_
is	_	_
a	_	_
matrix	_	_
controlling	_	_
the	_	_
non-rigid	_	_
shape	_	_
parameters	_	_
,	_	_
which	_	_
correspond	_	_
to	_	_
deviations	_	_
of	_	_
landmarks	_	_
locations	_	_
from	_	_
an	_	_
average	_	_
(	_	_
neutral	_	_
)	_	_
face	_	_
.	_	_

#102
As	_	_
described	_	_
,	_	_
shape	_	_
parameters	_	_
of	_	_
the	_	_
face	_	_
carry	_	_
rich	_	_
information	_	_
about	_	_
head	_	_
pose	_	_
and	_	_
facial	_	_
expressions	_	_
.	_	_

#103
Hence	_	_
,	_	_
we	_	_
use	_	_
them	_	_
as	_	_
our	_	_
face	_	_
representations	_	_
.	_	_

#104
Figure	_	_
4	_	_
shows	_	_
68	_	_
facial	_	_
landmarks	_	_
that	_	_
we	_	_
have	_	_
considered	_	_
in	_	_
this	_	_
work	_	_
.	_	_

#105
We	_	_
have	_	_
also	_	_
included	_	_
a	_	_
visualization	_	_
of	_	_
non-rigid	_	_
shape	_	_
parameters	_	_
at	_	_
the	_	_
bottom	_	_
row	_	_
for	_	_
better	_	_
understanding	_	_
of	_	_
these	_	_
parameters	_	_
.	_	_

#106
3.3	_	_
.	_	_

#107
Affect-Shape	_	_
Dictionary	_	_
Different	_	_
facial	_	_
expressions	_	_
are	_	_
indicative	_	_
of	_	_
different	_	_
affective	_	_
states	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#108
Depending	_	_
on	_	_
the	_	_
affective	_	_
state	_	_
and	_	_
its	_	_
intensity	_	_
,	_	_
the	_	_
temporal	_	_
dynamics	_	_
and	_	_
changes	_	_
in	_	_
landmarks	_	_
can	_	_
happen	_	_
in	_	_
various	_	_
facial	_	_
expressions	_	_
such	_	_
as	_	_
a	_	_
smile	_	_
vs	_	_
a	_	_
frown	_	_
and	_	_
they	_	_
can	_	_
happen	_	_
slow	_	_
or	_	_
fast	_	_
depending	_	_
on	_	_
how	_	_
intense	_	_
is	_	_
the	_	_
affect	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#109
Based	_	_
on	_	_
this	_	_
knowledge	_	_
,	_	_
we	_	_
propose	_	_
the	_	_
following	_	_
strategies	_	_
to	_	_
learn	_	_
meaningful	_	_
distributions	_	_
of	_	_
face	_	_
shape	_	_
parameters	_	_
.	_	_

#110
Figure	_	_
5	_	_
shows	_	_
an	_	_
overview	_	_
of	_	_
our	_	_
affect-shape	_	_
dictionary	_	_
generation	_	_
.	_	_

#111
3.3.1	_	_
Affect	_	_
Clustering	_	_

#112
While	_	_
in	_	_
state	_	_
of	_	_
fear	_	_
landmarks	_	_
can	_	_
change	_	_
very	_	_
sudden	_	_
,	_	_
in	_	_
state	_	_
of	_	_
sadness	_	_
or	_	_
neutral	_	_
the	_	_
changes	_	_
in	_	_
locations	_	_
of	_	_
landmarks	_	_
will	_	_
be	_	_
smaller	_	_
and	_	_
slower	_	_
.	_	_

#113
In	_	_
order	_	_
to	_	_
account	_	_
for	_	_
this	_	_
point	_	_
,	_	_
we	_	_
first	_	_
cluster	_	_
the	_	_
calculated	_	_
shape	_	_
parameters	_	_
for	_	_
all	_	_
input	_	_
frames	_	_
into	_	_
8	_	_
clusters	_	_
,	_	_
using	_	_
k-means	_	_
clustering	_	_
[	_	_
14	_	_
]	_	_
where	_	_
clusters	_	_
denote	_	_
the	_	_
following	_	_
affective	_	_
states	_	_
:	_	_
Joy	_	_
,	_	_
Anger	_	_
,	_	_
Surprise	_	_
,	_	_
Fear	_	_
,	_	_
Contempt	_	_
,	_	_
Disgust	_	_
,	_	_
Sadness	_	_
and	_	_
Neutral	_	_
.	_	_

#114
This	_	_
stage	_	_
gives	_	_
us	_	_
distributions	_	_
of	_	_
possible	_	_
facial	_	_
expressions	_	_
for	_	_
each	_	_
affective	_	_
state	_	_
.	_	_

#115
3.3.2	_	_
Inter	_	_
Affect	_	_
Clustering	_	_

#116
The	_	_
output	_	_
of	_	_
the	_	_
first	_	_
clustering	_	_
step	_	_
will	_	_
give	_	_
us	_	_
groupings	_	_
of	_	_
shape	_	_
parameters	_	_
in	_	_
8	_	_
clusters	_	_
corresponding	_	_
to	_	_
each	_	_
category	_	_
of	_	_
affective	_	_
states	_	_
.	_	_

#117
Each	_	_
affective	_	_
state	_	_
can	_	_
have	_	_
various	_	_
arousal	_	_
levels	_	_
(	_	_
a.k.a	_	_
intensity	_	_
)	_	_
[	_	_
24	_	_
]	_	_
.	_	_

#118
We	_	_
did	_	_
a	_	_
second	_	_
level	_	_
of	_	_
clustering	_	_
of	_	_
shape	_	_
parameters	_	_
inside	_	_
each	_	_
affect	_	_
cluster	_	_
.	_	_

#119
To	_	_
do	_	_
so	_	_
,	_	_
we	_	_
used	_	_
a	_	_
hierarchical	_	_
agglomerative	_	_
clustering	_	_
approach	_	_
[	_	_
11	_	_
]	_	_
,	_	_
where	_	_
we	_	_
put	_	_
the	_	_
constraint	_	_
that	_	_
each	_	_
cluster	_	_
should	deontic	_
contain	_	_
at	_	_
least	_	_
100	_	_
videos	_	_
.	_	_

#120
The	_	_
result	_	_
of	_	_
this	_	_
clustering	_	_
gives	_	_
us	_	_
3	_	_
to	_	_
9	_	_
sub	_	_
clusters	_	_
corresponding	_	_
to	_	_
different	_	_
arousal	_	_
levels	_	_
of	_	_
each	_	_
affective	_	_
state	_	_
.	_	_

#121
Having	_	_
information	_	_
about	_	_
the	_	_
possible	_	_
facial	_	_
expressions	_	_
that	_	_
can	_	_
happen	_	_
in	_	_
a	_	_
particular	_	_
affect	_	_
category	_	_
and	_	_
what	_	_
has	_	_
been	_	_
expressed	_	_
in	_	_
previous	_	_
frames	_	_
by	_	_
the	_	_
agent	_	_
,	_	_
we	_	_
can	_	_
sample	_	_
z	_	_
from	_	_
an	_	_
appropriate	_	_
distribution	_	_
which	_	_
results	_	_
in	_	_
an	_	_
expressive	_	_
and	_	_
temporally	_	_
smooth	_	_
sequence	_	_
of	_	_
facial	_	_
expressions	_	_
.	_	_

#122
To	_	_
account	_	_
for	_	_
the	_	_
temporal	_	_
constraints	_	_
,	_	_
for	_	_
generating	_	_
nth	_	_
frame	_	_
,	_	_
we	_	_
pick	_	_
z	_	_
from	_	_
the	_	_
appropriate	_	_
distribution	_	_
based	_	_
on	_	_
the	_	_
affective	_	_
state	_	_
which	_	_
is	_	_
the	_	_
closest	_	_
to	_	_
the	_	_
previously	_	_
chosen	_	_
z	_	_
in	_	_
terms	_	_
of	_	_
euclidean	_	_
distance	_	_
.	_	_

#123
3.4	_	_
.	_	_

#124
Conditional	_	_
LSTM	_	_
Long	_	_
Short	_	_
Term	_	_
Memory	_	_
networks	_	_
(	_	_
LSTM	_	_
)	_	_
have	_	_
been	_	_
known	_	_
for	_	_
their	_	_
ability	_	_
in	_	_
learning	_	_
long-term	_	_
dependencies	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#125
We	_	_
are	_	_
interested	_	_
in	_	_
learning	_	_
the	_	_
dynamics	_	_
of	_	_
facial	_	_
expression	_	_
,	_	_
hence	_	_
LSTMs	_	_
are	_	_
an	_	_
appropriate	_	_
candidate	_	_
for	_	_
our	_	_
problem	_	_
.	_	_

#126
As	_	_
the	_	_
problem	_	_
we	_	_
are	_	_
solving	_	_
is	_	_
a	_	_
conditional	_	_
problem	_	_
,	_	_
we	_	_
will	_	_
use	_	_
a	_	_
conditional	_	_
LSTM	_	_
(	_	_
C-LSTM	_	_
)	_	_
[	_	_
2	_	_
]	_	_
.	_	_

#127
The	_	_
input	_	_
to	_	_
CLSTM	_	_
will	_	_
be	_	_
the	_	_
concatenated	_	_
vector	_	_
of	_	_
affective	_	_
state	_	_
of	_	_
the	_	_
interviewer	_	_
and	_	_
the	_	_
shape	_	_
parameters	_	_
of	_	_
the	_	_
interviewee	_	_
for	_	_
n	_	_
previous	_	_
frames	_	_
and	_	_
the	_	_
generated	_	_
output	_	_
will	_	_
be	_	_
the	_	_
future	_	_
shape	_	_
parameters	_	_
of	_	_
the	_	_
interviewee	_	_
.	_	_

#128
We	_	_
fix	_	_
n	_	_
to	_	_
be	_	_
100	_	_
frames	_	_
in	_	_
our	_	_
experiments	_	_
.	_	_

#129
We	_	_
use	_	_
conditional	_	_
LSTMS	_	_
in	_	_
two	_	_
different	_	_
settings	_	_
which	_	_
are	_	_
described	_	_
in	_	_
the	_	_
following	_	_
subsections	_	_
.	_	_

#130
Figure	_	_
6	_	_
shows	_	_
an	_	_
overview	_	_
of	_	_
our	_	_
C-LSTM	_	_
approach	_	_
.	_	_

#131
Overlapping	_	_
C-LSTM	_	_
:	_	_
In	_	_
overlapping	_	_
conditional	_	_
LSTMS	_	_
,	_	_
we	_	_
consider	_	_
information	_	_
across	_	_
100	_	_
frames	_	_
as	_	_
inFigure	_	_
5	_	_
.	_	_

#132
Overview	_	_
of	_	_
our	_	_
affect-shape	_	_
approach	_	_
for	_	_
generating	_	_
clusters	_	_
of	_	_
face	_	_
shape	_	_
parameters	_	_
.	_	_

#133
We	_	_
first	_	_
extract	_	_
shape	_	_
descriptors	_	_
for	_	_
all	_	_
faces	_	_
in	_	_
the	_	_
dataset	_	_
.	_	_

#134
Then	_	_
we	_	_
cluster	_	_
them	_	_
into	_	_
8	_	_
affect	_	_
classes	_	_
corresponding	_	_
to	_	_
Joy	_	_
,	_	_
Anger	_	_
,	_	_
Surprise	_	_
,	_	_
Fear	_	_
,	_	_
Contempt	_	_
,	_	_
Disgust	_	_
,	_	_
Sadness	_	_
and	_	_
Neutral	_	_
.	_	_

#135
Then	_	_
,	_	_
we	_	_
do	_	_
a	_	_
further	_	_
inter	_	_
class	_	_
clustering	_	_
for	_	_
each	_	_
affect	_	_
class	_	_
.	_	_

#136
The	_	_
purpose	_	_
of	_	_
the	_	_
second	_	_
clustering	_	_
is	_	_
to	_	_
find	_	_
the	_	_
inter	_	_
affect	_	_
clusters	_	_
,	_	_
such	_	_
as	_	_
very	_	_
joyful	_	_
,	_	_
joyful	_	_
or	_	_
little	_	_
joyful.These	_	_
clusters	_	_
form	_	_
the	_	_
distributions	_	_
from	_	_
which	_	_
the	_	_
z	_	_
vector	_	_
is	_	_
sampled	_	_
in	_	_
our	_	_
generative	_	_
models	_	_
.	_	_

#137
Figure	_	_
6	_	_
.	_	_

#138
Overview	_	_
of	_	_
our	_	_
conditional	_	_
LSTM	_	_
approach	_	_
for	_	_
generating	_	_
facial	_	_
shape	_	_
parameters	_	_
.	_	_

#139
In	_	_
this	_	_
approach	_	_
we	_	_
consider	_	_
the	_	_
affective	_	_
state	_	_
of	_	_
the	_	_
interviewee	_	_
and	_	_
past	_	_
frames	_	_
of	_	_
the	_	_
interviewer’s	_	_
facial	_	_
expressions	_	_
as	_	_
conditioning	_	_
vector	_	_
and	_	_
previous	_	_
history	_	_
.	_	_

#140
We	_	_
then	_	_
learn	_	_
to	_	_
generate	_	_
the	_	_
future	_	_
frames	_	_
of	_	_
interviewer’s	_	_
facial	_	_
expressions	_	_
using	_	_
a	_	_
conditional	_	_
LSTM	_	_
.	_	_

#141
The	_	_
output	_	_
result	_	_
of	_	_
this	_	_
generation	_	_
is	_	_
the	_	_
input	_	_
for	_	_
z	_	_
vector	_	_
in	_	_
our	_	_
generative	_	_
models	_	_
.	_	_

#142
put	_	_
of	_	_
the	_	_
model	_	_
and	_	_
the	_	_
output	_	_
of	_	_
the	_	_
model	_	_
is	_	_
generation	_	_
of	_	_
shape	_	_
parameters	_	_
for	_	_
one	_	_
future	_	_
frame	_	_
.	_	_

#143
In	_	_
each	_	_
step	_	_
,	_	_
the	_	_
newly	_	_
generated	_	_
frame	_	_
is	_	_
added	_	_
to	_	_
the	_	_
history	_	_
and	_	_
the	_	_
information	_	_
from	_	_
the	_	_
first	_	_
input	_	_
frame	_	_
is	_	_
removed	_	_
instead	_	_
(	_	_
in	_	_
first	_	_
in	_	_
,	_	_
first	_	_
out	_	_
manner	_	_
)	_	_
,	_	_
to	_	_
predict	_	_
futures	_	_
frames	_	_
.	_	_

#144
Non-overlapping	_	_
C-LSTM	_	_
:	_	_
In	_	_
non-overlapping	_	_
CLSTMs	_	_
the	_	_
input	_	_
to	_	_
the	_	_
models	_	_
is	_	_
information	_	_
across	_	_
100	_	_
frames	_	_
and	_	_
the	_	_
output	_	_
is	_	_
generation	_	_
of	_	_
shape	_	_
parameters	_	_
for	_	_
future	_	_
100	_	_
frames	_	_
,	_	_
with	_	_
no	_	_
overlap	_	_
between	_	_
the	_	_
frames	_	_
.	_	_

#145
4	_	_
.	_	_

#146
Experiments	_	_
To	_	_
assess	_	_
the	_	_
effectiveness	_	_
of	_	_
our	_	_
proposed	_	_
approaches	_	_
in	_	_
generating	_	_
appropriate	_	_
and	_	_
temporally	_	_
smooth	_	_
facial	_	_
expressions	_	_
,	_	_
we	_	_
performed	_	_
a	_	_
number	_	_
of	_	_
experiments	_	_
using	_	_
the	_	_
proposed	_	_
frameworks	_	_
approaches	_	_
.	_	_

#147
Our	_	_
experiments	_	_
show	_	_
the	_	_
comparison	_	_
between	_	_
our	_	_
models	_	_
in	_	_
terms	_	_
of	_	_
result	_	_
and	_	_
their	_	_
pros	_	_
and	_	_
cons	_	_
.	_	_

#148
4.1	_	_
.	_	_

#149
Dataset	_	_
The	_	_
dataset	_	_
used	_	_
in	_	_
this	_	_
paper	_	_
is	_	_
31	_	_
pair	_	_
of	_	_
dyadic	_	_
interactions	_	_
(	_	_
videos	_	_
)	_	_
which	_	_
are	_	_
interviews	_	_
for	_	_
undergraduate	_	_
Figure	_	_
7	_	_
.	_	_

#150
Examples	_	_
of	_	_
generated	_	_
sketches	_	_
using	_	_
both	_	_
of	_	_
our	_	_
approaches	_	_
.	_	_

#151
The	_	_
first	_	_
sequence	_	_
corresponds	_	_
to	_	_
the	_	_
result	_	_
of	_	_
affect-shape	_	_
dictionary	_	_
approach	_	_
and	_	_
the	_	_
second	_	_
row	_	_
is	_	_
generated	_	_
using	_	_
the	_	_
conditional	_	_
LSTM	_	_
approach	_	_
.	_	_

#152
Overall	_	_
,	_	_
our	_	_
first	_	_
approach	_	_
generates	_	_
smoother	_	_
sequences	_	_
.	_	_

#153
admissions	_	_
process	_	_
.	_	_

#154
The	_	_
purpose	_	_
of	_	_
interviews	_	_
is	_	_
to	_	_
assess	_	_
English	_	_
speaking	_	_
ability	_	_
of	_	_
the	_	_
prospective	_	_
college	_	_
.	_	_

#155
There	_	_
are	_	_
16	_	_
male	_	_
and	_	_
15	_	_
female	_	_
candidates	_	_
and	_	_
each	_	_
candidate	_	_
is	_	_
interviewed	_	_
by	_	_
the	_	_
same	_	_
interviewer	_	_
(	_	_
Caucasian	_	_
female	_	_
)	_	_
who	_	_
followed	_	_
a	_	_
predetermined	_	_
set	_	_
of	_	_
academic	_	_
and	_	_
nonacademic	_	_
questions	_	_
designed	_	_
to	_	_
encourage	_	_
open	_	_
conversation	_	_
.	_	_

#156
The	_	_
interviews	_	_
were	_	_
conducted	_	_
using	_	_
Skype	_	_
video	_	_
conferencing	_	_
so	_	_
the	_	_
participants	_	_
could	capability	_
see	_	_
and	_	_
hear	_	_
each	_	_
other	_	_
and	_	_
the	_	_
video	_	_
data	_	_
from	_	_
each	_	_
dyadic	_	_
interaction	_	_
was	_	_
captured	_	_
.	_	_

#157
The	_	_
duration	_	_
of	_	_
interviews	_	_
varies	_	_
from	_	_
8	_	_
to	_	_
37	_	_
minutes	_	_
and	_	_
a	_	_
total	_	_
of	_	_
24	_	_
hours	_	_
of	_	_
video	_	_
data	_	_
.	_	_

#158
We	_	_
have	_	_
used	_	_
70,000	_	_
short	_	_
video	_	_
clips	_	_
for	_	_
training	_	_
data	_	_
and	_	_
7000	_	_
videos	_	_
for	_	_
evaluation	_	_
.	_	_

#159
4.2	_	_
.	_	_

#160
Affect	_	_
Recognition	_	_
Framework	_	_
For	_	_
estimating	_	_
the	_	_
affective	_	_
state	_	_
of	_	_
the	_	_
interviewee	_	_
,	_	_
we	_	_
have	_	_
used	_	_
Emotients	_	_
Facet	_	_
SDK	_	_
[	_	_
1	_	_
]	_	_
to	_	_
process	_	_
the	_	_
frames	_	_
and	_	_
estimate	_	_
8-	_	_
dimensional	_	_
affect	_	_
descriptor	_	_
vectors	_	_
,	_	_
representing	_	_
the	_	_
likelihood	_	_
of	_	_
the	_	_
following	_	_
classes	_	_
:	_	_
joy	_	_
,	_	_
anger	_	_
,	_	_
surprise	_	_
,	_	_
fear	_	_
,	_	_
contempt	_	_
,	_	_
disgust	_	_
,	_	_
sadness	_	_
and	_	_
neutral	_	_
.	_	_

#161
4.3	_	_
.	_	_

#162
Implementation	_	_
Details	_	_
To	_	_
create	_	_
our	_	_
training	_	_
set	_	_
we	_	_
randomly	_	_
sampled	_	_
70,000	_	_
video	_	_
clips	_	_
of	_	_
100	_	_
frames	_	_
each	_	_
(	_	_
3.3	_	_
seconds	_	_
)	_	_
from	_	_
the	_	_
interviewee	_	_
videos	_	_
and	_	_
extracted	_	_
their	_	_
affect	_	_
descriptor	_	_
using	_	_
Facet	_	_
SDK	_	_
.	_	_

#163
These	_	_
affect	_	_
descriptors	_	_
are	_	_
then	_	_
combined	_	_
into	_	_
a	_	_
single	_	_
vector	_	_
using	_	_
the	_	_
approach	_	_
proposed	_	_
by	_	_
Huang	_	_
et	_	_
al.	_	_
[	_	_
18	_	_
]	_	_
.	_	_

#164
For	_	_
each	_	_
interviewee	_	_
video	_	_
clip	_	_
a	_	_
single	_	_
frame	_	_
from	_	_
the	_	_
corresponding	_	_
interviewer	_	_
video	_	_
is	_	_
also	_	_
sampled	_	_
for	_	_
training	_	_
.	_	_

#165
Our	_	_
approach	_	_
is	_	_
to	_	_
train	_	_
the	_	_
model	_	_
to	_	_
generate	_	_
a	_	_
single	_	_
frame	_	_
of	_	_
the	_	_
interviewer	_	_
conditioned	_	_
on	_	_
facial	_	_
expression	_	_
descriptors	_	_
from	_	_
the	_	_
preceding	_	_
100	_	_
frames	_	_
of	_	_
the	_	_
interviewee	_	_
.	_	_

#166
All	_	_
face	_	_
images	_	_
are	_	_
aligned	_	_
and	_	_
landmarks	_	_
are	_	_
estimated	_	_
by	_	_
the	_	_
method	_	_
proposed	_	_
by	_	_
kazem	_	_
et	_	_
al.	_	_
[	_	_
22	_	_
]	_	_
to	_	_
generate	_	_
ground	_	_
truth	_	_
face	_	_
sketches	_	_
.	_	_

#167
For	_	_
creating	_	_
our	_	_
test	_	_
data	_	_
,	_	_
we	_	_
randomly	_	_
sampled	_	_
7000	_	_
interviewee	_	_
video	_	_
clips	_	_
of	_	_
400	_	_
frames	_	_
each	_	_
(	_	_
no	_	_
overlap	_	_
with	_	_
the	_	_
training	_	_
set	_	_
)	_	_
and	_	_
used	_	_
these	_	_
as	_	_
input	_	_
to	_	_
our	_	_
two	_	_
model	_	_
to	_	_
generate	_	_
7000	_	_
facial	_	_
expression	_	_
video	_	_
clips	_	_
.	_	_

#168
For	_	_
training	_	_
the	_	_
first	_	_
stage	_	_
of	_	_
our	_	_
network	_	_
(	_	_
affect	_	_
to	_	_
sketch	_	_
)	_	_
,	_	_
we	_	_
first	_	_
estimate	_	_
the	_	_
landmarks	_	_
of	_	_
the	_	_
interviewer	_	_
using	_	_
the	_	_
proposed	_	_
method	_	_
by	_	_
Kazemi	_	_
et	_	_
al.	_	_
[	_	_
22	_	_
]	_	_
which	_	_
gives	_	_
us	_	_
68	_	_
facial	_	_
landmarks	_	_
(	_	_
See	_	_
Figure	_	_
4	_	_
)	_	_
.	_	_

#169
We	_	_
then	_	_
connect	_	_
these	_	_
points	_	_
by	_	_
linear	_	_
lines	_	_
of	_	_
one	_	_
pixel	_	_
width	_	_
to	_	_
generate	_	_
the	_	_
sketch	_	_
images	_	_
.	_	_

#170
The	_	_
generated	_	_
sketches	_	_
are	_	_
used	_	_
in	_	_
a	_	_
pair	_	_
with	_	_
the	_	_
corresponding	_	_
image	_	_
of	_	_
the	_	_
interviewer	_	_
for	_	_
training	_	_
the	_	_
second	_	_
network	_	_
.	_	_

#171
We	_	_
masked	_	_
all	_	_
training	_	_
image	_	_
pairs	_	_
by	_	_
an	_	_
oval	_	_
mask	_	_
after	_	_
roughly	_	_
aligned	_	_
all	_	_
faces	_	_
,	_	_
to	_	_
reduce	_	_
the	_	_
effect	_	_
of	_	_
appearance	_	_
and	_	_
lightening	_	_
variation	_	_
of	_	_
the	_	_
the	_	_
face	_	_
in	_	_
generating	_	_
our	_	_
videos	_	_
.	_	_

#172
In	_	_
the	_	_
generator	_	_
,	_	_
a	_	_
sketch	_	_
image	_	_
is	_	_
passed	_	_
through	_	_
an	_	_
encoder-decoder	_	_
network	_	_
[	_	_
20	_	_
]	_	_
each	_	_
containing	_	_
8	_	_
layers	_	_
of	_	_
down	_	_
sampling	_	_
and	_	_
up	_	_
sampling	_	_
,	_	_
in	_	_
order	_	_
to	_	_
generate	_	_
the	_	_
final	_	_
image	_	_
.	_	_

#173
We	_	_
have	_	_
adopted	_	_
the	_	_
idea	_	_
proposed	_	_
by	_	_
Ronneberger	_	_
et	_	_
al.	_	_
[	_	_
37	_	_
]	_	_
and	_	_
have	_	_
connected	_	_
feature	_	_
maps	_	_
at	_	_
layer	_	_
i	_	_
and	_	_
layer	_	_
n-i	_	_
where	_	_
n	_	_
is	_	_
total	_	_
number	_	_
of	_	_
layers	_	_
in	_	_
the	_	_
network	_	_
.	_	_

#174
In	_	_
this	_	_
network	_	_
receptive	_	_
fields	_	_
after	_	_
convolution	_	_
are	_	_
concatenated	_	_
with	_	_
the	_	_
receptive	_	_
fields	_	_
in	_	_
up-convolving	_	_
process	_	_
,	_	_
which	_	_
allows	_	_
network	_	_
to	_	_
use	_	_
original	_	_
features	_	_
in	_	_
addition	_	_
to	_	_
features	_	_
after	_	_
up-convolution	_	_
and	_	_
results	_	_
in	_	_
overall	_	_
better	_	_
performance	_	_
than	_	_
a	_	_
network	_	_
that	_	_
has	_	_
access	_	_
to	_	_
only	_	_
features	_	_
after	_	_
up-sampling	_	_
.	_	_

#175
We	_	_
adopted	_	_
the	_	_
architecture	_	_
proposed	_	_
by	_	_
Radford	_	_
et	_	_
.	_	_

#176
al	_	_
[	_	_
35	_	_
]	_	_
for	_	_
our	_	_
generation	_	_
framework	_	_
and	_	_
deep	_	_
convolutional	_	_
structure	_	_
for	_	_
generator	_	_
and	_	_
discriminator	_	_
.	_	_

#177
We	_	_
used	_	_
modules	_	_
of	_	_
the	_	_
form	_	_
convolution-BatchNorm-ReLu	_	_
[	_	_
19	_	_
]	_	_
to	_	_
stabilize	_	_
optimization	_	_
.	_	_

#178
In	_	_
the	_	_
training	_	_
phase	_	_
,	_	_
we	_	_
used	_	_
mini-batch	_	_
SGD	_	_
and	_	_
applied	_	_
the	_	_
Adam	_	_
solver	_	_
.	_	_

#179
To	_	_
avoid	_	_
the	_	_
fast	_	_
conFigure	_	_
8	_	_
.	_	_

#180
Examples	_	_
of	_	_
generated	_	_
face	_	_
images	_	_
using	_	_
the	_	_
second	_	_
stage	_	_
of	_	_
our	_	_
architecture	_	_
.	_	_

#181
The	_	_
first	_	_
sequence	_	_
corresponds	_	_
to	_	_
a	_	_
single	_	_
step	_	_
generation	_	_
without	_	_
considering	_	_
the	_	_
sketches	_	_
and	_	_
generating	_	_
face	_	_
images	_	_
directly	_	_
from	_	_
the	_	_
affect	_	_
vector	_	_
and	_	_
noise	_	_
.	_	_

#182
The	_	_
second	_	_
row	_	_
shows	_	_
the	_	_
results	_	_
related	_	_
to	_	_
generation	_	_
of	_	_
faces	_	_
using	_	_
our	_	_
proposed	_	_
two	_	_
stage	_	_
architecture	_	_
that	_	_
we	_	_
have	_	_
used	_	_
in	_	_
our	_	_
experiments	_	_
.	_	_

#183
Note	_	_
that	_	_
the	_	_
quality	_	_
of	_	_
images	_	_
in	_	_
the	_	_
second	_	_
row	_	_
is	_	_
much	_	_
better	_	_
than	_	_
the	_	_
first	_	_
row	_	_
.	_	_

#184
vergence	_	_
of	_	_
discriminators	_	_
,	_	_
generators	_	_
were	_	_
updated	_	_
twice	_	_
for	_	_
each	_	_
discriminator	_	_
update	_	_
,	_	_
which	_	_
differs	_	_
from	_	_
original	_	_
setting	_	_
[	_	_
35	_	_
]	_	_
in	_	_
that	_	_
the	_	_
discriminator	_	_
and	_	_
generator	_	_
update	_	_
alternately	_	_
.	_	_

#185
In	_	_
order	_	_
to	_	_
generate	_	_
temporally	_	_
smooth	_	_
sequences	_	_
,	_	_
for	_	_
each	_	_
new	_	_
frame	_	_
,	_	_
z	_	_
is	_	_
sampled	_	_
from	_	_
a	_	_
meaningful	_	_
distribution	_	_
and	_	_
based	_	_
on	_	_
the	_	_
previous	_	_
frames	_	_
.	_	_

#186
Note	_	_
that	_	_
the	_	_
appropriateness	_	_
of	_	_
z	_	_
for	_	_
a	_	_
frame	_	_
is	_	_
decided	_	_
based	_	_
on	_	_
the	_	_
possible	_	_
variation	_	_
between	_	_
two	_	_
adjacent	_	_
frames	_	_
in	_	_
distribution	_	_
of	_	_
face	_	_
shape	_	_
parameters	_	_
training	_	_
data	_	_
which	_	_
correspond	_	_
to	_	_
the	_	_
movements	_	_
of	_	_
facial	_	_
landmarks	_	_
and	_	_
head	_	_
pose	_	_
.	_	_

#187
4.4	_	_
.	_	_

#188
Results	_	_
and	_	_
Discussions	_	_
We	_	_
generated	_	_
sequences	_	_
of	_	_
non-verbal	_	_
facial	_	_
behaviors	_	_
for	_	_
an	_	_
agent	_	_
,	_	_
based	_	_
on	_	_
affect-shape	_	_
strategy	_	_
and	_	_
conditional	_	_
LSTM	_	_
.	_	_

#189
Figure	_	_
7	_	_
shows	_	_
the	_	_
output	_	_
sketches	_	_
generated	_	_
by	_	_
these	_	_
two	_	_
models	_	_
.	_	_

#190
The	_	_
first	_	_
row	_	_
corresponds	_	_
to	_	_
the	_	_
affect-shape	_	_
dictionary	_	_
approach	_	_
and	_	_
the	_	_
second	_	_
row	_	_
shows	_	_
the	_	_
results	_	_
based	_	_
on	_	_
Conditional	_	_
LSTM	_	_
.	_	_

#191
We	_	_
observed	_	_
that	_	_
the	_	_
results	_	_
from	_	_
dictionary	_	_
based	_	_
is	_	_
more	_	_
smooth	_	_
.	_	_

#192
In	_	_
results	_	_
of	_	_
conditional	_	_
LSTM	_	_
there	_	_
seems	_	_
to	_	_
be	_	_
grouping	_	_
of	_	_
smooth	_	_
sequences	_	_
followed	_	_
by	_	_
sudden	_	_
jumps	_	_
between	_	_
the	_	_
sequences	_	_
(	_	_
In	_	_
Figure	_	_
7	_	_
see	_	_
the	_	_
transition	_	_
between	_	_
last	_	_
and	_	_
one	_	_
before	_	_
last	_	_
frame	_	_
in	_	_
the	_	_
second	_	_
row	_	_
)	_	_
.	_	_

#193
We	_	_
think	_	_
that	_	_
this	_	_
can	_	_
happen	_	_
because	_	_
of	_	_
number	_	_
of	_	_
frames	_	_
we	_	_
have	_	_
considered	_	_
as	_	_
history	_	_
,	_	_
as	_	_
in	_	_
LSTM	_	_
frameworks	_	_
all	_	_
of	_	_
the	_	_
history	_	_
gets	_	_
summarized	_	_
and	_	_
in	_	_
unrolling	_	_
step	_	_
the	_	_
time	_	_
steps	_	_
are	_	_
not	_	_
captured	_	_
accurately	_	_
and	_	_
the	_	_
generation	_	_
is	_	_
more	_	_
based	_	_
on	_	_
an	_	_
average	_	_
of	_	_
all	_	_
frames	_	_
.	_	_

#194
We	_	_
compare	_	_
the	_	_
two	_	_
proposed	_	_
strategies	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#195
The	_	_
advantage	_	_
of	_	_
the	_	_
dictionary	_	_
based	_	_
approach	_	_
is	_	_
that	_	_
it	_	_
does	_	_
not	_	_
require	_	_
previous	_	_
history	_	_
of	_	_
the	_	_
face	_	_
and	_	_
it	_	_
can	_	_
run	_	_
in	_	_
real	_	_
time	_	_
.	_	_

#196
However	_	_
,	_	_
since	_	_
z	_	_
is	_	_
randomly	_	_
sampled	_	_
from	_	_
the	_	_
possible	_	_
distribution	_	_
space	_	_
,	_	_
it	_	_
generates	_	_
a	_	_
different	_	_
sequence	_	_
each	_	_
time	_	_
and	_	_
it	_	_
is	_	_
hard	_	_
to	_	_
evaluate	_	_
this	_	_
approach	_	_
.	_	_

#197
In	_	_
table	_	_
2	_	_
we	_	_
show	_	_
an	_	_
evaluation	_	_
of	_	_
conditional	_	_
LSTM	_	_
approach	_	_
for	_	_
generating	_	_
the	_	_
facial	_	_
expressions	_	_
.	_	_

#198
We	_	_
have	_	_
generated	_	_
the	_	_
facial	_	_
expressions	_	_
using	_	_
C-LSTM	_	_
by	_	_
considering	_	_
100	_	_
frames	_	_
as	_	_
history	_	_
and	_	_
1	_	_
)	_	_
generating	_	_
the	_	_
101	_	_
st	_	_
frame	_	_
(	_	_
complete	_	_
overlap	_	_
)	_	_
.	_	_

#199
2	_	_
)	_	_
generating	_	_
the	_	_
next	_	_
100	_	_
frames	_	_
(	_	_
No	_	_
overlap	_	_
)	_	_
.	_	_

#200
As	_	_
this	_	_
method	_	_
,	_	_
tries	_	_
to	_	_
generate	_	_
the	_	_
closest	_	_
sequence	_	_
to	_	_
ground	_	_
truth	_	_
,	_	_
we	_	_
have	_	_
calculated	_	_
mean	_	_
squared	_	_
error	_	_
as	_	_
an	_	_
evaluation	_	_
metric	_	_
.	_	_

#201
As	_	_
you	_	_
can	_	_
see	_	_
,	_	_
C-LSTM	_	_
with	_	_
100	_	_
frame	_	_
overlap	_	_
has	_	_
a	_	_
smaller	_	_
error	_	_
value	_	_
compared	_	_
to	_	_
the	_	_
C-LSTM	_	_
with	_	_
no	_	_
overlap	_	_
.	_	_

#202
Intuitively	_	_
,	_	_
it	_	_
is	_	_
harder	_	_
to	_	_
predict	_	_
future	_	_
with	_	_
no	_	_
overlap	_	_
than	_	_
having	_	_
overlap	_	_
.	_	_

#203
Properties	_	_
Affect-Shape	_	_
Dictionary	_	_
Conditional	_	_
LSTM	_	_
History	_	_
No	_	_
need	_	_
for	_	_
history	_	_
.	_	_

#204
Needs	_	_
history	_	_
.	_	_

#205
Diversity	_	_
Generates	_	_
diverse	_	_
sequence	_	_
.	_	_

#206
Generates	_	_
one	_	_
particular	_	_
sequence	_	_
.	_	_

#207
Evaluation	_	_
Hard	_	_
to	_	_
evaluate	_	_
.	_	_

#208
Easier	_	_
to	_	_
evaluate	_	_
.	_	_

#209
Speed	_	_
Real	_	_
time	_	_
Not	_	_
real	_	_
time	_	_
Table	_	_
1	_	_
.	_	_

#210
Comparison	_	_
between	_	_
Affect-shape	_	_
dictionary	_	_
and	_	_
conditional	_	_
LSTM	_	_
approaches	_	_
in	_	_
generating	_	_
face	_	_
shape	_	_
parameters	_	_
.	_	_

#211
Method	_	_
C-LSTM	_	_
w	_	_
overlap	_	_
C-	_	_
LSTM	_	_
w/o	_	_
overlap	_	_
MSE	_	_
0.101	_	_
0.183	_	_
Table	_	_
2	_	_
.	_	_

#212
Comparison	_	_
between	_	_
performance	_	_
of	_	_
conditional	_	_
LSTMs	_	_
with	_	_
and	_	_
without	_	_
overlap	_	_
in	_	_
history	_	_
.	_	_

#213
We	_	_
have	_	_
also	_	_
generated	_	_
the	_	_
final	_	_
face	_	_
images	_	_
using	_	_
our	_	_
CGAN	_	_
network	_	_
.	_	_

#214
Figure	_	_
8	_	_
shows	_	_
the	_	_
generated	_	_
images	_	_
of	_	_
the	_	_
interviewer	_	_
which	_	_
is	_	_
the	_	_
final	_	_
result	_	_
of	_	_
the	_	_
our	_	_
second	_	_
network	_	_
,	_	_
sketch-image	_	_
GAN	_	_
.	_	_

#215
The	_	_
top	_	_
row	_	_
shows	_	_
the	_	_
results	_	_
of	_	_
such	_	_
images	_	_
if	_	_
we	_	_
only	_	_
had	_	_
one	_	_
network	_	_
and	_	_
directly	_	_
went	_	_
from	_	_
affect	_	_
vector	_	_
and	_	_
z	_	_
into	_	_
face	_	_
images	_	_
.	_	_

#216
The	_	_
second	_	_
row	_	_
shows	_	_
the	_	_
results	_	_
of	_	_
a	_	_
two	_	_
step	_	_
network	_	_
where	_	_
we	_	_
first	_	_
generate	_	_
face	_	_
sketches	_	_
and	_	_
then	_	_
face	_	_
images	_	_
.	_	_

#217
Note	_	_
that	_	_
quality	_	_
of	_	_
the	_	_
images	_	_
in	_	_
second	_	_
row	_	_
is	_	_
significantly	_	_
better	_	_
than	_	_
the	_	_
first	_	_
row	_	_
.	_	_

#218
Also	_	_
,	_	_
as	_	_
our	_	_
final	_	_
goal	_	_
is	_	_
to	_	_
transfer	_	_
these	_	_
expressions	_	_
to	_	_
an	_	_
avatar’s	_	_
face	_	_
,	_	_
our	_	_
mid-level	_	_
results	_	_
can	_	_
be	_	_
used	_	_
to	_	_
transfer	_	_
the	_	_
expressions	_	_
to	_	_
various	_	_
virtual	_	_
characters	_	_
.	_	_

#219
5	_	_
.	_	_

#220
Conclusions	_	_
and	_	_
Future	_	_
Work	_	_
Human	_	_
beings	_	_
react	_	_
to	_	_
each	_	_
other’s	_	_
affective	_	_
state	_	_
and	_	_
adjust	_	_
their	_	_
behaviors	_	_
accordingly	_	_
.	_	_

#221
In	_	_
this	_	_
paper	_	_
we	_	_
have	_	_
proposed	_	_
a	_	_
method	_	_
for	_	_
generating	_	_
facial	_	_
behaviors	_	_
in	_	_
a	_	_
dyadic	_	_
interaction	_	_
.	_	_

#222
Our	_	_
model	_	_
learns	_	_
semantically	_	_
meaningful	_	_
facial	_	_
behaviors	_	_
such	_	_
as	_	_
head	_	_
pose	_	_
and	_	_
movements	_	_
of	_	_
landmarks	_	_
and	_	_
generates	_	_
appropriate	_	_
and	_	_
temporally	_	_
smooth	_	_
sequences	_	_
of	_	_
facial	_	_
expressions	_	_
.	_	_

#223
In	_	_
our	_	_
future	_	_
work	_	_
,	_	_
we	_	_
are	_	_
interested	_	_
in	_	_
transferring	_	_
the	_	_
generated	_	_
facial	_	_
expressions	_	_
to	_	_
an	_	_
avatar’s	_	_
face	_	_
and	_	_
generating	_	_
sequences	_	_
of	_	_
expressive	_	_
avatars	_	_
reacting	_	_
to	_	_
the	_	_
interviewee’s	_	_
affective	_	_
state	_	_
using	_	_
the	_	_
proposed	_	_
approaches	_	_
in	_	_
this	_	_
work	_	_
.	_	_

#224
We	_	_
then	_	_
will	_	_
run	_	_
a	_	_
user	_	_
study	_	_
using	_	_
pairs	_	_
of	_	_
interviewee	_	_
and	_	_
generated	_	_
avatar’s	_	_
videos	_	_
to	_	_
study	_	_
the	_	_
most	_	_
appropriate	_	_
responses	_	_
for	_	_
each	_	_
video	_	_
and	_	_
will	_	_
use	_	_
the	_	_
result	_	_
in	_	_
building	_	_
our	_	_
final	_	_
models	_	_
of	_	_
emotionally	_	_
intelligent	_	_
agents	_	_
.	_	_