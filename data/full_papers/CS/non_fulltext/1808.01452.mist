#0
Traits	_	_
&	_	_
Transferability	_	_
of	_	_
Adversarial	_	_
Examples	_	_
against	_	_
Instance	_	_
Segmentation	_	_
&	_	_
Object	_	_
Detection	_	_
Raghav	_	_
Gurbaxani	_	_
Shivank	_	_
Mishra	_	_
University	_	_
of	_	_
Illinois	_	_
at	_	_
Urbana	_	_
Champaign	_	_
{	_	_
raghavg3	_	_
,	_	_
smishr25	_	_
}	_	_
@	_	_
illinois.edu	_	_

#1
Abstract	_	_

#2
Despite	_	_
the	_	_
recent	_	_
advancements	_	_
in	_	_
deploying	_	_
neural	_	_
networks	_	_
for	_	_
image	_	_
classification	_	_
,	_	_
it	_	_
has	_	_
been	_	_
found	_	_
that	_	_
adversarial	_	_
examples	_	_
are	_	_
able	_	_
to	_	_
fool	_	_
these	_	_
models	_	_
leading	_	_
them	_	_
to	_	_
misclassify	_	_
the	_	_
images	_	_
.	_	_

#3
Since	_	_
these	_	_
models	_	_
are	_	_
now	_	_
being	_	_
widely	_	_
deployed	_	_
,	_	_
we	_	_
provide	_	_
an	_	_
insight	_	_
on	_	_
the	_	_
threat	_	_
of	_	_
these	_	_
adversarial	_	_
examples	_	_
by	_	_
evaluating	_	_
their	_	_
characteristics	_	_
and	_	_
transferability	_	_
to	_	_
more	_	_
complex	_	_
models	_	_
that	_	_
utilize	_	_
Image	_	_
Classification	_	_
as	_	_
a	_	_
subtask	_	_
.	_	_

#4
We	_	_
demonstrate	_	_
the	_	_
ineffectiveness	_	_
of	_	_
adversarial	_	_
examples	_	_
when	_	_
applied	_	_
to	_	_
Instance	_	_
Segmentation	_	_
&	_	_
Object	_	_
Detection	_	_
models	_	_
.	_	_

#5
We	_	_
show	_	_
that	_	_
this	_	_
ineffectiveness	_	_
arises	_	_
from	_	_
the	_	_
inability	_	_
of	_	_
adversarial	_	_
examples	_	_
to	_	_
withstand	_	_
transformations	_	_
such	_	_
as	_	_
scaling	_	_
or	_	_
a	_	_
change	_	_
in	_	_
lighting	_	_
conditions	_	_
.	_	_

#6
Moreover	_	_
,	_	_
we	_	_
show	_	_
that	_	_
there	_	_
exists	_	_
a	_	_
small	_	_
threshold	_	_
below	_	_
which	_	_
the	_	_
adversarial	_	_
property	_	_
is	_	_
retained	_	_
while	_	_
applying	_	_
these	_	_
input	_	_
transformations	_	_
.	_	_

#7
Additionally	_	_
,	_	_
these	_	_
attacks	_	_
demonstrate	_	_
weak	_	_
crossnetwork	_	_
transferability	_	_
across	_	_
neural	_	_
network	_	_
architectures	_	_
,	_	_
e.g.	_	_
VGG16	_	_
and	_	_
ResNet50	_	_
,	_	_
however	_	_
,	_	_
the	_	_
attack	_	_
may	_	_
fool	_	_
both	_	_
the	_	_
networks	_	_
if	_	_
passed	_	_
sequentially	_	_
through	_	_
networks	_	_
during	_	_
its	_	_
formation	_	_
.	_	_

#8
The	_	_
lack	_	_
of	_	_
scalability	_	_
and	_	_
transferability	_	_
challenges	_	_
the	_	_
question	_	_
of	_	_
how	_	_
adversarial	_	_
images	_	_
would	_	_
be	_	_
effective	_	_
in	_	_
the	_	_
real	_	_
world	_	_
.	_	_

#9
1	_	_
.	_	_

#10
Introduction	_	_
With	_	_
recent	_	_
advancements	_	_
in	_	_
Computer	_	_
Vision	_	_
,	_	_
Neural	_	_
Networks	_	_
have	_	_
been	_	_
able	_	_
to	_	_
achieve	_	_
state-of-the-art	_	_
results	_	_
in	_	_
Image	_	_
Recognition	_	_
and	_	_
have	_	_
been	_	_
able	_	_
to	_	_
outperform	_	_
human-level	_	_
performance	_	_
.	_	_

#11
The	_	_
success	_	_
of	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
[	_	_
7	_	_
]	_	_
,	_	_
[	_	_
18	_	_
]	_	_
,	_	_
[	_	_
6	_	_
]	_	_
,	_	_
[	_	_
19	_	_
]	_	_
on	_	_
the	_	_
Imagnet	_	_
[	_	_
16	_	_
]	_	_
dataset	_	_
(	_	_
for	_	_
the	_	_
Large-Scale	_	_
Image	_	_
Recognition	_	_
Challenge	_	_
)	_	_
has	_	_
propelled	_	_
neural	_	_
networks	_	_
to	_	_
achieve	_	_
significant	_	_
results	_	_
in	_	_
various	_	_
visual	_	_
recognition	_	_
tasks	_	_
.	_	_

#12
Consequently	_	_
,	_	_
they	_	_
are	_	_
now	_	_
widely	_	_
deployed	_	_
at	_	_
an	_	_
unprecedented	_	_
scale	_	_
across	_	_
a	_	_
variety	_	_
of	_	_
applications	_	_
such	_	_
as	_	_
Robotics	_	_
,	_	_
Drones	_	_
,	_	_
Self-Driving	_	_
Cars	_	_
,	_	_
and	_	_
Surveillance	_	_
.	_	_

#13
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
the	_	_
discovery	_	_
of	_	_
adversarial	_	_
examples	_	_
[	_	_
20	_	_
]	_	_
,	_	_
[	_	_
13	_	_
]	_	_
,	_	_
[	_	_
3	_	_
]	_	_
has	_	_
threatened	_	_
these	_	_
achievements	_	_
by	_	_
fooling	_	_
neural	_	_
networks	_	_
to	_	_
misclassify	_	_
the	_	_
image	_	_
by	_	_
introducing	_	_
almost	_	_
visually	_	_
imperceptible	_	_
perturbations	_	_
.	_	_

#14
Adversarial	_	_
examples	_	_
have	_	_
been	_	_
able	_	_
to	_	_
fool	_	_
the	_	_
state	_	_
of	_	_
the	_	_
Image	_	_
Classification	_	_
models	_	_
with	_	_
a	_	_
high	_	_
confidence	_	_
score	_	_
.	_	_

#15
as	_	_
shown	_	_
in	_	_
Figure1	_	_
Moreover	_	_
,	_	_
[	_	_
8	_	_
]	_	_
showed	_	_
that	_	_
adversarial	_	_
can	_	_
exist	_	_
in	_	_
the	_	_
physical	_	_
world	_	_
.	_	_

#16
To	_	_
put	_	_
this	_	_
in	_	_
perspective	_	_
,	_	_
an	_	_
adversarial	_	_
image	_	_
of	_	_
a	_	_
stop	_	_
sign	_	_
on	_	_
road	_	_
could	_	_
be	_	_
misclassified	_	_
as	_	_
a	_	_
green	_	_
light	_	_
-	_	_
thus	_	_
raising	_	_
a	_	_
huge	_	_
security	_	_
threat	_	_
.	_	_

#17
As	_	_
Image	_	_
classification	_	_
based	_	_
models	_	_
are	_	_
now	_	_
being	_	_
widely	_	_
deployed	_	_
across	_	_
the	_	_
industry	_	_
,	_	_
it	_	_
is	_	_
consequential	_	_
to	_	_
study	_	_
of	_	_
the	_	_
effects	_	_
of	_	_
these	_	_
adversarial	_	_
perturbations	_	_
.	_	_

#18
Keeping	_	_
in	_	_
mind	_	_
the	_	_
harmful	_	_
ramifications	_	_
,	_	_
in	_	_
this	_	_
paper	_	_
we	_	_
study	_	_
the	_	_
properties	_	_
of	_	_
adversarial	_	_
examples	_	_
in	_	_
real	_	_
world	_	_
conditions	_	_
and	_	_
evaluate	_	_
their	_	_
effectiveness	_	_
across	_	_
different	_	_
neural	_	_
networks	_	_
architectures	_	_
.	_	_

#19
The	_	_
advancements	_	_
of	_	_
neural	_	_
networks	_	_
Classification	_	_
tasks	_	_
has	_	_
propelled	_	_
research	_	_
in	_	_
developing	_	_
more	_	_
complex	_	_
algorithms	_	_
such	_	_
as	_	_
Object	_	_
Detection	_	_
[	_	_
15	_	_
]	_	_
,	_	_
[	_	_
9	_	_
]	_	_
which	_	_
use	_	_
Image	_	_
Classifiers	_	_
as	_	_
a	_	_
subtask	_	_
and	_	_
to	_	_
identify	_	_
all	_	_
the	_	_
objects	_	_
in	_	_
an	_	_
image	_	_
,	_	_
as	_	_
well	_	_
as	_	_
localize	_	_
their	_	_
position	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#20
Moreover	_	_
,	_	_
Instance	_	_
Segmentation	_	_
[	_	_
5	_	_
]	_	_
further	_	_
extends	_	_
Object	_	_
Detection	_	_
by	_	_
adding	_	_
a	_	_
segmentation	_	_
pipeline	_	_
to	_	_
it	_	_
,	_	_
thus	_	_
allowing	_	_
it	_	_
to	_	_
not	_	_
only	_	_
detect	_	_
objects	_	_
but	_	_
also	_	_
segment	_	_
each	_	_
pixel	_	_
on	_	_
those	_	_
objects	_	_
leading	_	_
to	_	_
a	_	_
more	_	_
fine-grained	_	_
classification	_	_
.	_	_

#21
With	_	_
the	_	_
advent	_	_
of	_	_
these	_	_
complex	_	_
models	_	_
,	_	_
which	_	_
use	_	_
image	_	_
classification	_	_
as	_	_
a	_	_
sub-task	_	_
and	_	_
taking	_	_
into	_	_
account	_	_
the	_	_
susceptibility	_	_
of	_	_
Image	_	_
classifiers	_	_
against	_	_
adversarial	_	_
images	_	_
,	_	_
it	_	_
becomes	_	_
important	_	_
to	_	_
study	_	_
whether	_	_
the	_	_
adversarial	_	_
image	_	_
is	_	_
able	_	_
to	_	_
fool	_	_
these	_	_
advanced	_	_
pipelines	_	_
as	_	_
well	_	_
.	_	_

#22
With	_	_
the	_	_
advent	_	_
of	_	_
these	_	_
complex	_	_
models	_	_
and	_	_
taking	_	_
into	_	_
account	_	_
the	_	_
threat	_	_
of	_	_
adversarial	_	_
attacks	_	_
,	_	_
it	_	_
becomes	_	_
necessary	_	_
to	_	_
examine	_	_
whether	_	_
these	_	_
attacks	_	_
are	_	_
able	_	_
to	_	_
mislead	_	_
these	_	_
complex	_	_
advanced	_	_
model	_	_
pipelines	_	_
as	_	_
well	_	_
.	_	_

#23
To	_	_
study	_	_
the	_	_
various	_	_
properties	_	_
of	_	_
adversarial	_	_
examples	_	_
we	_	_
use	_	_
the	_	_
FGSM	_	_
[	_	_
3	_	_
]	_	_
,	_	_
DeepFool	_	_
[	_	_
12	_	_
]	_	_
,	_	_
and	_	_
Carlini	_	_
&	_	_
Wagner	_	_
L2	_	_
[	_	_
1	_	_
]	_	_
attacks	_	_
in	_	_
a	_	_
targeted	_	_
and	_	_
untargeted	_	_
setting	_	_
.	_	_

#24
To	_	_
simulate	_	_
the	_	_
real	_	_
world	_	_
we	_	_
a	_	_
variety	_	_
of	_	_
scaling	_	_
,	_	_
rotation	_	_
and	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
8	_	_
.	_	_

#25
2v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
4	_	_
A	_	_
ug	_	_
2	_	_
lighting	_	_
conditions	_	_
and	_	_
demonstrate	_	_
that	_	_
these	_	_
attacks	_	_
do	_	_
not	_	_
retain	_	_
their	_	_
adversarial	_	_
property	_	_
under	_	_
these	_	_
situations	_	_
.	_	_

#26
Interestingly	_	_
,	_	_
we	_	_
discovered	_	_
that	_	_
in	_	_
case	_	_
of	_	_
these	_	_
transformations	_	_
,	_	_
there	_	_
exists	_	_
a	_	_
minimum	_	_
threshold	_	_
below	_	_
which	_	_
the	_	_
adversarial	_	_
property	_	_
is	_	_
retained	_	_
and	_	_
is	_	_
lost	_	_
in	_	_
the	_	_
latter	_	_
case	_	_
.	_	_

#27
We	_	_
then	_	_
study	_	_
the	_	_
transferability	_	_
of	_	_
these	_	_
attacks	_	_
to	_	_
various	_	_
network	_	_
architectures	_	_
on	_	_
a	_	_
Cross	_	_
Network	_	_
(	_	_
same	_	_
dataset	_	_
,	_	_
different	_	_
architectures	_	_
)	_	_
and	_	_
a	_	_
Cross	_	_
Task	_	_
Transfer	_	_
(	_	_
different	_	_
dataset	_	_
,	_	_
similar	_	_
architecture	_	_
)	_	_
.	_	_

#28
We	_	_
notice	_	_
that	_	_
adversarial	_	_
attack	_	_
demonstrate	_	_
weak	_	_
transferability	_	_
across	_	_
different	_	_
networks	_	_
(	_	_
VGG	_	_
,	_	_
ResNet	_	_
,	_	_
etc	_	_
.	_	_
)	_	_

#29
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
.	_	_

#30
Moreover	_	_
,	_	_
these	_	_
transformations	_	_
deem	_	_
these	_	_
attacks	_	_
ineffective	_	_
against	_	_
Cross	_	_
Task	_	_
models	_	_
such	_	_
as	_	_
Object	_	_
Detection	_	_
&	_	_
Instance	_	_
Segmentation	_	_
models	_	_
which	_	_
involve	_	_
cropping	_	_
objects	_	_
in	_	_
the	_	_
image	_	_
and	_	_
classifying	_	_
them	_	_
individually	_	_
to	_	_
their	_	_
respective	_	_
classes	_	_
.	_	_

#31
We	_	_
further	_	_
chose	_	_
a	_	_
third	_	_
proxy	_	_
task	_	_
of	_	_
Image	_	_
Captioning	_	_
and	_	_
show	_	_
that	_	_
adversarial	_	_
examples	_	_
are	_	_
not	_	_
transferable	_	_
unless	_	_
the	_	_
setup	_	_
is	_	_
as	_	_
maintained	_	_
by	_	_
[	_	_
2	_	_
]	_	_
.	_	_

#32
We	_	_
further	_	_
extend	_	_
the	_	_
study	_	_
conducted	_	_
by	_	_
[	_	_
11	_	_
]	_	_
to	_	_
Instance	_	_
Segmentation	_	_
and	_	_
Captioning	_	_
models	_	_
and	_	_
study	_	_
the	_	_
scalability	_	_
of	_	_
attacks	_	_
in	_	_
the	_	_
real	_	_
world	_	_
.	_	_

#33
(	_	_
a	_	_
)	_	_
Box	_	_
Turtle	_	_
(	_	_
b	_	_
)	_	_
African	_	_
Chameleon	_	_
Figure	_	_
1	_	_
:	_	_
Example	_	_
of	_	_
adversarial	_	_
misclassification	_	_
2	_	_
.	_	_

#34
Adversarial	_	_
Attacks	_	_
2.1	_	_
.	_	_

#35
Fast	_	_
Gradient	_	_
Sign	_	_
Method	_	_
The	_	_
Fast	_	_
Gradient	_	_
Sign	_	_
Method	_	_
,	_	_
as	_	_
introduced	_	_
by	_	_
[	_	_
3	_	_
]	_	_
exploits	_	_
the	_	_
linearity	_	_
of	_	_
the	_	_
neural	_	_
network	_	_
models	_	_
in	_	_
a	_	_
high	_	_
dimensional	_	_
space	_	_
.	_	_

#36
We	_	_
used	_	_
the	_	_
targeted	_	_
FGSM	_	_
attack	_	_
where	_	_
the	_	_
attacker	_	_
specifies	_	_
the	_	_
adversarial	_	_
image	_	_
class	_	_
,	_	_
to	_	_
which	_	_
is	_	_
should	_	_
be	_	_
misclassified	_	_
.	_	_

#37
Consider	_	_
x	_	_
as	_	_
the	_	_
original	_	_
image	_	_
,	_	_
y	_	_
the	_	_
corresponding	_	_
label	_	_
of	_	_
x	_	_
,	_	_
θ	_	_
the	_	_
parameters	_	_
of	_	_
the	_	_
network	_	_
and	_	_
L	_	_
(	_	_
θ	_	_
,	_	_
x	_	_
,	_	_
y	_	_
)	_	_
the	_	_
loss	_	_
function	_	_
used	_	_
to	_	_
train	_	_
the	_	_
network	_	_
.	_	_

#38
η	_	_
=	_	_
εsign	_	_
(	_	_
∇xL	_	_
(	_	_
θ	_	_
,	_	_
x	_	_
,	_	_
y	_	_
)	_	_
)	_	_
The	_	_
attack	_	_
,	_	_
essentially	_	_
an	_	_
optimization	_	_
problem	_	_
,	_	_
tries	_	_
to	_	_
increase	_	_
the	_	_
loss	_	_
of	_	_
the	_	_
classifier	_	_
.	_	_

#39
It	_	_
takes	_	_
the	_	_
derivative	_	_
of	_	_
the	_	_
loss	_	_
function	_	_
in	_	_
the	_	_
x-direction	_	_
-	_	_
∇xL	_	_
(	_	_
θ	_	_
,	_	_
x	_	_
,	_	_
y	_	_
)	_	_
and	_	_
the	_	_
sign	_	_
of	_	_
the	_	_
derivative	_	_
to	_	_
determine	_	_
the	_	_
direction	_	_
of	_	_
the	_	_
pixel	_	_
change	_	_
.	_	_

#40
The	_	_
sign	_	_
term	_	_
ensures	_	_
that	_	_
the	_	_
loss	_	_
is	_	_
maximized	_	_
.	_	_

#41
Moreover	_	_
,	_	_
this	_	_
is	_	_
multiplied	_	_
by	_	_
a	_	_
small	_	_
constant	_	_
ε	_	_
,	_	_
which	_	_
ensures	_	_
that	_	_
the	_	_
perturbation	_	_
doesnt	_	_
go	_	_
too	_	_
far	_	_
from	_	_
the	_	_
original	_	_
image	_	_
and	_	_
restricts	_	_
its	_	_
norm	_	_
that	_	_
gives	_	_
us	_	_
the	_	_
perturbation	_	_
η	_	_
.	_	_

#42
This	_	_
perturbation	_	_
can	_	_
now	_	_
be	_	_
added	_	_
to	_	_
the	_	_
original	_	_
image	_	_
x	_	_
to	_	_
result	_	_
in	_	_
the	_	_
adversarial	_	_
image	_	_
,	_	_
capable	_	_
of	_	_
fooling	_	_
the	_	_
classifier	_	_
.	_	_

#43
xadv	_	_
=	_	_
x	_	_
+	_	_
η	_	_
For	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
used	_	_
the	_	_
Cleverhans	_	_
[	_	_
4	_	_
]	_	_
library	_	_
to	_	_
generate	_	_
the	_	_
adversarial	_	_
images	_	_
on	_	_
pre-trained	_	_
Imagenet	_	_
models	_	_
with	_	_
y	_	_
target	_	_
set	_	_
to	_	_
class	_	_
’tennis	_	_
ball’	_	_
and	_	_
ε	_	_
set	_	_
to	_	_
0.3	_	_
.	_	_

#44
2.2	_	_
.	_	_

#45
DeepFool	_	_
The	_	_
perturbations	_	_
generated	_	_
by	_	_
Deepfool	_	_
are	_	_
smaller	_	_
than	_	_
the	_	_
perturbations	_	_
compared	_	_
to	_	_
FGSM	_	_
in	_	_
terms	_	_
of	_	_
the	_	_
norm	_	_
but	_	_
have	_	_
similar	_	_
effectiveness	_	_
in	_	_
terms	_	_
of	_	_
fooling	_	_
ratios	_	_
as	_	_
shown	_	_
in	_	_
Figure2	_	_
.	_	_

#46
It	_	_
also	_	_
reduces	_	_
the	_	_
intensity	_	_
of	_	_
the	_	_
perturbations	_	_
and	_	_
was	_	_
used	_	_
in	_	_
the	_	_
untargeted	_	_
attacking	_	_
case	_	_
where	_	_
the	_	_
attacker	_	_
can	_	_
only	_	_
specify	_	_
that	_	_
the	_	_
adversarial	_	_
image	_	_
is	_	_
classified	_	_
differently	_	_
than	_	_
the	_	_
original	_	_
image	_	_
.	_	_

#47
This	_	_
method	_	_
[	_	_
12	_	_
]	_	_
was	_	_
used	_	_
to	_	_
iteratively	_	_
minimize	_	_
the	_	_
distortion	_	_
which	_	_
leads	_	_
the	_	_
image	_	_
to	_	_
switch	_	_
classes	_	_
by	_	_
projecting	_	_
the	_	_
input	_	_
image	_	_
to	_	_
the	_	_
closest	_	_
separating	_	_
hyperplane	_	_
(	_	_
considering	_	_
each	_	_
class	_	_
of	_	_
classifier	_	_
is	_	_
separated	_	_
by	_	_
a	_	_
hyperplane	_	_
)	_	_
and	_	_
by	_	_
linearizing	_	_
the	_	_
decision	_	_
boundaries	_	_
in	_	_
which	_	_
the	_	_
image	_	_
confines	_	_
.	_	_

#48
For	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
used	_	_
the	_	_
Foolbox	_	_
[	_	_
14	_	_
]	_	_
library	_	_
to	_	_
generate	_	_
the	_	_
adversarial	_	_
images	_	_
on	_	_
pre-trained	_	_
Imagenet	_	_
models	_	_
with	_	_
50	_	_
iterations	_	_
and	_	_
ε	_	_
set	_	_
to	_	_
0.3	_	_
.	_	_

#49
(	_	_
a	_	_
)	_	_
Ballplayer	_	_
(	_	_
b	_	_
)	_	_
Chain	_	_
Mail	_	_
Figure	_	_
2	_	_
:	_	_
Example	_	_
of	_	_
Original	_	_
&	_	_
Adversarial	_	_
Image	_	_
Generated	_	_
from	_	_
DeepFool	_	_
Attack	_	_
2.3	_	_
.	_	_

#50
Carlini	_	_
&	_	_
Wagner	_	_
L2	_	_
The	_	_
Carlini	_	_
&	_	_
Wagner	_	_
L2	_	_
(	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
)	_	_
attack	_	_
[	_	_
1	_	_
]	_	_
was	_	_
introduced	_	_
to	_	_
deal	_	_
with	_	_
the	_	_
defensive	_	_
distillation	_	_
strategy	_	_
,	_	_
which	_	_
made	_	_
the	_	_
perturbations	_	_
quasi-imperceptible	_	_
.	_	_

#51
Although	_	_
the	_	_
distortions	_	_
produced	_	_
by	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
are	_	_
smaller	_	_
than	_	_
the	_	_
previously	_	_
mentioned	_	_
attacks	_	_
,	_	_
it	_	_
is	_	_
also	_	_
a	_	_
slower	_	_
attack	_	_
than	_	_
FGSM	_	_
.	_	_

#52
These	_	_
attacks	_	_
limit	_	_
the	_	_
perturbation	_	_
by	_	_
restricting	_	_
their	_	_
L2	_	_
norm	_	_
.	_	_

#53
3	_	_
.	_	_

#54
Experimental	_	_
Setup	_	_
In	_	_
this	_	_
section	_	_
we	_	_
describe	_	_
the	_	_
datasets	_	_
,	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
models	_	_
,	_	_
adversarial	_	_
attacks	_	_
methods	_	_
,	_	_
the	_	_
evaluation	_	_
metrics	_	_
and	_	_
the	_	_
deep	_	_
learning	_	_
frameworks	_	_
that	_	_
we	_	_
used	_	_
to	_	_
carry	_	_
out	_	_
our	_	_
experiments	_	_
.	_	_

#55
3.1	_	_
.	_	_

#56
Datasets	_	_
In	_	_
order	_	_
to	_	_
generate	_	_
the	_	_
adversarial	_	_
examples	_	_
,	_	_
we	_	_
used	_	_
the	_	_
pre-trained	_	_
models	_	_
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
.	_	_

#57
Then	_	_
,	_	_
to	_	_
evaluate	_	_
the	_	_
generated	_	_
adversarial	_	_
examples	_	_
on	_	_
the	_	_
Object	_	_
detectors	_	_
and	_	_
Instance	_	_
Segmentation	_	_
models	_	_
,	_	_
we	_	_
used	_	_
models	_	_
pre-trained	_	_
on	_	_
the	_	_
MSCOCO	_	_
[	_	_
10	_	_
]	_	_
dataset	_	_
.	_	_

#58
For	_	_
Image	_	_
Captioning	_	_
,	_	_
we	_	_
used	_	_
models	_	_
pre-trained	_	_
on	_	_
the	_	_
MSCOCO	_	_
dataset	_	_
which	_	_
comprises	_	_
of	_	_
82783	_	_
training	_	_
images	_	_
,	_	_
and	_	_
40504	_	_
and	_	_
40775	_	_
validation	_	_
and	_	_
test	_	_
images	_	_
respectively	_	_
with	_	_
each	_	_
image	_	_
corresponding	_	_
to	_	_
5	_	_
captions	_	_
each	_	_
.	_	_

#59
3.2	_	_
.	_	_

#60
Models	_	_
For	_	_
object	_	_
detection	_	_
we	_	_
used	_	_
the	_	_
Faster	_	_
RCNN	_	_
model	_	_
and	_	_
the	_	_
RetinaNet	_	_
models	_	_
,	_	_
each	_	_
using	_	_
the	_	_
ResNet	_	_
50	_	_
pipeline	_	_
.	_	_

#61
We	_	_
also	_	_
used	_	_
the	_	_
Mask	_	_
RCNN	_	_
Instance	_	_
Segmentation	_	_
module	_	_
,	_	_
which	_	_
utilizes	_	_
a	_	_
Feature	_	_
Pyramid	_	_
Network	_	_
and	_	_
a	_	_
ResNet	_	_
101	_	_
CNN	_	_
backbone	_	_
.	_	_

#62
Lastly	_	_
,	_	_
We	_	_
used	_	_
the	_	_
Image	_	_
Captioning	_	_
pipeline	_	_
from	_	_
Show	_	_
and	_	_
Tell	_	_
based	_	_
on	_	_
a	_	_
VGG	_	_
16	_	_
CNN	_	_
.	_	_

#63
The	_	_
adversarial	_	_
examples	_	_
based	_	_
on	_	_
the	_	_
three	_	_
attacks	_	_
were	_	_
generated	_	_
using	_	_
VGG16	_	_
,	_	_
ResNet	_	_
50	_	_
and	_	_
Resnet	_	_
101	_	_
models	_	_
pre-trained	_	_
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
.	_	_

#64
3.3	_	_
.	_	_

#65
Attacks	_	_
We	_	_
employed	_	_
the	_	_
FGSM	_	_
,	_	_
DeepFool	_	_
and	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
using	_	_
pre-trained	_	_
models	_	_
on	_	_
ImageNet	_	_
.	_	_

#66
We	_	_
have	_	_
mentioned	_	_
the	_	_
hyperparameters	_	_
that	_	_
we	_	_
used	_	_
for	_	_
the	_	_
respective	_	_
attacks	_	_
in	_	_
Section	_	_
2	_	_
.	_	_

#67
We	_	_
used	_	_
the	_	_
FGSM	_	_
and	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
attacks	_	_
in	_	_
a	_	_
targeted	_	_
setting	_	_
whereas	_	_
the	_	_
DeepFool	_	_
attack	_	_
in	_	_
an	_	_
untargeted	_	_
setting	_	_
.	_	_

#68
3.4	_	_
.	_	_

#69
Metrics	_	_
We	_	_
used	_	_
Intersection	_	_
over	_	_
Union	_	_
(	_	_
IoU	_	_
)	_	_
as	_	_
the	_	_
primary	_	_
metric	_	_
while	_	_
evaluating	_	_
Instance	_	_
Segmentation	_	_
and	_	_
Object	_	_
Detection	_	_
models	_	_
.	_	_

#70
For	_	_
Image	_	_
Classification	_	_
,	_	_
we	_	_
utilized	_	_
accuracy	_	_
as	_	_
a	_	_
metric	_	_
.	_	_

#71
For	_	_
Image	_	_
Captioning	_	_
,	_	_
we	_	_
utilized	_	_
the	_	_
BLEU	_	_
score	_	_
.	_	_

#72
3.5	_	_
.	_	_

#73
Deep	_	_
Learning	_	_
Frameworks	_	_
We	_	_
used	_	_
the	_	_
pretrained	_	_
models	_	_
on	_	_
ImageNet	_	_
using	_	_
the	_	_
Keras	_	_
Library	_	_
which	_	_
uses	_	_
Tensorflow	_	_
backend	_	_
.	_	_

#74
In	_	_
order	_	_
to	_	_
generate	_	_
the	_	_
FGSM	_	_
and	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
attacks	_	_
,	_	_
we	_	_
used	_	_
the	_	_
novel	_	_
Cleverhans	_	_
library	_	_
released	_	_
by	_	_
Goodfellow	_	_
et	_	_
.	_	_

#75
al	_	_
.	_	_

#76
For	_	_
the	_	_
DeepFool	_	_
attack	_	_
we	_	_
employed	_	_
the	_	_
Foolbox	_	_
library	_	_
.	_	_

#77
4	_	_
.	_	_

#78
Characteristics	_	_
of	_	_
Adversarial	_	_
Attacks	_	_
To	_	_
gain	_	_
complete	_	_
understanding	_	_
of	_	_
Adversarial	_	_
examples	_	_
,	_	_
we	_	_
have	_	_
surveyed	_	_
some	_	_
of	_	_
their	_	_
properties	_	_
in	_	_
this	_	_
section	_	_
by	_	_
subjecting	_	_
them	_	_
to	_	_
several	_	_
scaling	_	_
,	_	_
rotation	_	_
,	_	_
and	_	_
lighting	_	_
conditions	_	_
.	_	_

#79
4.1	_	_
.	_	_

#80
Cropping	_	_
For	_	_
the	_	_
latter	_	_
question	_	_
,	_	_
we	_	_
were	_	_
able	_	_
to	_	_
notice	_	_
that	_	_
when	_	_
the	_	_
image	_	_
was	_	_
cropped	_	_
from	_	_
the	_	_
original	_	_
size	_	_
of	_	_
224	_	_
x	_	_
224	_	_
to	_	_
224	_	_
x	_	_
223	_	_
(	_	_
1	_	_
row	_	_
cropped	_	_
)	_	_
,	_	_
it	_	_
retained	_	_
its	_	_
adversarial	_	_
property	_	_
.	_	_

#81
However	_	_
,	_	_
it	_	_
suffered	_	_
a	_	_
significant	_	_
drop	_	_
in	_	_
adversarial	_	_
confidence	_	_
score	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
3	_	_
.	_	_

#82
On	_	_
further	_	_
cropping	_	_
,	_	_
it	_	_
loses	_	_
its	_	_
attacking	_	_
capability	_	_
and	_	_
the	_	_
classification	_	_
switches	_	_
back	_	_
to	_	_
the	_	_
original	_	_
class	_	_
(	_	_
Labrador	_	_
retriever	_	_
in	_	_
this	_	_
case	_	_
)	_	_
.	_	_

#83
This	_	_
would	_	_
indicate	_	_
towards	_	_
the	_	_
idea	_	_
that	_	_
there	_	_
exists	_	_
a	_	_
global	_	_
pattern	_	_
for	_	_
each	_	_
adversarial	_	_
image	_	_
that	_	_
must	_	_
be	_	_
preserved	_	_
for	_	_
it	_	_
to	_	_
be	_	_
able	_	_
to	_	_
attack	_	_
classifiers	_	_
.	_	_

#84
(	_	_
a	_	_
)	_	_
224	_	_
x	_	_
224	_	_
(	_	_
b	_	_
)	_	_
223	_	_
x	_	_
224	_	_
(	_	_
c	_	_
)	_	_
223	_	_
x	_	_
222	_	_
Predictions	_	_
:	_	_
a	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
75.65	_	_
%	_	_
)	_	_
b	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
50.25	_	_
%	_	_
)	_	_
c	_	_
)	_	_
golden	_	_
retriever	_	_
(	_	_
56.49	_	_
%	_	_
)	_	_
Figure	_	_
3	_	_
:	_	_
Demonstrates	_	_
the	_	_
a	_	_
)	_	_
Adversarial	_	_
Image	_	_
b	_	_
)	_	_
Adversarial	_	_
Image	_	_
after	_	_
1	_	_
row	_	_
cropped	_	_
c	_	_
)	_	_
Cropped	_	_
further	_	_
cropping	_	_
beyong	_	_
a	_	_
certain	_	_
limit	_	_
causes	_	_
the	_	_
image	_	_
to	_	_
lose	_	_
it’s	_	_
adversarial	_	_
class	_	_
)	_	_
.	_	_

#85
4.2	_	_
.	_	_

#86
Magnification	_	_
Magnification	_	_
can	_	_
be	_	_
considered	_	_
as	_	_
a	_	_
consequence	_	_
of	_	_
cropping	_	_
as	_	_
it	_	_
leads	_	_
to	_	_
enlargement	_	_
of	_	_
the	_	_
cropped	_	_
area	_	_
and	_	_
thus	_	_
also	_	_
leads	_	_
to	_	_
a	_	_
loss	_	_
of	_	_
attacking	_	_
ability	_	_
for	_	_
the	_	_
adversarial	_	_
image	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
4	_	_
.	_	_

#87
4.3	_	_
.	_	_

#88
Rotation	_	_
Similar	_	_
to	_	_
cropping	_	_
and	_	_
magnification	_	_
,	_	_
adversarial	_	_
images	_	_
were	_	_
not	_	_
able	_	_
to	_	_
maintain	_	_
their	_	_
attacking	_	_
property	_	_
on	_	_
being	_	_
rotated	_	_
.	_	_

#89
We	_	_
were	_	_
curious	_	_
to	_	_
determine	_	_
a	_	_
threshold	_	_
for	_	_
the	_	_
susceptibility	_	_
to	_	_
rotation	_	_
at	_	_
which	_	_
the	_	_
image	_	_
would	_	_
lose	_	_
it’s	_	_
attacking	_	_
ability	_	_
.	_	_

#90
However	_	_
,	_	_
we	_	_
found	_	_
that	_	_
the	_	_
images	_	_
lose	_	_
their	_	_
attacking	_	_
capability	_	_
even	_	_
with	_	_
a	_	_
1-degree	_	_
rotation	_	_
(	_	_
in	_	_
both	_	_
clockwise	_	_
and	_	_
anti-clockwise	_	_
directions	_	_
)	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
5	_	_
.	_	_

#91
(	_	_
a	_	_
)	_	_
Adversarial	_	_
(	_	_
b	_	_
)	_	_
Magnified	_	_
Adversarial	_	_
Predictions	_	_
:	_	_
a	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
75.65	_	_
%	_	_
)	_	_
b	_	_
)	_	_
Labrador	_	_
retriever	_	_
(	_	_
52.66	_	_
%	_	_
)	_	_
Figure	_	_
4	_	_
:	_	_
The	_	_
effect	_	_
of	_	_
Magnification	_	_
-	_	_
In	_	_
Fig	_	_
b	_	_
)	_	_
magnification	_	_
magnification	_	_
causes	_	_
adversarial	_	_
image	_	_
to	_	_
lose	_	_
it’s	_	_
attacking	_	_
class	_	_
(	_	_
a	_	_
)	_	_
Adversarial	_	_
Image	_	_
(	_	_
b	_	_
)	_	_
Rotated	_	_
Adversarial	_	_
Image	_	_
Predictions	_	_
:	_	_
a	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
75.65	_	_
%	_	_
)	_	_
b	_	_
)	_	_
Labrador	_	_
retriever	_	_
(	_	_
45.20	_	_
%	_	_
)	_	_
Figure	_	_
5	_	_
:	_	_
The	_	_
effect	_	_
of	_	_
Rotation	_	_
-	_	_
In	_	_
Fig	_	_
b	_	_
)	_	_
rotation	_	_
(	_	_
clockwise	_	_
&	_	_
anti-	_	_
clockwise	_	_
)	_	_
causes	_	_
adversarial	_	_
image	_	_
to	_	_
lose	_	_
it’s	_	_
attacking	_	_
class	_	_
4.4	_	_
.	_	_

#92
Change	_	_
in	_	_
brightness	_	_
In	_	_
order	_	_
to	_	_
consider	_	_
how	_	_
adversarial	_	_
images	_	_
would	_	_
perform	_	_
in	_	_
the	_	_
real	_	_
world	_	_
under	_	_
myriad	_	_
of	_	_
lighting	_	_
conditions	_	_
,	_	_
we	_	_
subjected	_	_
these	_	_
images	_	_
to	_	_
change	_	_
in	_	_
exposure	_	_
.	_	_

#93
Our	_	_
experiments	_	_
show	_	_
that	_	_
adversarial	_	_
examples	_	_
are	_	_
robust	_	_
to	_	_
change	_	_
in	_	_
exposure	_	_
and	_	_
retain	_	_
their	_	_
attacking	_	_
capability	_	_
until	_	_
a	_	_
certain	_	_
threshold	_	_
.	_	_

#94
As	_	_
demonstrated	_	_
in	_	_
Figure	_	_
6	_	_
,	_	_
a	_	_
50	_	_
%	_	_
increase	_	_
in	_	_
exposure	_	_
in	_	_
the	_	_
adversarial	_	_
image	_	_
retains	_	_
the	_	_
adversarial	_	_
class	_	_
,	_	_
but	_	_
a	_	_
100	_	_
%	_	_
increase	_	_
in	_	_
exposure	_	_
causes	_	_
it	_	_
to	_	_
return	_	_
to	_	_
its	_	_
original	_	_
class	_	_
.	_	_

#95
5	_	_
.	_	_

#96
Transferability	_	_
of	_	_
Attacks	_	_
across	_	_
Architectures	_	_
We	_	_
evaluate	_	_
the	_	_
transferability	_	_
of	_	_
of	_	_
adversarial	_	_
images	_	_
across	_	_
1	_	_
.	_	_

#97
Cross-Network	_	_
Transfer	_	_
:	_	_
Using	_	_
different	_	_
architectures	_	_
(	_	_
a	_	_
)	_	_
No	_	_
change	_	_
in	_	_
brightness	_	_
(	_	_
b	_	_
)	_	_
50	_	_
%	_	_
increase	_	_
(	_	_
c	_	_
)	_	_
100	_	_
%	_	_
increase	_	_
Predictions	_	_
:	_	_
a	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
75.65	_	_
%	_	_
)	_	_
b	_	_
)	_	_
tennis	_	_
ball	_	_
(	_	_
76.42	_	_
%	_	_
)	_	_
c	_	_
)	_	_
golden	_	_
retriever	_	_
(	_	_
43.11	_	_
%	_	_
)	_	_
Figure	_	_
6	_	_
:	_	_
The	_	_
effect	_	_
of	_	_
Brightness	_	_
-	_	_
In	_	_
Fig	_	_
b	_	_
)	_	_
adversarial	_	_
image	_	_
with	_	_
50	_	_
%	_	_
increase	_	_
remains	_	_
adversarial	_	_
,	_	_
however	_	_
in	_	_
Fig.	_	_
c	_	_
)	_	_
100	_	_
%	_	_
increase	_	_
reverts	_	_
to	_	_
original	_	_
class	_	_
on	_	_
a	_	_
common	_	_
dataset	_	_
2	_	_
.	_	_

#98
Cross-Task	_	_
Transfer	_	_
:	_	_
Using	_	_
different	_	_
architectures	_	_
on	_	_
different	_	_
datasets	_	_
5.1	_	_
.	_	_

#99
Cross-Network	_	_
Transferability	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
investigate	_	_
how	_	_
well	_	_
do	_	_
adversarial	_	_
images	_	_
generalize	_	_
over	_	_
different	_	_
neural	_	_
network	_	_
architectures	_	_
while	_	_
keeping	_	_
the	_	_
dataset	_	_
and	_	_
the	_	_
task	_	_
constant	_	_
.	_	_

#100
In	_	_
order	_	_
to	_	_
do	_	_
this	_	_
task	_	_
,	_	_
we	_	_
use	_	_
a	_	_
computed	_	_
adversarial	_	_
image	_	_
from	_	_
one	_	_
model	_	_
and	_	_
use	_	_
it	_	_
to	_	_
attack	_	_
different	_	_
model	_	_
architecture	_	_
.	_	_

#101
We	_	_
tried	_	_
the	_	_
following	_	_
experiment	_	_
in	_	_
a	_	_
targeted	_	_
attack	_	_
setting	_	_
on	_	_
various	_	_
models	_	_
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
.	_	_

#102
As	_	_
demonstrated	_	_
in	_	_
Figure	_	_
7	_	_
,	_	_
we	_	_
generated	_	_
the	_	_
adversarial	_	_
image	_	_
using	_	_
a	_	_
ResNet	_	_
50	_	_
based	_	_
classifier	_	_
that	_	_
was	_	_
able	_	_
to	_	_
fool	_	_
the	_	_
ResNet	_	_
50	_	_
pipeline	_	_
with	_	_
high	_	_
confidence	_	_
.	_	_

#103
However	_	_
,	_	_
when	_	_
we	_	_
transferred	_	_
this	_	_
adversarial	_	_
image	_	_
for	_	_
evaluation	_	_
on	_	_
a	_	_
different	_	_
architecture	_	_
such	_	_
as	_	_
the	_	_
VGG	_	_
16	_	_
,	_	_
it	_	_
reverted	_	_
to	_	_
its	_	_
original	_	_
(	_	_
non-adversarial	_	_
)	_	_
class	_	_
.	_	_

#104
We	_	_
were	_	_
able	_	_
to	_	_
achieve	_	_
similar	_	_
results	_	_
while	_	_
performing	_	_
the	_	_
experiments	_	_
vice-versa	_	_
and	_	_
with	_	_
other	_	_
models	_	_
such	_	_
as	_	_
ResNet	_	_
101	_	_
and	_	_
Inception	_	_
V3	_	_
.	_	_

#105
This	_	_
indicates	_	_
that	_	_
the	_	_
adversarial	_	_
examples	_	_
are	_	_
weakly	_	_
transferable	_	_
across	_	_
neural	_	_
network	_	_
architectures	_	_
on	_	_
the	_	_
same	_	_
task	_	_
.	_	_

#106
5.2	_	_
.	_	_

#107
Cross-Task	_	_
Transferability	_	_
In	_	_
order	_	_
to	_	_
evaluate	_	_
Cross	_	_
Task	_	_
Transferability	_	_
,	_	_
we	_	_
used	_	_
adversarial	_	_
images	_	_
generated	_	_
through	_	_
pre-trained	_	_
models	_	_
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
and	_	_
evaluate	_	_
their	_	_
transferability	_	_
across	_	_
three	_	_
tasks	_	_
that	_	_
have	_	_
a	_	_
different	_	_
model	_	_
pipeline	_	_
and	_	_
were	_	_
trained	_	_
on	_	_
a	_	_
different	_	_
dataset	_	_
.	_	_

#108
5.2.1	_	_
Object	_	_
Detection	_	_

#109
We	_	_
first	_	_
evaluate	_	_
our	_	_
cross-task	_	_
task	_	_
transferability	_	_
on	_	_
object	_	_
detection	_	_
models	_	_
Faster	_	_
RCNN	_	_
and	_	_
RetinaNet	_	_
which	_	_
(	_	_
a	_	_
)	_	_
Original	_	_
Image	_	_
(	_	_
b	_	_
)	_	_
Adversarial	_	_
Image	_	_
Predictions	_	_
:	_	_
a	_	_
)	_	_
Resnet	_	_
50	_	_
-	_	_
Labrador	_	_
retriever	_	_
(	_	_
58.70	_	_
%	_	_
)	_	_
b1	_	_
)	_	_
Resnet	_	_
50	_	_
-	_	_
tennis	_	_
ball	_	_
(	_	_
75.65	_	_
%	_	_
)	_	_
b2	_	_
)	_	_
VGG	_	_
16	_	_
-	_	_
golden	_	_
retriever	_	_
(	_	_
42.38	_	_
%	_	_
)	_	_
Figure	_	_
7	_	_
:	_	_
Cross	_	_
Network	_	_
Transferability	_	_
:	_	_
Fig	_	_
a	_	_
)	_	_
shows	_	_
the	_	_
classification	_	_
for	_	_
original	_	_
image	_	_
on	_	_
ResNet	_	_
50	_	_
,	_	_
for	_	_
Fig	_	_
b1	_	_
)	_	_
shows	_	_
adversarial	_	_
fooling	_	_
on	_	_
ResNet	_	_
50	_	_
,	_	_
Fig	_	_
b2	_	_
)	_	_
however	_	_
when	_	_
adversarial	_	_
image	_	_
is	_	_
passed	_	_
to	_	_
VGG-	_	_
fails	_	_
to	_	_
transfer	_	_
and	_	_
reverts	_	_
to	_	_
original	_	_
class	_	_
used	_	_
ResNet	_	_
50	_	_
based	_	_
Image	_	_
Classification	_	_
model	_	_
as	_	_
a	_	_
subroutine	_	_
.	_	_

#110
Object	_	_
detection	_	_
models	_	_
form	_	_
bounding	_	_
boxes	_	_
over	_	_
the	_	_
objects	_	_
and	_	_
then	_	_
pass	_	_
it	_	_
to	_	_
the	_	_
classification	_	_
model	_	_
-	_	_
it	_	_
effectively	_	_
translates	_	_
to	_	_
cropping	_	_
the	_	_
objects	_	_
under	_	_
the	_	_
bounding	_	_
box	_	_
leading	_	_
to	_	_
classification	_	_
under	_	_
different	_	_
resolutions	_	_
.	_	_

#111
As	_	_
seen	_	_
in	_	_
section	_	_
4.1	_	_
,	_	_
the	_	_
cropping	_	_
operation	_	_
renders	_	_
the	_	_
adversarial	_	_
image	_	_
ineffective	_	_
for	_	_
an	_	_
attack	_	_
,	_	_
thus	_	_
detectors	_	_
are	_	_
not	_	_
affected	_	_
by	_	_
these	_	_
adversarial	_	_
images	_	_
.	_	_

#112
The	_	_
Figure	_	_
8	_	_
below	_	_
demonstrates	_	_
that	_	_
the	_	_
adversarial	_	_
attack	_	_
has	_	_
no	_	_
visible	_	_
impact	_	_
on	_	_
the	_	_
results	_	_
of	_	_
the	_	_
detection	_	_
pipeline	_	_
.	_	_

#113
(	_	_
a	_	_
)	_	_
Original	_	_
Detection	_	_
(	_	_
b	_	_
)	_	_
Adversarial	_	_
Detection	_	_
Figure	_	_
8	_	_
:	_	_
Object	_	_
detection	_	_
,	_	_
with	_	_
and	_	_
without	_	_
adversarial	_	_
attack	_	_
.	_	_

#114
5.2.2	_	_
Instance	_	_
Segmentation	_	_

#115
In	_	_
order	_	_
to	_	_
extend	_	_
the	_	_
results	_	_
obtained	_	_
on	_	_
object	_	_
detectors	_	_
in	_	_
the	_	_
previous	_	_
section	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
attacks	_	_
on	_	_
the	_	_
Instance	_	_
Segmentation	_	_
models	_	_
.	_	_

#116
Despite	_	_
passing	_	_
the	_	_
adversarial	_	_
image	_	_
to	_	_
the	_	_
models	_	_
,	_	_
there	_	_
is	_	_
no	_	_
discernible	_	_
difference	_	_
in	_	_
the	_	_
results	_	_
of	_	_
the	_	_
Mask	_	_
RCNN	_	_
pipelines	_	_
between	_	_
the	_	_
original	_	_
and	_	_
the	_	_
adversarial	_	_
images	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
9	_	_
.	_	_

#117
The	_	_
instance	_	_
segmentation	_	_
models	_	_
utilize	_	_
object	_	_
detection	_	_
as	_	_
a	_	_
subroutine	_	_
and	_	_
thus	_	_
adversarial	_	_
attack	_	_
does	_	_
not	_	_
cause	_	_
a	_	_
change	_	_
in	_	_
result	_	_
due	_	_
to	_	_
the	_	_
susceptibility	_	_
to	_	_
the	_	_
cropping	_	_
operation	_	_
during	_	_
formation	_	_
of	_	_
the	_	_
bounding	_	_
boxes	_	_
.	_	_

#118
(	_	_
a	_	_
)	_	_
Original	_	_
Inst	_	_
.	_	_

#119
Segmentation	_	_
(	_	_
b	_	_
)	_	_
Adversarial	_	_
Inst	_	_
.	_	_

#120
Segmentation	_	_
Figure	_	_
9	_	_
:	_	_
Instance	_	_
Segmentation	_	_
:	_	_
with	_	_
and	_	_
without	_	_
Adversarial	_	_
attack	_	_

#121
5.2.3	_	_
Image	_	_
Captioning	_	_

#122
Lastly	_	_
,	_	_
we	_	_
assess	_	_
the	_	_
transferability	_	_
of	_	_
adversarial	_	_
attacks	_	_
to	_	_
the	_	_
Image	_	_
Captioning	_	_
models	_	_
.	_	_

#123
The	_	_
intent	_	_
was	_	_
to	_	_
recognize	_	_
how	_	_
the	_	_
adversarial	_	_
perturbations	_	_
affect	_	_
an	_	_
RNN	_	_
based	_	_
model	_	_
and	_	_
how	_	_
they	_	_
alter	_	_
the	_	_
captions	_	_
and	_	_
their	_	_
semantic	_	_
properties	_	_
.	_	_

#124
For	_	_
the	_	_
experiment	_	_
,	_	_
we	_	_
used	_	_
a	_	_
captioning	_	_
model	_	_
based	_	_
on	_	_
a	_	_
CNN-LSTM	_	_
(	_	_
encoder-decoder	_	_
)	_	_
pipeline	_	_
,	_	_
pre-trained	_	_
on	_	_
the	_	_
MSCOCO	_	_
dataset	_	_
.	_	_

#125
The	_	_
model	_	_
used	_	_
with	_	_
k	_	_
beam	_	_
search	_	_
inference	_	_
technique	_	_
keeping	_	_
k	_	_
=1	_	_
.	_	_

#126
Following	_	_
are	_	_
some	_	_
of	_	_
the	_	_
results	_	_
obtained	_	_
.	_	_

#127
(	_	_
a	_	_
)	_	_
Original	_	_
Image	_	_
(	_	_
b	_	_
)	_	_
Adversarial	_	_
Image	_	_
Generated	_	_
Captions	_	_
:	_	_
a	_	_
)	_	_
A	_	_
man	_	_
flying	_	_
through	_	_
the	_	_
air	_	_
while	_	_
riding	_	_
a	_	_
snowboard	_	_
.	_	_

#128
(	_	_
p=0.021026	_	_
)	_	_
b	_	_
)	_	_
A	_	_
person	_	_
jumping	_	_
a	_	_
snow	_	_
board	_	_
in	_	_
the	_	_
air	_	_
(	_	_
p=0.007723	_	_
)	_	_
Figure	_	_
10	_	_
:	_	_
Demonstrates	_	_
a	_	_
)	_	_
original	_	_
and	_	_
b	_	_
)	_	_
adversarial	_	_
image	_	_
and	_	_
respective	_	_
captions	_	_
generated	_	_
.	_	_

#129
It	_	_
is	_	_
apparent	_	_
in	_	_
Figures	_	_
10	_	_
&	_	_
11	_	_
that	_	_
the	_	_
introduction	_	_
of	_	_
adversarial	_	_
perturbations	_	_
does	_	_
not	_	_
fool	_	_
the	_	_
captioning	_	_
pipeline	_	_
as	_	_
the	_	_
captions	_	_
obtained	_	_
by	_	_
adversarial	_	_
and	_	_
original	_	_
images	_	_
are	_	_
identical	_	_
.	_	_

#130
(	_	_
a	_	_
)	_	_
Original	_	_
Image	_	_
(	_	_
b	_	_
)	_	_
Adversarial	_	_
Image	_	_
Generated	_	_
Captions	_	_
:	_	_
a	_	_
)	_	_
A	_	_
baseball	_	_
player	_	_
throwing	_	_
a	_	_
baseball	_	_
on	_	_
a	_	_
field	_	_
.	_	_

#131
(	_	_
p=0.001463	_	_
)	_	_
b	_	_
)	_	_
A	_	_
baseball	_	_
player	_	_
swinging	_	_
a	_	_
bat	_	_
at	_	_
a	_	_
ball	_	_
(	_	_
p=0.001510	_	_
)	_	_
Figure	_	_
11	_	_
:	_	_
Demonstrates	_	_
a	_	_
)	_	_
original	_	_
and	_	_
b	_	_
)	_	_
adversarial	_	_
image	_	_
and	_	_
respective	_	_
captions	_	_
generated	_	_
.	_	_

#132
5.3	_	_
.	_	_

#133
Attacks	_	_
Capable	_	_
of	_	_
Fooling	_	_
Multiple	_	_
Networks	_	_
As	_	_
an	_	_
extension	_	_
of	_	_
the	_	_
study	_	_
described	_	_
in	_	_
section	_	_
5.1	_	_
,	_	_
we	_	_
examined	_	_
whether	_	_
adversarial	_	_
examples	_	_
could	_	_
fool	_	_
multiple	_	_
network	_	_
architectures	_	_
simultaneously	_	_
.	_	_

#134
We	_	_
passed	_	_
the	_	_
adversarial	_	_
image	_	_
generated	_	_
from	_	_
one	_	_
model	_	_
architecture	_	_
as	_	_
the	_	_
input	_	_
for	_	_
generation	_	_
to	_	_
the	_	_
next	_	_
model	_	_
architecture	_	_
.	_	_

#135
In	_	_
several	_	_
cases	_	_
,	_	_
we	_	_
discovered	_	_
that	_	_
the	_	_
resulting	_	_
adversarial	_	_
image	_	_
was	_	_
capable	_	_
of	_	_
fooling	_	_
both	_	_
the	_	_
networks	_	_
architectures	_	_
.	_	_

#136
In	_	_
the	_	_
image	_	_
below	_	_
,	_	_
we	_	_
carried	_	_
out	_	_
the	_	_
aforementioned	_	_
process	_	_
by	_	_
generating	_	_
the	_	_
adversarial	_	_
image	_	_
using	_	_
a	_	_
ResNet	_	_
50	_	_
pipeline	_	_
.	_	_

#137
We	_	_
then	_	_
transferred	_	_
it	_	_
for	_	_
generation	_	_
using	_	_
a	_	_
VGG16	_	_
model	_	_
and	_	_
found	_	_
that	_	_
the	_	_
resulting	_	_
image	_	_
was	_	_
able	_	_
to	_	_
fool	_	_
both	_	_
the	_	_
classification	_	_
models	_	_
(	_	_
tennis	_	_
ball	_	_
(	_	_
99.49	_	_
%	_	_
)	_	_
against	_	_
vgg16	_	_
and	_	_
tennis	_	_
ball	_	_
(	_	_
67.52	_	_
%	_	_
)	_	_
against	_	_
resnet	_	_
50	_	_
)	_	_
.	_	_

#138
However	_	_
,	_	_
the	_	_
resulting	_	_
image	_	_
also	_	_
experienced	_	_
more	_	_
visual	_	_
distortion	_	_
.	_	_

#139
This	_	_
experiment	_	_
would	_	_
also	_	_
suggest	_	_
that	_	_
it	_	_
might	speculation	_
possible	_	_
to	_	_
devise	_	_
an	_	_
adversarial	_	_
example	_	_
,	_	_
capable	_	_
of	_	_
fooling	_	_
all	_	_
the	_	_
major	_	_
neural	_	_
network	_	_
architectures	_	_
.	_	_

#140
6.	_	_
Future	_	_
Work	_	_

#141
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
suggest	_	_
some	_	_
directions	_	_
for	_	_
future	_	_
researchers	_	_
to	_	_
consider	_	_

#142
6.1.	_	_
Change	_	_
Loss	_	_
Function	_	_
in	_	_
Object	_	_
Detection	_	_
&	_	_
Instance	_	_
Segmentation	_	_

#143
As	_	_
demonstrated	_	_
by	_	_
Chen	_	_
et	_	_
.	_	_

#144
al	_	_
that	_	_
adversarial	_	_
images	_	_
can	_	_
be	_	_
transferred	_	_
to	_	_
Image	_	_
Captioning	_	_
models	_	_
by	_	_
altering	_	_
the	_	_
loss	_	_
function	_	_
of	_	_
the	_	_
model	_	_
.	_	_

#145
Whether	_	_
similar	_	_
results	_	_
can	_	_
be	_	_
achieved	_	_
on	_	_
Object	_	_
Detectors	_	_
and	_	_
Instance	_	_
Segmentation	_	_
models	_	_
remains	_	_
to	_	_
be	_	_
seen	_	_
.	_	_

#146
6.2	_	_
.	_	_

#147
Effect	_	_
of	_	_
adversarial	_	_
examples	_	_
on	_	_
Generative	_	_
models	_	_
So	_	_
far	_	_
,	_	_
adversarial	_	_
images	_	_
have	_	_
been	_	_
widely	_	_
deployed	_	_
against	_	_
discriminative	_	_
models	_	_
.	_	_

#148
However	_	_
,	_	_
the	_	_
effects	_	_
of	_	_
adversarial	_	_
examples	_	_
on	_	_
generative	_	_
models	_	_
such	_	_
as	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
and	_	_
Autoencoders	_	_
needs	_	_
to	_	_
be	_	_
researched	_	_
.	_	_

#149
Xiao	_	_
et	_	_
al.	_	_
[	_	_
21	_	_
]	_	_
and	_	_
[	_	_
17	_	_
]	_	_
showed	_	_
how	_	_
GANs	_	_
can	_	_
be	_	_
employed	_	_
for	_	_
adversarial	_	_
attack	_	_
and	_	_
defense	_	_
respectively	_	_
.	_	_

#150
However	_	_
detailed	_	_
studies	_	_
on	_	_
VAEs	_	_
and	_	_
GANs	_	_
for	_	_
adversarial	_	_
images	_	_
have	_	_
yet	_	_
to	_	_
be	_	_
conducted	_	_
.	_	_

#151
One	_	_
question	_	_
that	_	_
needs	_	_
to	_	_
be	_	_
answered	_	_
is	_	_
whether	_	_
adversarial	_	_
examples	_	_
can	_	_
be	_	_
generated	_	_
on	_	_
a	_	_
large	_	_
scale	_	_
using	_	_
generative	_	_
models	_	_
without	_	_
a	_	_
loss	_	_
of	_	_
clarity	_	_
and	_	_
attacking	_	_
ability	_	_
.	_	_

#152
6.3	_	_
.	_	_

#153
Effect	_	_
of	_	_
precision	_	_
on	_	_
adversarial	_	_
examples	_	_
Most	_	_
of	_	_
the	_	_
existing	_	_
research	_	_
on	_	_
adversarial	_	_
examples	_	_
has	_	_
been	_	_
carried	_	_
out	_	_
on	_	_
32-bit	_	_
float	_	_
models	_	_
.	_	_

#154
As	_	_
most	_	_
vision	_	_
models	_	_
in	_	_
real-life	_	_
applications	_	_
are	_	_
being	_	_
deployed	_	_
using	_	_
lower	_	_
precision	_	_
of	_	_
these	_	_
neural	_	_
networks	_	_
weights	_	_
such	_	_
as	_	_
16-bit	_	_
float	_	_
or	_	_
8-bit	_	_
integer	_	_
.	_	_

#155
It	_	_
would	_	_
be	_	_
intriguing	_	_
to	_	_
examine	_	_
the	_	_
effects	_	_
of	_	_
adversarial	_	_
examples	_	_
with	_	_
lower	_	_
precision	_	_
going	_	_
down	_	_
till	_	_
binary	_	_
.	_	_

#156
6.4	_	_
.	_	_

#157
Comparison	_	_
of	_	_
network	_	_
architectures	_	_
instead	_	_
of	_	_
fooling	_	_
capability	_	_
Another	_	_
possible	_	_
direction	_	_
would	_	_
the	_	_
effect	_	_
of	_	_
adversarial	_	_
examples	_	_
against	_	_
various	_	_
neural	_	_
network	_	_
architectures	_	_
.	_	_

#158
It	_	_
is	_	_
important	_	_
to	_	_
see	_	_
how	_	_
various	_	_
attacks	_	_
fare	_	_
against	_	_
different	_	_
architectures	_	_
;	_	_
which	_	_
architectures	_	_
are	_	_
able	_	_
to	_	_
defend	_	_
against	_	_
these	_	_
attacks	_	_
better	_	_
.	_	_

#159
7	_	_
.	_	_

#160
Conclusion	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
showed	_	_
that	_	_
adversarial	_	_
examples	_	_
are	_	_
weakly	_	_
transferable	_	_
across	_	_
neural	_	_
network	_	_
architectures	_	_
especially	_	_
in	_	_
more	_	_
complex	_	_
vision	_	_
tasks	_	_
.	_	_

#161
Using	_	_
the	_	_
FGSM	_	_
,	_	_
DeepFool	_	_
and	_	_
C	_	_
&	_	_
W	_	_
L2	_	_
attacks	_	_
,	_	_
we	_	_
evaluated	_	_
the	_	_
crossnetwork	_	_
transferability	_	_
of	_	_
adversarial	_	_
images	_	_
across	_	_
network	_	_
architectures	_	_
on	_	_
the	_	_
ImageNet	_	_
dataset	_	_
.	_	_

#162
We	_	_
extended	_	_
our	_	_
study	_	_
whether	_	_
these	_	_
attacks	_	_
are	_	_
transferable	_	_
to	_	_
more	_	_
complex	_	_
Computer	_	_
Vision	_	_
tasks	_	_
such	_	_
as	_	_
Instance	_	_
Segmentation	_	_
and	_	_
Object	_	_
Detection	_	_
As	_	_
in	_	_
the	_	_
real	_	_
world	_	_
,	_	_
adversarial	_	_
images	_	_
may	_	_
be	_	_
encountered	_	_
in	_	_
a	_	_
variety	_	_
of	_	_
scale	_	_
and	_	_
lighting	_	_
conditions	_	_
we	_	_
subjected	_	_
these	_	_
images	_	_
to	_	_
various	_	_
input	_	_
transformations	_	_
such	_	_
as	_	_
cropping	_	_
,	_	_
magnification	_	_
,	_	_
rotation	_	_
,	_	_
and	_	_
change	_	_
in	_	_
lighting	_	_
conditions	_	_
.	_	_

#163
We	_	_
found	_	_
that	_	_
there	_	_
exists	_	_
a	_	_
threshold	_	_
beyond	_	_
which	_	_
they	_	_
lose	_	_
their	_	_
adversarial	_	_
property	_	_
(	_	_
example-	_	_
if	_	_
an	_	_
image	_	_
is	_	_
cropped	_	_
beyond	_	_
a	_	_
few	_	_
pixels	_	_
)	_	_
.	_	_

#164
As	_	_
a	_	_
result	_	_
,	_	_
adversarial	_	_
images	_	_
fail	_	_
to	_	_
fool	_	_
object	_	_
detection	_	_
,	_	_
instance	_	_
segmentation	_	_
,	_	_
and	_	_
image	_	_
captioning	_	_
models	_	_
.	_	_

#165
We	_	_
also	_	_
found	_	_
that	_	_
when	_	_
an	_	_
adversarial	_	_
image	_	_
is	_	_
generated	_	_
sequentially	_	_
across	_	_
two	_	_
networks	_	_
,	_	_
it	_	_
may	_	_
be	_	_
able	_	_
to	_	_
fool	_	_
the	_	_
networks	_	_
simultaneously	_	_
.	_	_

#166
Our	_	_
work	_	_
shows	_	_
due	_	_
to	_	_
the	_	_
lack	_	_
of	_	_
transferability	_	_
and	_	_
scalability	_	_
of	_	_
adversarial	_	_
examples	_	_
,	_	_
they	_	_
are	_	_
yet	_	_
to	_	_
become	_	_
a	_	_
threat	_	_
in	_	_
the	_	_
real	_	_
world	_	_
and	_	_
are	_	_
currently	_	_
only	_	_
effective	_	_
in	_	_
controlled	_	_
experiments	_	_
.	_	_

#167
8	_	_
.	_	_

#168
Acknowledgement	_	_
The	_	_
authors	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
the	_	_
anonymous	_	_
reviewer	_	_
(	_	_
s	_	_
)	_	_
for	_	_
their	_	_
valuable	_	_
comments	_	_
and	_	_
suggestions	_	_
and	_	_
the	_	_
University	_	_
of	_	_
Illinois	_	_
at	_	_
Urbana	_	_
Champaign	_	_
for	_	_
their	_	_
GPU	_	_
computing	_	_
without	_	_
whose	_	_
support	_	_
the	_	_
paper	_	_
would	_	_
not	_	_
have	_	_
been	_	_
possible	_	_
.	_	_