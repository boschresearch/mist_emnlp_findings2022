#0
Batch	_	_
Normalization	_	_
and	_	_
the	_	_
impact	_	_
of	_	_
batch	_	_
structure	_	_
on	_	_
the	_	_
behavior	_	_
of	_	_
deep	_	_
convolution	_	_
networks	_	_
Mohamed	_	_
Hajaj	_	_
Duncan	_	_
Gillies	_	_
Department	_	_
of	_	_
Computing	_	_
,	_	_
Imperial	_	_
College	_	_
London	_	_

#1
Abstract	_	_

#2
Batch	_	_
normalization	_	_
was	_	_
introduced	_	_
in	_	_
2015	_	_
to	_	_
speed	_	_
up	_	_
training	_	_
of	_	_
deep	_	_
convolution	_	_
networks	_	_
by	_	_
normalizing	_	_
the	_	_
activations	_	_
across	_	_
the	_	_
current	_	_
batch	_	_
to	_	_
have	_	_
zero	_	_
mean	_	_
and	_	_
unity	_	_
variance	_	_
.	_	_

#3
The	_	_
results	_	_
presented	_	_
here	_	_
show	_	_
an	_	_
interesting	_	_
aspect	_	_
of	_	_
batch	_	_
normalization	_	_
,	_	_
where	_	_
controlling	_	_
the	_	_
shape	_	_
of	_	_
the	_	_
training	_	_
batches	_	_
can	_	_
influence	_	_
what	_	_
the	_	_
network	_	_
will	_	_
learn	_	_
.	_	_

#4
If	_	_
training	_	_
batches	_	_
are	_	_
structured	_	_
as	_	_
balanced	_	_
batches	_	_
(	_	_
one	_	_
image	_	_
per	_	_
class	_	_
)	_	_
,	_	_
and	_	_
inference	_	_
is	_	_
also	_	_
carried	_	_
out	_	_
on	_	_
balanced	_	_
test	_	_
batches	_	_
,	_	_
using	_	_
the	_	_
batch’s	_	_
own	_	_
means	_	_
and	_	_
variances	_	_
,	_	_
then	_	_
the	_	_
conditional	_	_
results	_	_
will	_	_
improve	_	_
considerably	_	_
.	_	_

#5
The	_	_
network	_	_
uses	_	_
the	_	_
strong	_	_
information	_	_
about	_	_
easy	_	_
images	_	_
in	_	_
a	_	_
balanced	_	_
batch	_	_
,	_	_
and	_	_
propagates	_	_
it	_	_
through	_	_
the	_	_
shared	_	_
means	_	_
and	_	_
variances	_	_
to	_	_
help	_	_
decide	_	_
the	_	_
identity	_	_
of	_	_
harder	_	_
images	_	_
on	_	_
the	_	_
same	_	_
batch	_	_
.	_	_

#6
Balancing	_	_
the	_	_
test	_	_
batches	_	_
requires	_	_
the	_	_
labels	_	_
of	_	_
the	_	_
test	_	_
images	_	_
,	_	_
which	_	_
are	_	_
not	_	_
available	_	_
in	_	_
practice	_	_
,	_	_
however	_	_
further	_	_
investigation	_	_
can	_	_
be	_	_
done	_	_
using	_	_
batch	_	_
structures	_	_
that	_	_
are	_	_
less	_	_
strict	_	_
and	_	_
might	options	_
not	_	_
require	_	_
the	_	_
test	_	_
image	_	_
labels	_	_
.	_	_

#7
The	_	_
conditional	_	_
results	_	_
show	_	_
the	_	_
error	_	_
rate	_	_
almost	_	_
reduced	_	_
to	_	_
zero	_	_
for	_	_
nontrivial	_	_
datasets	_	_
with	_	_
small	_	_
number	_	_
of	_	_
classes	_	_
such	_	_
as	_	_
the	_	_
CIFAR10	_	_
.	_	_

#8
1	_	_
Introduction	_	_

#9
When	_	_
training	_	_
deep	_	_
convolution	_	_
networks	_	_
,	_	_
batch	_	_
normalization	_	_
BN	_	_
[	_	_
Ioffe	_	_
and	_	_
Szegedy	_	_
,	_	_
2015	_	_
]	_	_
reduces	_	_
the	_	_
dependency	_	_
of	_	_
the	_	_
gradients	_	_
on	_	_
the	_	_
scale	_	_
of	_	_
the	_	_
parameters	_	_
and	_	_
their	_	_
initial	_	_
values	_	_
,	_	_
which	_	_
allows	_	_
for	_	_
much	_	_
higher	_	_
learning	_	_
rates	_	_
,	_	_
much	_	_
faster	_	_
convergence	_	_
,	_	_
and	_	_
better	_	_
final	_	_
results	_	_
.	_	_

#10
The	_	_
experiments	_	_
presented	_	_
here	_	_
show	_	_
that	_	_
by	_	_
the	_	_
nature	_	_
of	_	_
its	_	_
implementation	_	_
,	_	_
batch	_	_
normalization	_	_
has	_	_
more	_	_
to	_	_
it	_	_
than	_	_
just	_	_
speeding	_	_
up	_	_
training	_	_
.	_	_

#11
Because	_	_
activations	_	_
are	_	_
normalized	_	_
using	_	_
means	_	_
and	_	_
variances	_	_
that	_	_
are	_	_
shared	_	_
across	_	_
all	_	_
images	_	_
in	_	_
the	_	_
current	_	_
batch	_	_
,	_	_
the	_	_
output	_	_
activations	_	_
of	_	_
one	_	_
image	_	_
are	_	_
influenced	_	_
by	_	_
all	_	_
the	_	_
other	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
.	_	_

#12
This	_	_
means	_	_
that	_	_
the	_	_
behavior	_	_
of	_	_
the	_	_
network	_	_
is	_	_
not	_	_
only	_	_
receptive	_	_
to	_	_
the	_	_
individual	_	_
images	_	_
but	_	_
also	_	_
to	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
training	_	_
batches	_	_
.	_	_

#13
Therefore	_	_
,	_	_
with	_	_
BN	_	_
there	_	_
is	_	_
an	_	_
extra	_	_
level	_	_
of	_	_
control	_	_
that	_	_
can	_	_
be	_	_
used	_	_
to	_	_
guide	_	_
the	_	_
training	_	_
of	_	_
these	_	_
networks	_	_
,	_	_
which	_	_
is	_	_
based	_	_
on	_	_
how	_	_
batches	_	_
are	_	_
constructed	_	_
.	_	_

#14
The	_	_
results	_	_
presented	_	_
here	_	_
show	_	_
that	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
make	_	_
the	_	_
network	_	_
learn	_	_
something	_	_
based	_	_
on	_	_
how	_	_
batches	_	_
are	_	_
constructed	_	_
.	_	_

#15
Balanced	_	_
batches	_	_
are	_	_
batches	_	_
that	_	_
have	_	_
a	_	_
single	_	_
instance	_	_
from	_	_
each	_	_
class	_	_
with	_	_
size	_	_
equal	_	_
to	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
.	_	_

#16
If	_	_
the	_	_
network	_	_
is	_	_
trained	_	_
only	_	_
on	_	_
balanced	_	_
batches	_	_
,	_	_
then	_	_
in	_	_
addition	_	_
to	_	_
learning	_	_
how	_	_
to	_	_
classify	_	_
single	_	_
images	_	_
,	_	_
BN	_	_
will	_	_
allow	_	_
the	_	_
network	_	_
to	_	_
learn	_	_
an	_	_
extra	_	_
logic	_	_
based	_	_
on	_	_
the	_	_
structure	_	_
of	_	_
balanced	_	_
batches	_	_
.	_	_

#17
Because	_	_
it	_	_
was	_	_
only	_	_
exposed	_	_
to	_	_
balanced	_	_
batches	_	_
in	_	_
the	_	_
training	_	_
phase	_	_
,	_	_
the	_	_
network	_	_
will	_	_
learn	_	_
an	_	_
association	_	_
mechanism	_	_
between	_	_
the	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
through	_	_
the	_	_
shared	_	_
means	_	_
and	_	_
variances	_	_
of	_	_
BN	_	_
to	_	_
always	_	_
expect	_	_
balanced	_	_
batches	_	_
.	_	_

#18
If	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
network	_	_
is	_	_
measured	_	_
in	_	_
the	_	_
standard	_	_
way	_	_
on	_	_
single	_	_
test	_	_
images	_	_
,	_	_
then	_	_
this	_	_
association	_	_
mechanism	_	_
based	_	_
on	_	_
batch	_	_
structure	_	_
can	_	_
not	_	_
be	_	_
noticed	_	_
.	_	_

#19
In	_	_
order	_	_
to	_	_
measure	_	_
it	_	_
,	_	_
the	_	_
network	_	_
needs	_	_
to	_	_
be	_	_
tested	_	_
on	_	_
balanced	_	_
test	_	_
batches	_	_
using	_	_
each	_	_
batch’s	_	_
own	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#20
The	_	_
practical	_	_
difficulty	_	_
here	_	_
is	_	_
balancing	_	_
the	_	_
test	_	_
batches	_	_
,	_	_
which	_	_
requires	_	_
the	_	_
labels	_	_
of	_	_
the	_	_
test	_	_
images	_	_
.	_	_

#21
Therefore	_	_
,	_	_
the	_	_
purpose	_	_
of	_	_
these	_	_
experiments	_	_
is	_	_
to	_	_
investigate	_	_
this	_	_
extra	_	_
dimension	_	_
of	_	_
control	_	_
based	_	_
on	_	_
batch	_	_
structure	_	_
that	_	_
is	_	_
made	_	_
possible	_	_
through	_	_
BN	_	_
,	_	_
and	_	_
it	_	_
is	_	_
not	_	_
claiming	_	_
state	_	_
of	_	_
the	_	_
art	_	_
results	_	_
.	_	_

#22
However	_	_
,	_	_
because	_	_
of	_	_
the	_	_
scale	_	_
of	_	_
improvement	_	_
(	_	_
conditional	_	_
)	_	_
achieved	_	_
on	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
,	_	_
the	_	_
subject	_	_
is	_	_
worth	_	_
further	_	_
investigation	_	_
maybe	_	_
using	_	_
other	_	_
batch	_	_
structures	_	_
that	_	_
do	_	_
not	_	_
require	_	_
the	_	_
test	_	_
image	_	_
labels	_	_
.	_	_

#23
2	_	_
Implementation	_	_

#24
2.1	_	_
Batch	_	_
Normalization	_	_

#25
For	_	_
a	_	_
deep	_	_
neural	_	_
network	_	_
,	_	_
the	_	_
input	_	_
distribution	_	_
of	_	_
layer	_	_
l	_	_
depends	_	_
on	_	_
the	_	_
parameters	_	_
of	_	_
all	_	_
previous	_	_
layers	_	_
,	_	_
and	_	_
as	_	_
these	_	_
parameters	_	_
change	_	_
,	_	_
the	_	_
input	_	_
distribution	_	_
of	_	_
layer	_	_
l	_	_
will	_	_
also	_	_
change	_	_
.	_	_

#26
Layer	_	_
l	_	_
will	_	_
try	_	_
to	_	_
adapt	_	_
to	_	_
an	_	_
input	_	_
distribution	_	_
that	_	_
keeps	_	_
changing	_	_
throughout	_	_
training	_	_
,	_	_
and	_	_
that	_	_
slows	_	_
up	_	_
training	_	_
.	_	_

#27
This	_	_
problem	_	_
is	_	_
called	_	_
internal	_	_
covariance	_	_
shifts	_	_
[	_	_
Shimodaira	_	_
,	_	_
2000	_	_
]	_	_
,	_	_
and	_	_
BN	_	_
tries	_	_
to	_	_
reduce	_	_
this	_	_
problem	_	_
by	_	_
performing	_	_
a	_	_
simplified	_	_
version	_	_
of	_	_
complete	_	_
whitening	_	_
to	_	_
the	_	_
inputs	_	_
of	_	_
each	_	_
layer	_	_
.	_	_

#28
First	_	_
,	_	_
BN	_	_
assumes	_	_
that	_	_
input	_	_
features	_	_
are	_	_
independent	_	_
,	_	_
and	_	_
therefore	_	_
can	_	_
be	_	_
normalized	_	_
independently	_	_
to	_	_
have	_	_
zero	_	_
mean	_	_
and	_	_
unity	_	_
variance	_	_
.	_	_

#29
Second	_	_
,	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
are	_	_
calculated	_	_
across	_	_
the	_	_
current	_	_
batch	_	_
,	_	_
and	_	_
not	_	_
over	_	_
the	_	_
entire	_	_
training	_	_
data	_	_
(	_	_
hence	_	_
the	_	_
name	_	_
batch	_	_
normalization	_	_
)	_	_
.	_	_

#30
ar	_	_
X	_	_
iv	_	_
:1	_	_
2	_	_
.	_	_

#31
0v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
1	_	_
Fe	_	_
b	_	_
Because	_	_
the	_	_
networks	_	_
used	_	_
in	_	_
this	_	_
study	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2016a	_	_
]	_	_
are	_	_
fully	_	_
convolutional	_	_
,	_	_
BN	_	_
will	_	_
only	_	_
be	_	_
applied	_	_
to	_	_
convolution	_	_
layers	_	_
.	_	_

#32
For	_	_
each	_	_
output	_	_
channel	_	_
a	_	_
mean	_	_
and	_	_
variance	_	_
are	_	_
calculated	_	_
across	_	_
all	_	_
images	_	_
in	_	_
the	_	_
current	_	_
batch	_	_
.	_	_

#33
If	_	_
the	_	_
channel	_	_
size	_	_
is	_	_
h	_	_
×	_	_
w	_	_
,	_	_
and	_	_
the	_	_
batch	_	_
size	_	_
is	_	_
m	_	_
images	_	_
,	_	_
and	_	_
m′	_	_
=	_	_
m	_	_
×	_	_
h	_	_
×	_	_
w	_	_
,	_	_
then	_	_
the	_	_
mean	_	_
and	_	_
variance	_	_
are	_	_
calculated	_	_
using	_	_
equations	_	_
(	_	_
1	_	_
)	_	_
and	_	_
(	_	_
2	_	_
)	_	_
,	_	_
and	_	_
each	_	_
location	_	_
xi	_	_
in	_	_
that	_	_
output	_	_
channel	_	_
across	_	_
all	_	_
the	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
is	_	_
normalized	_	_
using	_	_
equation	_	_
(	_	_
3	_	_
)	_	_
.	_	_

#34
A	_	_
convolution	_	_
layer	_	_
with	_	_
N	_	_
output	_	_
channels	_	_
will	_	_
need	_	_
N	_	_
pairs	_	_
of	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#35
The	_	_
algorithm	_	_
also	_	_
adds	_	_
a	_	_
linear	_	_
transformation	_	_
y	_	_
=	_	_
αx	_	_
+	_	_
β	_	_
(	_	_
α	_	_
,	_	_
β	_	_
are	_	_
trainable	_	_
parameters	_	_
)	_	_
after	_	_
the	_	_
normalization	_	_
step	_	_
to	_	_
restore	_	_
the	_	_
expressive	_	_
capabilities	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#36
For	_	_
the	_	_
backward	_	_
error	_	_
propagation	_	_
equations	_	_
refer	_	_
to	_	_
[	_	_
Ioffe	_	_
and	_	_
Szegedy	_	_
,	_	_
2015	_	_
]	_	_
.	_	_

#37
µ	_	_
=	_	_
m′	_	_
m′∑	_	_
i=1	_	_
xi	_	_
(	_	_
1	_	_
)	_	_
σ2	_	_
=	_	_
−µ2	_	_
+	_	_
m′	_	_
m′∑	_	_
i=1	_	_
x2	_	_
i	_	_
(	_	_
2	_	_
)	_	_
x̂i	_	_
=	_	_
xi	_	_
−	_	_
µ√	_	_
σ2	_	_
+	_	_
ε	_	_
(	_	_
3	_	_
)	_	_

#38
2.2	_	_
Balanced	_	_
Batches	_	_

#39
A	_	_
balanced	_	_
batch	_	_
contains	_	_
a	_	_
single	_	_
instance	_	_
from	_	_
each	_	_
class	_	_
,	_	_
and	_	_
therefore	_	_
the	_	_
batch	_	_
size	_	_
of	_	_
a	_	_
balanced	_	_
batch	_	_
will	_	_
always	_	_
be	_	_
equal	_	_
to	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
.	_	_

#40
With	_	_
balanced	_	_
batches	_	_
,	_	_
the	_	_
standard	_	_
BN	_	_
algorithm	_	_
will	_	_
be	_	_
used	_	_
with	_	_
the	_	_
following	_	_
considerations	_	_
at	_	_
the	_	_
training	_	_
and	_	_
inference	_	_
stages	_	_
.	_	_

#41
•	_	_
Training	_	_
:	_	_
-	_	_
instead	_	_
of	_	_
being	_	_
constructed	_	_
randomly	_	_
,	_	_
training	_	_
batches	_	_
are	_	_
created	_	_
to	_	_
be	_	_
balanced	_	_
,	_	_
and	_	_
to	_	_
contain	_	_
a	_	_
single	_	_
instance	_	_
from	_	_
each	_	_
class	_	_
.	_	_

#42
If	_	_
training	_	_
images	_	_
were	_	_
shuffled	_	_
to	_	_
prevent	_	_
an	_	_
image	_	_
from	_	_
always	_	_
appearing	_	_
in	_	_
the	_	_
same	_	_
batch	_	_
(	_	_
to	_	_
improve	_	_
performance	_	_
)	_	_
,	_	_
then	_	_
the	_	_
shuffling	_	_
subroutine	_	_
needs	_	_
to	_	_
be	_	_
changed	_	_
to	_	_
always	_	_
generate	_	_
balanced	_	_
batches	_	_
.	_	_

#43
•	_	_
Inference	_	_
:	_	_
-	_	_
Standard	_	_
BN	_	_
calculates	_	_
a	_	_
fixed	_	_
set	_	_
of	_	_
means	_	_
and	_	_
variances	_	_
over	_	_
the	_	_
entire	_	_
training	_	_
data	_	_
to	_	_
be	_	_
used	_	_
at	_	_
the	_	_
inference	_	_
stage	_	_
instead	_	_
of	_	_
using	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
of	_	_
the	_	_
current	_	_
test	_	_
batch	_	_
.	_	_

#44
This	_	_
is	_	_
done	_	_
to	_	_
make	_	_
the	_	_
prediction	_	_
of	_	_
the	_	_
network	_	_
deterministic	_	_
and	_	_
depends	_	_
only	_	_
on	_	_
the	_	_
image	_	_
itself	_	_
,	_	_
and	_	_
not	_	_
on	_	_
the	_	_
other	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
.	_	_

#45
In	_	_
order	_	_
to	_	_
measure	_	_
the	_	_
effect	_	_
of	_	_
training	_	_
the	_	_
network	_	_
on	_	_
balanced	_	_
batches	_	_
,	_	_
test	_	_
images	_	_
are	_	_
arranged	_	_
as	_	_
balanced	_	_
batches	_	_
and	_	_
the	_	_
current	_	_
means	_	_
and	_	_
variances	_	_
of	_	_
the	_	_
test	_	_
batch	_	_
itself	_	_
are	_	_
used	_	_
in	_	_
the	_	_
inference	_	_
process	_	_
.	_	_

#46
2.3	_	_
Network	_	_
Structure	_	_

#47
Deep	_	_
residual	_	_
convolution	_	_
networks	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2016a	_	_
]	_	_
were	_	_
used	_	_
in	_	_
this	_	_
study	_	_
,	_	_
where	_	_
a	_	_
standard	_	_
deep	_	_
residual	_	_
network	_	_
starts	_	_
with	_	_
a	_	_
single	_	_
convolution	_	_
layer	_	_
,	_	_
followed	_	_
by	_	_
multiple	_	_
residual	_	_
building	_	_
blocks	_	_
,	_	_
followed	_	_
by	_	_
one	_	_
fully	_	_
connected	_	_
layer	_	_
,	_	_
which	_	_
is	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#48
A	_	_
residual	_	_
block	_	_
figure	_	_
(	_	_
1	_	_
)	_	_
is	_	_
made	_	_
of	_	_
2	_	_
or	_	_
3	_	_
stacked	_	_
convolution	_	_
layers	_	_
warped	_	_
by	_	_
identity	_	_
mapping	_	_
so	_	_
that	_	_
the	_	_
output	_	_
of	_	_
the	_	_
block	_	_
is	_	_
the	_	_
combination	_	_
of	_	_
the	_	_
input	_	_
signal	_	_
to	_	_
that	_	_
block	_	_
and	_	_
the	_	_
output	_	_
signal	_	_
of	_	_
the	_	_
stacked	_	_
convolution	_	_
layers	_	_
.	_	_

#49
Residual	_	_
learning	_	_
makes	_	_
it	_	_
possible	_	_
to	_	_
train	_	_
very	_	_
deep	_	_
networks	_	_
with	_	_
up	_	_
to	_	_
hundreds	_	_
of	_	_
layers	_	_
by	_	_
eliminating	_	_
the	_	_
degradation	_	_
problem	_	_
[	_	_
He	_	_
and	_	_
Sun	_	_
,	_	_
2015	_	_
,	_	_
Srivastava	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
]	_	_
exhibited	_	_
by	_	_
standard	_	_
very	_	_
deep	_	_
stacked	_	_
networks	_	_
.	_	_

#50
Figure	_	_
1	_	_
:	_	_
Residual	_	_
Block	_	_

#51
3	_	_
Experiments	_	_
and	_	_
Results	_	_

#52
3.1	_	_
CIFAR10	_	_
Experiment	_	_

#53
The	_	_
first	_	_
experiments	_	_
to	_	_
measure	_	_
the	_	_
effect	_	_
of	_	_
using	_	_
balanced	_	_
batches	_	_
with	_	_
BN	_	_
were	_	_
done	_	_
using	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
.	_	_

#54
Table	_	_
(	_	_
1	_	_
)	_	_
shows	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
two	_	_
deep	_	_
residual	_	_
networks	_	_
with	_	_
depths	_	_
equal	_	_
to	_	_
20	_	_
and	_	_
44	_	_
layers	_	_
used	_	_
in	_	_
this	_	_
experiment	_	_
,	_	_
and	_	_
they	_	_
are	_	_
very	_	_
similar	_	_
to	_	_
the	_	_
ones	_	_
used	_	_
by	_	_
He	_	_
et	_	_
al.	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2016a	_	_
]	_	_
for	_	_
CIFAR10	_	_
.	_	_

#55
The	_	_
same	_	_
treatment	_	_
of	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
by	_	_
enlarging	_	_
the	_	_
images	_	_
with	_	_
zero-padding	_	_
from	_	_
32	_	_
×	_	_
32	_	_
to	_	_
40	_	_
×	_	_
40	_	_
pixels	_	_
is	_	_
used	_	_
here	_	_
.	_	_

#56
The	_	_
main	_	_
stream	_	_
approach	_	_
in	_	_
dealing	_	_
with	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
uses	_	_
cropping	_	_
without	_	_
scaling	_	_
because	_	_
the	_	_
images	_	_
are	_	_
very	_	_
small	_	_
,	_	_
32	_	_
×	_	_
32	_	_
pixels	_	_
.	_	_

#57
However	_	_
,	_	_
we	_	_
found	_	_
that	_	_
using	_	_
scaling	_	_
has	_	_
improved	_	_
the	_	_
results	_	_
despite	_	_
the	_	_
images	_	_
being	_	_
very	_	_
small	_	_
.	_	_

#58
The	_	_
weight	_	_
initialization	_	_
method	_	_
in	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
]	_	_
,	_	_
and	_	_
the	_	_
standard	_	_
color	_	_
augmentation	_	_
method	_	_
in	_	_
[	_	_
Krizhevsky	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
]	_	_
are	_	_
used	_	_
.	_	_

#59
Simple	_	_
gradient	_	_
descent	_	_
with	_	_
weight	_	_
decay	_	_
is	_	_
used	_	_
to	_	_
update	_	_
the	_	_
network	_	_
parameters	_	_
.	_	_

#60
output	_	_
size	_	_
20	_	_
Layers	_	_
44	_	_
Layers	_	_
36×	_	_
36	_	_
conv	_	_
,	_	_
3×	_	_
3	_	_
,	_	_
64	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
64	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
64	_	_
]	_	_
×	_	_
3	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
64	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
64	_	_
]	_	_
×	_	_
7	_	_
18×	_	_
18	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
128	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
128	_	_
]	_	_
×	_	_
3	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
128	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
128	_	_
]	_	_
×	_	_
7	_	_
9×	_	_
9	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
256	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
256	_	_
]	_	_
×	_	_
3	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
256	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
256	_	_
]	_	_
×	_	_
7	_	_
1×	_	_
1	_	_
global	_	_
avg	_	_
pool	_	_
9×	_	_
9	_	_
,	_	_
10-d	_	_
fc	_	_
,	_	_
softmax	_	_
Table	_	_
1	_	_
:	_	_
Network	_	_
Structure	_	_
In	_	_
the	_	_
first	_	_
part	_	_
of	_	_
the	_	_
experiment	_	_
,	_	_
both	_	_
training	_	_
and	_	_
inference	_	_
were	_	_
performed	_	_
in	_	_
the	_	_
standard	_	_
way	_	_
,	_	_
where	_	_
training	_	_
batches	_	_
were	_	_
created	_	_
randomly	_	_
,	_	_
and	_	_
inference	_	_
were	_	_
carried	_	_
out	_	_
on	_	_
individual	_	_
images	_	_
using	_	_
a	_	_
fixed	_	_
set	_	_
of	_	_
means	_	_
and	_	_
variances	_	_
computed	_	_
using	_	_
the	_	_
entire	_	_
training	_	_
data	_	_
after	_	_
training	_	_
was	_	_
completed	_	_
.	_	_

#61
In	_	_
the	_	_
second	_	_
part	_	_
of	_	_
the	_	_
experiment	_	_
,	_	_
training	_	_
was	_	_
done	_	_
using	_	_
balanced	_	_
batches	_	_
,	_	_
and	_	_
inference	_	_
was	_	_
carried	_	_
out	_	_
twice	_	_
:	_	_
First	_	_
,	_	_
inference	_	_
was	_	_
done	_	_
in	_	_
the	_	_
standard	_	_
way	_	_
,	_	_
where	_	_
images	_	_
are	_	_
tested	_	_
individually	_	_
using	_	_
fixed	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#62
Second	_	_
,	_	_
to	_	_
measure	_	_
the	_	_
effect	_	_
of	_	_
batch	_	_
structure	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
network	_	_
,	_	_
test	_	_
images	_	_
were	_	_
arranged	_	_
the	_	_
same	_	_
way	_	_
as	_	_
training	_	_
images	_	_
;	_	_
test	_	_
images	_	_
were	_	_
arranged	_	_
as	_	_
balanced	_	_
batches	_	_
,	_	_
and	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
of	_	_
the	_	_
current	_	_
test	_	_
batch	_	_
itself	_	_
were	_	_
used	_	_
in	_	_
the	_	_
inference	_	_
process	_	_
.	_	_

#63
Table	_	_
(	_	_
2	_	_
)	_	_
shows	_	_
the	_	_
results	_	_
for	_	_
both	_	_
experiments	_	_
.	_	_

#64
It	_	_
is	_	_
clear	_	_
that	_	_
balancing	_	_
the	_	_
training	_	_
batches	_	_
doesn’t	_	_
change	_	_
the	_	_
results	_	_
,	_	_
if	_	_
the	_	_
inference	_	_
process	_	_
is	_	_
carried	_	_
out	_	_
in	_	_
a	_	_
standard	_	_
way	_	_
.	_	_

#65
However	_	_
,	_	_
if	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
network	_	_
trained	_	_
using	_	_
balanced	_	_
batches	_	_
is	_	_
also	_	_
tested	_	_
using	_	_
balanced	_	_
test	_	_
batches	_	_
,	_	_
the	_	_
error	_	_
rate	_	_
is	_	_
reduced	_	_
by	_	_
about	_	_
80	_	_
%	_	_
for	_	_
both	_	_
network	_	_
models	_	_
.	_	_

#66
The	_	_
error	_	_
rate	_	_
was	_	_
almost	_	_
eliminated	_	_
for	_	_
the	_	_
non-trivial	_	_
CIFAR10	_	_
.	_	_

#67
Training	_	_
Inference	_	_
20	_	_
Layers	_	_
44	_	_
Layers	_	_
standard	_	_
standard	_	_
4.45	_	_
%	_	_
3.89	_	_
%	_	_
Balanced	_	_
Batches	_	_
standard	_	_
4.47	_	_
%	_	_
3.9	_	_
%	_	_
Balanced	_	_
Batches	_	_
Balanced	_	_
Batches	_	_
0.97	_	_
%	_	_
0.69	_	_
%	_	_
Table	_	_
2	_	_
:	_	_
CIFAR10	_	_
Results	_	_
,	_	_
when	_	_
training	_	_
is	_	_
done	_	_
using	_	_
random	_	_
vs	_	_
balanced	_	_
batches	_	_
,	_	_
and	_	_
inference	_	_
is	_	_
done	_	_
on	_	_
individual	_	_
images	_	_
vs	_	_
balanced	_	_
batches	_	_
.	_	_

#68
Figure	_	_
(	_	_
2	_	_
)	_	_
shows	_	_
the	_	_
error	_	_
curves	_	_
measured	_	_
on	_	_
the	_	_
central	_	_
crop	_	_
of	_	_
the	_	_
test	_	_
set	_	_
images	_	_
as	_	_
training	_	_
progresses	_	_
for	_	_
the	_	_
44-layer	_	_
network	_	_
.	_	_

#69
The	_	_
red	_	_
curve	_	_
was	_	_
obtained	_	_
by	_	_
training	_	_
the	_	_
network	_	_
on	_	_
balanced	_	_
batches	_	_
,	_	_
and	_	_
the	_	_
blue	_	_
curve	_	_
was	_	_
obtained	_	_
by	_	_
training	_	_
the	_	_
network	_	_
on	_	_
randomly	_	_
constructed	_	_
batches	_	_
,	_	_
but	_	_
because	_	_
both	_	_
were	_	_
measured	_	_
on	_	_
individual	_	_
test	_	_
images	_	_
using	_	_
the	_	_
running	_	_
averages	_	_
of	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
,	_	_
the	_	_
results	_	_
were	_	_
very	_	_
similar	_	_
.	_	_

#70
However	_	_
,	_	_
if	_	_
the	_	_
progress	_	_
of	_	_
the	_	_
network	_	_
trained	_	_
using	_	_
balanced	_	_
batches	_	_
was	_	_
also	_	_
measured	_	_
on	_	_
balanced	_	_
test	_	_
batches	_	_
using	_	_
the	_	_
current	_	_
means	_	_
and	_	_
variances	_	_
of	_	_
the	_	_
batch	_	_
itself	_	_
,	_	_
then	_	_
a	_	_
big	_	_
reduction	_	_
in	_	_
the	_	_
test	_	_
error	_	_
can	_	_
be	_	_
measured	_	_
from	_	_
the	_	_
start	_	_
to	_	_
the	_	_
end	_	_
of	_	_
the	_	_
training	_	_
process	_	_
as	_	_
the	_	_
black	_	_
curve	_	_
shows	_	_
.	_	_

#71
These	_	_
three	_	_
curves	_	_
agree	_	_
with	_	_
the	_	_
final	_	_
results	_	_
shown	_	_
in	_	_
the	_	_
three	_	_
rows	_	_
in	_	_
table	_	_
(	_	_
2	_	_
)	_	_
.	_	_

#72
Figure	_	_
2	_	_
:	_	_
validation	_	_
(	_	_
test	_	_
)	_	_
error	_	_
curves	_	_
as	_	_
training	_	_
progresses	_	_
for	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
.	_	_

#73
blue	_	_
curve	_	_
:	_	_
standard	_	_
training	_	_
and	_	_
inference	_	_
,	_	_
red	_	_
curve	_	_
:	_	_
training	_	_
using	_	_
balanced	_	_
batches	_	_
,	_	_
black	_	_
curve	_	_
:	_	_
both	_	_
training	_	_
and	_	_
inference	_	_
using	_	_
balanced	_	_
batches	_	_
.	_	_

#74
3.2	_	_
Experiment	_	_
Two	_	_

#75
Three	_	_
datasets	_	_
with	_	_
10	_	_
,	_	_
50	_	_
,	_	_
and	_	_
100	_	_
classes	_	_
were	_	_
randomly	_	_
sampled	_	_
from	_	_
ImageNet	_	_
,	_	_
and	_	_
will	_	_
be	_	_
used	_	_
here	_	_
.	_	_

#76
The	_	_
reason	_	_
for	_	_
not	_	_
using	_	_
the	_	_
entire	_	_
ImageNet	_	_
dataset	_	_
is	_	_
because	_	_
it	_	_
has	_	_
1000	_	_
classes	_	_
,	_	_
and	_	_
using	_	_
balanced	_	_
batches	_	_
will	_	_
require	_	_
a	_	_
batch	_	_
size	_	_
of	_	_
1000	_	_
image	_	_
,	_	_
which	_	_
can	_	_
not	_	_
be	_	_
supported	_	_
by	_	_
our	_	_
hardware	_	_
.	_	_

#77
The	_	_
reason	_	_
for	_	_
sampling	_	_
three	_	_
datasets	_	_
instead	_	_
of	_	_
just	_	_
one	_	_
is	_	_
to	_	_
measure	_	_
the	_	_
impact	_	_
of	_	_
batch	_	_
size	_	_
on	_	_
the	_	_
results	_	_
obtained	_	_
using	_	_
balanced	_	_
batches	_	_
.	_	_

#78
All	_	_
sampled	_	_
classes	_	_
have	_	_
1300	_	_
training	_	_
images	_	_
,	_	_
and	_	_
50	_	_
test	_	_
images	_	_
.	_	_

#79
Table	_	_
(	_	_
4	_	_
)	_	_
shows	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
34-layer	_	_
deep	_	_
residual	_	_
network	_	_
used	_	_
in	_	_
this	_	_
experiment	_	_
for	_	_
all	_	_
three	_	_
datasets	_	_
,	_	_
which	_	_
is	_	_
similar	_	_
to	_	_
that	_	_
used	_	_
by	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2016a	_	_
]	_	_
for	_	_
ImageNet	_	_
.	_	_

#80
Data	_	_
augmentation	_	_
in	_	_
[	_	_
Szegedy	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
]	_	_
,	_	_
weight	_	_
initialization	_	_
in	_	_
[	_	_
He	_	_
et	_	_
al.	_	_
,	_	_
2015	_	_
]	_	_
,	_	_
and	_	_
color	_	_
augmentation	_	_
in	_	_
[	_	_
Krizhevsky	_	_
et	_	_
al.	_	_
,	_	_
2012	_	_
]	_	_
were	_	_
used	_	_
.	_	_

#81
The	_	_
RMSProp	_	_
optimization	_	_
method	_	_
is	_	_
used	_	_
instead	_	_
of	_	_
gradient	_	_
decent	_	_
with	_	_
momentum	_	_
to	_	_
update	_	_
the	_	_
network	_	_
parameters	_	_
,	_	_
using	_	_
a	_	_
decay	_	_
value	_	_
of	_	_
0.999	_	_
to	_	_
calculate	_	_
the	_	_
running	_	_
average	_	_
per	_	_
parameter	_	_
.	_	_

#82
Figure	_	_
3	_	_
:	_	_
the	_	_
validation	_	_
(	_	_
test	_	_
)	_	_
error	_	_
curves	_	_
as	_	_
training	_	_
progresses	_	_
,	_	_
blue	_	_
curve	_	_
using	_	_
standard	_	_
training	_	_
and	_	_
inference	_	_
,	_	_
red	_	_
curve	_	_
training	_	_
using	_	_
balanced	_	_
batches	_	_
,	_	_
black	_	_
curve	_	_
both	_	_
training	_	_
and	_	_
inference	_	_
using	_	_
balanced	_	_
batches	_	_
.	_	_

#83
left	_	_
:	_	_
for	_	_
the	_	_
10-classes	_	_
dataset	_	_
,	_	_
middle	_	_
:	_	_
for	_	_
the	_	_
50-classes	_	_
dataset	_	_
,	_	_
right	_	_
:	_	_
for	_	_
the	_	_
100-classes	_	_
dataset	_	_
.	_	_

#84
Training	_	_
Inference	_	_

#85
10	_	_
Classes	_	_

#86
dataset	_	_

#87
50	_	_
Classes	_	_

#88
dataset	_	_

#89
100	_	_
Classes	_	_

#90
dataset	_	_
standard	_	_
standard	_	_
5.97	_	_
%	_	_
7.3	_	_
%	_	_
10.1	_	_
%	_	_
Balanced	_	_
Batches	_	_
standard	_	_
5.9	_	_
%	_	_
7.34	_	_
%	_	_
10.14	_	_
%	_	_
Balanced	_	_
Batches	_	_
Balanced	_	_
Batches	_	_
1.1	_	_
%	_	_
3.32	_	_
%	_	_
6.74	_	_
%	_	_
Table	_	_
3	_	_
:	_	_
results	_	_
for	_	_
3	_	_
datasets	_	_
,	_	_
when	_	_
training	_	_
is	_	_
done	_	_
using	_	_
random	_	_
vs	_	_
balanced	_	_
batches	_	_
,	_	_
and	_	_
inference	_	_
is	_	_
done	_	_
on	_	_
individual	_	_
images	_	_
vs	_	_
balanced	_	_
batches	_	_
.	_	_

#91
output	_	_
size	_	_
34	_	_
Layers	_	_
112×	_	_
112	_	_
conv	_	_
,	_	_
5×	_	_
5	_	_
,	_	_
96	_	_
56×	_	_
56	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
96	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
96	_	_
]	_	_
×	_	_
3	_	_
28×	_	_
28	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
192	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
192	_	_
]	_	_
×	_	_
4	_	_
14×	_	_
14	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
384	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
384	_	_
]	_	_
×	_	_
6	_	_
7×	_	_
7	_	_
max	_	_
pool	_	_
3×	_	_
3	_	_
,	_	_
stride	_	_
2	_	_
[	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
768	_	_
conv	_	_
,	_	_
3×3	_	_
,	_	_
768	_	_
]	_	_
×	_	_
3	_	_
1×	_	_
1	_	_
global	_	_
avg	_	_
pool	_	_
9×9	_	_
10−d	_	_
fc	_	_
,	_	_
softmax	_	_
Table	_	_
4	_	_
:	_	_
Network	_	_
Structure	_	_
Table	_	_
(	_	_
3	_	_
)	_	_
shows	_	_
the	_	_
results	_	_
for	_	_
the	_	_
three	_	_
datasets	_	_
,	_	_
and	_	_
compares	_	_
the	_	_
results	_	_
when	_	_
training	_	_
and	_	_
inference	_	_
are	_	_
done	_	_
in	_	_
the	_	_
standard	_	_
way	_	_
versus	_	_
using	_	_
balanced	_	_
batches	_	_
.	_	_

#92
As	_	_
with	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
,	_	_
training	_	_
the	_	_
network	_	_
using	_	_
balanced	_	_
batches	_	_
doesn’t	_	_
change	_	_
the	_	_
results	_	_
if	_	_
inference	_	_
is	_	_
carried	_	_
out	_	_
on	_	_
individual	_	_
images	_	_
using	_	_
fixed	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#93
However	_	_
,	_	_
if	_	_
the	_	_
network	_	_
trained	_	_
on	_	_
balanced	_	_
batches	_	_
,	_	_
is	_	_
also	_	_
tested	_	_
using	_	_
balanced	_	_
test	_	_
batches	_	_
,	_	_
the	_	_
error	_	_
rate	_	_
is	_	_
reduced	_	_
considerably	_	_
.	_	_

#94
When	_	_
using	_	_
balanced	_	_
batches	_	_
,	_	_
the	_	_
batch	_	_
size	_	_
is	_	_
equal	_	_
to	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
,	_	_
and	_	_
therefore	_	_
,	_	_
the	_	_
batch	_	_
sizes	_	_
used	_	_
here	_	_
are	_	_
10	_	_
,	_	_
50	_	_
,	_	_
and	_	_
100	_	_
images	_	_
,	_	_
for	_	_
datasets	_	_
with	_	_
10	_	_
,	_	_
50	_	_
,	_	_
and	_	_
100	_	_
classes	_	_
respectively	_	_
.	_	_

#95
From	_	_
table	_	_
(	_	_
3	_	_
)	_	_
The	_	_
relative	_	_
reduction	_	_
in	_	_
the	_	_
error	_	_
when	_	_
training	_	_
and	_	_
inference	_	_
were	_	_
done	_	_
using	_	_
balanced	_	_
batches	_	_
,	_	_
is	_	_
very	_	_
dependent	_	_
on	_	_
the	_	_
batch	_	_
size	_	_
.	_	_

#96
For	_	_
the	_	_
10-classes	_	_
dataset	_	_
the	_	_
reduction	_	_
was	_	_
81	_	_
%	_	_
,	_	_
for	_	_
the	_	_
50-classes	_	_
dataset	_	_
it	_	_
was	_	_
54	_	_
%	_	_
,	_	_
and	_	_
for	_	_
the	_	_
100-classes	_	_
dataset	_	_
it	_	_
was	_	_
33	_	_
%	_	_
.	_	_

#97
This	_	_
is	_	_
not	_	_
surprising	_	_
because	_	_
balancing	_	_
the	_	_
test	_	_
batches	_	_
means	_	_
that	_	_
the	_	_
real	_	_
task	_	_
here	_	_
is	_	_
not	_	_
really	_	_
classifying	_	_
the	_	_
images	_	_
but	_	_
rather	_	_
identifying	_	_
the	_	_
identity	_	_
of	_	_
each	_	_
image	_	_
in	_	_
a	_	_
balanced	_	_
batch	_	_
.	_	_

#98
And	_	_
it	_	_
makes	_	_
sense	_	_
that	_	_
this	_	_
task	_	_
gets	_	_
harder	_	_
as	_	_
the	_	_
size	_	_
of	_	_
the	_	_
batch	_	_
gets	_	_
bigger	_	_
,	_	_
and	_	_
as	_	_
the	_	_
batch	_	_
size	_	_
(	_	_
thus	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
)	_	_
goes	_	_
to	_	_
infinity	_	_
(	_	_
or	_	_
a	_	_
large	_	_
number	_	_
)	_	_
the	_	_
performance	_	_
measured	_	_
using	_	_
balanced	_	_
batches	_	_
converges	_	_
to	_	_
the	_	_
performance	_	_
measured	_	_
on	_	_
individual	_	_
images	_	_
.	_	_

#99
It	_	_
is	_	_
interesting	_	_
to	_	_
notice	_	_
that	_	_
the	_	_
81	_	_
%	_	_
conditional	_	_
gains	_	_
for	_	_
the	_	_
10-classes	_	_
dataset	_	_
is	_	_
very	_	_
close	_	_
to	_	_
the	_	_
80	_	_
%	_	_
conditional	_	_
gains	_	_
obtained	_	_
with	_	_
the	_	_
CIFAR10	_	_
data	_	_
sets	_	_
,	_	_
which	_	_
also	_	_
has	_	_
10	_	_
classes	_	_
.	_	_

#100
Figure	_	_
(	_	_
3	_	_
)	_	_
shows	_	_
the	_	_
error	_	_
curves	_	_
measured	_	_
on	_	_
the	_	_
central	_	_
crop	_	_
for	_	_
all	_	_
three	_	_
datasets	_	_
as	_	_
training	_	_
progresses	_	_
.	_	_

#101
Again	_	_
,	_	_
the	_	_
red	_	_
and	_	_
blue	_	_
curves	_	_
show	_	_
that	_	_
training	_	_
the	_	_
network	_	_
on	_	_
balanced	_	_
batches	_	_
will	_	_
produce	_	_
similar	_	_
results	_	_
to	_	_
training	_	_
it	_	_
on	_	_
randomly	_	_
constructed	_	_
batches	_	_
if	_	_
the	_	_
performance	_	_
is	_	_
measured	_	_
on	_	_
individual	_	_
test	_	_
images	_	_
using	_	_
the	_	_
running	_	_
averages	_	_
of	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#102
The	_	_
black	_	_
curves	_	_
show	_	_
the	_	_
gains	_	_
when	_	_
both	_	_
training	_	_
and	_	_
inference	_	_
are	_	_
done	_	_
on	_	_
balanced	_	_
batches	_	_
using	_	_
the	_	_
batch’s	_	_
own	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#103
These	_	_
learning	_	_
curves	_	_
agree	_	_
with	_	_
the	_	_
final	_	_
results	_	_
shown	_	_
in	_	_
the	_	_
three	_	_
rows	_	_
of	_	_
table	_	_
(	_	_
3	_	_
)	_	_
.	_	_

#104
3.3	_	_
Shuffling	_	_
the	_	_
balanced	_	_
test	_	_
batches	_	_

#105
In	_	_
the	_	_
training	_	_
phase	_	_
shuffling	_	_
the	_	_
training	_	_
images	_	_
improves	_	_
the	_	_
results	_	_
if	_	_
BN	_	_
is	_	_
implemented	_	_
in	_	_
the	_	_
network	_	_
.	_	_

#106
The	_	_
results	_	_
also	_	_
show	_	_
that	_	_
if	_	_
balanced	_	_
test	_	_
batches	_	_
are	_	_
shuffled	_	_
,	_	_
the	_	_
results	_	_
will	_	_
improve	_	_
Figure	_	_
4	_	_
:	_	_
a	_	_
balanced	_	_
test	_	_
batch	_	_
made	_	_
of	_	_
images	_	_
classified	_	_
with	_	_
high	_	_
confidence	_	_
,	_	_
left	_	_
:	_	_
a	_	_
misclassified	_	_
test	_	_
image	_	_
made	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
batch	_	_
,	_	_
middle	_	_
:	_	_
correctly	_	_
classified	_	_
test	_	_
image	_	_
(	_	_
belong	_	_
to	_	_
class	_	_
no	_	_
.	_	_

#107
4	_	_
)	_	_
made	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
batch	_	_
,	_	_
right	_	_
:	_	_
2	_	_
misclassified	_	_
test	_	_
images	_	_
made	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
batch	_	_
.	_	_

#108
even	_	_
further	_	_
.	_	_

#109
It	_	_
seems	_	_
that	_	_
there	_	_
is	_	_
an	_	_
advantage	_	_
in	_	_
presenting	_	_
the	_	_
different	_	_
crops	_	_
of	_	_
a	_	_
test	_	_
image	_	_
with	_	_
different	_	_
batches	_	_
rather	_	_
than	_	_
presenting	_	_
all	_	_
of	_	_
them	_	_
with	_	_
the	_	_
same	_	_
batch	_	_
.	_	_

#110
One	_	_
possible	_	_
explanation	_	_
is	_	_
that	_	_
if	_	_
images	_	_
are	_	_
not	_	_
shuffled	_	_
,	_	_
the	_	_
current	_	_
balanced	_	_
batch	_	_
may	_	_
contain	_	_
multiple	_	_
similar	_	_
images	_	_
that	_	_
belong	_	_
to	_	_
different	_	_
classes	_	_
,	_	_
and	_	_
that	_	_
may	_	_
confuse	_	_
the	_	_
network	_	_
.	_	_

#111
By	_	_
shuffling	_	_
the	_	_
images	_	_
at	_	_
inference	_	_
time	_	_
,	_	_
the	_	_
chances	_	_
of	_	_
presenting	_	_
all	_	_
the	_	_
crops	_	_
of	_	_
an	_	_
image	_	_
with	_	_
another	_	_
similar	_	_
image	_	_
from	_	_
a	_	_
different	_	_
class	_	_
will	_	_
be	_	_
reduced	_	_
.	_	_

#112
Table	_	_
(	_	_
5	_	_
)	_	_
shows	_	_
the	_	_
error	_	_
rate	_	_
measured	_	_
on	_	_
shuffled	_	_
and	_	_
balanced	_	_
test	_	_
batches	_	_
for	_	_
the	_	_
three	_	_
datasets	_	_
sampled	_	_
from	_	_
ImageNet	_	_
.	_	_

#113
Training	_	_
&	_	_
Inference	_	_

#114
10	_	_
Classes	_	_

#115
dataset	_	_

#116
50	_	_
Classes	_	_

#117
dataset	_	_

#118
100	_	_
Classes	_	_

#119
dataset	_	_
Balanced	_	_
Batches	_	_
0.2	_	_
%	_	_
2.04	_	_
%	_	_
5.5	_	_
%	_	_
Table	_	_
5	_	_
:	_	_
Results	_	_
obtained	_	_
by	_	_
shuffling	_	_
balanced	_	_
test	_	_
batches	_	_

#120
4	_	_
Inference	_	_
using	_	_
the	_	_
batch’s	_	_

#121
own	_	_
means	_	_
and	_	_
variances	_	_
The	_	_
results	_	_
in	_	_
the	_	_
previous	_	_
sections	_	_
show	_	_
that	_	_
if	_	_
batches	_	_
are	_	_
structured	_	_
as	_	_
balanced	_	_
batches	_	_
at	_	_
the	_	_
training	_	_
and	_	_
inference	_	_
stages	_	_
,	_	_
then	_	_
the	_	_
results	_	_
improve	_	_
considerably	_	_
.	_	_

#122
The	_	_
experiment	_	_
presented	_	_
here	_	_
shows	_	_
how	_	_
the	_	_
network	_	_
uses	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
balanced	_	_
test	_	_
batch	_	_
in	_	_
the	_	_
classification	_	_
process	_	_
.	_	_

#123
This	_	_
experiment	_	_
is	_	_
done	_	_
on	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
using	_	_
a	_	_
single	_	_
balanced	_	_
test	_	_
batch	_	_
made	_	_
of	_	_
images	_	_
identified	_	_
with	_	_
high	_	_
confidence	_	_
by	_	_
the	_	_
network	_	_
.	_	_

#124
In	_	_
the	_	_
first	_	_
run	_	_
a	_	_
misclassified	_	_
image	_	_
(	_	_
using	_	_
standard	_	_
inference	_	_
)	_	_
is	_	_
selected	_	_
and	_	_
used	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
created	_	_
test	_	_
batch	_	_
by	_	_
replacing	_	_
one	_	_
image	_	_
at	_	_
a	_	_
time	_	_
.	_	_

#125
In	_	_
the	_	_
second	_	_
run	_	_
an	_	_
image	_	_
that	_	_
has	_	_
been	_	_
classified	_	_
by	_	_
the	_	_
network	_	_
with	_	_
high	_	_
confidence	_	_
(	_	_
using	_	_
standard	_	_
inference	_	_
)	_	_
is	_	_
made	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
batch	_	_
.	_	_

#126
Because	_	_
the	_	_
number	_	_
of	_	_
classes	_	_
for	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
is	_	_
10	_	_
,	_	_
then	_	_
each	_	_
time	_	_
the	_	_
selected	_	_
image	_	_
will	_	_
replace	_	_
one	_	_
of	_	_
the	_	_
10	_	_
images	_	_
that	_	_
make	_	_
up	_	_
the	_	_
balanced	_	_
batch	_	_
,	_	_
and	_	_
the	_	_
classification	_	_
of	_	_
all	_	_
the	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
will	_	_
be	_	_
reported	_	_
.	_	_

#127
Figure	_	_
(	_	_
4	_	_
)	_	_
shows	_	_
the	_	_
classification	_	_
results	_	_
,	_	_
where	_	_
each	_	_
column	_	_
shows	_	_
a	_	_
single	_	_
step	_	_
,	_	_
and	_	_
represents	_	_
a	_	_
single	_	_
batch	_	_
.	_	_

#128
Figure	_	_
(	_	_
4	_	_
,	_	_
left	_	_
)	_	_
shows	_	_
what	_	_
happened	_	_
to	_	_
the	_	_
weak	_	_
(	_	_
misclassified	_	_
)	_	_
image	_	_
,	_	_
where	_	_
each	_	_
time	_	_
the	_	_
network	_	_
classifies	_	_
the	_	_
image	_	_
to	_	_
belong	_	_
to	_	_
the	_	_
missing	_	_
class	_	_
.	_	_

#129
Figure	_	_
(	_	_
4	_	_
,	_	_
middle	_	_
)	_	_
shows	_	_
the	_	_
results	_	_
for	_	_
the	_	_
strong	_	_
(	_	_
classified	_	_
with	_	_
high	_	_
confidence	_	_
)	_	_
image	_	_
,	_	_
where	_	_
the	_	_
image	_	_
was	_	_
always	_	_
classified	_	_
correctly	_	_
,	_	_
and	_	_
the	_	_
identity	_	_
of	_	_
the	_	_
image	_	_
wasn’t	_	_
changed	_	_
each	_	_
time	_	_
to	_	_
match	_	_
the	_	_
identity	_	_
of	_	_
the	_	_
replaced	_	_
image	_	_
.	_	_

#130
The	_	_
third	_	_
run	_	_
is	_	_
a	_	_
generalization	_	_
of	_	_
the	_	_
first	_	_
one	_	_
,	_	_
where	_	_
two	_	_
weak	_	_
images	_	_
are	_	_
made	_	_
to	_	_
circulate	_	_
through	_	_
the	_	_
balanced	_	_
batch	_	_
,	_	_
replacing	_	_
two	_	_
images	_	_
at	_	_
a	_	_
time	_	_
,	_	_
and	_	_
figure	_	_
(	_	_
4	_	_
,	_	_
right	_	_
)	_	_
shows	_	_
how	_	_
the	_	_
network	_	_
has	_	_
interpreted	_	_
their	_	_
identity	_	_
to	_	_
replace	_	_
the	_	_
missing	_	_
classes	_	_
(	_	_
or	_	_
one	_	_
of	_	_
them	_	_
)	_	_
.	_	_

#131
The	_	_
results	_	_
show	_	_
that	_	_
when	_	_
the	_	_
network	_	_
is	_	_
confident	_	_
about	_	_
the	_	_
identity	_	_
of	_	_
the	_	_
test	_	_
image	_	_
,	_	_
then	_	_
it	_	_
ignores	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
batch	_	_
.	_	_

#132
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
if	_	_
the	_	_
network	_	_
is	_	_
not	_	_
confident	_	_
about	_	_
the	_	_
identity	_	_
of	_	_
the	_	_
image	_	_
,	_	_
then	_	_
it	_	_
relies	_	_
mainly	_	_
on	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
batch	_	_
to	_	_
classify	_	_
that	_	_
image	_	_
.	_	_

#133
Based	_	_
on	_	_
the	_	_
results	_	_
shown	_	_
in	_	_
figure	_	_
(	_	_
4	_	_
)	_	_
,	_	_
the	_	_
network	_	_
uses	_	_
the	_	_
structure	_	_
of	_	_
a	_	_
balanced	_	_
batch	_	_
as	_	_
follows	_	_
:	_	_
if	_	_
a	_	_
balanced	_	_
test	_	_
batch	_	_
made	_	_
up	_	_
of	_	_
n	_	_
images	_	_
is	_	_
passed	_	_
through	_	_
the	_	_
network	_	_
,	_	_
and	_	_
the	_	_
network	_	_
is	_	_
confident	_	_
of	_	_
the	_	_
identity	_	_
of	_	_
m	_	_
out	_	_
of	_	_
the	_	_
total	_	_
n	_	_
images	_	_
,	_	_
then	_	_
the	_	_
network	_	_
will	_	_
restrict	_	_
the	_	_
prediction	_	_
of	_	_
the	_	_
remaining	_	_
n	_	_
−m	_	_
images	_	_
to	_	_
the	_	_
remaining	_	_
n−m	_	_
classes	_	_
.	_	_

#134
(	_	_
m	_	_
out	_	_
of	_	_
n	_	_
images	_	_
identified	_	_
)	_	_
&	_	_
&	_	_
(	_	_
batch	_	_
is	_	_
balanced	_	_
)	_	_
⇒	_	_
(	_	_
remaining	_	_
images	_	_
belong	_	_
to	_	_
remaining	_	_
n-m	_	_
classes	_	_
)	_	_
Figure	_	_
(	_	_
4	_	_
,	_	_
left	_	_
)	_	_
showed	_	_
how	_	_
the	_	_
network	_	_
was	_	_
only	_	_
confident	_	_
of	_	_
the	_	_
identity	_	_
of	_	_
9	_	_
out	_	_
of	_	_
the	_	_
10	_	_
images	_	_
,	_	_
and	_	_
therefore	_	_
the	_	_
tenth	_	_
image	_	_
was	_	_
always	_	_
interpreted	_	_
as	_	_
the	_	_
missing	_	_
class	_	_
.	_	_

#135
Figure	_	_
(	_	_
4	_	_
,	_	_
right	_	_
)	_	_
showed	_	_
how	_	_
the	_	_
network	_	_
was	_	_
only	_	_
confident	_	_
of	_	_
the	_	_
identity	_	_
of	_	_
8	_	_
out	_	_
of	_	_
the	_	_
10	_	_
images	_	_
,	_	_
and	_	_
therefore	_	_
the	_	_
identity	_	_
Figure	_	_
5	_	_
:	_	_
inference	_	_
for	_	_
a	_	_
balanced	_	_
batch	_	_
,	_	_
left	_	_
:	_	_
with	_	_
precomputed	_	_
fixed	_	_
means	_	_
and	_	_
variances	_	_
,	_	_
right	_	_
:	_	_
with	_	_
current	_	_
means	_	_
and	_	_
variances	_	_
.	_	_

#136
of	_	_
the	_	_
2	_	_
remaining	_	_
images	_	_
were	_	_
always	_	_
restricted	_	_
to	_	_
the	_	_
2	_	_
missing	_	_
classes	_	_
.	_	_

#137
This	_	_
logic	_	_
(	_	_
highlighted	_	_
in	_	_
blue	_	_
)	_	_
can	_	_
not	_	_
be	_	_
implemented	_	_
by	_	_
the	_	_
network	_	_
if	_	_
the	_	_
prediction	_	_
of	_	_
one	_	_
image	_	_
is	_	_
independent	_	_
from	_	_
all	_	_
the	_	_
other	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
,	_	_
and	_	_
therefore	_	_
it	_	_
can	_	_
not	_	_
be	_	_
implemented	_	_
without	_	_
BN	_	_
.	_	_

#138
The	_	_
network	_	_
was	_	_
able	_	_
to	_	_
implement	_	_
this	_	_
logic	_	_
because	_	_
it	_	_
was	_	_
only	_	_
exposed	_	_
to	_	_
balanced	_	_
batches	_	_
in	_	_
the	_	_
training	_	_
stage	_	_
,	_	_
and	_	_
because	_	_
BN	_	_
gave	_	_
the	_	_
network	_	_
the	_	_
capacity	_	_
to	_	_
learn	_	_
something	_	_
based	_	_
on	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
training	_	_
batches	_	_
by	_	_
using	_	_
the	_	_
shared	_	_
means	_	_
and	_	_
variances	_	_
as	_	_
a	_	_
communication	_	_
tool	_	_
between	_	_
the	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
.	_	_

#139
Figure	_	_
(	_	_
5	_	_
)	_	_
explains	_	_
the	_	_
process	_	_
of	_	_
making	_	_
a	_	_
decision	_	_
about	_	_
a	_	_
weak	_	_
image	_	_
in	_	_
a	_	_
balanced	_	_
batch	_	_
,	_	_
where	_	_
the	_	_
network	_	_
snoops	_	_
on	_	_
the	_	_
decisions	_	_
about	_	_
the	_	_
other	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
through	_	_
the	_	_
shared	_	_
means	_	_
and	_	_
variances	_	_
to	_	_
help	_	_
decide	_	_
on	_	_
the	_	_
weak	_	_
image	_	_
.	_	_

#140
What	_	_
is	_	_
interesting	_	_
is	_	_
that	_	_
this	_	_
process	_	_
happens	_	_
in	_	_
one	_	_
forward	_	_
pass	_	_
,	_	_
where	_	_
the	_	_
network	_	_
identifies	_	_
all	_	_
images	_	_
in	_	_
the	_	_
current	_	_
batch	_	_
at	_	_
the	_	_
same	_	_
time	_	_
.	_	_

#141
As	_	_
signals	_	_
travel	_	_
forward	_	_
,	_	_
the	_	_
network	_	_
found	_	_
a	_	_
way	_	_
to	_	_
use	_	_
the	_	_
shared	_	_
means	_	_
and	_	_
variances	_	_
at	_	_
each	_	_
layer	_	_
to	_	_
help	_	_
guide	_	_
the	_	_
prediction	_	_
of	_	_
all	_	_
the	_	_
images	_	_
in	_	_
the	_	_
batch	_	_
,	_	_
based	_	_
on	_	_
the	_	_
fact	_	_
that	_	_
batches	_	_
are	_	_
balanced	_	_
.	_	_

#142
The	_	_
results	_	_
presented	_	_
here	_	_
show	_	_
that	_	_
with	_	_
BN	_	_
,	_	_
controlling	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
training	_	_
batches	_	_
,	_	_
adds	_	_
another	_	_
layer	_	_
of	_	_
control	_	_
over	_	_
what	_	_
the	_	_
network	_	_
can	_	_
learn	_	_
.	_	_

#143
5	_	_
Difficulty	_	_
balancing	_	_
the	_	_
test	_	_

#144
batches	_	_
It	_	_
is	_	_
tempting	_	_
to	_	_
try	_	_
to	_	_
translate	_	_
these	_	_
big	_	_
conditional	_	_
gains	_	_
into	_	_
actual	_	_
gains	_	_
,	_	_
however	_	_
these	_	_
results	_	_
are	_	_
very	_	_
hard	_	_
to	_	_
achieve	_	_
in	_	_
practice	_	_
because	_	_
structuring	_	_
test	_	_
images	_	_
as	_	_
balanced	_	_
batches	_	_
requires	_	_
the	_	_
test	_	_
image	_	_
labels	_	_
.	_	_

#145
One	_	_
attempt	_	_
is	_	_
to	_	_
use	_	_
the	_	_
labels	_	_
generated	_	_
by	_	_
standard	_	_
inference	_	_
to	_	_
balance	_	_
the	_	_
test	_	_
batches	_	_
,	_	_
and	_	_
then	_	_
use	_	_
these	_	_
semi-balanced	_	_
batches	_	_
to	_	_
generate	_	_
more	_	_
accurate	_	_
labels	_	_
,	_	_
where	_	_
the	_	_
process	_	_
is	_	_
repeated	_	_
until	_	_
no	_	_
improvements	_	_
can	_	_
be	_	_
achieved	_	_
.	_	_

#146
Figure	_	_
(	_	_
6	_	_
)	_	_
shows	_	_
this	_	_
process	_	_
repeated	_	_
20	_	_
times	_	_
for	_	_
the	_	_
CIFAR10	_	_
dataset	_	_
,	_	_
but	_	_
unfortunately	_	_
it	_	_
didn’t	_	_
lead	_	_
to	_	_
more	_	_
accurate	_	_
results	_	_
(	_	_
better	_	_
labels	_	_
)	_	_
.	_	_

#147
The	_	_
error	_	_
rate	_	_
stayed	_	_
almost	_	_
constant	_	_
as	_	_
a	_	_
horizontal	_	_
line	_	_
.	_	_

#148
From	_	_
table	_	_
(	_	_
2	_	_
)	_	_
the	_	_
error	_	_
rate	_	_
using	_	_
standard	_	_
inference	_	_
is	_	_
3.89	_	_
%	_	_
,	_	_
which	_	_
means	_	_
3.89	_	_
%	_	_
of	_	_
the	_	_
images	_	_
will	_	_
be	_	_
misclassified	_	_
,	_	_
and	_	_
those	_	_
are	_	_
the	_	_
toughest	_	_
images	_	_
where	_	_
inference	_	_
using	_	_
fully	_	_
balanced	_	_
batches	_	_
made	_	_
gains	_	_
to	_	_
reduce	_	_
the	_	_
error	_	_
to	_	_
0.69	_	_
%	_	_
.	_	_

#149
From	_	_
the	_	_
previous	_	_
section	_	_
,	_	_
figure	_	_
(	_	_
4	_	_
)	_	_
shows	_	_
what	_	_
happens	_	_
to	_	_
misclassified	_	_
images	_	_
when	_	_
placed	_	_
in	_	_
semi-balanced	_	_
batches	_	_
,	_	_
the	_	_
network	_	_
often	_	_
misinterprets	_	_
their	_	_
identity	_	_
to	_	_
replace	_	_
the	_	_
missing	_	_
class	_	_
.	_	_

#150
Therefore	_	_
,	_	_
standard	_	_
inference	_	_
is	_	_
not	_	_
adequate	_	_
to	_	_
solve	_	_
the	_	_
problem	_	_
of	_	_
balancing	_	_
the	_	_
test	_	_
batches	_	_
,	_	_
because	_	_
it	_	_
will	_	_
misclassify	_	_
the	_	_
important	_	_
images	_	_
,	_	_
where	_	_
gains	_	_
need	_	_
to	_	_
be	_	_
made	_	_
.	_	_

#151
Figure	_	_
6	_	_
:	_	_
the	_	_
repeated	_	_
process	_	_
of	_	_
balancing	_	_
test	_	_
batches	_	_
starting	_	_
from	_	_
labels	_	_
generated	_	_
by	_	_
standard	_	_
inference	_	_
fails	_	_
.	_	_