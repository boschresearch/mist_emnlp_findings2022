#0
ar	_	_
X	_	_
iv	_	_
:1	_	_
5	_	_
.	_	_

#1
1v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
4	_	_
M	_	_
ay	_	_
2	_	_
Energy	_	_
Efficient	_	_
Hadamard	_	_
Neural	_	_
Networks	_	_
T.	_	_
Ceren	_	_
Deveci	_	_
,	_	_
Serdar	_	_
Cakir	_	_
and	_	_
A.	_	_
Enis	_	_
Cetin	_	_
April	_	_
2018	_	_

#2
Abstract	_	_

#3
Deep	_	_
learning	_	_
has	_	_
made	_	_
significant	_	_
improvements	_	_
at	_	_
many	_	_
image	_	_
processing	_	_
tasks	_	_
in	_	_
recent	_	_
years	_	_
,	_	_
such	_	_
as	_	_
image	_	_
classification	_	_
,	_	_
object	_	_
recognition	_	_
and	_	_
object	_	_
detection	_	_
.	_	_

#4
Convolutional	_	_
neural	_	_
networks	_	_
(	_	_
CNN	_	_
)	_	_
,	_	_
which	_	_
is	_	_
a	_	_
popular	_	_
deep	_	_
learning	_	_
architecture	_	_
designed	_	_
to	_	_
process	_	_
data	_	_
in	_	_
multiple	_	_
array	_	_
form	_	_
,	_	_
show	_	_
great	_	_
success	_	_
to	_	_
almost	_	_
all	_	_
detection	_	_
&	_	_
recognition	_	_
problems	_	_
and	_	_
computer	_	_
vision	_	_
tasks	_	_
.	_	_

#5
However	_	_
,	_	_
the	_	_
number	_	_
of	_	_
parameters	_	_
in	_	_
a	_	_
CNN	_	_
is	_	_
too	_	_
high	_	_
such	_	_
that	_	_
the	_	_
computers	_	_
require	_	_
more	_	_
energy	_	_
and	_	_
larger	_	_
memory	_	_
size	_	_
.	_	_

#6
In	_	_
order	_	_
to	_	_
solve	_	_
this	_	_
problem	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
energy	_	_
efficient	_	_
model	_	_
Binary	_	_
Weight	_	_
and	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
(	_	_
BWHIN	_	_
)	_	_
,	_	_
which	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
Binary	_	_
Weight	_	_
Network	_	_
(	_	_
BWN	_	_
)	_	_
and	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
(	_	_
HIN	_	_
)	_	_
.	_	_

#7
It	_	_
is	_	_
observed	_	_
that	_	_
energy	_	_
efficiency	_	_
is	_	_
achieved	_	_
with	_	_
a	_	_
slight	_	_
sacrifice	_	_
at	_	_
classification	_	_
accuracy	_	_
.	_	_

#8
Among	_	_
all	_	_
energy	_	_
efficient	_	_
networks	_	_
,	_	_
our	_	_
novel	_	_
ensemble	_	_
model	_	_
outperforms	_	_
other	_	_
energy	_	_
efficient	_	_
models	_	_
.	_	_

#9
1	_	_
Introduction	_	_

#10
In	_	_
the	_	_
recent	_	_
years	_	_
,	_	_
artificial	_	_
neural	_	_
networks	_	_
(	_	_
ANN	_	_
)	_	_
have	_	_
become	_	_
very	_	_
popular	_	_
amongst	_	_
researchers	_	_
due	_	_
to	_	_
their	_	_
great	_	_
success	_	_
on	_	_
image	_	_
classification	_	_
,	_	_
feature	_	_
extraction	_	_
,	_	_
segmentation	_	_
,	_	_
object	_	_
recognition	_	_
and	_	_
detection	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#11
Deep	_	_
learning	_	_
is	_	_
a	_	_
more	_	_
sophisticated	_	_
and	_	_
particular	_	_
form	_	_
of	_	_
machine	_	_
learning	_	_
,	_	_
which	_	_
enables	_	_
the	_	_
user	_	_
to	_	_
form	_	_
complex	_	_
models	_	_
which	_	_
are	_	_
composed	_	_
of	_	_
multiple	_	_
hidden	_	_
layers	_	_
.	_	_

#12
Deep	_	_
learning	_	_
methods	_	_
have	_	_
enhanced	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
in	_	_
object	_	_
recognition	_	_
&	_	_
detection	_	_
and	_	_
computer	_	_
vision	_	_
tasks	_	_
.	_	_

#13
Deep	_	_
learning	_	_
is	_	_
also	_	_
advantageous	_	_
for	_	_
processing	_	_
raw	_	_
data	_	_
such	_	_
that	_	_
it	_	_
can	_	_
automatically	_	_
find	_	_
a	_	_
suitable	_	_
representation	_	_
for	_	_
detection	_	_
or	_	_
classification	_	_
[	_	_
2	_	_
]	_	_
.	_	_

#14
Convolutional	_	_
neural	_	_
network	_	_
(	_	_
CNN	_	_
)	_	_
is	_	_
a	_	_
specific	_	_
deep	_	_
learning	_	_
architecture	_	_
for	_	_
processing	_	_
data	_	_
which	_	_
is	_	_
composed	_	_
of	_	_
multiple	_	_
arrays	_	_
.	_	_

#15
Images	_	_
can	_	_
be	_	_
a	_	_
good	_	_
example	_	_
of	_	_
input	_	_
to	_	_
CNN	_	_
with	_	_
its	_	_
2D	_	_
grid	_	_
of	_	_
pixels	_	_
.	_	_

#16
Convolutional	_	_
Neural	_	_
Networks	_	_
have	_	_
become	_	_
popular	_	_
with	_	_
the	_	_
introduction	_	_
of	_	_
its	_	_
modern	_	_
version	_	_
LeNet-5	_	_
for	_	_
the	_	_
recognition	_	_
of	_	_
handwritten	_	_
numbers	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#17
Besides	_	_
,	_	_
AlexNet	_	_
,	_	_
the	_	_
winner	_	_
of	_	_
ILSVRC	_	_
object	_	_
recognition	_	_
challenge	_	_
in	_	_
2012	_	_
,	_	_
aroused	_	_
both	_	_
commercial	_	_
and	_	_
scientific	_	_
interest	_	_
in	_	_
CNN	_	_
and	_	_
it	_	_
is	_	_
the	_	_
main	_	_
reason	_	_
of	_	_
the	_	_
intense	_	_
popularity	_	_
of	_	_
CNN	_	_
architectures	_	_
for	_	_
deep	_	_
learning	_	_
applications	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#18
The	_	_
usage	_	_
of	_	_
CNN	_	_
in	_	_
AlexNet	_	_
obtained	_	_
remarkable	_	_
results	_	_
such	_	_
that	_	_
the	_	_
network	_	_
halved	_	_
the	_	_
error	_	_
rate	_	_
of	_	_
its	_	_
previous	_	_
competitors	_	_
.	_	_

#19
Thanks	_	_
to	_	_
this	_	_
great	_	_
achievement	_	_
,	_	_
CNN	_	_
is	_	_
the	_	_
most	_	_
preferred	_	_
approach	_	_
for	_	_
most	_	_
detection	_	_
and	_	_
recognition	_	_
problems	_	_
and	_	_
computer	_	_
vision	_	_
tasks	_	_
.	_	_

#20
Although	_	_
CNNs	_	_
are	_	_
suitable	_	_
for	_	_
efficient	_	_
hardware	_	_
implementations	_	_
such	_	_
as	_	_
in	_	_
GPUs	_	_
or	_	_
FPGAs	_	_
,	_	_
the	_	_
training	_	_
is	_	_
computationally	_	_
expensive	_	_
due	_	_
to	_	_
the	_	_
high	_	_
number	_	_
of	_	_
parameters	_	_
.	_	_

#21
As	_	_
a	_	_
result	_	_
,	_	_
excessive	_	_
amount	_	_
of	_	_
energy	_	_
consumption	_	_
and	_	_
memory	_	_
usage	_	_
make	_	_
the	_	_
implementation	_	_
of	_	_
neural	_	_
networks	_	_
ineffective	_	_
.	_	_

#22
According	_	_
to	_	_
[	_	_
5	_	_
]	_	_
,	_	_
especially	_	_
matrix	_	_
multiplications	_	_
at	_	_
the	_	_
layers	_	_
of	_	_
a	_	_
neural	_	_
network	_	_
consume	_	_
too	_	_
much	_	_
energy	_	_
compared	_	_
to	_	_
addition	_	_
or	_	_
activation	_	_
function	_	_
and	_	_
becomes	_	_
a	_	_
major	_	_
problem	_	_
for	_	_
mobile	_	_
devices	_	_
with	_	_
limited	_	_
batteries	_	_
.	_	_

#23
As	_	_
a	_	_
result	_	_
,	_	_
replacing	_	_
the	_	_
multiplication	_	_
operation	_	_
becomes	_	_
the	_	_
main	_	_
concern	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
energy	_	_
efficiency	_	_
.	_	_

#24
Many	_	_
solutions	_	_
are	_	_
proposed	_	_
in	_	_
order	_	_
to	_	_
handle	_	_
the	_	_
energy	_	_
efficiency	_	_
problem	_	_
.	_	_

#25
An	_	_
energy	_	_
efficient	_	_
ℓ1-norm	_	_
based	_	_
operator	_	_
is	_	_
introduced	_	_
in	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#26
This	_	_
multiplier-less	_	_
operator	_	_
is	_	_
first	_	_
used	_	_
in	_	_
image	_	_
processing	_	_
tasks	_	_
such	_	_
as	_	_
cancer	_	_
cell	_	_
detection	_	_
and	_	_
licence	_	_
plate	_	_
recognition	_	_
in	_	_
[	_	_
7	_	_
,	_	_
8	_	_
]	_	_
.	_	_

#27
Multiplication-free	_	_
neural	_	_
networks	_	_
(	_	_
MFNN	_	_
)	_	_
based	_	_
on	_	_
this	_	_
operator	_	_
are	_	_
studied	_	_
in	_	_
[	_	_
9–11	_	_
]	_	_
.	_	_

#28
This	_	_
operator	_	_
achieved	_	_
promising	_	_
performance	_	_
especially	_	_
at	_	_
image	_	_
classification	_	_
on	_	_
MNIST	_	_
dataset	_	_
with	_	_
multi-layer	_	_
perceptron	_	_
(	_	_
MLP	_	_
)	_	_
models	_	_
[	_	_
10	_	_
]	_	_
.	_	_

#29
Han	_	_
et	_	_
al.	_	_
reduces	_	_
both	_	_
the	_	_
computation	_	_
and	_	_
storage	_	_
in	_	_
three	_	_
steps	_	_
:	_	_
First	_	_
,	_	_
the	_	_
network	_	_
is	_	_
trained	_	_
to	_	_
learn	_	_
the	_	_
important	_	_
connections	_	_
.	_	_

#30
Then	_	_
,	_	_
the	_	_
redundant	_	_
connections	_	_
are	_	_
discarded	_	_
for	_	_
a	_	_
sparser	_	_
network	_	_
.	_	_

#31
Finally	_	_
,	_	_
the	_	_
remaining	_	_
network	_	_
is	_	_
retrained	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#32
Using	_	_
neuromorphic	_	_
processors	_	_
with	_	_
its	_	_
special	_	_
chip	_	_
architecture	_	_
is	_	_
another	_	_
solution	_	_
for	_	_
energy	_	_
efficiency	_	_
[	_	_
12	_	_
]	_	_
.	_	_

#33
In	_	_
order	_	_
to	_	_
improve	_	_
energy	_	_
consumption	_	_
,	_	_
Sarwar	_	_
et	_	_
al.	_	_
exploits	_	_
the	_	_
error	_	_
resiliency	_	_
of	_	_
artificial	_	_
neurons	_	_
and	_	_
approximates	_	_
the	_	_
multiplication	_	_
operation	_	_
and	_	_
defines	_	_
a	_	_
Multiplier-less	_	_
Artificial	_	_
Neuron	_	_
(	_	_
MAN	_	_
)	_	_
by	_	_
using	_	_
Alphabet	_	_
Set	_	_
Multiplier	_	_
(	_	_
ASM	_	_
)	_	_
.	_	_

#34
In	_	_
ASM	_	_
,	_	_
the	_	_
multiplication	_	_
is	_	_
approximated	_	_
as	_	_
shifting	_	_
and	_	_
adding	_	_
in	_	_
bitwise	_	_
manner	_	_
with	_	_
some	_	_
previously	_	_
defined	_	_
alphabets	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#35
Binary	_	_
Weight	_	_
Networks	_	_
are	_	_
energy	_	_
efficient	_	_
neural	_	_
networks	_	_
whose	_	_
filters	_	_
at	_	_
the	_	_
convolutional	_	_
layers	_	_
are	_	_
approximated	_	_
as	_	_
binary	_	_
weights	_	_
.	_	_

#36
With	_	_
these	_	_
binary	_	_
weights	_	_
,	_	_
convolution	_	_
operation	_	_
can	_	_
be	_	_
computed	_	_
only	_	_
with	_	_
addition	_	_
and	_	_
subtraction	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#37
There	_	_
is	_	_
also	_	_
a	_	_
computationally	_	_
inexpensive	_	_
method	_	_
called	_	_
distillation	_	_
[	_	_
15	_	_
]	_	_
.	_	_

#38
A	_	_
very	_	_
large	_	_
network	_	_
or	_	_
an	_	_
emsemble	_	_
model	_	_
is	_	_
first	_	_
trained	_	_
and	_	_
transfers	_	_
its	_	_
knowledge	_	_
to	_	_
a	_	_
much	_	_
smaller	_	_
,	_	_
distilled	_	_
network	_	_
.	_	_

#39
Using	_	_
this	_	_
small	_	_
and	_	_
compact	_	_
model	_	_
is	_	_
much	_	_
more	_	_
advantageous	_	_
in	_	_
mobile	_	_
devices	_	_
in	_	_
terms	_	_
of	_	_
speed	_	_
and	_	_
memory	_	_
size	_	_
.	_	_

#40
This	_	_
method	_	_
shows	_	_
promising	_	_
results	_	_
at	_	_
image	_	_
processing	_	_
tasks	_	_
such	_	_
as	_	_
facial	_	_
expression	_	_
recognition	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#41
In	_	_
this	_	_
study	_	_
,	_	_
an	_	_
energy-efficient	_	_
neural	_	_
network	_	_
framework	_	_
based	_	_
on	_	_
Binary	_	_
Weight	_	_
Network	_	_
(	_	_
BWN	_	_
)	_	_
[	_	_
14	_	_
]	_	_
and	_	_
Hadamard	_	_
Transform	_	_
is	_	_
developed	_	_
.	_	_

#42
The	_	_
weights	_	_
at	_	_
the	_	_
convolutional	_	_
layers	_	_
of	_	_
the	_	_
BWN	_	_
are	_	_
approximated	_	_
to	_	_
binary	_	_
values	_	_
,	_	_
+1	_	_
or	_	_
−1	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#43
Instead	_	_
of	_	_
utilizing	_	_
the	_	_
original	_	_
images	_	_
as	_	_
network	_	_
inputs	_	_
,	_	_
the	_	_
network	_	_
is	_	_
modified	_	_
to	_	_
use	_	_
compressed	_	_
images	_	_
.	_	_

#44
This	_	_
network	_	_
is	_	_
called	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
(	_	_
HIN	_	_
)	_	_
.	_	_

#45
Since	_	_
Hadamard	_	_
transform	_	_
is	_	_
implemented	_	_
by	_	_
Fast	_	_
Walsh-Hadamard	_	_
Transform	_	_
algorithm	_	_
which	_	_
requires	_	_
only	_	_
addition	_	_
or	_	_
subtraction	_	_
[	_	_
17	_	_
]	_	_
,	_	_
the	_	_
HIN	_	_
network	_	_
is	_	_
energy	_	_
efficient	_	_
.	_	_

#46
Our	_	_
main	_	_
contribution	_	_
is	_	_
the	_	_
combination	_	_
of	_	_
BWN	_	_
and	_	_
HIN	_	_
models	_	_
:	_	_
Binary	_	_
Weight	_	_
and	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
(	_	_
BWHIN	_	_
)	_	_
.	_	_

#47
The	_	_
combination	_	_
is	_	_
carried	_	_
out	_	_
after	_	_
the	_	_
energy	_	_
efficient	_	_
layers	_	_
,	_	_
i.e.	_	_
convolutional	_	_
layers	_	_
with	_	_
two	_	_
different	_	_
averaging	_	_
techniques	_	_
.	_	_

#48
All	_	_
of	_	_
the	_	_
energy	_	_
efficient	_	_
models	_	_
are	_	_
also	_	_
examined	_	_
with	_	_
different	_	_
CNN	_	_
architectures	_	_
.	_	_

#49
One	_	_
of	_	_
them	_	_
(	_	_
ConvoPool-CNN	_	_
)	_	_
contains	_	_
pooling	_	_
layers	_	_
along	_	_
with	_	_
convolutional	_	_
layers	_	_
,	_	_
while	_	_
the	_	_
other	_	_
(	_	_
All-CNN	_	_
[	_	_
18	_	_
]	_	_
)	_	_
uses	_	_
strided	_	_
convolution	_	_
instead	_	_
of	_	_
pooling	_	_
layer	_	_
[	_	_
18	_	_
]	_	_
.	_	_

#50
We	_	_
analyze	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
models	_	_
on	_	_
two	_	_
famous	_	_
image	_	_
datasets	_	_
MNIST	_	_
and	_	_
CIFAR-10	_	_
.	_	_

#51
While	_	_
working	_	_
on	_	_
MNIST	_	_
,	_	_
we	_	_
also	_	_
study	_	_
the	_	_
effects	_	_
of	_	_
certain	_	_
hyperparameters	_	_
on	_	_
the	_	_
classification	_	_
accuracy	_	_
of	_	_
energy	_	_
efficient	_	_
neural	_	_
networks	_	_
.	_	_

#52
2	_	_
Methodology	_	_

#53
Firstly	_	_
,	_	_
we	_	_
investigate	_	_
the	_	_
efficiency	_	_
BWN	_	_
proposed	_	_
in	_	_
[	_	_
14	_	_
]	_	_
which	_	_
approximates	_	_
the	_	_
weights	_	_
to	_	_
binary	_	_
values	_	_
.	_	_

#54
Similar	_	_
to	_	_
BWN	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
(	_	_
HIN	_	_
)	_	_
which	_	_
uses	_	_
the	_	_
Hadamard-transformed	_	_
images	_	_
with	_	_
binarized	_	_
weights	_	_
.	_	_

#55
Lastly	_	_
,	_	_
a	_	_
combined	_	_
network	_	_
is	_	_
introduced	_	_
and	_	_
its	_	_
performance	_	_
is	_	_
compared	_	_
with	_	_
BWN	_	_
and	_	_
HIN	_	_
frameworks	_	_
.	_	_

#56
2.1	_	_
Binary	_	_
Weight	_	_
Networks	_	_
(	_	_
BWN	_	_
)	_	_

#57
Binary-Weight	_	_
Network	_	_
(	_	_
BWN	_	_
)	_	_
is	_	_
proposed	_	_
in	_	_
[	_	_
14	_	_
]	_	_
as	_	_
an	_	_
efficient	_	_
approximation	_	_
to	_	_
standard	_	_
convolutional	_	_
neural	_	_
networks	_	_
.	_	_

#58
In	_	_
BWNs	_	_
,	_	_
the	_	_
filters	_	_
,	_	_
i.e.	_	_
weights	_	_
of	_	_
the	_	_
CNN	_	_
are	_	_
approximated	_	_
to	_	_
binary	_	_
values	_	_
+1	_	_
and	_	_
−1	_	_
.	_	_

#59
While	_	_
a	_	_
conventional	_	_
convolutional	_	_
neural	_	_
network	_	_
needs	_	_
multiplication	_	_
,	_	_
addition	_	_
and	_	_
subtraction	_	_
for	_	_
convolution	_	_
operation	_	_
,	_	_
convolution	_	_
with	_	_
binary	_	_
weights	_	_
can	_	_
be	_	_
estimated	_	_
by	_	_
only	_	_
addition	_	_
and	_	_
subtraction	_	_
.	_	_

#60
Convolution	_	_
operation	_	_
can	_	_
be	_	_
approximated	_	_
as	_	_
I	_	_
∗W	_	_
≈	_	_
(	_	_
I⊕B	_	_
)	_	_
α	_	_
where	_	_
I	_	_
is	_	_
the	_	_
input	_	_
tensor	_	_
,	_	_
W	_	_
is	_	_
the	_	_
weight	_	_
(	_	_
filter	_	_
)	_	_
,	_	_
B	_	_
is	_	_
the	_	_
binary	_	_
weight	_	_
tensor	_	_
which	_	_
has	_	_
the	_	_
same	_	_
size	_	_
with	_	_
W	_	_
and	_	_
α	_	_
∈	_	_
R	_	_
+	_	_
is	_	_
the	_	_
scaling	_	_
factor	_	_
such	_	_
that	_	_
W	_	_
≈	_	_
αB	_	_
.	_	_

#61
⊕	_	_
operation	_	_
indicates	_	_
convolution	_	_
only	_	_
with	_	_
addition	_	_
and	_	_
subtraction	_	_
.	_	_

#62
Since	_	_
the	_	_
weight	_	_
values	_	_
are	_	_
only	_	_
+1	_	_
and	_	_
−1	_	_
,	_	_
convolution	_	_
operation	_	_
can	_	_
be	_	_
implemented	_	_
in	_	_
a	_	_
multiplier-less	_	_
manner	_	_
.	_	_

#63
After	_	_
solving	_	_
an	_	_
optimization	_	_
problem	_	_
to	_	_
estimate	_	_
W	_	_
,	_	_
B	_	_
and	_	_
α	_	_
is	_	_
found	_	_
as	_	_
:	_	_
B	_	_
=	_	_
sign	_	_
(	_	_
W	_	_
)	_	_
(	_	_
1	_	_
)	_	_
α	_	_
=	_	_
∑	_	_
|Wi|	_	_
n	_	_
=	_	_
n	_	_
||W	_	_
||ℓ1	_	_
(	_	_
2	_	_
)	_	_
In	_	_
Equation	_	_
2	_	_
,	_	_
n	_	_
=	_	_
c×w×h	_	_
where	_	_
c	_	_
is	_	_
the	_	_
channel	_	_
,	_	_
h	_	_
is	_	_
the	_	_
height	_	_
and	_	_
w	_	_
is	_	_
the	_	_
width	_	_
of	_	_
weight	_	_
tensor	_	_
W	_	_
,	_	_
and	_	_
of	_	_
B	_	_
as	_	_
well	_	_
.	_	_

#64
Equations	_	_
1	_	_
and	_	_
2	_	_
show	_	_
that	_	_
binary	_	_
weight	_	_
filter	_	_
is	_	_
simply	_	_
the	_	_
sign	_	_
of	_	_
weight	_	_
values	_	_
and	_	_
scaling	_	_
factor	_	_
is	_	_
the	_	_
average	_	_
of	_	_
absolute	_	_
weight	_	_
values	_	_
.	_	_

#65
While	_	_
training	_	_
a	_	_
CNN	_	_
with	_	_
binary	_	_
weights	_	_
,	_	_
the	_	_
weights	_	_
are	_	_
only	_	_
binarized	_	_
in	_	_
forward	_	_
pass	_	_
and	_	_
back	_	_
propagation	_	_
steps	_	_
of	_	_
the	_	_
training	_	_
.	_	_

#66
At	_	_
the	_	_
parameter-update	_	_
stage	_	_
,	_	_
the	_	_
real-valued	_	_
weights	_	_
(	_	_
not	_	_
binarized	_	_
)	_	_
are	_	_
used	_	_
.	_	_

#67
Another	_	_
significant	_	_
point	_	_
about	_	_
this	_	_
network	_	_
is	_	_
that	_	_
convolutional	_	_
filters	_	_
here	_	_
don’t	_	_
have	_	_
bias	_	_
terms	_	_
,	_	_
and	_	_
this	_	_
convolution	_	_
approximation	_	_
is	_	_
only	_	_
held	_	_
in	_	_
convolutional	_	_
layers	_	_
.	_	_

#68
Fully	_	_
connected	_	_
layers	_	_
still	_	_
do	_	_
have	_	_
bias	_	_
terms	_	_
and	_	_
standard	_	_
multiplication	_	_
.	_	_

#69
2.2	_	_
Hadamard-transformed	_	_
Image	_	_
Networks	_	_
(	_	_
HIN	_	_
)	_	_

#70
In	_	_
the	_	_
literature	_	_
,	_	_
transform	_	_
domain	_	_
features	_	_
are	_	_
also	_	_
used	_	_
as	_	_
input	_	_
data	_	_
in	_	_
deep	_	_
learning	_	_
structures	_	_
.	_	_

#71
Although	_	_
feature	_	_
extraction	_	_
requires	_	_
extra	_	_
computation	_	_
time	_	_
and	_	_
energy	_	_
,	_	_
transform	_	_
domain	_	_
features	_	_
as	_	_
input	_	_
to	_	_
the	_	_
convolutional	_	_
neural	_	_
networks	_	_
can	_	_
be	_	_
preferred	_	_
due	_	_
to	_	_
simpler	_	_
implementation	_	_
,	_	_
effective	_	_
computation	_	_
and	_	_
improved	_	_
model	_	_
accuracy	_	_
.	_	_

#72
Discrete	_	_
Cosine	_	_
Transform	_	_
(	_	_
DCT	_	_
)	_	_
domain	_	_
data	_	_
as	_	_
the	_	_
input	_	_
data	_	_
can	_	_
outperform	_	_
the	_	_
state-of-the-art	_	_
results	_	_
as	_	_
shown	_	_
in	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#73
Wu	_	_
et	_	_
al.	_	_
uses	_	_
DCT	_	_
because	_	_
JPEG	_	_
,	_	_
MPEG	_	_
video	_	_
coding	_	_
standards	_	_
are	_	_
based	_	_
on	_	_
DCT	_	_
.	_	_

#74
In	_	_
this	_	_
study	_	_
,	_	_
we	_	_
also	_	_
use	_	_
transform	_	_
domain	_	_
features	_	_
to	_	_
feed	_	_
them	_	_
into	_	_
the	_	_
our	_	_
CNN	_	_
model	_	_
.	_	_

#75
This	_	_
network	_	_
model	_	_
is	_	_
called	_	_
Hadamard-transformed	_	_
Image	_	_
Networks	_	_
(	_	_
HIN	_	_
)	_	_
.	_	_

#76
Compressed	_	_
domain	_	_
data	_	_
is	_	_
also	_	_
used	_	_
as	_	_
input	_	_
in	_	_
deep	_	_
learning	_	_
structures	_	_
.	_	_

#77
Discrete	_	_
Cosine	_	_
Transform	_	_
(	_	_
DCT	_	_
)	_	_
domain	_	_
data	_	_
as	_	_
the	_	_
input	_	_
data	_	_
can	_	_
outperform	_	_
the	_	_
state-of-the-art	_	_
results	_	_
as	_	_
shown	_	_
in	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#78
Compressed	_	_
domain	_	_
video	_	_
frames	_	_
as	_	_
input	_	_
to	_	_
the	_	_
convolutional	_	_
neural	_	_
networks	_	_
are	_	_
preferred	_	_
rather	_	_
than	_	_
RGB	_	_
frames	_	_
,	_	_
since	_	_
data	_	_
decompression	_	_
requires	_	_
extra	_	_
computation	_	_
time	_	_
and	_	_
energy	_	_
.	_	_

#79
As	_	_
a	_	_
result	_	_
;	_	_
simpler	_	_
implementation	_	_
,	_	_
effective	_	_
computation	_	_
and	_	_
improved	_	_
model	_	_
accuracy	_	_
are	_	_
achieved	_	_
.	_	_

#80
Wu	_	_
et	_	_
al.	_	_
uses	_	_
DCT	_	_
because	_	_
JPEG	_	_
,	_	_
MPEG	_	_
video	_	_
coding	_	_
standards	_	_
are	_	_
based	_	_
on	_	_
DCT	_	_
.	_	_

#81
In	_	_
this	_	_
study	_	_
,	_	_
we	_	_
also	_	_
use	_	_
transform	_	_
domain	_	_
features	_	_
to	_	_
feed	_	_
them	_	_
into	_	_
the	_	_
our	_	_
CNN	_	_
model	_	_
.	_	_

#82
This	_	_
network	_	_
model	_	_
is	_	_
called	_	_
Hadamard-transformed	_	_
Image	_	_
Networks	_	_
(	_	_
HIN	_	_
)	_	_
.	_	_

#83
Hadamard	_	_
Transform	_	_
,	_	_
also	_	_
called	_	_
as	_	_
Hadamard-ordered	_	_
Walsh-Hadamard	_	_
Transform	_	_
,	_	_
is	_	_
an	_	_
image	_	_
transformation	_	_
technique	_	_
which	_	_
is	_	_
also	_	_
used	_	_
to	_	_
compress	_	_
images	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#84
Hadamard	_	_
Transform	_	_
coefficients	_	_
consists	_	_
of	_	_
binary	_	_
values	_	_
+1	_	_
and	_	_
-1	_	_
.	_	_

#85
Thus	_	_
,	_	_
Hadamard	_	_
Transform	_	_
can	_	_
be	_	_
considered	_	_
as	_	_
an	_	_
efficient	_	_
alternative	_	_
to	_	_
the	_	_
other	_	_
image	_	_
transforms	_	_
such	_	_
that	_	_
it	_	_
can	_	_
be	_	_
implemented	_	_
without	_	_
any	_	_
multiplication	_	_
and	_	_
division	_	_
[	_	_
21	_	_
]	_	_
.	_	_

#86
1-D	_	_
Hadamard	_	_
Transform	_	_
is	_	_
expressed	_	_
by	_	_
Equation	_	_
3	_	_
.	_	_

#87
In	_	_
this	_	_
formula	_	_
,	_	_
g	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
elements	_	_
of	_	_
1-D	_	_
array	_	_
g	_	_
and	_	_
bi	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
ith	_	_
bit	_	_
(	_	_
from	_	_
right	_	_
to	_	_
left	_	_
)	_	_
in	_	_
the	_	_
binary	_	_
representation	_	_
of	_	_
x	_	_
.	_	_

#88
The	_	_
scaling	_	_
factor	_	_
(	_	_
1√	_	_
)	_	_
m	_	_
is	_	_
used	_	_
to	_	_
make	_	_
the	_	_
Hadamard	_	_
matrix	_	_
orthonormal	_	_
,	_	_
hence	_	_
it	_	_
is	_	_
mostly	_	_
kept	_	_
in	_	_
the	_	_
calculations	_	_
.	_	_

#89
T	_	_
(	_	_
u	_	_
)	_	_
=	_	_
(	_	_
1√	_	_
)	_	_
m	_	_
2m−1∑	_	_
x=0	_	_
g	_	_
(	_	_
x	_	_
)	_	_
(	_	_
−1	_	_
)	_	_
m−1∑	_	_
i=0	_	_
bi	_	_
(	_	_
x	_	_
)	_	_
bm−1−i	_	_
(	_	_
u	_	_
)	_	_
(	_	_
3	_	_
)	_	_
2-D	_	_
Hadamard	_	_
Transform	_	_
is	_	_
a	_	_
straightforward	_	_
extension	_	_
of	_	_
1-D	_	_
Hadamard	_	_
Transform	_	_
[	_	_
22	_	_
]	_	_
:	_	_
T	_	_
(	_	_
u	_	_
,	_	_
v	_	_
)	_	_
=	_	_
(	_	_
)	_	_
m	_	_
2m−1∑	_	_
x=0	_	_
2m−1∑	_	_
y=0	_	_
g	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
(	_	_
−1	_	_
)	_	_
m−1∑	_	_
i=0	_	_
(	_	_
bi	_	_
(	_	_
x	_	_
)	_	_
pi	_	_
(	_	_
u	_	_
)	_	_
+bi	_	_
(	_	_
y	_	_
)	_	_
pi	_	_
(	_	_
v	_	_
)	_	_
)	_	_
(	_	_
4	_	_
)	_	_
In	_	_
Equation	_	_
4	_	_
,	_	_
pi	_	_
(	_	_
u	_	_
)	_	_
is	_	_
computed	_	_
using	_	_
:	_	_
p0	_	_
(	_	_
u	_	_
)	_	_
=	_	_
bm−1	_	_
(	_	_
u	_	_
)	_	_
p1	_	_
(	_	_
u	_	_
)	_	_
=	_	_
bm−1	_	_
(	_	_
u	_	_
)	_	_
+	_	_
bm−2	_	_
(	_	_
u	_	_
)	_	_
...	_	_
pm−1	_	_
(	_	_
u	_	_
)	_	_
=	_	_
b1	_	_
(	_	_
u	_	_
)	_	_
+	_	_
b0	_	_
(	_	_
u	_	_
)	_	_
(	_	_
5	_	_
)	_	_
2-D	_	_
Hadamard	_	_
Transform	_	_
is	_	_
separable	_	_
and	_	_
symmetric	_	_
,	_	_
hence	_	_
it	_	_
can	_	_
be	_	_
implemented	_	_
by	_	_
using	_	_
row-column	_	_
or	_	_
column-row	_	_
passes	_	_
of	_	_
the	_	_
corresponding	_	_
1-D	_	_
transform	_	_
.	_	_

#90
There	_	_
is	_	_
an	_	_
algorithm	_	_
called	_	_
Fast	_	_
Walsh-Hadamard	_	_
Transform	_	_
(	_	_
FWTHh	_	_
)	_	_
which	_	_
requires	_	_
less	_	_
storage	_	_
and	_	_
is	_	_
fast	_	_
and	_	_
efficient	_	_
to	_	_
compute	_	_
Hadamard	_	_
Transform	_	_
[	_	_
17	_	_
]	_	_
.	_	_

#91
The	_	_
implementation	_	_
of	_	_
this	_	_
algorithm	_	_
can	_	_
be	_	_
realized	_	_
by	_	_
only	_	_
addition	_	_
and	_	_
subtraction	_	_
operations	_	_
which	_	_
can	_	_
be	_	_
summarized	_	_
in	_	_
a	_	_
butterfly	_	_
structure	_	_
.	_	_

#92
While	_	_
the	_	_
complexity	_	_
of	_	_
Hadamard	_	_
Transform	_	_
is	_	_
O	_	_
(	_	_
N2	_	_
)	_	_
,	_	_
complexity	_	_
of	_	_
fast	_	_
algorithm	_	_
is	_	_
O	_	_
(	_	_
Nlog2N	_	_
)	_	_
where	_	_
N	_	_
=	_	_
2m.If	_	_
the	_	_
length	_	_
of	_	_
the	_	_
input	_	_
1-D	_	_
array	_	_
is	_	_
less	_	_
than	_	_
a	_	_
power	_	_
of	_	_
2	_	_
,	_	_
the	_	_
array	_	_
is	_	_
padded	_	_
with	_	_
zeros	_	_
up	_	_
to	_	_
the	_	_
next	_	_
greater	_	_
power	_	_
of	_	_
two	_	_
.	_	_

#93
Since	_	_
2-D	_	_
Hadamard	_	_
Transform	_	_
is	_	_
separable	_	_
,	_	_
we	_	_
can	_	_
treat	_	_
columns	_	_
and	_	_
rows	_	_
of	_	_
the	_	_
2-D	_	_
array	_	_
as	_	_
separate	_	_
1-D	_	_
arrays	_	_
.	_	_

#94
Training	_	_
of	_	_
HIN	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
training	_	_
of	_	_
BWN	_	_
;	_	_
the	_	_
only	_	_
difference	_	_
is	_	_
that	_	_
the	_	_
input	_	_
images	_	_
are	_	_
Hadamard-transformed	_	_
as	_	_
explained	_	_
above	_	_
.	_	_

#95
Training	_	_
proceeds	_	_
as	_	_
explained	_	_
in	_	_
Section	_	_
2.1	_	_
,	_	_
but	_	_
at	_	_
the	_	_
beginning	_	_
Hadamard-transformed	_	_
input	_	_
data	_	_
is	_	_
fed	_	_
in	_	_
to	_	_
the	_	_
network	_	_
instead	_	_
.	_	_

#96
As	_	_
in	_	_
BWN	_	_
,	_	_
binarized	_	_
weights	_	_
are	_	_
used	_	_
and	_	_
no	_	_
bias	_	_
terms	_	_
are	_	_
defined	_	_
.	_	_

#97
2.3	_	_
Combination	_	_
of	_	_
Models	_	_
:	_	_
Binary	_	_
Weight	_	_
&	_	_
Hadamard	_	_

#98
Transformed	_	_
Image	_	_
Network	_	_
(	_	_
BWHIN	_	_
)	_	_
Combination	_	_
of	_	_
the	_	_
neural	_	_
networks	_	_
can	_	_
improve	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
neural	_	_
networks	_	_
by	_	_
a	_	_
few	_	_
percent	_	_
.	_	_

#99
Since	_	_
combining	_	_
the	_	_
neural	_	_
networks	_	_
reduces	_	_
the	_	_
test	_	_
error	_	_
and	_	_
tends	_	_
to	_	_
keep	_	_
the	_	_
training	_	_
error	_	_
the	_	_
same	_	_
,	_	_
it	_	_
can	_	_
be	_	_
viewed	_	_
as	_	_
a	_	_
regularization	_	_
technique	_	_
.	_	_

#100
One	_	_
of	_	_
the	_	_
popular	_	_
techniques	_	_
of	_	_
the	_	_
combination	_	_
is	_	_
called	_	_
“model	_	_
ensembles”	_	_
which	_	_
combines	_	_
the	_	_
multiple	_	_
hypotheses	_	_
that	_	_
explain	_	_
the	_	_
same	_	_
training	_	_
data	_	_
[	_	_
23,24	_	_
]	_	_
.	_	_

#101
In	_	_
model	_	_
ensembles	_	_
,	_	_
the	_	_
error	_	_
made	_	_
by	_	_
averaging	_	_
prediction	_	_
of	_	_
all	_	_
models	_	_
in	_	_
the	_	_
ensemble	_	_
decreases	_	_
linearly	_	_
with	_	_
the	_	_
ensemble	_	_
size	_	_
,	_	_
i.e.	_	_
the	_	_
number	_	_
of	_	_
models	_	_
in	_	_
the	_	_
ensemble	_	_
.	_	_

#102
However	_	_
,	_	_
since	_	_
they	_	_
need	_	_
longer	_	_
time	_	_
and	_	_
higher	_	_
amount	_	_
of	_	_
memory	_	_
to	_	_
evaluate	_	_
on	_	_
test	_	_
example	_	_
,	_	_
we	_	_
try	_	_
to	_	_
avoid	_	_
increasing	_	_
the	_	_
ensemble	_	_
size	_	_
for	_	_
energy	_	_
efficiency	_	_
.	_	_

#103
In	_	_
multi-network	_	_
frameworks	_	_
,	_	_
different	_	_
networks	_	_
are	_	_
trained	_	_
independently	_	_
and	_	_
separately	_	_
before	_	_
performing	_	_
a	_	_
combination	_	_
layer	_	_
.	_	_

#104
Bilinear	_	_
CNNs	_	_
[	_	_
25	_	_
]	_	_
is	_	_
a	_	_
good	_	_
example	_	_
for	_	_
such	_	_
combination	_	_
models	_	_
.	_	_

#105
In	_	_
bilinear	_	_
CNN	_	_
,	_	_
there	_	_
are	_	_
two	_	_
sub-networks	_	_
which	_	_
are	_	_
Figure	_	_
1	_	_
:	_	_
Our	_	_
approach	_	_
to	_	_
combine	_	_
BWN	_	_
and	_	_
HIN	_	_
:	_	_
The	_	_
architecture	_	_
of	_	_
BWHIN	_	_
[	_	_
25	_	_
]	_	_
.	_	_

#106
standard	_	_
CNN	_	_
units	_	_
.	_	_

#107
After	_	_
these	_	_
CNN	_	_
units	_	_
,	_	_
the	_	_
image	_	_
regions	_	_
which	_	_
extract	_	_
features	_	_
are	_	_
combined	_	_
with	_	_
a	_	_
matrix	_	_
dot	_	_
product	_	_
and	_	_
then	_	_
average	_	_
pooled	_	_
to	_	_
obtain	_	_
the	_	_
bilinear	_	_
vector	_	_
.	_	_

#108
In	_	_
order	_	_
to	_	_
perform	_	_
these	_	_
operations	_	_
properly	_	_
,	_	_
those	_	_
image	_	_
regions	_	_
have	_	_
to	_	_
be	_	_
of	_	_
the	_	_
same	_	_
size	_	_
.	_	_

#109
This	_	_
vector	_	_
is	_	_
passed	_	_
through	_	_
a	_	_
fully-connected	_	_
and	_	_
softmax	_	_
layer	_	_
to	_	_
obtain	_	_
class	_	_
predictions	_	_
.	_	_

#110
Our	_	_
approach	_	_
to	_	_
combine	_	_
BWN	_	_
and	_	_
HIN	_	_
is	_	_
quite	_	_
similar	_	_
to	_	_
Bilinear-CNN	_	_
,	_	_
but	_	_
simpler	_	_
.	_	_

#111
After	_	_
convolutional	_	_
,	_	_
ReLU	_	_
and	_	_
pooling	_	_
layers	_	_
,	_	_
the	_	_
output	_	_
tensor	_	_
is	_	_
reshaped	_	_
for	_	_
fully	_	_
connected	_	_
layer	_	_
as	_	_
a	_	_
1-D	_	_
tensor	_	_
.	_	_

#112
Afterwards	_	_
,	_	_
these	_	_
same	_	_
sized	_	_
1-D	_	_
tensors	_	_
of	_	_
each	_	_
sub-network	_	_
will	_	_
be	_	_
averaged	_	_
instead	_	_
of	_	_
dot	_	_
product	_	_
.	_	_

#113
Since	_	_
multiplication	_	_
consumes	_	_
power	_	_
,	_	_
dot	_	_
product	_	_
is	_	_
avoided	_	_
and	_	_
averaging	_	_
is	_	_
preferred	_	_
.	_	_

#114
Two	_	_
averaging	_	_
methods	_	_
are	_	_
used	_	_
:	_	_
Simple	_	_
averaging	_	_
and	_	_
weighted	_	_
averaging	_	_
.	_	_

#115
[	_	_
26	_	_
]	_	_
.	_	_

#116
Simple	_	_
averaging	_	_
is	_	_
the	_	_
conventional	_	_
averaging	_	_
technique	_	_
which	_	_
calculates	_	_
the	_	_
output	_	_
by	_	_
averaging	_	_
the	_	_
sum	_	_
of	_	_
outputs	_	_
from	_	_
each	_	_
ensemble	_	_
member	_	_
.	_	_

#117
Weighted	_	_
averaging	_	_
technique	_	_
assigns	_	_
a	_	_
weight	_	_
to	_	_
each	_	_
ensemble	_	_
member	_	_
and	_	_
calculated	_	_
the	_	_
output	_	_
by	_	_
taking	_	_
these	_	_
weights	_	_
into	_	_
account	_	_
.	_	_

#118
The	_	_
total	_	_
weight	_	_
of	_	_
each	_	_
ensemble	_	_
is	_	_
1	_	_
.	_	_

#119
In	_	_
order	_	_
to	_	_
implement	_	_
this	_	_
technique	_	_
,	_	_
we	_	_
define	_	_
a	_	_
random	_	_
number	_	_
which	_	_
behaves	_	_
like	_	_
a	_	_
weight	_	_
.	_	_

#120
If	_	_
Y	_	_
Ybinary	_	_
is	_	_
defined	_	_
as	_	_
the	_	_
1-D	_	_
tensor	_	_
of	_	_
BWN	_	_
and	_	_
Y	_	_
Yhadamard	_	_
is	_	_
the	_	_
1-D	_	_
tensor	_	_
of	_	_
HIN	_	_
,	_	_
the	_	_
weighted	_	_
averaging	_	_
is	_	_
defines	_	_
as	_	_
follows	_	_
:	_	_
Y	_	_
Ycombined	_	_
=	_	_
(	_	_
Wcombined	_	_
×	_	_
Y	_	_
Ybinary	_	_
)	_	_
+	_	_
(	_	_
(	_	_
1	_	_
−Wcombined	_	_
)	_	_
)	_	_
×	_	_
Y	_	_
Yhadamard	_	_
(	_	_
6	_	_
)	_	_
where	_	_
Wcombined	_	_
is	_	_
initialized	_	_
according	_	_
to	_	_
truncated	_	_
normal	_	_
distribution	_	_
which	_	_
can	_	_
only	_	_
take	_	_
values	_	_
in	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
.	_	_

#121
After	_	_
the	_	_
averaging	_	_
operation	_	_
,	_	_
the	_	_
resultant	_	_
1-D	_	_
tensor	_	_
is	_	_
processed	_	_
through	_	_
a	_	_
fully	_	_
connected	_	_
and	_	_
softmax	_	_
layers	_	_
.	_	_

#122
The	_	_
architecture	_	_
of	_	_
our	_	_
combined	_	_
network	_	_
Binary	_	_
Weight	_	_
&	_	_
Hadamard-Transformed	_	_
Image	_	_
Network	_	_
(	_	_
BWHIN	_	_
)	_	_
is	_	_
summarized	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#123
By	_	_
looking	_	_
at	_	_
Figure	_	_
1	_	_
,	_	_
one	_	_
can	_	_
observe	_	_
that	_	_
the	_	_
combination	_	_
is	_	_
applied	_	_
after	_	_
the	_	_
convolutional	_	_
layers	_	_
of	_	_
each	_	_
network	_	_
,	_	_
which	_	_
are	_	_
energy	_	_
efficient	_	_
layers	_	_
.	_	_

#124
With	_	_
this	_	_
combination	_	_
model	_	_
,	_	_
we	_	_
still	_	_
want	_	_
to	_	_
maintain	_	_
the	_	_
energy	_	_
efficiency	_	_
of	_	_
the	_	_
entire	_	_
network	_	_
.	_	_

#125
3	_	_
Experimental	_	_
Studies	_	_

#126
3.1	_	_
Image	_	_
Datasets	_	_
and	_	_
Implementation	_	_

#127
We	_	_
analyze	_	_
our	_	_
proposed	_	_
algorithm	_	_
on	_	_
two	_	_
well-known	_	_
datasets	_	_
:	_	_
MNIST	_	_
and	_	_
CIFAR-10	_	_
.	_	_

#128
The	_	_
MNIST	_	_
(	_	_
Modified	_	_
National	_	_
Institute	_	_
of	_	_
Standards	_	_
and	_	_
Technology	_	_
)	_	_
database	_	_
of	_	_
handwritten	_	_
digit	_	_
images	_	_
is	_	_
a	_	_
very	_	_
popular	_	_
digit	_	_
database	_	_
for	_	_
implementing	_	_
learning	_	_
techniques	_	_
and	_	_
pattern	_	_
recognition	_	_
methods	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#129
It	_	_
contains	_	_
60,000	_	_
training	_	_
images	_	_
and	_	_
10,000	_	_
test	_	_
images	_	_
in	_	_
ten	_	_
classes	_	_
.	_	_

#130
These	_	_
black	_	_
and	_	_
white	_	_
images	_	_
are	_	_
of	_	_
size	_	_
28×	_	_
28	_	_
pixels	_	_
.	_	_

#131
CIFAR-10	_	_
(	_	_
Canadian	_	_
Institute	_	_
for	_	_
Advanced	_	_
Research-10	_	_
)	_	_
is	_	_
also	_	_
a	_	_
popular	_	_
dataset	_	_
used	_	_
for	_	_
image	_	_
classification	_	_
tasks	_	_
[	_	_
28	_	_
]	_	_
.	_	_

#132
It	_	_
consists	_	_
of	_	_
50,000	_	_
training	_	_
and	_	_
10,000	_	_
test	_	_
images	_	_
.	_	_

#133
These	_	_
images	_	_
,	_	_
which	_	_
are	_	_
of	_	_
size	_	_
32×	_	_
32	_	_
,	_	_
are	_	_
collected	_	_
in	_	_
ten	_	_
different	_	_
classes	_	_
of	_	_
objects	_	_
(	_	_
airplane	_	_
,	_	_
automobile	_	_
,	_	_
bird	_	_
,	_	_
cat	_	_
,	_	_
deer	_	_
,	_	_
dog	_	_
,	_	_
frog	_	_
,	_	_
horse	_	_
,	_	_
ship	_	_
,	_	_
truck	_	_
)	_	_
.	_	_

#134
Tensorflow	_	_
is	_	_
chosen	_	_
to	_	_
implement	_	_
all	_	_
of	_	_
the	_	_
deep	_	_
neural	_	_
networks	_	_
for	_	_
this	_	_
work	_	_
.	_	_

#135
Tensorflow	_	_
is	_	_
an	_	_
open-source	_	_
software	_	_
library	_	_
for	_	_
machine	_	_
intelligence	_	_
,	_	_
which	_	_
is	_	_
developed	_	_
by	_	_
Google	_	_
Brain	_	_
Team	_	_
in	_	_
Python	_	_
language	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#136
Since	_	_
our	_	_
main	_	_
purpose	_	_
is	_	_
to	_	_
investigate	_	_
the	_	_
energy	_	_
efficieny	_	_
of	_	_
the	_	_
proposed	_	_
neural	_	_
network	_	_
models	_	_
,	_	_
simple	_	_
architectures	_	_
are	_	_
chosen	_	_
and	_	_
no	_	_
pretraining	_	_
such	_	_
as	_	_
feature	_	_
extraction	_	_
or	_	_
unsupervised	_	_
learning	_	_
techniques	_	_
is	_	_
performed	_	_
.	_	_

#137
Images	_	_
in	_	_
both	_	_
datasets	_	_
are	_	_
normalized	_	_
such	_	_
that	_	_
the	_	_
pixel	_	_
values	_	_
are	_	_
in	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
.	_	_

#138
All	_	_
experiments	_	_
are	_	_
carried	_	_
out	_	_
on	_	_
a	_	_
single	_	_
GPU	_	_
,	_	_
which	_	_
is	_	_
NVIDIA	_	_
GeForce	_	_
940M	_	_
.	_	_

#139
Thanks	_	_
to	_	_
our	_	_
CUDA-enabled	_	_
GPU	_	_
,	_	_
we	_	_
are	_	_
able	_	_
to	_	_
run	_	_
Tensorflow	_	_
with	_	_
GPU	_	_
support	_	_
and	_	_
we	_	_
achieve	_	_
faster	_	_
computation	_	_
.	_	_

#140
3.2	_	_
Neural	_	_
Network	_	_
Architecture	_	_
and	_	_
Hyperparamaters	_	_

#141
3.2.1	_	_
CNN	_	_
Architectures	_	_

#142
In	_	_
order	_	_
to	_	_
implement	_	_
the	_	_
proposed	_	_
networks	_	_
and	_	_
analyze	_	_
their	_	_
performances	_	_
,	_	_
two	_	_
different	_	_
CNN	_	_
architectures	_	_
are	_	_
utilized	_	_
.	_	_

#143
First	_	_
type	_	_
of	_	_
architecture	_	_
for	_	_
MNIST	_	_
database	_	_
is	_	_
very	_	_
similar	_	_
to	_	_
LeNet-5	_	_
in	_	_
[	_	_
3	_	_
]	_	_
with	_	_
convolutional	_	_
and	_	_
pooling	_	_
layers	_	_
.	_	_

#144
Second	_	_
architecture	_	_
is	_	_
built	_	_
according	_	_
to	_	_
AllConvolutionalNeuralNetwork	_	_
[	_	_
18	_	_
]	_	_
with	_	_
strided	_	_
convolution	_	_
.	_	_

#145
Strided	_	_
convolution	_	_
is	_	_
that	_	_
some	_	_
positions	_	_
of	_	_
the	_	_
kernel	_	_
are	_	_
skipped	_	_
over	_	_
in	_	_
order	_	_
to	_	_
reduce	_	_
the	_	_
computational	_	_
burden	_	_
while	_	_
implementing	_	_
the	_	_
convolution	_	_
operation	_	_
.	_	_

#146
Strided	_	_
convolution	_	_
is	_	_
equivalent	_	_
to	_	_
downsampling	_	_
the	_	_
output	_	_
of	_	_
the	_	_
full	_	_
convolution	_	_
function	_	_
.	_	_

#147
The	_	_
reason	_	_
is	_	_
to	_	_
investigate	_	_
the	_	_
effect	_	_
of	_	_
the	_	_
pooling	_	_
layer	_	_
and	_	_
strided	_	_
convolution	_	_
on	_	_
energy	_	_
efficiency	_	_
and	_	_
test	_	_
accuracy	_	_
.	_	_

#148
Both	_	_
neural	_	_
network	_	_
architectures	_	_
used	_	_
for	_	_
MNIST	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#149
First	_	_
architecture	_	_
is	_	_
built	_	_
as	_	_
a	_	_
[	_	_
ConvReLUConvReLUPoolConvReLUPoolFCSoftmax	_	_
]	_	_
structure	_	_
while	_	_
second	_	_
architecture	_	_
is	_	_
built	_	_
as	_	_
[	_	_
ConvReLUStridedConvReLUStridedConvReLUFCSoftmax	_	_
]	_	_
.	_	_

#150
The	_	_
sizes	_	_
of	_	_
three	_	_
convoConvPoolCNN	_	_
All-CNN	_	_
Input	_	_
28×28	_	_
gray-scale	_	_
image	_	_
6×6	_	_
conv	_	_
.	_	_

#151
6	_	_
ReLU	_	_
6×6	_	_
conv	_	_
.	_	_

#152
6	_	_
ReLU	_	_
5×5	_	_
conv	_	_
.	_	_

#153
12	_	_
ReLU	_	_
5×5	_	_
conv	_	_
.	_	_

#154
12	_	_
ReLU	_	_
with	_	_
stride	_	_
22×2	_	_
max-pooling	_	_
,	_	_
stride	_	_
2	_	_
4×4	_	_
conv	_	_
.	_	_

#155
24	_	_
ReLU	_	_
4×4	_	_
conv	_	_
.	_	_

#156
24	_	_
ReLU	_	_
with	_	_
stride	_	_
22×2	_	_
max-pooling	_	_
,	_	_
stride	_	_
2	_	_
Fully	_	_
connected	_	_
layer	_	_
with	_	_
200	_	_
neurons	_	_
,	_	_
dropout	_	_
10-way	_	_
softmax	_	_
layer	_	_
Table	_	_
1	_	_
:	_	_
Model	_	_
description	_	_
of	_	_
the	_	_
two	_	_
architectures	_	_
for	_	_
MNIST	_	_
dataset	_	_
.	_	_

#157
lutional	_	_
layers	_	_
and	_	_
1	_	_
fully	_	_
connected	_	_
layer	_	_
are	_	_
determined	_	_
as	_	_
6	_	_
,	_	_
12	_	_
,	_	_
24	_	_
and	_	_
200	_	_
,	_	_
respectively	_	_
.	_	_

#158
These	_	_
configurations	_	_
are	_	_
determined	_	_
after	_	_
a	_	_
large	_	_
scale	_	_
experimentation	_	_
over	_	_
the	_	_
datasets	_	_
.	_	_

#159
Both	_	_
pooling	_	_
and	_	_
strided	_	_
convolutional	_	_
operations	_	_
are	_	_
used	_	_
to	_	_
shrink	_	_
the	_	_
input	_	_
size	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
two	_	_
in	_	_
order	_	_
to	_	_
reduce	_	_
the	_	_
computational	_	_
and	_	_
statistical	_	_
burden	_	_
on	_	_
the	_	_
next	_	_
layer	_	_
.	_	_

#160
Filter	_	_
sizes	_	_
are	_	_
determined	_	_
heuristically	_	_
.	_	_

#161
Since	_	_
5×5	_	_
filters	_	_
are	_	_
used	_	_
in	_	_
LeNet5	_	_
,	_	_
filter	_	_
sizes	_	_
are	_	_
selected	_	_
to	_	_
be	_	_
close	_	_
to	_	_
this	_	_
size	_	_
.	_	_

#162
In	_	_
order	_	_
to	_	_
preserve	_	_
the	_	_
input	_	_
size	_	_
for	_	_
conventional	_	_
convolutional	_	_
layers	_	_
,	_	_
stride	_	_
is	_	_
chosen	_	_
as	_	_
1	_	_
and	_	_
zero	_	_
padding	_	_
is	_	_
used	_	_
accordingly	_	_
.	_	_

#163
For	_	_
strided	_	_
convolutional	_	_
layers	_	_
,	_	_
stride	_	_
is	_	_
2	_	_
to	_	_
decrease	_	_
the	_	_
height	_	_
and	_	_
width	_	_
of	_	_
the	_	_
image	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
2	_	_
.	_	_

#164
For	_	_
non-overlapping	_	_
max-pooling	_	_
operation	_	_
,	_	_
2×	_	_
2	_	_
filters	_	_
with	_	_
stride	_	_
2	_	_
is	_	_
chosen	_	_
.	_	_

#165
The	_	_
architectures	_	_
which	_	_
are	_	_
applied	_	_
to	_	_
CIFAR-10	_	_
dataset	_	_
are	_	_
described	_	_
in	_	_
Table	_	_
2	_	_
.	_	_

#166
Since	_	_
CIFAR-10	_	_
dataset	_	_
has	_	_
colored	_	_
and	_	_
high	_	_
resolutions	_	_
images	_	_
compared	_	_
to	_	_
the	_	_
images	_	_
in	_	_
MNIST	_	_
,	_	_
models	_	_
adapted	_	_
for	_	_
higher	_	_
capacity	_	_
are	_	_
preferred	_	_
.	_	_

#167
Model	_	_
capacity	_	_
is	_	_
expanded	_	_
by	_	_
increasing	_	_
both	_	_
the	_	_
number	_	_
of	_	_
layers	_	_
and	_	_
the	_	_
number	_	_
of	_	_
neurons	_	_
at	_	_
the	_	_
hidden	_	_
layers	_	_
.	_	_

#168
The	_	_
architecture	_	_
with	_	_
pooling	_	_
layers	_	_
is	_	_
built	_	_
as	_	_
[	_	_
ConvReLUConvReLUPoolConvReLUConvReLUPoolFCSoftmax	_	_
]	_	_
,	_	_
while	_	_
all-CNN	_	_
architecture	_	_
is	_	_
build	_	_
as	_	_
[	_	_
ConvReLUStridedConvReLUConvReLUStridedConvReLUFCSoftmax	_	_
]	_	_
.	_	_

#169
Since	_	_
we	_	_
want	_	_
to	_	_
preserve	_	_
the	_	_
energy	_	_
efficieny	_	_
as	_	_
far	_	_
as	_	_
possible	_	_
,	_	_
we	_	_
use	_	_
more	_	_
convolutional	_	_
layers	_	_
,	_	_
which	_	_
can	_	_
be	_	_
modified	_	_
as	_	_
energy	_	_
efficient	_	_
layers	_	_
,	_	_
and	_	_
only	_	_
one	_	_
fully-connected	_	_
layer	_	_
.	_	_

#170
The	_	_
sizes	_	_
of	_	_
these	_	_
4	_	_
convolutional	_	_
layers	_	_
and	_	_
1	_	_
fully-connected	_	_
layer	_	_
are	_	_
determined	_	_
as	_	_
32	_	_
,	_	_
32	_	_
,	_	_
64	_	_
,	_	_
64	_	_
,	_	_
and	_	_
512	_	_
,	_	_
respectively	_	_
.	_	_

#171
The	_	_
number	_	_
of	_	_
neurons	_	_
in	_	_
a	_	_
layer	_	_
and	_	_
the	_	_
filter	_	_
sizes	_	_
are	_	_
selected	_	_
empirically	_	_
.	_	_

#172
A	_	_
critical	_	_
point	_	_
in	_	_
CIFAR-10	_	_
architectures	_	_
is	_	_
that	_	_
more	_	_
dropout	_	_
is	_	_
used	_	_
due	_	_
to	_	_
the	_	_
increased	_	_
capacity	_	_
.	_	_

#173
Note	_	_
that	_	_
the	_	_
size	_	_
of	_	_
an	_	_
image	_	_
in	_	_
the	_	_
MNIST	_	_
dataset	_	_
is	_	_
altered	_	_
from	_	_
28×28×1	_	_
to	_	_
32×	_	_
32×	_	_
1	_	_
after	_	_
Hadamard	_	_
transform	_	_
.	_	_

#174
As	_	_
a	_	_
result	_	_
,	_	_
the	_	_
outputs	_	_
of	_	_
the	_	_
BWN	_	_
and	_	_
HIN	_	_
will	_	_
not	_	_
compatible	_	_
in	_	_
the	_	_
combined	_	_
model	_	_
.	_	_

#175
In	_	_
order	_	_
to	_	_
overcome	_	_
this	_	_
problem	_	_
in	_	_
the	_	_
MNIST	_	_
architectures	_	_
,	_	_
the	_	_
filter	_	_
size	_	_
in	_	_
the	_	_
first	_	_
convolutional	_	_
layer	_	_
whose	_	_
input	_	_
is	_	_
Hadamard-transformed	_	_
image	_	_
is	_	_
modified	_	_
as	_	_
5	_	_
×	_	_
5	_	_
and	_	_
zero-padding	_	_
is	_	_
not	_	_
used	_	_
.	_	_

#176
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
we	_	_
will	_	_
not	_	_
have	_	_
that	_	_
issue	_	_
for	_	_
CIFAR-10	_	_
database	_	_
.	_	_

#177
Since	_	_
the	_	_
width	_	_
&	_	_
height	_	_
of	_	_
an	_	_
image	_	_
in	_	_
CIFAR-10	_	_
is	_	_
32	_	_
,	_	_
a	_	_
power	_	_
of	_	_
2	_	_
,	_	_
the	_	_
size	_	_
will	_	_
remain	_	_
unchanged	_	_
(	_	_
32	_	_
×	_	_
32	_	_
×	_	_
3	_	_
)	_	_
after	_	_
Hadamard	_	_
ConvPool-CNN	_	_
All-CNN	_	_
Input	_	_
32×32	_	_
RGB	_	_
image	_	_
3×3	_	_
conv	_	_
.	_	_

#178
32	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#179
32	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#180
32	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#181
32	_	_
ReLU	_	_
with	_	_
stride	_	_
22×2	_	_
max-pooling	_	_
,	_	_
stride	_	_
2	_	_
Dropout	_	_
3×3	_	_
conv	_	_
.	_	_

#182
64	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#183
64	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#184
64	_	_
ReLU	_	_
3×3	_	_
conv	_	_
.	_	_

#185
64	_	_
ReLU	_	_
with	_	_
stride	_	_
22×2	_	_
max-pooling	_	_
,	_	_
stride	_	_
2	_	_
Dropout	_	_
Fully	_	_
connected	_	_
layer	_	_
with	_	_
512	_	_
neurons	_	_
,	_	_
dropout	_	_
10-way	_	_
softmax	_	_
layer	_	_
Table	_	_
2	_	_
:	_	_
Model	_	_
description	_	_
of	_	_
the	_	_
two	_	_
architectures	_	_
for	_	_
CIFAR-10	_	_
dataset	_	_
.	_	_

#186
transform	_	_
.	_	_

#187
3.2.2	_	_
Network	_	_
Hyperparamaters	_	_

#188
In	_	_
order	_	_
to	_	_
make	_	_
a	_	_
fair	_	_
comparison	_	_
,	_	_
similar	_	_
methods	_	_
are	_	_
used	_	_
for	_	_
both	_	_
networks	_	_
and	_	_
architectures	_	_
.	_	_

#189
As	_	_
optimization	_	_
method	_	_
,	_	_
ADAM	_	_
is	_	_
selected	_	_
as	_	_
suggested	_	_
in	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#190
ADAM	_	_
is	_	_
an	_	_
practical	_	_
optimizer	_	_
which	_	_
is	_	_
fairly	_	_
robust	_	_
to	_	_
the	_	_
choice	_	_
of	_	_
hyperparameters	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#191
As	_	_
regularization	_	_
technique	_	_
,	_	_
dropout	_	_
is	_	_
chosen	_	_
.	_	_

#192
In	_	_
case	_	_
of	_	_
dropout	_	_
,	_	_
a	_	_
neuron	_	_
is	_	_
kept	_	_
active	_	_
with	_	_
a	_	_
fixed	_	_
probability	_	_
of	_	_
p	_	_
independent	_	_
of	_	_
other	_	_
units	_	_
.	_	_

#193
p	_	_
can	_	_
be	_	_
considered	_	_
as	_	_
a	_	_
hyperparameter	_	_
and	_	_
it	_	_
can	_	_
be	_	_
set	_	_
to	_	_
a	_	_
number	_	_
or	_	_
determined	_	_
by	_	_
cross-validation	_	_
.	_	_

#194
Although	_	_
p	_	_
=	_	_
0.5	_	_
is	_	_
a	_	_
reasonable	_	_
choice	_	_
for	_	_
hidden	_	_
units	_	_
,	_	_
the	_	_
optimal	_	_
probability	_	_
is	_	_
usually	_	_
closer	_	_
to	_	_
1	_	_
than	_	_
to	_	_
0.5	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#195
In	_	_
addition	_	_
to	_	_
ReLU	_	_
layers	_	_
in	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
ReLU	_	_
is	_	_
also	_	_
used	_	_
as	_	_
the	_	_
activation	_	_
function	_	_
for	_	_
the	_	_
fully-connected	_	_
layer	_	_
.	_	_

#196
ReLU	_	_
is	_	_
also	_	_
an	_	_
energy	_	_
efficient	_	_
function	_	_
since	_	_
the	_	_
definition	_	_
function	_	_
of	_	_
ReLU	_	_
f	_	_
(	_	_
x	_	_
)	_	_
=	_	_
max	_	_
(	_	_
0	_	_
,	_	_
x	_	_
)	_	_
simply	_	_
requires	_	_
a	_	_
comparison	_	_
.	_	_

#197
Other	_	_
hyperparameter	_	_
settings	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
3	_	_
.	_	_

#198
MNIST	_	_
CIFAR-10	_	_
Weight	_	_
Initialization	_	_
W	_	_
∼	_	_
N	_	_
(	_	_
0	_	_
,	_	_
0.1	_	_
)	_	_
Xavier	_	_
init	_	_
.	_	_

#199
Bias	_	_
Initialization	_	_
0.1	_	_
0.1	_	_
Mini-batch	_	_
Size	_	_
100	_	_
100	_	_
Initial	_	_
Learning	_	_
Rate	_	_
0.003	_	_
0.0005	_	_
Exponential	_	_
Decay	_	_
Rate	_	_
of	_	_
Learning	_	_
Rate	_	_
0.0001	_	_
10−6	_	_
Probability	_	_
of	_	_
Retention	_	_
for	_	_
Dropout	_	_
,	_	_
p	_	_
0.75	_	_
0.75	_	_
,	_	_
0.75	_	_
,	_	_
0.5	_	_
Table	_	_
3	_	_
:	_	_
Hyperparameter	_	_
settings	_	_
for	_	_
different	_	_
datasets	_	_
.	_	_

#200
3.3	_	_
Experimental	_	_
Results	_	_

#201
In	_	_
the	_	_
experiments	_	_
,	_	_
five	_	_
CNN	_	_
models	_	_
(	_	_
including	_	_
the	_	_
standard	_	_
CNN	_	_
)	_	_
with	_	_
two	_	_
different	_	_
architectures	_	_
are	_	_
trained	_	_
on	_	_
MNIST	_	_
and	_	_
CIFAR-10	_	_
database	_	_
.	_	_

#202
These	_	_
CNN	_	_
models	_	_
include	_	_
the	_	_
standard	_	_
CNN	_	_
,	_	_
previously	_	_
studied	_	_
BWN	_	_
and	_	_
our	_	_
energy	_	_
efficient	_	_
neural	_	_
networks	_	_
.	_	_

#203
The	_	_
performance	_	_
of	_	_
the	_	_
neural	_	_
networks	_	_
is	_	_
evaluated	_	_
based	_	_
on	_	_
test	_	_
accuracies	_	_
.	_	_

#204
Networks	_	_
trained	_	_
on	_	_
MNIST	_	_
dataset	_	_
has	_	_
10000	_	_
iterations	_	_
,	_	_
while	_	_
networks	_	_
of	_	_
CIFAR-10	_	_
dataset	_	_
are	_	_
trained	_	_
in	_	_
150000	_	_
iterations	_	_
.	_	_

#205
This	_	_
means	_	_
the	_	_
number	_	_
of	_	_
epochs	_	_
for	_	_
MNIST	_	_
and	_	_
CIFAR-10	_	_
training	_	_
is	_	_
chosen	_	_
as	_	_
17	_	_
and	_	_
300	_	_
,	_	_
respectively	_	_
.	_	_

#206
Before	_	_
learning	_	_
procedure	_	_
is	_	_
performed	_	_
,	_	_
the	_	_
input	_	_
images	_	_
are	_	_
normalized	_	_
to	_	_
the	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
.	_	_

#207
The	_	_
overall	_	_
test	_	_
accuracies	_	_
and	_	_
test	_	_
accuracies	_	_
corresponding	_	_
to	_	_
each	_	_
number	_	_
of	_	_
iterations	_	_
are	_	_
presented	_	_
in	_	_
Table	_	_
4	_	_
and	_	_
Figure	_	_
2	_	_
,	_	_
respectively	_	_
.	_	_

#208
MNIST	_	_
CIFAR-10	_	_
ConvoPool-CNN	_	_
All-CNN	_	_
ConvoPool-CNN	_	_
All-CNN	_	_
CNN	_	_
99.48	_	_
99.31	_	_
82.64	_	_
77.32	_	_
BWN	_	_
98.88	_	_
98.37	_	_
68.72	_	_
65.36	_	_
HIN	_	_
98.32	_	_
97.84	_	_
61.36	_	_
11.76	_	_
BWHIN-NormalAvg	_	_
98.79	_	_
98.40	_	_
72.10	_	_
67.70	_	_
BWHIN-RandomAvg	_	_
98.96	_	_
98.61	_	_
72.65	_	_
67.30	_	_
Table	_	_
4	_	_
:	_	_
Test	_	_
accuracy	_	_
results	_	_
(	_	_
in	_	_
percentage	_	_
)	_	_
for	_	_
CIFAR-10	_	_
dataset	_	_
.	_	_

#209
By	_	_
looking	_	_
at	_	_
the	_	_
results	_	_
presented	_	_
in	_	_
Table	_	_
4	_	_
and	_	_
Figure	_	_
2	_	_
,	_	_
one	_	_
can	_	_
observe	_	_
that	_	_
our	_	_
energy-efficient	_	_
solution	_	_
sacrifice	_	_
the	_	_
classification	_	_
performance	_	_
slightly	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
a	_	_
greater	_	_
energy	_	_
efficiency	_	_
.	_	_

#210
While	_	_
the	_	_
proposed	_	_
networks	_	_
almost	_	_
achieve	_	_
the	_	_
state-of-the-art	_	_
result	_	_
for	_	_
MNIST	_	_
dataset	_	_
,	_	_
there	_	_
is	_	_
an	_	_
considerable	_	_
accuracy	_	_
gap	_	_
between	_	_
the	_	_
state-of-the-art	_	_
results	_	_
and	_	_
the	_	_
results	_	_
of	_	_
the	_	_
proposed	_	_
networks	_	_
.	_	_

#211
This	_	_
problem	_	_
concerning	_	_
CIFAR-10	_	_
dataset	_	_
can	_	_
be	_	_
solved	_	_
by	_	_
modifying	_	_
the	_	_
size	_	_
of	_	_
the	_	_
network	_	_
,	_	_
since	_	_
binarized	_	_
neural	_	_
networks	_	_
may	_	_
need	_	_
larger	_	_
network	_	_
structures	_	_
than	_	_
the	_	_
networks	_	_
with	_	_
real-valued	_	_
parameters	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#212
As	_	_
observed	_	_
from	_	_
Table	_	_
4	_	_
,	_	_
it’s	_	_
clear	_	_
that	_	_
our	_	_
CNN-based	_	_
energy	_	_
efficient	_	_
neural	_	_
networks	_	_
with	_	_
ConvPool	_	_
architecture	_	_
have	_	_
better	_	_
classification	_	_
results	_	_
than	_	_
the	_	_
All-CNN	_	_
networks	_	_
.	_	_

#213
Although	_	_
networks	_	_
with	_	_
pooling	_	_
layer	_	_
have	_	_
greater	_	_
test	_	_
accuracies	_	_
,	_	_
All-CNN	_	_
can	_	_
be	_	_
preferred	_	_
over	_	_
ConvPool-CNN	_	_
,	_	_
since	_	_
they	_	_
are	_	_
more	_	_
energy-efficient	_	_
.	_	_

#214
Here	_	_
,	_	_
both	_	_
a	_	_
convolutional	_	_
layer	_	_
with	_	_
stride	_	_
1	_	_
and	_	_
pooling	_	_
layer	_	_
is	_	_
replaced	_	_
by	_	_
a	_	_
convolutional	_	_
layer	_	_
which	_	_
has	_	_
stride	_	_
greater	_	_
than	_	_
1	_	_
.	_	_

#215
Number	_	_
of	_	_
multiplications	_	_
in	_	_
the	_	_
convolutional	_	_
layer	_	_
is	_	_
decreased	_	_
by	_	_
a	_	_
significant	_	_
amount	_	_
with	_	_
strided	_	_
convolution	_	_
.	_	_

#216
While	_	_
the	_	_
downsampling	_	_
is	_	_
performed	_	_
along	_	_
with	_	_
full	_	_
convolution	_	_
,	_	_
a	_	_
large	_	_
number	_	_
of	_	_
values	_	_
are	_	_
already	_	_
computed	_	_
in	_	_
convolutional	_	_
layer	_	_
,	_	_
then	_	_
many	_	_
of	_	_
these	_	_
values	_	_
are	_	_
discarded	_	_
with	_	_
pooling	_	_
operation	_	_
.	_	_

#217
This	_	_
is	_	_
computationally	_	_
wasteful	_	_
;	_	_
it	_	_
will	_	_
take	_	_
more	_	_
time	_	_
and	_	_
use	_	_
more	_	_
memory	_	_
than	_	_
strided	_	_
convolutional	_	_
layer	_	_
.	_	_

#218
If	_	_
we	_	_
take	_	_
the	_	_
risk	_	_
of	_	_
less	_	_
test	_	_
accuracy	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
energy	_	_
efficiency	_	_
,	_	_
strided	_	_
convolution	_	_
should	_	_
be	_	_
used	_	_
instead	_	_
of	_	_
pooling	_	_
.	_	_

#219
There	_	_
is	_	_
also	_	_
a	_	_
significant	_	_
point	_	_
about	_	_
All-CNN	_	_
.	_	_

#220
If	_	_
we	_	_
increase	_	_
the	_	_
capacity	_	_
of	_	_
All-CNN	_	_
by	_	_
increasing	_	_
the	_	_
size	_	_
of	_	_
the	_	_
layers	_	_
and/or	_	_
making	_	_
a	_	_
deeper	_	_
network	_	_
,	_	_
image	_	_
classification	_	_
can	_	_
be	_	_
performed	_	_
without	_	_
the	_	_
loss	_	_
of	_	_
accuracy	_	_
.	_	_

#221
In	_	_
some	_	_
cases	_	_
,	_	_
it	_	_
may	_	_
even	_	_
give	_	_
better	_	_
results	_	_
[	_	_
18	_	_
]	_	_
.	_	_

#222
(	_	_
a	_	_
)	_	_
MINIST	_	_
(	_	_
b	_	_
)	_	_
CIFAR-10	_	_
Figure	_	_
2	_	_
:	_	_
Test	_	_
accuracy	_	_
results	_	_
.	_	_

#223
Binary	_	_
Weight	_	_
Network	_	_
is	_	_
a	_	_
very	_	_
robust	_	_
network	_	_
to	_	_
the	_	_
changes	_	_
of	_	_
the	_	_
hyperparameters	_	_
.	_	_

#224
As	_	_
seen	_	_
from	_	_
Table	_	_
4	_	_
,	_	_
this	_	_
model	_	_
could	_	_
train	_	_
the	_	_
network	_	_
for	_	_
any	_	_
cases	_	_
.	_	_

#225
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
Hadamard-transformed	_	_
Image	_	_
Network	_	_
is	_	_
slightly	_	_
worse	_	_
than	_	_
BWN	_	_
.	_	_

#226
The	_	_
reason	_	_
could	_	_
be	_	_
the	_	_
slight	_	_
change	_	_
in	_	_
the	_	_
original	_	_
architecture	_	_
.	_	_

#227
If	_	_
Hadamard	_	_
transform	_	_
is	_	_
applied	_	_
to	_	_
an	_	_
image	_	_
whose	_	_
width	_	_
&	_	_
height	_	_
is	_	_
not	_	_
a	_	_
power	_	_
of	_	_
2	_	_
,	_	_
size	_	_
of	_	_
the	_	_
image	_	_
changes	_	_
such	_	_
that	_	_
the	_	_
width	_	_
and	_	_
height	_	_
is	_	_
increased	_	_
to	_	_
the	_	_
next	_	_
power	_	_
of	_	_
2	_	_
of	_	_
their	_	_
original	_	_
values	_	_
.	_	_

#228
Hence	_	_
,	_	_
if	_	_
we	_	_
want	_	_
to	_	_
feed	_	_
the	_	_
Hadamard-transformed	_	_
images	_	_
to	_	_
the	_	_
network	_	_
as	_	_
input	_	_
images	_	_
,	_	_
we	_	_
may	_	_
have	_	_
to	_	_
modify	_	_
the	_	_
architecture	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#229
That	_	_
might	speculation	_
be	_	_
the	_	_
reason	_	_
why	_	_
HIN	_	_
has	_	_
worse	_	_
performance	_	_
and	_	_
is	_	_
a	_	_
lossy	_	_
network	_	_
than	_	_
BWN	_	_
.	_	_

#230
Combined	_	_
models	_	_
work	_	_
as	_	_
expected	_	_
;	_	_
they	_	_
have	_	_
better	_	_
test	_	_
accuracies	_	_
than	_	_
their	_	_
sub-networks	_	_
BWN	_	_
and	_	_
HIN	_	_
.	_	_

#231
When	_	_
the	_	_
conventional	_	_
averaging	_	_
has	_	_
the	_	_
lower	_	_
test	_	_
accuracies	_	_
than	_	_
BWN	_	_
,	_	_
the	_	_
random	_	_
averaging	_	_
is	_	_
always	_	_
better	_	_
than	_	_
both	_	_
BWN	_	_
and	_	_
BWHIN-Normal	_	_
models	_	_
.	_	_

#232
Hence	_	_
both	_	_
averaging	_	_
techniques	_	_
should	_	_
be	_	_
tried	_	_
and	_	_
it	_	_
should	_	_
be	_	_
observed	_	_
which	_	_
one	_	_
has	_	_
better	_	_
test	_	_
accuracies	_	_
even	_	_
though	_	_
the	_	_
random-average	_	_
works	_	_
for	_	_
most	_	_
of	_	_
the	_	_
cases	_	_
.	_	_

#233
Combined	_	_
models	_	_
are	_	_
also	_	_
a	_	_
good	_	_
solution	_	_
to	_	_
the	_	_
failure	_	_
of	_	_
one	_	_
sub-network	_	_
.	_	_

#234
For	_	_
example	_	_
,	_	_
when	_	_
HIN	_	_
is	_	_
not	_	_
trained	_	_
,	_	_
BWN	_	_
compensates	_	_
this	_	_
failure	_	_
and	_	_
BWN-Normal/BWN-Random	_	_
gives	_	_
better	_	_
results	_	_
than	_	_
both	_	_
BWN	_	_
and	_	_
HIN	_	_
.	_	_

#235
Hence	_	_
it	_	_
can	_	_
be	_	_
said	_	_
that	_	_
our	_	_
combined	_	_
model	_	_
is	_	_
also	_	_
robust	_	_
in	_	_
case	_	_
one	_	_
of	_	_
the	_	_
subnetworks	_	_
can	_	_
not	_	_
be	_	_
trained	_	_
.	_	_

#236
4	_	_
Conclusion	_	_

#237
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
study	_	_
CNN-based	_	_
enery	_	_
efficient	_	_
neural	_	_
networks	_	_
for	_	_
the	_	_
image	_	_
classification	_	_
tasks	_	_
of	_	_
MNIST	_	_
and	_	_
CIFAR-10	_	_
databases	_	_
.	_	_

#238
These	_	_
models	_	_
are	_	_
BWN	_	_
,	_	_
HIN	_	_
,	_	_
BWHIN-Normal	_	_
and	_	_
BWHIN-Random	_	_
.	_	_

#239
As	_	_
observed	_	_
from	_	_
these	_	_
networks	_	_
,	_	_
energy	_	_
efficiency	_	_
comes	_	_
with	_	_
a	_	_
small	_	_
loss	_	_
of	_	_
accuracy	_	_
.	_	_

#240
BWN	_	_
gives	_	_
satisfying	_	_
results	_	_
,	_	_
while	_	_
HIN	_	_
sometimes	_	_
may	_	_
struggle	_	_
to	_	_
train	_	_
the	_	_
network	_	_
.	_	_

#241
The	_	_
combined	_	_
models	_	_
BWHIN-Normal	_	_
and	_	_
BWHIN-Random	_	_
,	_	_
which	_	_
are	_	_
our	_	_
principle	_	_
contributions	_	_
,	_	_
certainly	_	_
improves	_	_
the	_	_
performance	_	_
of	_	_
their	_	_
sub-networks	_	_
BWN	_	_
and	_	_
HIN	_	_
.	_	_

#242
Hence	_	_
as	_	_
an	_	_
energy	_	_
efficient	_	_
model	_	_
,	_	_
the	_	_
combined	_	_
model	_	_
can	_	_
be	_	_
preferred	_	_
with	_	_
its	_	_
superior	_	_
classification	_	_
performance	_	_
.	_	_

#243
As	_	_
future	_	_
work	_	_
,	_	_
proposed	_	_
energy	_	_
efficient	_	_
neural	_	_
networks	_	_
can	_	_
be	_	_
used	_	_
for	_	_
the	_	_
classification	_	_
of	_	_
other	_	_
datasets	_	_
such	_	_
as	_	_
CIFAR-100	_	_
,	_	_
Street	_	_
View	_	_
House	_	_
Numbers	_	_
(	_	_
SVHN	_	_
)	_	_
dataset	_	_
or	_	_
ImageNet	_	_
.	_	_

#244
The	_	_
networks	_	_
can	_	_
be	_	_
also	_	_
implemented	_	_
with	_	_
bigger	_	_
capacity	_	_
by	_	_
increasing	_	_
the	_	_
size	_	_
of	_	_
a	_	_
hidden	_	_
layer	_	_
or	_	_
with	_	_
more	_	_
hidden	_	_
layers	_	_
,	_	_
i.e.	_	_
as	_	_
a	_	_
deeper	_	_
network	_	_
.	_	_

#245
Effects	_	_
of	_	_
data	_	_
preprocessing	_	_
or	_	_
usage	_	_
of	_	_
unsupervised	_	_
learning	_	_
as	_	_
pretraining	_	_
on	_	_
energy	_	_
efficient	_	_
NN	_	_
might	_	_
be	_	_
another	_	_
research	_	_
topic	_	_
.	_	_