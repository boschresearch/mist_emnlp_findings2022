#0
MONET	_	_
:	_	_
Multiview	_	_
Semi-supervised	_	_
Keypoint	_	_
Detection	_	_
via	_	_
Epipolar	_	_
Divergence	_	_
Yuan	_	_
Yao	_	_
University	_	_
of	_	_
Minnesota	_	_
yaoxx340	_	_
@	_	_
umn.edu	_	_
Yasamin	_	_
Jafarian	_	_
University	_	_
of	_	_
Minnesota	_	_
yasamin	_	_
@	_	_
umn.edu	_	_
Hyun	_	_
Soo	_	_
Park	_	_
University	_	_
of	_	_
Minnesota	_	_
hspark	_	_
@	_	_
umn.edu	_	_
Figure	_	_
1	_	_
:	_	_
This	_	_
paper	_	_
presents	_	_
MONET-an	_	_
semi-supervised	_	_
learning	_	_
for	_	_
keypoint	_	_
detection	_	_
,	_	_
which	_	_
is	_	_
able	_	_
to	_	_
localize	_	_
customized	_	_
keypoints	_	_
of	_	_
diverse	_	_
species	_	_
,	_	_
e.g.	_	_
,	_	_
humans	_	_
,	_	_
dogs	_	_
,	_	_
and	_	_
monkeys	_	_
with	_	_
very	_	_
limited	_	_
number	_	_
of	_	_
labeled	_	_
data	_	_
without	_	_
a	_	_
pre-trained	_	_
model	_	_
.	_	_

#1
The	_	_
right	_	_
most	_	_
figure	_	_
illustrates	_	_
3D	_	_
reconstruction	_	_
of	_	_
monkey	_	_
movement	_	_
using	_	_
our	_	_
pose	_	_
detection	_	_
.	_	_

#2
Abstract	_	_

#3
This	_	_
paper	_	_
presents	_	_
MONET—an	_	_
end-to-end	_	_
semi-supervised	_	_
learning	_	_
framework	_	_
for	_	_
a	_	_
keypoint	_	_
detector	_	_
using	_	_
multiview	_	_
image	_	_
streams	_	_
.	_	_

#4
In	_	_
particular	_	_
,	_	_
we	_	_
consider	_	_
general	_	_
subjects	_	_
such	_	_
as	_	_
non-human	_	_
species	_	_
where	_	_
attaining	_	_
a	_	_
large	_	_
scale	_	_
annotated	_	_
dataset	_	_
is	_	_
challenging	_	_
.	_	_

#5
While	_	_
multiview	_	_
geometry	_	_
can	_	_
be	_	_
used	_	_
to	_	_
self-supervise	_	_
the	_	_
unlabeled	_	_
data	_	_
,	_	_
integrating	_	_
the	_	_
geometry	_	_
into	_	_
learning	_	_
a	_	_
keypoint	_	_
detector	_	_
is	_	_
challenging	_	_
due	_	_
to	_	_
representation	_	_
mismatch	_	_
.	_	_

#6
We	_	_
address	_	_
this	_	_
mismatch	_	_
by	_	_
formulating	_	_
a	_	_
new	_	_
differentiable	_	_
representation	_	_
of	_	_
the	_	_
epipolar	_	_
constraint	_	_
called	_	_
epipolar	_	_
divergence—a	_	_
generalized	_	_
distance	_	_
from	_	_
the	_	_
epipolar	_	_
lines	_	_
to	_	_
the	_	_
corresponding	_	_
keypoint	_	_
distribution	_	_
.	_	_

#7
Epipolar	_	_
divergence	_	_
characterizes	_	_
when	_	_
two	_	_
view	_	_
keypoint	_	_
distributions	_	_
produce	_	_
zero	_	_
reprojection	_	_
error	_	_
.	_	_

#8
We	_	_
design	_	_
a	_	_
twin	_	_
network	_	_
that	_	_
minimizes	_	_
the	_	_
epipolar	_	_
divergence	_	_
through	_	_
stereo	_	_
rectification	_	_
that	_	_
can	_	_
significantly	_	_
alleviate	_	_
computational	_	_
complexity	_	_
and	_	_
sampling	_	_
aliasing	_	_
in	_	_
training	_	_
.	_	_

#9
We	_	_
demonstrate	_	_
that	_	_
our	_	_
framework	_	_
can	_	_
localize	_	_
customized	_	_
keypoints	_	_
of	_	_
diverse	_	_
species	_	_
,	_	_
e.g.	_	_
,	_	_
humans	_	_
,	_	_
dogs	_	_
,	_	_
and	_	_
monkeys	_	_
.	_	_

#10
1	_	_
.	_	_

#11
Introduction	_	_
Human	_	_
pose	_	_
detection	_	_
has	_	_
advanced	_	_
significantly	_	_
over	_	_
the	_	_
last	_	_
few	_	_
years	_	_
[	_	_
8	_	_
,	_	_
43	_	_
,	_	_
64	_	_
,	_	_
69	_	_
]	_	_
,	_	_
driven	_	_
in	_	_
large	_	_
part	_	_
to	_	_
new	_	_
approaches	_	_
based	_	_
on	_	_
deep	_	_
learning	_	_
.	_	_

#12
But	_	_
these	_	_
techniques	_	_
require	_	_
large	_	_
amounts	_	_
of	_	_
labeled	_	_
training	_	_
data	_	_
.	_	_

#13
For	_	_
this	_	_
reason	_	_
,	_	_
pose	_	_
detection	_	_
is	_	_
almost	_	_
always	_	_
demonstrated	_	_
on	_	_
humans	_	_
,	_	_
for	_	_
which	_	_
large-scale	_	_
datasets	_	_
are	_	_
available	_	_
(	_	_
e.g.	_	_
,	_	_
MS	_	_
COCO	_	_
[	_	_
37	_	_
]	_	_
and	_	_
MPII	_	_
[	_	_
2	_	_
]	_	_
)	_	_
.	_	_

#14
What	_	_
about	_	_
pose	_	_
detectors	_	_
for	_	_
other	_	_
animals	_	_
,	_	_
such	_	_
as	_	_
monkeys	_	_
,	_	_
mice	_	_
,	_	_
and	_	_
dogs	_	_
?	_	_

#15
Such	_	_
algorithms	_	_
could	_	_
have	_	_
enormous	_	_
scientific	_	_
impact	_	_
[	_	_
41	_	_
]	_	_
,	_	_
but	_	_
obtaining	_	_
large-scale	_	_
labeled	_	_
training	_	_
data	_	_
would	_	_
be	_	_
a	_	_
substantial	_	_
challenge	_	_
:	_	_
each	_	_
individual	_	_
species	_	_
may	_	_
need	_	_
its	_	_
own	_	_
dataset	_	_
,	_	_
some	_	_
species	_	_
have	_	_
large	_	_
intra-class	_	_
variations	_	_
,	_	_
and	_	_
domain	_	_
experts	_	_
may	_	_
be	_	_
needed	_	_
to	_	_
perform	_	_
the	_	_
labeling	_	_
accurately	_	_
.	_	_

#16
Moreover	_	_
,	_	_
while	_	_
there	_	_
is	_	_
significant	_	_
commercial	_	_
interest	_	_
in	_	_
human	_	_
pose	_	_
recognition	_	_
,	_	_
there	_	_
may	_	_
be	_	_
little	_	_
incentive	_	_
for	_	_
companies	_	_
and	_	_
research	_	_
labs	_	_
to	_	_
invest	_	_
in	_	_
collecting	_	_
large-scale	_	_
datasets	_	_
for	_	_
other	_	_
species	_	_
.	_	_

#17
This	_	_
paper	_	_
addresses	_	_
this	_	_
annotation	_	_
challenge	_	_
by	_	_
leveraging	_	_
multiview	_	_
image	_	_
streams	_	_
.	_	_

#18
Our	_	_
insight	_	_
is	_	_
that	_	_
the	_	_
manual	_	_
effort	_	_
of	_	_
annotation	_	_
can	_	_
be	_	_
significantly	_	_
reduced	_	_
by	_	_
using	_	_
the	_	_
redundant	_	_
visual	_	_
information	_	_
embedded	_	_
in	_	_
the	_	_
multiview	_	_
imagery	_	_
,	_	_
allowing	_	_
cross-view	_	_
self-supervision	_	_
:	_	_
one	_	_
image	_	_
can	_	_
provide	_	_
a	_	_
supervisionary	_	_
signal	_	_
to	_	_
another	_	_
image	_	_
through	_	_
epipolar	_	_
geometry	_	_
without	_	_
3D	_	_
reconstruction	_	_
.	_	_

#19
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
design	_	_
a	_	_
novel	_	_
end-to-end	_	_
semi-supervised	_	_
framework	_	_
to	_	_
utilize	_	_
a	_	_
large	_	_
set	_	_
of	_	_
unlabeled	_	_
multiview	_	_
images	_	_
using	_	_
cross-view	_	_
supervision	_	_
.	_	_

#20
The	_	_
key	_	_
challenge	_	_
of	_	_
integrating	_	_
the	_	_
epipolar	_	_
geometry	_	_
for	_	_
building	_	_
a	_	_
strong	_	_
keypoint	_	_
(	_	_
pose	_	_
)	_	_
detector	_	_
lies	_	_
in	_	_
a	_	_
representational	_	_
mismatch	_	_
:	_	_
the	_	_
geometric	_	_
quantities	_	_
such	_	_
as	_	_
points	_	_
,	_	_
lines	_	_
,	_	_
and	_	_
planes	_	_
are	_	_
represented	_	_
as	_	_
a	_	_
vectors	_	_
[	_	_
18	_	_
]	_	_
(	_	_
Figure	_	_
2	_	_
(	_	_
a	_	_
)	_	_
left	_	_
)	_	_
while	_	_
the	_	_
raster	_	_
representation	_	_
via	_	_
pixel	_	_
response	_	_
(	_	_
heatmap	_	_
[	_	_
8	_	_
,	_	_
43	_	_
,	_	_
69	_	_
]	_	_
)	_	_
has	_	_
been	_	_
shown	_	_
strong	_	_
performance	_	_
on	_	_
keypoint	_	_
detection	_	_
.	_	_

#21
For	_	_
instance	_	_
,	_	_
applying	_	_
the	_	_
epipolar	_	_
constraint	_	_
[	_	_
40	_	_
]	_	_
—	_	_
a	_	_
point	_	_
x	_	_
∈	_	_
R2	_	_
must	deontic	_
lie	_	_
in	_	_
the	_	_
corresponding	_	_
epipolar	_	_
line	_	_
l	_	_
∈	_	_
P2	_	_
—	_	_
can	_	_
be	_	_
expressed	_	_
as	_	_
:	_	_
(	_	_
x̃∗	_	_
)	_	_
Tl∗	_	_
=	_	_
0	_	_
s.t.	_	_
x∗	_	_
=	_	_
argmax	_	_
x	_	_
Pp	_	_
(	_	_
x	_	_
)	_	_
,	_	_
l∗	_	_
=	_	_
argmax	_	_
l	_	_
Pe	_	_
(	_	_
l	_	_
)	_	_
,	_	_
where	_	_
x̃	_	_
is	_	_
the	_	_
homogeneous	_	_
representation	_	_
of	_	_
x	_	_
,	_	_
and	_	_
Pp	_	_
and	_	_
Pe	_	_
are	_	_
the	_	_
distributions	_	_
of	_	_
keypoints	_	_
and	_	_
epipolar	_	_
lines	_	_
.	_	_

#22
Note	_	_
that	_	_
the	_	_
raster	_	_
representation	_	_
involves	_	_
non-differentiable	_	_
argmax	_	_
operations	_	_
,	_	_
which	_	_
are	_	_
not	_	_
trainable	_	_
.	_	_

#23
This	_	_
challenge	_	_
leads	_	_
to	_	_
offline	_	_
reconstruction	_	_
[	_	_
7,58,67	_	_
]	_	_
,	_	_
data	_	_
driven	_	_
depth	_	_
prediction	_	_
[	_	_
31	_	_
,	_	_
53	_	_
,	_	_
54	_	_
,	_	_
65	_	_
,	_	_
74	_	_
]	_	_
,	_	_
or	_	_
the	_	_
usage	_	_
of	_	_
the	_	_
soft-argmax	_	_
operation	_	_
[	_	_
13	_	_
]	_	_
,	_	_
which	_	_
shows	_	_
inferior	_	_
performance	_	_
(	_	_
see	_	_
Figure	_	_
6	_	_
)	_	_
.	_	_

#24
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
formulate	_	_
a	_	_
new	_	_
raster	_	_
representation	_	_
of	_	_
the	_	_
epipolar	_	_
geometry	_	_
that	_	_
eliminates	_	_
the	_	_
argmax	_	_
operations	_	_
.	_	_

#25
We	_	_
prove	_	_
that	_	_
the	_	_
minimization	_	_
of	_	_
geometric	_	_
error	_	_
(	_	_
i.e.	_	_
,	_	_
|x̃Tl|	_	_
)	_	_
is	_	_
equivalent	_	_
to	_	_
minimizing	_	_
epipolar	_	_
divergence—	_	_
a	_	_
generalized	_	_
distance	_	_
from	_	_
the	_	_
epipolar	_	_
lines	_	_
to	_	_
the	_	_
corresponding	_	_
keypoint	_	_
distribution	_	_
.	_	_

#26
With	_	_
this	_	_
measure	_	_
,	_	_
we	_	_
design	_	_
a	_	_
new	_	_
end-to-end	_	_
semi-supervised	_	_
network	_	_
called	_	_
MONET	_	_
(	_	_
Multiview	_	_
Optical	_	_
Supervision	_	_
Network	_	_
)	_	_
.	_	_

#27
The	_	_
network	_	_
efficiently	_	_
leverages	_	_
the	_	_
unlabeled	_	_
multiview	_	_
image	_	_
streams	_	_
with	_	_
limited	_	_
numbers	_	_
of	_	_
manual	_	_
annotations	_	_
(	_	_
<	_	_
1	_	_
%	_	_
)	_	_
.	_	_

#28
We	_	_
integrate	_	_
this	_	_
raster	_	_
formulation	_	_
into	_	_
the	_	_
network	_	_
by	_	_
incorporating	_	_
it	_	_
with	_	_
stereo	_	_
rectification	_	_
,	_	_
which	_	_
reduces	_	_
the	_	_
computational	_	_
complexity	_	_
and	_	_
sampling	_	_
artifacts	_	_
while	_	_
training	_	_
the	_	_
network	_	_
.	_	_

#29
The	_	_
key	_	_
features	_	_
of	_	_
MONET	_	_
include	_	_
that	_	_
(	_	_
1	_	_
)	_	_
it	_	_
does	_	_
not	_	_
require	_	_
offline	_	_
triangulation	_	_
that	_	_
involves	_	_
non-differentiable	_	_
argmax	_	_
and	_	_
RANSAC	_	_
operations	_	_
[	_	_
58	_	_
]	_	_
(	_	_
Figure	_	_
2	_	_
(	_	_
b	_	_
)	_	_
)	_	_
;	_	_
(	_	_
2	_	_
)	_	_
it	_	_
does	_	_
not	_	_
require	_	_
3D	_	_
prediction	_	_
[	_	_
53	_	_
,	_	_
54	_	_
,	_	_
70	_	_
]	_	_
(	_	_
Figure	_	_
2	_	_
(	_	_
c	_	_
)	_	_
)	_	_
,	_	_
i.e.	_	_
,	_	_
it	_	_
deterministically	_	_
transfers	_	_
keypoint	_	_
detections	_	_
in	_	_
one	_	_
image	_	_
to	_	_
the	_	_
other	_	_
via	_	_
epipolar	_	_
geometry	_	_
(	_	_
Figure	_	_
2	_	_
(	_	_
d	_	_
)	_	_
)	_	_
2	_	_
;	_	_
(	_	_
3	_	_
)	_	_
it	_	_
is	_	_
compatible	_	_
with	_	_
any	_	_
keypoint	_	_
detector	_	_
design	_	_
including	_	_
CPM	_	_
[	_	_
69	_	_
]	_	_
and	_	_
Hourglass	_	_
[	_	_
43	_	_
]	_	_
which	_	_
localizes	_	_
keypoints	_	_
through	_	_
a	_	_
raster	_	_
representation	_	_
;	_	_
and	_	_
(	_	_
4	_	_
)	_	_
it	_	_
can	_	_
apply	_	_
to	_	_
general	_	_
multi-camera	_	_
systems	_	_
(	_	_
e.g.	_	_
,	_	_
different	_	_
multi-camera	_	_
rigs	_	_
,	_	_
number	_	_
of	_	_
cameras	_	_
,	_	_
and	_	_
intrinsic	_	_
parameters	_	_
)	_	_
.	_	_

#30
1See	_	_
Section	_	_
3.1	_	_
,	_	_
respectively	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
(	_	_
a	_	_
)	_	_
for	_	_
computation	_	_
of	_	_
Pe	_	_
.	_	_

#31
2This	_	_
is	_	_
analogous	_	_
to	_	_
the	_	_
fundamental	_	_
matrix	_	_
computation	_	_
without	_	_
3D	_	_
estimation	_	_
[	_	_
18	_	_
,	_	_
40	_	_
]	_	_
.	_	_

#32
The	_	_
main	_	_
contributions	_	_
of	_	_
this	_	_
paper	_	_
include	_	_
:	_	_
(	_	_
1	_	_
)	_	_
introducing	_	_
a	_	_
novel	_	_
measure	_	_
called	_	_
the	_	_
epipolar	_	_
divergence	_	_
,	_	_
which	_	_
measures	_	_
the	_	_
geometric	_	_
consistency	_	_
between	_	_
two	_	_
view	_	_
key-point	_	_
distributions	_	_
;	_	_
(	_	_
2	_	_
)	_	_
a	_	_
network	_	_
called	_	_
MONET	_	_
that	_	_
efficiently	_	_
minimizes	_	_
the	_	_
epipolar	_	_
divergence	_	_
via	_	_
stereo	_	_
rectification	_	_
of	_	_
keypoint	_	_
distributions	_	_
;	_	_
(	_	_
3	_	_
)	_	_
a	_	_
technique	_	_
for	_	_
large-scale	_	_
spatiotemporal	_	_
data	_	_
augmentation	_	_
using	_	_
3D	_	_
reconstruction	_	_
of	_	_
keypoint	_	_
trajectories	_	_
;	_	_
(	_	_
4	_	_
)	_	_
experimental	_	_
results	_	_
that	_	_
demonstrate	_	_
that	_	_
MONET	_	_
is	_	_
flexible	_	_
enough	_	_
to	_	_
detect	_	_
keypoints	_	_
in	_	_
various	_	_
subjects	_	_
(	_	_
humans	_	_
,	_	_
dogs	_	_
,	_	_
and	_	_
monkeys	_	_
)	_	_
in	_	_
different	_	_
camera	_	_
rigs	_	_
and	_	_
to	_	_
outperform	_	_
existing	_	_
baselines	_	_
in	_	_
terms	_	_
of	_	_
localization	_	_
accuracy	_	_
and	_	_
precision	_	_
(	_	_
re-projection	_	_
error	_	_
)	_	_
.	_	_

#33
2	_	_
.	_	_

#34
Related	_	_
Work	_	_
The	_	_
physical	_	_
and	_	_
social	_	_
behaviors	_	_
of	_	_
non-human	_	_
species	_	_
such	_	_
as	_	_
rhesus	_	_
macaque	_	_
monkeys	_	_
have	_	_
been	_	_
widely	_	_
used	_	_
as	_	_
a	_	_
window	_	_
to	_	_
study	_	_
human	_	_
activities	_	_
in	_	_
neuroscience	_	_
and	_	_
psychology	_	_
.	_	_

#35
While	_	_
measuring	_	_
their	_	_
subtle	_	_
behaviors	_	_
in	_	_
the	_	_
form	_	_
of	_	_
3D	_	_
anatomic	_	_
landmarks	_	_
is	_	_
key	_	_
,	_	_
implementing	_	_
markerbased	_	_
3D	_	_
tracking	_	_
systems	_	_
is	_	_
challenging	_	_
due	_	_
to	_	_
the	_	_
animal’s	_	_
sensitivity	_	_
to	_	_
reflective	_	_
markers	_	_
and	_	_
occlusion	_	_
by	_	_
fur	_	_
,	_	_
which	_	_
limits	_	_
its	_	_
applications	_	_
to	_	_
restricted	_	_
body	_	_
motions	_	_
(	_	_
e.g.	_	_
,	_	_
body	_	_
tied	_	_
to	_	_
a	_	_
chair	_	_
)	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#36
Vision-based	_	_
marker-less	_	_
motion	_	_
capture	_	_
is	_	_
a	_	_
viable	_	_
solution	_	_
to	_	_
measure	_	_
their	_	_
free	_	_
ranging	_	_
behaviors	_	_
[	_	_
16	_	_
,	_	_
42	_	_
,	_	_
55	_	_
]	_	_
.	_	_

#37
In	_	_
general	_	_
,	_	_
the	_	_
number	_	_
of	_	_
3D	_	_
pose	_	_
configurations	_	_
of	_	_
a	_	_
deformable	_	_
articulated	_	_
body	_	_
is	_	_
exponential	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
number	_	_
of	_	_
joints	_	_
.	_	_

#38
The	_	_
2D	_	_
projections	_	_
of	_	_
the	_	_
3D	_	_
body	_	_
introduces	_	_
substantial	_	_
variability	_	_
in	_	_
illumination	_	_
,	_	_
appearance	_	_
,	_	_
and	_	_
occlusion	_	_
,	_	_
which	_	_
makes	_	_
pose	_	_
estimation	_	_
challenging	_	_
.	_	_

#39
But	_	_
the	_	_
space	_	_
of	_	_
possible	_	_
pose	_	_
configurations	_	_
has	_	_
structure	_	_
that	_	_
can	_	_
be	_	_
captured	_	_
by	_	_
efficient	_	_
spatial	_	_
representations	_	_
such	_	_
as	_	_
pictorial	_	_
structures	_	_
[	_	_
3,4,14,25,50,51,71	_	_
]	_	_
,	_	_
hierarchical	_	_
and	_	_
non-tree	_	_
models	_	_
[	_	_
12	_	_
,	_	_
32	_	_
,	_	_
35	_	_
,	_	_
57	_	_
,	_	_
60	_	_
,	_	_
62	_	_
,	_	_
68	_	_
]	_	_
and	_	_
convolutional	_	_
architectures	_	_
[	_	_
9	_	_
,	_	_
10	_	_
,	_	_
33	_	_
,	_	_
39	_	_
,	_	_
44	_	_
,	_	_
48	_	_
,	_	_
49	_	_
,	_	_
63	_	_
,	_	_
64	_	_
]	_	_
,	_	_
and	_	_
inference	_	_
on	_	_
these	_	_
structures	_	_
can	_	_
be	_	_
performed	_	_
efficiently	_	_
using	_	_
clever	_	_
algorithms	_	_
,	_	_
e.g.	_	_
,	_	_
dynamic	_	_
programming	_	_
,	_	_
convex	_	_
relaxation	_	_
,	_	_
and	_	_
approximate	_	_
algorithms	_	_
.	_	_

#40
Albeit	_	_
efficient	_	_
and	_	_
accurate	_	_
on	_	_
canonical	_	_
images	_	_
,	_	_
they	_	_
exhibit	_	_
inferior	_	_
performance	_	_
on	_	_
images	_	_
in	_	_
the	_	_
long-tail	_	_
distribution	_	_
,	_	_
e.g.	_	_
,	_	_
a	_	_
pigeon	_	_
pose	_	_
of	_	_
yoga	_	_
.	_	_

#41
Fully	_	_
supervised	_	_
learning	_	_
frameworks	_	_
using	_	_
millions	_	_
of	_	_
perceptrons	_	_
in	_	_
convolutional	_	_
neural	_	_
networks	_	_
(	_	_
CNNs	_	_
)	_	_
[	_	_
8	_	_
,	_	_
43	_	_
,	_	_
64	_	_
,	_	_
69	_	_
]	_	_
can	_	_
address	_	_
this	_	_
long-tail	_	_
distribution	_	_
issue	_	_
by	_	_
leveraging	_	_
a	_	_
sheer	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
annotated	_	_
by	_	_
crowd	_	_
workers	_	_
[	_	_
2	_	_
,	_	_
37	_	_
,	_	_
56	_	_
]	_	_
.	_	_

#42
However	_	_
,	_	_
due	_	_
to	_	_
the	_	_
number	_	_
of	_	_
parameters	_	_
in	_	_
a	_	_
CNN	_	_
,	_	_
the	_	_
trained	_	_
model	_	_
can	_	_
be	_	_
highly	_	_
biased	_	_
when	_	_
the	_	_
number	_	_
of	_	_
data	_	_
samples	_	_
is	_	_
not	_	_
sufficient	_	_
(	_	_
<	_	_
1M	_	_
)	_	_
.	_	_

#43
Semi-supervised	_	_
and	_	_
weakly-supervised	_	_
learning	_	_
frameworks	_	_
train	_	_
CNN	_	_
models	_	_
with	_	_
limited	_	_
number	_	_
of	_	_
training	_	_
data	_	_
[	_	_
5,23,36,38,45,46,59,61,66,75	_	_
]	_	_
.	_	_

#44
For	_	_
instance	_	_
,	_	_
temporal	_	_
consistency	_	_
derived	_	_
by	_	_
tracking	_	_
during	_	_
training	_	_
can	_	_
provide	_	_
a	_	_
supervisionary	_	_
signal	_	_
for	_	_
body	_	_
joint	_	_
detection	_	_
[	_	_
36	_	_
]	_	_
.	_	_

#45
Geometric	_	_
(	_	_
such	_	_
as	_	_
3DPS	_	_
model	_	_
[	_	_
5	_	_
]	_	_
)	_	_
and	_	_
spatial	_	_
[	_	_
59	_	_
]	_	_
relationship	_	_
are	_	_
another	_	_
way	_	_
to	_	_
supervise	_	_
body	_	_
keypoint	_	_
estimation	_	_
.	_	_

#46
Active	_	_
learning	_	_
that	_	_
finds	_	_
the	_	_
most	_	_
informative	_	_
images	_	_
to	_	_
be	_	_
annotated	_	_
can	_	_
alleviate	_	_
the	_	_
amount	_	_
of	_	_
labeling	_	_
effort	_	_
[	_	_
38	_	_
]	_	_
,	_	_
and	_	_
geometric	_	_
[	_	_
46	_	_
]	_	_
and	_	_
temporal	_	_
[	_	_
23	_	_
]	_	_
in	_	_
2D	_	_
[	_	_
30	_	_
]	_	_
and	_	_
3D	_	_
[	_	_
27,72	_	_
]	_	_
consistency	_	_
can	_	_
also	_	_
be	_	_
used	_	_
to	_	_
augment	_	_
annotation	_	_
data	_	_
.	_	_

#47
These	_	_
approaches	_	_
embed	_	_
underlying	_	_
spatial	_	_
structures	_	_
such	_	_
as	_	_
3D	_	_
skeletons	_	_
and	_	_
meshes	_	_
that	_	_
regularize	_	_
the	_	_
network	_	_
weights	_	_
.	_	_

#48
For	_	_
instance	_	_
,	_	_
motion	_	_
capture	_	_
data	_	_
can	_	_
be	_	_
used	_	_
to	_	_
jointly	_	_
learn	_	_
2D	_	_
and	_	_
3D	_	_
keypoints	_	_
[	_	_
75	_	_
]	_	_
,	_	_
and	_	_
scanned	_	_
human	_	_
body	_	_
models	_	_
are	_	_
used	_	_
to	_	_
validate	_	_
2D	_	_
pose	_	_
estimation	_	_
via	_	_
reprojection	_	_
[	_	_
17,29,31,73,76	_	_
]	_	_
,	_	_
e.g.	_	_
,	_	_
by	_	_
using	_	_
a	_	_
DoubleFusion	_	_
system	_	_
that	_	_
can	_	_
simultaneously	_	_
reconstruct	_	_
the	_	_
inner	_	_
body	_	_
shape	_	_
and	_	_
pose	_	_
.	_	_

#49
The	_	_
outer	_	_
surface	_	_
geometry	_	_
and	_	_
motion	_	_
in	_	_
real-time	_	_
by	_	_
using	_	_
a	_	_
single	_	_
depth	_	_
camera	_	_
[	_	_
73	_	_
]	_	_
and	_	_
recovery	_	_
human	_	_
meshes	_	_
that	_	_
can	_	_
reconstruct	_	_
a	_	_
full	_	_
3D	_	_
mesh	_	_
of	_	_
human	_	_
bodies	_	_
from	_	_
a	_	_
single	_	_
RGB	_	_
camera	_	_
by	_	_
having	_	_
2D	_	_
ground	_	_
truth	_	_
annotations	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#50
Graphical	_	_
models	_	_
can	_	_
also	_	_
be	_	_
applied	_	_
for	_	_
animal	_	_
shape	_	_
reconstruction	_	_
by	_	_
learning	_	_
a	_	_
3D	_	_
model	_	_
based	_	_
on	_	_
a	_	_
small	_	_
set	_	_
of	_	_
3D	_	_
scans	_	_
of	_	_
toy	_	_
figurines	_	_
in	_	_
arbitrary	_	_
poses	_	_
and	_	_
refining	_	_
the	_	_
model	_	_
and	_	_
initial	_	_
registration	_	_
of	_	_
scans	_	_
together	_	_
,	_	_
and	_	_
then	_	_
generalizing	_	_
it	_	_
by	_	_
fitting	_	_
the	_	_
model	_	_
to	_	_
real	_	_
images	_	_
of	_	_
animal	_	_
species	_	_
out	_	_
of	_	_
the	_	_
training	_	_
set	_	_
[	_	_
76	_	_
]	_	_
.	_	_

#51
Notably	_	_
,	_	_
a	_	_
multi-camera	_	_
system	_	_
can	_	_
be	_	_
used	_	_
to	_	_
cross-view	_	_
supervise	_	_
multiview	_	_
synchronized	_	_
images	_	_
using	_	_
iterative	_	_
process	_	_
of	_	_
3D	_	_
reconstruction	_	_
and	_	_
network	_	_
training	_	_
[	_	_
54	_	_
,	_	_
58	_	_
]	_	_
.	_	_

#52
Unlike	_	_
existing	_	_
methods	_	_
,	_	_
MONET	_	_
does	_	_
not	_	_
rely	_	_
on	_	_
a	_	_
spatial	_	_
model	_	_
.	_	_

#53
To	_	_
our	_	_
knowledge	_	_
,	_	_
this	_	_
is	_	_
the	_	_
first	_	_
paper	_	_
that	_	_
jointly	_	_
reconstructs	_	_
and	_	_
trains	_	_
a	_	_
keypoint	_	_
detector	_	_
without	_	_
iterative	_	_
processes	_	_
using	_	_
epipolar	_	_
geometry	_	_
.	_	_

#54
We	_	_
integrate	_	_
reconstruction	_	_
and	_	_
learning	_	_
through	_	_
a	_	_
new	_	_
measure	_	_
of	_	_
keypoint	_	_
distributions	_	_
called	_	_
epipolar	_	_
divergence	_	_
,	_	_
which	_	_
can	_	_
apply	_	_
to	_	_
general	_	_
subjects	_	_
including	_	_
non-human	_	_
species	_	_
where	_	_
minimal	_	_
manual	_	_
annotations	_	_
are	_	_
available	_	_
.	_	_

#55
3	_	_
.	_	_

#56
MONET	_	_
We	_	_
present	_	_
a	_	_
semi-supervised	_	_
learning	_	_
framework	_	_
for	_	_
training	_	_
a	_	_
keypoint	_	_
detector	_	_
by	_	_
leveraging	_	_
multiview	_	_
image	_	_
streams	_	_
for	_	_
which	_	_
|DU	_	_
|	_	_
|DL|	_	_
,	_	_
where	_	_
DL	_	_
and	_	_
DU	_	_
are	_	_
labeled	_	_
and	_	_
unlabeled	_	_
data	_	_
,	_	_
respectively	_	_
.	_	_

#57
We	_	_
learn	_	_
a	_	_
network	_	_
model	_	_
that	_	_
takes	_	_
an	_	_
input	_	_
image	_	_
I	_	_
and	_	_
outputs	_	_
a	_	_
keypoint	_	_
distribution	_	_
,	_	_
i.e.	_	_
,	_	_
φ	_	_
(	_	_
I	_	_
;	_	_
w	_	_
)	_	_
∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
W×H×C	_	_
where	_	_
I	_	_
is	_	_
an	_	_
input	_	_
image	_	_
,	_	_
w	_	_
is	_	_
the	_	_
learned	_	_
network	_	_
weights	_	_
,	_	_
and	_	_
W	_	_
,	_	_
H	_	_
,	_	_
and	_	_
C	_	_
are	_	_
the	_	_
width	_	_
,	_	_
height	_	_
,	_	_
and	_	_
the	_	_
number	_	_
of	_	_
keypoints	_	_
.	_	_

#58
To	_	_
enable	_	_
end-to-end	_	_
cross-view	_	_
supervision	_	_
without	_	_
3D	_	_
reconstruction	_	_
,	_	_
we	_	_
formulate	_	_
a	_	_
novel	_	_
raster	_	_
representation	_	_
of	_	_
epipolar	_	_
geometry	_	_
in	_	_
Section	_	_
3.1	_	_
,	_	_
and	_	_
show	_	_
how	_	_
to	_	_
implement	_	_
it	_	_
in	_	_
practice	_	_
using	_	_
stereo	_	_
rectification	_	_
in	_	_
Section	_	_
3.2	_	_
.	_	_

#59
The	_	_
full	_	_
learning	_	_
framework	_	_
is	_	_
described	_	_
in	_	_
Section	_	_
3.3	_	_
by	_	_
incorporating	_	_
a	_	_
bootstrapping	_	_
prior	_	_
.	_	_

#60
3.1.	_	_
Epipolar	_	_
Divergence	_	_

#61
A	_	_
point	_	_
in	_	_
the	_	_
ith	_	_
image	_	_
xi	_	_
∈	_	_
R2	_	_
is	_	_
transferred	_	_
to	_	_
form	_	_
a	_	_
corresponding	_	_
epipolar	_	_
line	_	_
in	_	_
the	_	_
jth	_	_
image	_	_
via	_	_
the	_	_
fundamental	_	_
matrix	_	_
F	_	_
between	_	_
two	_	_
relative	_	_
camera	_	_
poses	_	_
,	_	_
which	_	_
measures	_	_
geometric	_	_
consistency	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
corresponding	_	_
point	_	_
xj	_	_
must	deontic	_
lie	_	_
in	_	_
the	_	_
epipolar	_	_
line	_	_
[	_	_
18	_	_
]	_	_
:	_	_
D	_	_
(	_	_
xi	_	_
,	_	_
xj	_	_
)	_	_
=	_	_
∣∣x̃T	_	_
j	_	_
(	_	_
Fx̃i	_	_
)	_	_
∣∣	_	_
∝	_	_
inf	_	_
x∈Fx̃i	_	_
‖x−	_	_
xj‖.	_	_
(	_	_
1	_	_
)	_	_

#62
The	_	_
infimum	_	_
operation	_	_
measures	_	_
the	_	_
distance	_	_
between	_	_
the	_	_
closest	_	_
point	_	_
in	_	_
the	_	_
epipolar	_	_
line	_	_
(	_	_
Fx̃i	_	_
)	_	_
and	_	_
xj	_	_
in	_	_
the	_	_
jth	_	_
image	_	_
.	_	_

#63
We	_	_
generalize	_	_
the	_	_
epipolar	_	_
line	_	_
transfer	_	_
to	_	_
define	_	_
the	_	_
distance	_	_
between	_	_
keypoint	_	_
distributions	_	_
.	_	_

#64
Let	_	_
Pi	_	_
:	_	_
R2	_	_
→	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
be	_	_
the	_	_
keypoint	_	_
distribution	_	_
given	_	_
the	_	_
ith	_	_
image	_	_
computed	_	_
by	_	_
a	_	_
keypoint	_	_
detector	_	_
,	_	_
i.e.	_	_
,	_	_
Pi	_	_
(	_	_
x	_	_
)	_	_
=	_	_
φ	_	_
(	_	_
Ii	_	_
;	_	_
w	_	_
)	_	_
|x	_	_
,	_	_
and	_	_
Pj→i	_	_
:	_	_
R	_	_
2	_	_
→	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
be	_	_
the	_	_
keypoint	_	_
distribution	_	_
in	_	_
the	_	_
ith	_	_
image	_	_
transferred	_	_
from	_	_
the	_	_
jth	_	_
image	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
3	_	_
(	_	_
a	_	_
)	_	_
.	_	_

#65
Note	_	_
that	_	_
we	_	_
abuse	_	_
notation	_	_
by	_	_
omitting	_	_
the	_	_
keypoint	_	_
index	_	_
,	_	_
as	_	_
each	_	_
keypoint	_	_
is	_	_
considered	_	_
independently	_	_
.	_	_

#66
Consider	_	_
a	_	_
max-pooling	_	_
operation	_	_
along	_	_
a	_	_
line	_	_
,	_	_
g	_	_
:	_	_
g	_	_
(	_	_
l	_	_
;	_	_
P	_	_
)	_	_
=	_	_
sup	_	_
x∈l	_	_
P	_	_
(	_	_
x	_	_
)	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
P	_	_
:	_	_
R2	_	_
→	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
is	_	_
a	_	_
distribution	_	_
and	_	_
l	_	_
∈	_	_
P2	_	_
is	_	_
a	_	_
2D	_	_
line	_	_
parameter	_	_
.	_	_

#67
g	_	_
takes	_	_
the	_	_
maximum	_	_
value	_	_
along	_	_
the	_	_
line	_	_
in	_	_
P	_	_
.	_	_

#68
Given	_	_
the	_	_
keypoint	_	_
distribution	_	_
in	_	_
the	_	_
jth	_	_
image	_	_
Pj	_	_
,	_	_
the	_	_
transferred	_	_
keypoint	_	_
distribution	_	_
can	_	_
be	_	_
obtained	_	_
:	_	_
Pj→i	_	_
(	_	_
xi	_	_
)	_	_
=	_	_
g	_	_
(	_	_
Fx̃i	_	_
;	_	_
Pj	_	_
)	_	_
.	_	_

#69
(	_	_
3	_	_
)	_	_
The	_	_
supremum	_	_
operation	_	_
is	_	_
equivalent	_	_
to	_	_
the	_	_
infimum	_	_
operation	_	_
in	_	_
Equation	_	_
(	_	_
1	_	_
)	_	_
,	_	_
where	_	_
it	_	_
finds	_	_
the	_	_
most	_	_
likely	_	_
(	_	_
closest	_	_
)	_	_
correspondences	_	_
along	_	_
the	_	_
epipolar	_	_
line	_	_
.	_	_

#70
The	_	_
first	_	_
two	_	_
images	_	_
in	_	_
Figure	_	_
3	_	_
(	_	_
a	_	_
)	_	_
illustrate	_	_
the	_	_
keypoint	_	_
distribution	_	_
transfer	_	_
via	_	_
Equation	_	_
(	_	_
3	_	_
)	_	_
.	_	_

#71
The	_	_
keypoint	_	_
distribution	_	_
in	_	_
the	_	_
ith	_	_
image	_	_
is	_	_
deterministically	_	_
transformed	_	_
to	_	_
the	_	_
rasterized	_	_
epipolar	_	_
line	_	_
distribution	_	_
in	_	_
the	_	_
jth	_	_
image	_	_
,	_	_
i.e.	_	_
,	_	_
no	_	_
explicit	_	_
3D	_	_
reconstruction	_	_
(	_	_
triangulation	_	_
or	_	_
depth	_	_
prediction	_	_
)	_	_
is	_	_
needed	_	_
.	_	_

#72
In	_	_
fact	_	_
,	_	_
the	_	_
transferred	_	_
distribution	_	_
is	_	_
a	_	_
posterior	_	_
distribution	_	_
of	_	_
a	_	_
3D	_	_
keypoint	_	_
given	_	_
a	_	_
uniform	_	_
depth	_	_
prior	_	_
.	_	_

#73
Pi	_	_
and	_	_
Pj→i	_	_
can	_	_
not	_	_
be	_	_
directly	_	_
matched	_	_
because	_	_
Pi	_	_
is	_	_
a	_	_
point	_	_
distribution	_	_
while	_	_
Pj→i	_	_
is	_	_
a	_	_
line	_	_
distribution	_	_
.	_	_

#74
A	_	_
key	_	_
observation	_	_
is	_	_
that	_	_
points	_	_
that	_	_
lie	_	_
on	_	_
the	_	_
same	_	_
epipolar	_	_
line	_	_
in	_	_
Pj→i	_	_
have	_	_
the	_	_
same	_	_
probability	_	_
,	_	_
i.e.	_	_
,	_	_
Pi→j	_	_
(	_	_
xj	_	_
)	_	_
=	_	_
Pi→j	_	_
(	_	_
yj	_	_
)	_	_
if	_	_
FTx̃j	_	_
∝	_	_
FTỹj	_	_
as	_	_
shown	_	_
in	_	_
the	_	_
second	_	_
image	_	_
of	_	_
Figure	_	_
3	_	_
(	_	_
a	_	_
)	_	_
.	_	_

#75
This	_	_
indicates	_	_
that	_	_
the	_	_
transferred	_	_
distribution	_	_
can	_	_
be	_	_
parametrized	_	_
by	_	_
the	_	_
slope	_	_
of	_	_
an	_	_
epipolar	_	_
line	_	_
,	_	_
θ	_	_
∈	_	_
S	_	_
,	_	_
i.e.	_	_
,	_	_
Qj→i	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
g	_	_
(	_	_
li	_	_
(	_	_
θ	_	_
)	_	_
;	_	_
Pj→i	_	_
)	_	_
,	_	_
(	_	_
4	_	_
)	_	_
ixix	_	_
jP	_	_
j	_	_
iP	_	_
→	_	_
iP	_	_
(	_	_
)	_	_
i	_	_
θl	_	_
(	_	_
)	_	_
i	_	_
θl	_	_
(	_	_
)	_	_
j	_	_
iθ	_	_
=	_	_
l	_	_
Fx	_	_
(	_	_
)	_	_
j	_	_
iQ	_	_
θ→	_	_
(	_	_
)	_	_
iQ	_	_
θ	_	_
iy	_	_
θ	_	_
(	_	_
||	_	_
)	_	_
E	_	_
i	_	_
j	_	_
iD	_	_
Q	_	_
Q	_	_
→	_	_
(	_	_
a	_	_
)	_	_
Geometric	_	_
consistency	_	_
via	_	_
minimizing	_	_
epipolar	_	_
divergence	_	_
θ	_	_
Epipolar	_	_
plane	_	_
(	_	_
)	_	_
j	_	_
θl	_	_
(	_	_
)	_	_
i	_	_
θl	_	_
jC	_	_
iC	_	_
Baseline	_	_
(	_	_
b	_	_
)	_	_
Epipolar	_	_
plane	_	_
parametrization	_	_
Figure	_	_
3	_	_
:	_	_
(	_	_
a	_	_
)	_	_
The	_	_
keypoint	_	_
distribution	_	_
of	_	_
the	_	_
knee	_	_
joint	_	_
for	_	_
the	_	_
jth	_	_
image	_	_
,	_	_
Pj	_	_
,	_	_
is	_	_
transferred	_	_
to	_	_
the	_	_
ith	_	_
image	_	_
to	_	_
form	_	_
the	_	_
epipolar	_	_
line	_	_
distribution	_	_
Pj→i	_	_
(	_	_
xi	_	_
)	_	_
.	_	_

#76
Note	_	_
that	_	_
the	_	_
points	_	_
that	_	_
lie	_	_
in	_	_
the	_	_
same	_	_
epipolar	_	_
line	_	_
have	_	_
the	_	_
equal	_	_
transferred	_	_
distribution	_	_
,	_	_
Pj→i	_	_
(	_	_
xi	_	_
)	_	_
=	_	_
Pj→i	_	_
(	_	_
yi	_	_
)	_	_
,	_	_
and	_	_
therefore	_	_
(	_	_
b	_	_
)	_	_
the	_	_
distribution	_	_
can	_	_
be	_	_
reparametrized	_	_
by	_	_
the	_	_
1D	_	_
rotation	_	_
θ	_	_
∈	_	_
S	_	_
about	_	_
the	_	_
baseline	_	_
where	_	_
Ci	_	_
and	_	_
Cj	_	_
are	_	_
the	_	_
camera	_	_
optical	_	_
centers	_	_
.	_	_

#77
We	_	_
match	_	_
two	_	_
distributions	_	_
:	_	_
the	_	_
distribution	_	_
transferred	_	_
from	_	_
the	_	_
ith	_	_
image	_	_
Qj→i	_	_
(	_	_
θ	_	_
)	_	_
and	_	_
the	_	_
distribution	_	_
of	_	_
keypoint	_	_
in	_	_
the	_	_
jth	_	_
image	_	_
Qi	_	_
(	_	_
θ	_	_
)	_	_
.	_	_

#78
The	_	_
minimization	_	_
of	_	_
the	_	_
epipolar	_	_
divergence	_	_
DE	_	_
(	_	_
Qi||Qj→i	_	_
)	_	_
is	_	_
provably	_	_
equivalent	_	_
to	_	_
reprojection	_	_
error	_	_
minimization	_	_
.	_	_

#79
where	_	_
li	_	_
(	_	_
θ	_	_
)	_	_
is	_	_
the	_	_
line	_	_
passing	_	_
through	_	_
the	_	_
epipole	_	_
parametrized	_	_
by	_	_
θ	_	_
in	_	_
the	_	_
ith	_	_
image	_	_
,	_	_
and	_	_
Qj→i	_	_
:	_	_
S	_	_
→	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
is	_	_
a	_	_
flattened	_	_
1D	_	_
distribution	_	_
across	_	_
the	_	_
line	_	_
.	_	_

#80
Similarly	_	_
,	_	_
the	_	_
flattened	_	_
keypoint	_	_
distribution	_	_
of	_	_
Pi	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
Qi	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
g	_	_
(	_	_
li	_	_
(	_	_
θ	_	_
)	_	_
;	_	_
Pi	_	_
)	_	_
.	_	_

#81
Theorem	_	_
1	_	_
.	_	_

#82
Two	_	_
keypoint	_	_
distributions	_	_
Pi	_	_
and	_	_
Pj	_	_
are	_	_
geometrically	_	_
consistent	_	_
,	_	_
i.e.	_	_
,	_	_
zero	_	_
reprojection	_	_
error	_	_
,	_	_
if	_	_
Qi	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
Qj→i	_	_
(	_	_
θ	_	_
)	_	_
.	_	_

#83
See	_	_
the	_	_
proof	_	_
in	_	_
Appendix	_	_
.	_	_

#84
Theorem	_	_
1	_	_
states	_	_
the	_	_
necessary	_	_
condition	_	_
of	_	_
zero	_	_
reprojection	_	_
:	_	_
the	_	_
detected	_	_
keypoints	_	_
across	_	_
views	_	_
must	deontic	_
lie	_	_
in	_	_
the	_	_
same	_	_
epipolar	_	_
plane	_	_
in	_	_
3D	_	_
.	_	_

#85
Figure	_	_
11	_	_
illustrates	_	_
the	_	_
epipolar	_	_
plane	_	_
that	_	_
is	_	_
constructed	_	_
by	_	_
the	_	_
baseline	_	_
and	_	_
the	_	_
3D	_	_
ray	_	_
(	_	_
inverse	_	_
projection	_	_
)	_	_
of	_	_
the	_	_
detected	_	_
keypoint	_	_
.	_	_

#86
Matching	_	_
Qi	_	_
and	_	_
Qj→i	_	_
is	_	_
equivalent	_	_
to	_	_
matching	_	_
the	_	_
probabilities	_	_
of	_	_
epipolar	_	_
3D	_	_
planes	_	_
,	_	_
which	_	_
can	_	_
be	_	_
parametrized	_	_
by	_	_
their	_	_
surface	_	_
normal	_	_
(	_	_
θ	_	_
)	_	_
.	_	_

#87
To	_	_
match	_	_
their	_	_
distributions	_	_
,	_	_
we	_	_
define	_	_
an	_	_
epipolar	_	_
divergence	_	_
that	_	_
measures	_	_
the	_	_
difference	_	_
between	_	_
two	_	_
key-point	_	_
distributions	_	_
using	_	_
relative	_	_
entropy	_	_
inspired	_	_
by	_	_
Kullback–Leibler	_	_
(	_	_
KL	_	_
)	_	_
divergence	_	_
[	_	_
34	_	_
]	_	_
:	_	_
DE	_	_
(	_	_
Qi||Qj→i	_	_
)	_	_
=	_	_
∫	_	_
S	_	_
Qi	_	_
(	_	_
θ	_	_
)	_	_
log	_	_
Qi	_	_
(	_	_
θ	_	_
)	_	_
Qj→i	_	_
(	_	_
θ	_	_
)	_	_
dθ	_	_
.	_	_

#88
(	_	_
5	_	_
)	_	_
This	_	_
epipolar	_	_
divergence	_	_
measure	_	_
how	_	_
two	_	_
keypoint	_	_
distributions	_	_
are	_	_
geometrically	_	_
consistent	_	_
.	_	_

#89
3.2	_	_
.	_	_

#90
Cross-view	_	_
Supervision	_	_
via	_	_
Rectification	_	_
In	_	_
practice	_	_
,	_	_
embedding	_	_
Equation	_	_
(	_	_
5	_	_
)	_	_
into	_	_
an	_	_
end-to-end	_	_
neural	_	_
network	_	_
is	_	_
non-trivial	_	_
because	_	_
(	_	_
a	_	_
)	_	_
a	_	_
new	_	_
max-pooling	_	_
operation	_	_
over	_	_
oblique	_	_
epipolar	_	_
lines	_	_
in	_	_
Equation	_	_
(	_	_
3	_	_
)	_	_
needs	_	_
to	_	_
be	_	_
defined	_	_
;	_	_
(	_	_
b	_	_
)	_	_
the	_	_
sampling	_	_
interval	_	_
for	_	_
max-pooling	_	_
along	_	_
the	_	_
line	_	_
is	_	_
arbitrary	_	_
,	_	_
i.e.	_	_
,	_	_
uniform	_	_
sampling	_	_
does	_	_
not	_	_
encode	_	_
geometric	_	_
meaning	_	_
such	_	_
as	_	_
depth	_	_
;	_	_
and	_	_
(	_	_
c	_	_
)	_	_
the	_	_
sampling	_	_
interval	_	_
across	_	_
θ	_	_
is	_	_
also	_	_
arbitrary	_	_
.	_	_

#91
These	_	_
factors	_	_
increase	_	_
computational	_	_
complexity	_	_
and	_	_
sampling	_	_
artifacts	_	_
in	_	_
the	_	_
process	_	_
of	_	_
training	_	_
.	_	_

#92
We	_	_
introduce	_	_
a	_	_
new	_	_
operation	_	_
inspired	_	_
by	_	_
stereo	_	_
rectification	_	_
,	_	_
which	_	_
warps	_	_
a	_	_
keypoint	_	_
distribution	_	_
such	_	_
that	_	_
the	_	_
epipolar	_	_
lines	_	_
become	_	_
parallel	_	_
(	_	_
horizontal	_	_
)	_	_
as	_	_
shown	_	_
the	_	_
bottom	_	_
right	_	_
image	_	_
in	_	_
Figure	_	_
4	_	_
.	_	_

#93
This	_	_
rectification	_	_
allows	_	_
converting	_	_
the	_	_
max-pooling	_	_
operation	_	_
over	_	_
an	_	_
oblique	_	_
epipolar	_	_
line	_	_
into	_	_
regular	_	_
row-wise	_	_
max-pooling	_	_
,	_	_
i.e.	_	_
,	_	_
epipolar	_	_
line	_	_
can	_	_
be	_	_
parametrized	_	_
by	_	_
its	_	_
height	_	_
l	_	_
(	_	_
v	_	_
)	_	_
.	_	_

#94
Equation	_	_
(	_	_
2	_	_
)	_	_
can	_	_
be	_	_
re-written	_	_
with	_	_
the	_	_
rectified	_	_
keypoint	_	_
distribution	_	_
:	_	_
g	_	_
(	_	_
v	_	_
;	_	_
P	_	_
)	_	_
=	_	_
g	_	_
(	_	_
l	_	_
(	_	_
v	_	_
)	_	_
;	_	_
P	_	_
)	_	_
=	_	_
max	_	_
u	_	_
P	_	_
(	_	_
[	_	_
u	_	_
v	_	_
]	_	_
)	_	_
(	_	_
6	_	_
)	_	_
where	_	_
(	_	_
u	_	_
,	_	_
v	_	_
)	_	_
is	_	_
the	_	_
x	_	_
,	_	_
y-coordinate	_	_
of	_	_
a	_	_
point	_	_
in	_	_
the	_	_
rectified	_	_
keypoint	_	_
distribution	_	_
P	_	_
warped	_	_
from	_	_
P	_	_
,	_	_
i.e.	_	_
,	_	_
P	_	_
(	_	_
x	_	_
)	_	_
=	_	_
P	_	_
(	_	_
H−1r	_	_
x	_	_
)	_	_
where	_	_
Hr	_	_
is	_	_
the	_	_
homography	_	_
of	_	_
stereorectification	_	_
.	_	_

#95
P	_	_
is	_	_
computed	_	_
by	_	_
inverse	_	_
homography	_	_
warping	_	_
with	_	_
bilinear	_	_
interpolation	_	_
[	_	_
19	_	_
,	_	_
24	_	_
]	_	_
.	_	_

#96
This	_	_
rectification	_	_
simplifies	_	_
the	_	_
flattening	_	_
operation	_	_
in	_	_
Equation	_	_
(	_	_
4	_	_
)	_	_
:	_	_
Qj→i	_	_
(	_	_
v	_	_
)	_	_
=	_	_
g	_	_
(	_	_
v	_	_
;	_	_
P	_	_
j→i	_	_
)	_	_
=	_	_
g	_	_
(	_	_
av	_	_
+	_	_
b	_	_
;	_	_
P	_	_
j	_	_
)	_	_
,	_	_
Qi	_	_
(	_	_
v	_	_
)	_	_
=	_	_
g	_	_
(	_	_
v	_	_
;	_	_
P	_	_
i	_	_
)	_	_
,	_	_
(	_	_
7	_	_
)	_	_
where	_	_
a	_	_
and	_	_
b	_	_
are	_	_
re-scaling	_	_
factors	_	_
between	_	_
the	_	_
ith	_	_
and	_	_
jth	_	_
cameras	_	_
,	_	_
accounting	_	_
different	_	_
camera	_	_
intrinsic	_	_
and	_	_
cropping	_	_
parameters	_	_
.	_	_

#97
See	_	_
Appendix	_	_
for	_	_
more	_	_
details	_	_
.	_	_

#98
The	_	_
key	_	_
innovation	_	_
of	_	_
Equation	_	_
(	_	_
7	_	_
)	_	_
is	_	_
that	_	_
Qj→i	_	_
(	_	_
v	_	_
)	_	_
is	_	_
no	_	_
longer	_	_
parametrized	_	_
by	_	_
θ	_	_
where	_	_
an	_	_
additional	_	_
sampling	_	_
over	_	_
θ	_	_
is	_	_
not	_	_
necessary	_	_
.	_	_

#99
It	_	_
directly	_	_
accesses	_	_
P	_	_
j	_	_
to	_	_
max-pool	_	_
over	_	_
each	_	_
row	_	_
,	_	_
which	_	_
significantly	_	_
alleviates	_	_
computational	_	_
complexity	_	_
and	_	_
sampling	_	_
artifacts	_	_
.	_	_

#100
Moreover	_	_
,	_	_
sampling	_	_
over	_	_
the	_	_
x-coordinate	_	_
is	_	_
geometrically	_	_
meaningful	_	_
,	_	_
i.e.	_	_
,	_	_
uniform	_	_
sampling	_	_
is	_	_
equivalent	_	_
to	_	_
disparity	_	_
,	_	_
or	_	_
inverse	_	_
depth	_	_
.	_	_

#101
With	_	_
rectification	_	_
,	_	_
we	_	_
model	_	_
the	_	_
loss	_	_
for	_	_
multiview	_	_
cross-view	_	_
supervision	_	_
:	_	_
LE	_	_
=	_	_
C∑	_	_
c=1	_	_
S∑	_	_
i=1	_	_
∑	_	_
j∈Vi	_	_
H∑	_	_
v=1	_	_
Q	_	_
c	_	_
i	_	_
(	_	_
v	_	_
)	_	_
log	_	_
Q	_	_
c	_	_
i	_	_
(	_	_
v	_	_
)	_	_
Q	_	_
c	_	_
j→i	_	_
(	_	_
v	_	_
)	_	_
(	_	_
8	_	_
)	_	_
where	_	_
H	_	_
is	_	_
the	_	_
height	_	_
of	_	_
the	_	_
distribution	_	_
,	_	_
P	_	_
is	_	_
the	_	_
number	_	_
of	_	_
keypoints	_	_
,	_	_
S	_	_
is	_	_
the	_	_
number	_	_
of	_	_
cameras	_	_
,	_	_
and	_	_
Vi	_	_
is	_	_
the	_	_
set	_	_
of	_	_
paired	_	_
camera	_	_
indices	_	_
of	_	_
the	_	_
ith	_	_
camera	_	_
.	_	_

#102
We	_	_
use	_	_
the	_	_
superscript	_	_
in	_	_
Q	_	_
c	_	_
i	_	_
to	_	_
indicate	_	_
the	_	_
keypoint	_	_
index	_	_
.	_	_

#103
Figure	_	_
4	_	_
illustrates	_	_
our	_	_
twin	_	_
network	_	_
that	_	_
minimizes	_	_
the	_	_
epipolar	_	_
divergence	_	_
by	_	_
applying	_	_
stereo	_	_
rectification	_	_
,	_	_
epipolar	_	_
transfer	_	_
,	_	_
Keypoint	_	_
detector	_	_
Flattening	_	_
E	_	_
i	_	_
j	_	_
Transfer	_	_
iP	_	_
jP	_	_
iP	_	_
j	_	_
iP	_	_
→	_	_
iQ	_	_
j	_	_
iQ	_	_
→Rectification	_	_
jP	_	_
Rectification	_	_
(	_	_
;	_	_
)	_	_
iφ	_	_
w	_	_
(	_	_
;	_	_
)	_	_
jφ	_	_
w	_	_
Figure	_	_
4	_	_
:	_	_
We	_	_
design	_	_
a	_	_
twin	_	_
network	_	_
to	_	_
minimize	_	_
the	_	_
epipolar	_	_
divergence	_	_
between	_	_
Qi	_	_
and	_	_
Qj→i	_	_
.	_	_

#104
Stereo	_	_
rectification	_	_
is	_	_
used	_	_
to	_	_
simplify	_	_
the	_	_
max-pooling	_	_
operation	_	_
along	_	_
the	_	_
epipolar	_	_
line	_	_
,	_	_
and	_	_
reduce	_	_
computational	_	_
complexity	_	_
and	_	_
sampling	_	_
aliasing	_	_
.	_	_

#105
1P	_	_
2P	_	_
3P	_	_
4P	_	_
5P	_	_
6P	_	_
7P	_	_
2	_	_
1P	_	_
→	_	_
3	_	_
1P→	_	_
4	_	_
1P	_	_
→	_	_
5	_	_
1P→	_	_
6	_	_
1P→	_	_
7	_	_
1P→1iP→∑	_	_
Figure	_	_
5	_	_
:	_	_
Epipolar	_	_
cross-view	_	_
supervision	_	_
on	_	_
right	_	_
elbow	_	_
on	_	_
view	_	_
1	_	_
.	_	_

#106
Top	_	_
right	_	_
row	_	_
shows	_	_
elbow	_	_
detections	_	_
across	_	_
views	_	_
,	_	_
i.e.	_	_
,	_	_
P2	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
P7	_	_
.	_	_

#107
The	_	_
transferred	_	_
distribution	_	_
to	_	_
view	_	_
1	_	_
is	_	_
shown	_	_
on	_	_
the	_	_
bottom	_	_
right	_	_
row	_	_
,	_	_
i.e.	_	_
,	_	_
P2→1	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
P7→1	_	_
.	_	_

#108
These	_	_
transferred	_	_
probabilities	_	_
are	_	_
used	_	_
to	_	_
supervise	_	_
view	_	_
1	_	_
where	_	_
the	_	_
bottom	_	_
left	_	_
image	_	_
is	_	_
the	_	_
summation	_	_
of	_	_
cross-view	_	_
supervisions	_	_
.	_	_

#109
and	_	_
flattening	_	_
operations	_	_
,	_	_
which	_	_
can	_	_
perform	_	_
cross-view	_	_
supervision	_	_
from	_	_
unlabeled	_	_
data	_	_
.	_	_

#110
Since	_	_
the	_	_
epipolar	_	_
divergence	_	_
flattens	_	_
the	_	_
keypoint	_	_
distribution	_	_
,	_	_
cross-supervision	_	_
from	_	_
one	_	_
image	_	_
can	_	_
constrain	_	_
in	_	_
one	_	_
direction	_	_
.	_	_

#111
In	_	_
practice	_	_
,	_	_
we	_	_
find	_	_
a	_	_
set	_	_
of	_	_
images	_	_
given	_	_
the	_	_
ith	_	_
image	_	_
such	_	_
that	_	_
the	_	_
expected	_	_
epipolar	_	_
lines	_	_
are	_	_
not	_	_
parallel	_	_
.	_	_

#112
When	_	_
camera	_	_
centers	_	_
lie	_	_
on	_	_
a	_	_
co-planar	_	_
surface	_	_
,	_	_
a	_	_
3D	_	_
point	_	_
on	_	_
the	_	_
surface	_	_
produces	_	_
all	_	_
same	_	_
epipolar	_	_
lines	_	_
,	_	_
which	_	_
is	_	_
a	_	_
degenerate	_	_
case3	_	_
.	_	_

#113
Figure	_	_
5	_	_
illustrates	_	_
cross-view	_	_
supervision	_	_
on	_	_
a	_	_
right	_	_
elbow	_	_
on	_	_
view	_	_
1	_	_
.	_	_

#114
Elbow	_	_
detections	_	_
from	_	_
view	_	_
2	_	_
to	_	_
7	_	_
(	_	_
top	_	_
right	_	_
row	_	_
)	_	_
are	_	_
transferred	_	_
to	_	_
view	_	_
1	_	_
(	_	_
bottom	_	_
right	_	_
row	_	_
)	_	_
.	_	_

#115
These	_	_
transferred	_	_
probabilities	_	_
are	_	_
used	_	_
to	_	_
supervise	_	_
view	_	_
1	_	_
where	_	_
the	_	_
bottom	_	_
left	_	_
image	_	_
is	_	_
the	_	_
summation	_	_
of	_	_
cross-view	_	_
supervisions	_	_
.	_	_

#116
3.3	_	_
.	_	_

#117
Multiview	_	_
Semi-supervised	_	_
Learning	_	_
We	_	_
integrate	_	_
the	_	_
raster	_	_
formulation	_	_
of	_	_
the	_	_
epipolar	_	_
geometry	_	_
in	_	_
Section	_	_
3.2	_	_
into	_	_
a	_	_
semi-supervised	_	_
learning	_	_
framework	_	_
.	_	_

#118
The	_	_
keypoint	_	_
detector	_	_
is	_	_
trained	_	_
by	_	_
minimizing	_	_
the	_	_
following	_	_
loss	_	_
:	_	_
minimize	_	_
w	_	_
LL	_	_
+	_	_
λeLE	_	_
+	_	_
λpLB	_	_
,	_	_
(	_	_
9	_	_
)	_	_
3This	_	_
degenerate	_	_
case	_	_
does	_	_
not	_	_
apply	_	_
for	_	_
3D	_	_
point	_	_
triangulation	_	_
where	_	_
the	_	_
correspondence	_	_
is	_	_
known	_	_
.	_	_

#119
whereLL	_	_
,	_	_
LE	_	_
,	_	_
andLB	_	_
are	_	_
the	_	_
losses	_	_
for	_	_
labeled	_	_
supervision	_	_
,	_	_
multiview	_	_
cross-view	_	_
supervision	_	_
,	_	_
and	_	_
bootstrapping	_	_
prior	_	_
,	_	_
and	_	_
λe	_	_
and	_	_
λp	_	_
are	_	_
the	_	_
weights	_	_
that	_	_
control	_	_
their	_	_
importance	_	_
.	_	_

#120
Given	_	_
a	_	_
set	_	_
of	_	_
labeled	_	_
data	_	_
(	_	_
<	_	_
1	_	_
%	_	_
)	_	_
,	_	_
we	_	_
compute	_	_
the	_	_
labeled	_	_
loss	_	_
as	_	_
follows	_	_
:	_	_
LL	_	_
=	_	_
∑	_	_
i∈DL	_	_
‖φ	_	_
(	_	_
Ii	_	_
;	_	_
w	_	_
)	_	_
−	_	_
zi‖2	_	_
(	_	_
10	_	_
)	_	_
where	_	_
z	_	_
∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
W×H×C	_	_
is	_	_
the	_	_
labeled	_	_
likelihood	_	_
of	_	_
key-points	_	_
approximated	_	_
by	_	_
convolving	_	_
the	_	_
keypoint	_	_
location	_	_
with	_	_
a	_	_
Gaussian	_	_
kernel	_	_
.	_	_

#121
To	_	_
improve	_	_
performance	_	_
,	_	_
we	_	_
incorporate	_	_
with	_	_
offline	_	_
spatiotemporal	_	_
label	_	_
augmentation	_	_
by	_	_
reconstructing	_	_
3D	_	_
key-point	_	_
trajectories	_	_
using	_	_
the	_	_
multiview	_	_
labeled	_	_
data	_	_
inspired	_	_
by	_	_
the	_	_
multiview	_	_
bootstrapping	_	_
[	_	_
58	_	_
]	_	_
.	_	_

#122
Given	_	_
synchronized	_	_
labeled	_	_
images	_	_
,	_	_
we	_	_
triangulate	_	_
each	_	_
3D	_	_
keypoint	_	_
X	_	_
using	_	_
the	_	_
camera	_	_
projection	_	_
matrices	_	_
and	_	_
the	_	_
2D	_	_
labeled	_	_
keypoints	_	_
.	_	_

#123
The	_	_
3D	_	_
reconstructed	_	_
keypoint	_	_
is	_	_
projected	_	_
onto	_	_
the	_	_
rest	_	_
synchronized	_	_
unlabeled	_	_
images	_	_
,	_	_
which	_	_
automatically	_	_
produces	_	_
their	_	_
labels	_	_
.	_	_

#124
3D	_	_
tracking	_	_
[	_	_
27,72	_	_
]	_	_
further	_	_
increases	_	_
the	_	_
labeled	_	_
data	_	_
.	_	_

#125
For	_	_
each	_	_
keypoint	_	_
Xt	_	_
at	_	_
the	_	_
t	_	_
time	_	_
instant	_	_
,	_	_
we	_	_
project	_	_
the	_	_
point	_	_
onto	_	_
the	_	_
visible	_	_
set	_	_
of	_	_
cameras	_	_
.	_	_

#126
The	_	_
projected	_	_
point	_	_
is	_	_
tracked	_	_
in	_	_
2D	_	_
using	_	_
optical	_	_
flow	_	_
and	_	_
triangulated	_	_
with	_	_
RANSAC	_	_
[	_	_
15	_	_
]	_	_
to	_	_
form	_	_
Xt+1	_	_
.	_	_

#127
We	_	_
compute	_	_
the	_	_
visibility	_	_
of	_	_
the	_	_
point	_	_
to	_	_
reduce	_	_
tracking	_	_
drift	_	_
using	_	_
motion	_	_
and	_	_
appearance	_	_
cues	_	_
:	_	_
(	_	_
1	_	_
)	_	_
optical	_	_
flow	_	_
from	_	_
its	_	_
consecutive	_	_
image	_	_
is	_	_
compared	_	_
to	_	_
the	_	_
projected	_	_
3D	_	_
motion	_	_
vector	_	_
to	_	_
measure	_	_
motion	_	_
consistency	_	_
;	_	_
and	_	_
(	_	_
2	_	_
)	_	_
visual	_	_
appearance	_	_
is	_	_
matched	_	_
by	_	_
learning	_	_
a	_	_
linear	_	_
correlation	_	_
filter	_	_
[	_	_
6	_	_
]	_	_
on	_	_
PCA	_	_
HOG	_	_
[	_	_
11	_	_
]	_	_
,	_	_
which	_	_
can	_	_
reliably	_	_
track	_	_
longer	_	_
than	_	_
100	_	_
frames	_	_
forward	_	_
and	_	_
backward	_	_
.	_	_

#128
We	_	_
use	_	_
this	_	_
spatiotemporal	_	_
data	_	_
augmentation	_	_
to	_	_
define	_	_
the	_	_
bootstrapping	_	_
loss	_	_
:	_	_
LB	_	_
=	_	_
∑	_	_
i∈DU	_	_
‖φ	_	_
(	_	_
Ii	_	_
;	_	_
w	_	_
)	_	_
−	_	_
ẑi‖2	_	_
.	_	_

#129
(	_	_
11	_	_
)	_	_
where	_	_
ẑ	_	_
∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
W×H×C	_	_
is	_	_
the	_	_
augmented	_	_
labeled	_	_
likelihood	_	_
using	_	_
bootstrapping	_	_
approximated	_	_
by	_	_
convolving	_	_
the	_	_
keypoint	_	_
location	_	_
with	_	_
a	_	_
Gaussian	_	_
kernel	_	_
.	_	_

#130
4	_	_
.	_	_

#131
Result	_	_
We	_	_
build	_	_
a	_	_
keypoint	_	_
detector	_	_
for	_	_
each	_	_
species	_	_
without	_	_
a	_	_
pre-trained	_	_
model	_	_
,	_	_
using	_	_
the	_	_
CPM	_	_
network	_	_
(	_	_
5	_	_
stages	_	_
)	_	_
.	_	_

#132
The	_	_
code	_	_
can	_	_
be	_	_
found	_	_
in	_	_
https	_	_
:	_	_
//github.com/	_	_
MONET2018/MONET	_	_
.	_	_

#133
To	_	_
highlight	_	_
the	_	_
model	_	_
flexibility	_	_
,	_	_
we	_	_
include	_	_
implementations	_	_
with	_	_
two	_	_
state-of-the-art	_	_
pose	_	_
detectors	_	_
(	_	_
CPM	_	_
[	_	_
8	_	_
]	_	_
and	_	_
Hourglass	_	_
[	_	_
43	_	_
]	_	_
)	_	_
.	_	_

#134
λe	_	_
=	_	_
5	_	_
and	_	_
λp	_	_
=	_	_
1	_	_
are	_	_
used	_	_
.	_	_

#135
Our	_	_
detection	_	_
network	_	_
takes	_	_
an	_	_
input	_	_
image	_	_
(	_	_
368×368	_	_
)	_	_
,	_	_
and	_	_
outputs	_	_
a	_	_
distribution	_	_
(	_	_
46×46×C	_	_
)	_	_
.	_	_

#136
In	_	_
training	_	_
,	_	_
we	_	_
use	_	_
batch	_	_
size	_	_
30	_	_
,	_	_
learning	_	_
rate	_	_
10−4	_	_
,	_	_
and	_	_
learning	_	_
decay	_	_
rate	_	_
0.9	_	_
with	_	_
500	_	_
steps	_	_
.	_	_

#137
We	_	_
use	_	_
the	_	_
ADAM	_	_
optimizer	_	_
of	_	_
TensorFlow	_	_
with	_	_
single	_	_
nVidia	_	_
GTX	_	_
1080	_	_
.	_	_

#138
Datasets	_	_
We	_	_
validate	_	_
our	_	_
MONET	_	_
framework	_	_
on	_	_
multiple	_	_
sequences	_	_
of	_	_
diverse	_	_
subjects	_	_
including	_	_
humans	_	_
,	_	_
dogs	_	_
,	_	_
and	_	_
monkeys	_	_
.	_	_

#139
(	_	_
1	_	_
)	_	_
Monkey	_	_
subject	_	_
35	_	_
GoPro	_	_
HD	_	_
cameras	_	_
running	_	_
at	_	_
60	_	_
fps	_	_
are	_	_
installed	_	_
in	_	_
a	_	_
large	_	_
cage	_	_
(	_	_
9′×12′×9′	_	_
)	_	_
that	_	_
allows	_	_
the	_	_
free-ranging	_	_
behaviors	_	_
of	_	_
monkeys	_	_
.	_	_

#140
There	_	_
are	_	_
diverse	_	_
monkey	_	_
activities	_	_
include	_	_
grooming	_	_
,	_	_
hanging	_	_
,	_	_
and	_	_
walking	_	_
.	_	_

#141
The	_	_
camera	_	_
produces	_	_
1280×	_	_
960	_	_
images	_	_
.	_	_

#142
12	_	_
key-points	_	_
of	_	_
monkey’s	_	_
pose	_	_
in	_	_
85	_	_
images	_	_
out	_	_
of	_	_
63,000	_	_
images	_	_
are	_	_
manually	_	_
annotated	_	_
.	_	_

#143
(	_	_
2	_	_
)	_	_
Dog	_	_
subjects	_	_
Multi-camera	_	_
system	_	_
composed	_	_
of	_	_
69	_	_
synchronized	_	_
HD	_	_
cameras	_	_
(	_	_
1024×1280	_	_
at	_	_
30	_	_
fps	_	_
)	_	_
are	_	_
used	_	_
to	_	_
capture	_	_
the	_	_
behaviors	_	_
of	_	_
multiple	_	_
breeds	_	_
of	_	_
dogs	_	_
including	_	_
Dalmatian	_	_
and	_	_
Golden	_	_
Retrievers	_	_
.	_	_

#144
Less	_	_
than	_	_
1	_	_
%	_	_
of	_	_
data	_	_
are	_	_
manually	_	_
labeled	_	_
.	_	_

#145
(	_	_
3	_	_
)	_	_
Human	_	_
subject	_	_
I	_	_
A	_	_
multiview	_	_
behavioral	_	_
imaging	_	_
system	_	_
composed	_	_
of	_	_
69	_	_
synchronized	_	_
HD	_	_
cameras	_	_
capture	_	_
human	_	_
activities	_	_
at	_	_
30	_	_
fps	_	_
with	_	_
1024×1280	_	_
resolution	_	_
.	_	_

#146
30	_	_
images	_	_
out	_	_
of	_	_
20,700	_	_
images	_	_
are	_	_
manually	_	_
annotated	_	_
.	_	_

#147
This	_	_
dataset	_	_
includes	_	_
a	_	_
diverse	_	_
human	_	_
activities	_	_
such	_	_
as	_	_
dancing	_	_
,	_	_
jumping	_	_
,	_	_
and	_	_
sitting	_	_
.	_	_

#148
We	_	_
use	_	_
a	_	_
pre-trained	_	_
CPM	_	_
model	_	_
[	_	_
8	_	_
]	_	_
to	_	_
generate	_	_
the	_	_
ground	_	_
truth	_	_
data	_	_
.	_	_

#149
(	_	_
4	_	_
)	_	_
Human	_	_
subject	_	_
II	_	_
We	_	_
test	_	_
our	_	_
approach	_	_
on	_	_
two	_	_
publicly	_	_
available	_	_
datasets	_	_
for	_	_
human	_	_
subjects	_	_
:	_	_
Panoptic	_	_
Studio	_	_
dataset	_	_
[	_	_
26	_	_
]	_	_
and	_	_
Human3.6M	_	_
[	_	_
22	_	_
]	_	_
.	_	_

#150
For	_	_
the	_	_
Panoptic	_	_
Studio	_	_
dataset	_	_
,	_	_
we	_	_
use	_	_
31	_	_
HD	_	_
videos	_	_
(	_	_
1920	_	_
×	_	_
1080	_	_
at	_	_
30	_	_
Hz	_	_
)	_	_
.	_	_

#151
The	_	_
scenes	_	_
includes	_	_
diverse	_	_
subjects	_	_
with	_	_
social	_	_
interactions	_	_
that	_	_
introduce	_	_
severe	_	_
social	_	_
occlusion	_	_
.	_	_

#152
The	_	_
Human3.6M	_	_
dataset	_	_
is	_	_
captured	_	_
by	_	_
4	_	_
HD	_	_
cameras	_	_
that	_	_
includes	_	_
variety	_	_
of	_	_
single	_	_
actor	_	_
activities	_	_
,	_	_
e.g.	_	_
,	_	_
sitting	_	_
,	_	_
running	_	_
,	_	_
and	_	_
eating/drinking	_	_
.	_	_

#153
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
MONET	_	_
Keypoint	_	_
regression	_	_
Soft-argmax	_	_
3D	_	_
prediction	_	_
Figure	_	_
6	_	_
:	_	_
PCK	_	_
for	_	_
hypothesis	_	_
validation	_	_
Hypothesis	_	_
Validation	_	_
We	_	_
hypothesize	_	_
that	_	_
our	_	_
raster	_	_
formulation	_	_
is	_	_
superior	_	_
to	_	_
existing	_	_
multiview	_	_
cross-view	_	_
supervision	_	_
approaches	_	_
used	_	_
for	_	_
semi-supervised	_	_
learning	_	_
because	_	_
it	_	_
is	_	_
an	_	_
end-to-end	_	_
system	_	_
without	_	_
requiring	_	_
3D	_	_
prediction	_	_
.	_	_

#154
We	_	_
empirically	_	_
validate	_	_
our	_	_
hypothesis	_	_
by	_	_
comparing	_	_
to	_	_
three	_	_
approaches	_	_
on	_	_
multiview	_	_
monkey	_	_
data	_	_
from	_	_
35	_	_
views	_	_
(	_	_
300	_	_
labeled	_	_
and	_	_
600	_	_
unlabeled	_	_
time	_	_
instances	_	_
)	_	_
.	_	_

#155
No	_	_
pretrained	_	_
model	_	_
is	_	_
used	_	_
for	_	_
the	_	_
evaluation	_	_
.	_	_

#156
(	_	_
1	_	_
)	_	_
Keypoint	_	_
regression	_	_
:	_	_
a	_	_
vector	_	_
representation	_	_
of	_	_
keypoint	_	_
locations	_	_
is	_	_
directly	_	_
regressed	_	_
from	_	_
an	_	_
image	_	_
.	_	_

#157
We	_	_
use	_	_
DeepPose	_	_
[	_	_
64	_	_
]	_	_
to	_	_
detect	_	_
keypoints	_	_
and	_	_
use	_	_
the	_	_
fundamental	_	_
matrix	_	_
to	_	_
measure	_	_
the	_	_
distance	_	_
(	_	_
loss	_	_
)	_	_
between	_	_
the	_	_
epipolar	_	_
line	_	_
and	_	_
the	_	_
detected	_	_
points	_	_
,	_	_
|x̃T	_	_
2Fx̃1|	_	_
,	_	_
for	_	_
the	_	_
unlabeled	_	_
data	_	_
.	_	_

#158
(	_	_
2	_	_
)	_	_
Soft-argmax	_	_
:	_	_
a	_	_
vector	_	_
representation	_	_
can	_	_
be	_	_
approximated	_	_
by	_	_
the	_	_
raster	_	_
keypoint	_	_
distribution	_	_
using	_	_
a	_	_
soft-argmax	_	_
operation	_	_
:	_	_
xsoftmax	_	_
=	_	_
∑	_	_
x	_	_
P	_	_
(	_	_
x	_	_
)	_	_
x/	_	_
∑	_	_
x	_	_
P	_	_
(	_	_
x	_	_
)	_	_
,	_	_
which	_	_
is	_	_
reasonable	_	_
when	_	_
the	_	_
predicted	_	_
probability	_	_
is	_	_
nearly	_	_
unimodal	_	_
.	_	_

#159
This	_	_
is	_	_
differentiable	_	_
,	_	_
and	_	_
therefore	_	_
end-to-end	_	_
training	_	_
is	_	_
possible	_	_
.	_	_

#160
However	_	_
,	_	_
its	_	_
approximation	_	_
holds	_	_
when	_	_
the	_	_
predicted	_	_
distribution	_	_
is	_	_
unimodal	_	_
.	_	_

#161
We	_	_
use	_	_
CPM	_	_
[	_	_
69	_	_
]	_	_
to	_	_
build	_	_
a	_	_
semi-supervised	_	_
network	_	_
with	_	_
epipolar	_	_
distance	_	_
as	_	_
a	_	_
loss	_	_
.	_	_

#162
(	_	_
3	_	_
)	_	_
3D	_	_
prediction	_	_
:	_	_
each	_	_
3D	_	_
coordinate	_	_
is	_	_
predicted	_	_
from	_	_
a	_	_
single	_	_
view	_	_
image	_	_
where	_	_
the	_	_
projection	_	_
of	_	_
the	_	_
3D	_	_
prediction	_	_
is	_	_
used	_	_
as	_	_
cross-view	_	_
superivison	_	_
[	_	_
?	_	_
,	_	_
54	_	_
,	_	_
70	_	_
]	_	_
.	_	_

#163
We	_	_
augment	_	_
3D	_	_
prediction	_	_
layers	_	_
on	_	_
CPM	_	_
to	_	_
regress	_	_
the	_	_
depth	_	_
of	_	_
keypoints	_	_
[	_	_
47	_	_
]	_	_
.	_	_

#164
The	_	_
reprojection	_	_
error	_	_
is	_	_
used	_	_
for	_	_
the	_	_
loss	_	_
.	_	_

#165
Figure	_	_
6	_	_
illustrates	_	_
the	_	_
probability	_	_
of	_	_
correct	_	_
keypoint	_	_
(	_	_
PCK	_	_
)	_	_
curve	_	_
,	_	_
showing	_	_
that	_	_
our	_	_
approach	_	_
using	_	_
raster	_	_
epipolar	_	_
geometry	_	_
significantly	_	_
outperforms	_	_
other	_	_
approaches	_	_
.	_	_

#166
Baselines	_	_
We	_	_
compare	_	_
our	_	_
approach	_	_
with	_	_
5	_	_
different	_	_
baseline	_	_
algorithms	_	_
.	_	_

#167
For	_	_
all	_	_
algorithms	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
performance	_	_
on	_	_
the	_	_
unlabeled	_	_
data	_	_
.	_	_

#168
(	_	_
1	_	_
)	_	_
Supervised	_	_
learning	_	_
:	_	_
we	_	_
use	_	_
the	_	_
manually	_	_
annotated	_	_
images	_	_
to	_	_
train	_	_
the	_	_
network	_	_
in	_	_
a	_	_
fully	_	_
supervised	_	_
manner	_	_
.	_	_

#169
Due	_	_
to	_	_
the	_	_
limited	_	_
number	_	_
of	_	_
labeled	_	_
images	_	_
(	_	_
<	_	_
100	_	_
)	_	_
,	_	_
the	_	_
existing	_	_
distillation	_	_
methods	_	_
[	_	_
21,52	_	_
]	_	_
perform	_	_
similarly	_	_
.	_	_

#170
(	_	_
2	_	_
)	_	_
Spatial	_	_
augmentation	_	_
:	_	_
the	_	_
3D	_	_
keypoints	_	_
are	_	_
triangulated	_	_
and	_	_
projected	_	_
onto	_	_
the	_	_
synchronized	_	_
unlabeled	_	_
images	_	_
.	_	_

#171
This	_	_
models	_	_
visual	_	_
appearance	_	_
and	_	_
spatial	_	_
configuration	_	_
from	_	_
multiple	_	_
perspectives	_	_
,	_	_
which	_	_
can	_	_
greatly	_	_
improve	_	_
the	_	_
generalization	_	_
power	_	_
of	_	_
keypoint	_	_
detection	_	_
.	_	_

#172
(	_	_
3	_	_
)	_	_
Spatiotemporal	_	_
augmentation	_	_
:	_	_
we	_	_
track	_	_
the	_	_
3D	_	_
keypoints	_	_
over	_	_
time	_	_
using	_	_
multiview	_	_
optical	_	_
flow	_	_
[	_	_
27,72	_	_
]	_	_
.	_	_

#173
This	_	_
augmentation	_	_
can	_	_
model	_	_
different	_	_
geometric	_	_
configurations	_	_
of	_	_
3D	_	_
keypoints	_	_
.	_	_

#174
(	_	_
4	_	_
)	_	_
Bootstrapping	_	_
I	_	_
:	_	_
Given	_	_
the	_	_
spatiotemporal	_	_
data	_	_
augmentation	_	_
,	_	_
we	_	_
apply	_	_
the	_	_
multiview	_	_
bootstrapping	_	_
approach	_	_
[	_	_
58	_	_
]	_	_
to	_	_
obtain	_	_
pseudo-labels	_	_
computed	_	_
by	_	_
RANSAC-based	_	_
3D	_	_
triangulation	_	_
for	_	_
the	_	_
unlabeled	_	_
data	_	_
.	_	_

#175
(	_	_
5	_	_
)	_	_
Bootstrapping	_	_
II	_	_
:	_	_
the	_	_
Bootstrapping	_	_
I	_	_
model	_	_
is	_	_
refined	_	_
by	_	_
re-triangulation	_	_
and	_	_
re-training	_	_
.	_	_

#176
This	_	_
can	_	_
reduce	_	_
the	_	_
reprojection	_	_
errors	_	_
.	_	_

#177
We	_	_
evaluate	_	_
our	_	_
approach	_	_
based	_	_
on	_	_
1	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
Supervised	_	_
learning	_	_
Spatial	_	_
Augmentation	_	_
Spatiotemporal	_	_
Augmentation	_	_
Bootstrapping	_	_
I	_	_
Bootstrapping	_	_
II	_	_
MONET	_	_
(	_	_
a	_	_
)	_	_
Human	_	_
subject	_	_
PCK	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
No	_	_
Augmentation	_	_
Spatial	_	_
Augmentation	_	_
Spatiotemporal	_	_
Augmentation	_	_
Bootstrapping	_	_
I	_	_
Bootstrapping	_	_
II	_	_
MONET	_	_
(	_	_
b	_	_
)	_	_
Monkey	_	_
subject	_	_
PCK	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
nose	_	_
head	_	_
neck	_	_
R.shoulder	_	_
L.shoulder	_	_
R.hand	_	_
L.hand	_	_
L.pel	_	_
R.pelvis	_	_
L.foot	_	_
R.foot	_	_
nose	_	_
head	_	_
neck	_	_
R.shoulder	_	_
L.shoulder	_	_
R.hand	_	_
L.hand	_	_
hip	_	_
L.pelvis	_	_
R.pelvis	_	_
L.foot	_	_
R.foot	_	_
Supervised	_	_
learning	_	_
Spatial	_	_
Augmentation	_	_
Spatiotemporal	_	_
Augmentation	_	_
Bootstrapping	_	_
I	_	_
Bootstrapping	_	_
II	_	_
MONET	_	_
(	_	_
c	_	_
)	_	_
Dog	_	_
subject	_	_
PCK	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
Supervised	_	_
learning	_	_
Spatial	_	_
Augmentation	_	_
Bootstrapping	_	_
I	_	_
Bootstrapping	_	_
II	_	_
MONET	_	_
(	_	_
d	_	_
)	_	_
Panoptic	_	_
PCK	_	_
0	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
Confidence	_	_
(	_	_
keypoint	_	_
probability	_	_
)	_	_
Re	_	_
pr	_	_
oj	_	_
ec	_	_
tio	_	_
n	_	_
er	_	_
ro	_	_
r	_	_
(	_	_
pi	_	_
xe	_	_
l	_	_
)	_	_
Spatial	_	_
Augmentation	_	_
Bootstrapping	_	_
II	_	_
MONET	_	_
(	_	_
e	_	_
)	_	_
Reprojection	_	_
error	_	_
Figure	_	_
7	_	_
:	_	_
PCK	_	_
curves	_	_
for	_	_
(	_	_
a	_	_
)	_	_
humans	_	_
,	_	_
(	_	_
b	_	_
)	_	_
monkeys	_	_
,	_	_
(	_	_
c	_	_
)	_	_
dogs	_	_
and	_	_
(	_	_
d	_	_
)	_	_
the	_	_
CMU	_	_
Panoptic	_	_
dataset	_	_
[	_	_
28	_	_
]	_	_
.	_	_

#178
MONET	_	_
(	_	_
red	_	_
)	_	_
outperforms	_	_
5	_	_
baseline	_	_
algorithms	_	_
.	_	_

#179
(	_	_
e	_	_
)	_	_
MONET	_	_
is	_	_
designed	_	_
to	_	_
minimize	_	_
the	_	_
reprojection	_	_
error	_	_
,	_	_
and	_	_
we	_	_
achieve	_	_
far	_	_
stronger	_	_
performance	_	_
as	_	_
the	_	_
confidence	_	_
increases	_	_
.	_	_

#180
Geometric	_	_
inconsistency	_	_
Geometric	_	_
consistency	_	_
via	_	_
epipolar	_	_
divergence	_	_
Training	_	_
step	_	_
:	_	_
0	_	_
Training	_	_
step	_	_
:	_	_
4980	_	_
View	_	_
1	_	_
View	_	_
2	_	_
View	_	_
3	_	_
View	_	_
4	_	_
View	_	_
5	_	_
View	_	_
6	_	_
View	_	_
7	_	_
View	_	_
8	_	_
Figure	_	_
8	_	_
:	_	_
Erroneous	_	_
elbow	_	_
detections	_	_
from	_	_
multiview	_	_
images	_	_
converge	_	_
to	_	_
the	_	_
geometrically	_	_
consistent	_	_
location	_	_
through	_	_
training	_	_
.	_	_

#181
accuracy	_	_
and	_	_
precision	_	_
:	_	_
accuracy	_	_
measures	_	_
distance	_	_
from	_	_
the	_	_
ground	_	_
truth	_	_
keypoint	_	_
and	_	_
precision	_	_
measures	_	_
the	_	_
coherence	_	_
of	_	_
keypoint	_	_
detections	_	_
across	_	_
views	_	_
.	_	_

#182
(	_	_
6	_	_
)	_	_
Rhodin	_	_
et	_	_
al.	_	_
[	_	_
54	_	_
]	_	_
:	_	_
The	_	_
unlabeled	_	_
multi-view	_	_
image	_	_
pairs	_	_
are	_	_
used	_	_
to	_	_
generate	_	_
3D	_	_
point	_	_
cloud	_	_
of	_	_
body	_	_
first	_	_
during	_	_
unsupervised	_	_
training	_	_
,	_	_
and	_	_
then	_	_
the	_	_
model	_	_
is	_	_
trained	_	_
with	_	_
images	_	_
with	_	_
3D	_	_
ground	_	_
truth	_	_
to	_	_
learn	_	_
to	_	_
transfer	_	_
point	_	_
cloud	_	_
to	_	_
joint	_	_
positions	_	_
.	_	_

#183
Accuracy	_	_
We	_	_
use	_	_
PCK	_	_
curves	_	_
to	_	_
measure	_	_
the	_	_
accuracy	_	_
.	_	_

#184
The	_	_
distance	_	_
between	_	_
the	_	_
ground	_	_
truth	_	_
keypoint	_	_
and	_	_
the	_	_
detected	_	_
keypoint	_	_
is	_	_
normalized	_	_
by	_	_
the	_	_
size	_	_
of	_	_
the	_	_
width	_	_
of	_	_
the	_	_
detection	_	_
window	_	_
(	_	_
46	_	_
)	_	_
.	_	_

#185
Figure	_	_
7	_	_
shows	_	_
PCK	_	_
performance	_	_
on	_	_
human	_	_
,	_	_
monkey	_	_
,	_	_
and	_	_
dog	_	_
subjects	_	_
where	_	_
no	_	_
pre-trained	_	_
model	_	_
is	_	_
used	_	_
.	_	_

#186
Our	_	_
MONET	_	_
(	_	_
red	_	_
)	_	_
model	_	_
exhibits	_	_
accurate	_	_
detection	_	_
for	_	_
all	_	_
keypoints	_	_
,	_	_
and	_	_
outperforms	_	_
5	_	_
baselines	_	_
.	_	_

#187
For	_	_
the	_	_
monkey	_	_
data	_	_
,	_	_
higher	_	_
frame-rate	_	_
image	_	_
streams	_	_
(	_	_
60	_	_
fps	_	_
)	_	_
greatly	_	_
boost	_	_
the	_	_
performance	_	_
of	_	_
multiview	_	_
tracking	_	_
due	_	_
to	_	_
smaller	_	_
displacements	_	_
,	_	_
resulting	_	_
in	_	_
accurate	_	_
keypoint	_	_
detection	_	_
by	_	_
spatiotemporal	_	_
augmentation	_	_
.	_	_

#188
We	_	_
also	_	_
conducted	_	_
an	_	_
experiment	_	_
on	_	_
the	_	_
CMU	_	_
Panoptic	_	_
dataset	_	_
[	_	_
28	_	_
]	_	_
to	_	_
validate	_	_
the	_	_
generalization	_	_
power	_	_
of	_	_
our	_	_
approach	_	_
.	_	_

#189
This	_	_
dataset	_	_
differs	_	_
from	_	_
ours	_	_
in	_	_
terms	_	_
of	_	_
camera	_	_
parameters	_	_
,	_	_
placements	_	_
,	_	_
and	_	_
scene	_	_
(	_	_
e.g.	_	_
,	_	_
pose	_	_
,	_	_
illumination	_	_
,	_	_
background	_	_
,	_	_
and	_	_
subject	_	_
)	_	_
.	_	_

#190
MONET	_	_
outperforms	_	_
on	_	_
both	_	_
accuracy	_	_
(	_	_
PCK	_	_
)	_	_
and	_	_
precision	_	_
(	_	_
reprojection	_	_
error	_	_
)	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
7	_	_
(	_	_
d	_	_
)	_	_
.	_	_

#191
Precision	_	_
We	_	_
use	_	_
reprojection	_	_
error	_	_
to	_	_
evaluate	_	_
the	_	_
precision	_	_
of	_	_
detection	_	_
.	_	_

#192
Given	_	_
a	_	_
set	_	_
of	_	_
keypoint	_	_
detections	_	_
in	_	_
a	_	_
synchronized	_	_
frame	_	_
and	_	_
3D	_	_
camera	_	_
poses	_	_
,	_	_
we	_	_
triangulate	_	_
Human	_	_
Monkey	_	_
Dog	_	_
Panoptic	_	_
Supervised	_	_
learning	_	_
77.8±73.3	_	_
31.1±872	_	_
88.9±69.9	_	_
53.2±271.4	_	_
Spatial	_	_
aug.	_	_
69.0±66.2	_	_
12.9±26.6	_	_
37.5±47.1	_	_
22.2±40.4	_	_
Spatiotemporal	_	_
aug.	_	_
50.3±65.4	_	_
8.10±17.8	_	_
24.0±36.2	_	_
N/A	_	_
Bootstrapping	_	_
I	_	_
[	_	_
58	_	_
]	_	_
28.5±44.7	_	_
8.68±18.9	_	_
18.9±31.0	_	_
15.6±31.7	_	_
Bootstrapping	_	_
II	_	_
[	_	_
58	_	_
]	_	_
35.4±62.4	_	_
9.97±22.1	_	_
17.1±29.3	_	_
13.7±24.6	_	_
MONET	_	_
15.0±24.1	_	_
5.45±11.4	_	_
10.3±18.7	_	_
12.8±18.0	_	_
Table	_	_
1	_	_
:	_	_
Reprojection	_	_
error	_	_
(	_	_
Mean±Std	_	_
)	_	_
.	_	_

#193
Labeled	_	_
/	_	_
Unlabeled	_	_
Hips	_	_
R.Leg	_	_
R.Arm	_	_
Head	_	_
L.Hand	_	_
L.Foot	_	_
R.UpLeg	_	_
Neck	_	_
Total	_	_
S1	_	_
/	_	_
S5,6,7,8	_	_
13.0	_	_
3.1	_	_
3.4	_	_
1.0	_	_
6.6	_	_
6.2	_	_
10.9	_	_
1.6	_	_
5.5	_	_
S1,5	_	_
/	_	_
S6,7,8	_	_
12.7	_	_
2.2	_	_
2.9	_	_
1.0	_	_
5.2	_	_
3.3	_	_
10.9	_	_
1.6	_	_
5.2	_	_
S1,5,6	_	_
/	_	_
S7,8	_	_
7.1	_	_
2.0	_	_
2.7	_	_
0.9	_	_
5.0	_	_
4.7	_	_
5.6	_	_
1.5	_	_
4.3	_	_
Table	_	_
2	_	_
:	_	_
Mean	_	_
pixel	_	_
error	_	_
vs.	_	_
labeled	_	_
data	_	_
size	_	_
on	_	_
Human3.6M	_	_
dataset	_	_
the	_	_
3D	_	_
point	_	_
without	_	_
RANSAC	_	_
.	_	_

#194
The	_	_
3D	_	_
point	_	_
is	_	_
projected	_	_
back	_	_
to	_	_
each	_	_
camera	_	_
to	_	_
compute	_	_
the	_	_
reprojection	_	_
error	_	_
,	_	_
which	_	_
measures	_	_
geometric	_	_
consistency	_	_
across	_	_
all	_	_
views	_	_
.	_	_

#195
MONET	_	_
is	_	_
designed	_	_
to	_	_
minimize	_	_
the	_	_
reprojection	_	_
error	_	_
,	_	_
and	_	_
it	_	_
outperforms	_	_
baselines	_	_
significantly	_	_
in	_	_
Figure	_	_
7	_	_
(	_	_
e	_	_
)	_	_
.	_	_

#196
Our	_	_
MONET	_	_
performs	_	_
better	_	_
at	_	_
higher	_	_
keypoint	_	_
distribution	_	_
,	_	_
which	_	_
is	_	_
key	_	_
for	_	_
3D	_	_
reconstruction	_	_
because	_	_
it	_	_
indicates	_	_
which	_	_
points	_	_
to	_	_
triangulate	_	_
.	_	_

#197
Figure	_	_
8	_	_
shows	_	_
how	_	_
erroneous	_	_
detections	_	_
of	_	_
the	_	_
left	_	_
elbow	_	_
from	_	_
multiview	_	_
images	_	_
converge	_	_
to	_	_
geometrically	_	_
consistent	_	_
elbow	_	_
locations	_	_
as	_	_
the	_	_
training	_	_
progresses	_	_
.	_	_

#198
The	_	_
performance	_	_
for	_	_
each	_	_
subject	_	_
is	_	_
summarized	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#199
Robustness	_	_
We	_	_
evaluate	_	_
the	_	_
robustness	_	_
of	_	_
our	_	_
approach	_	_
by	_	_
varying	_	_
the	_	_
amount	_	_
of	_	_
labeled	_	_
data	_	_
on	_	_
Human3.6M	_	_
dataset	_	_
(	_	_
four	_	_
cameras	_	_
)	_	_
,	_	_
which	_	_
provides	_	_
motion	_	_
capture	_	_
ground	_	_
truth	_	_
Ground	_	_
truth	_	_
Spatial	_	_
aug.	_	_
Spatiotermpoal	_	_
aug.	_	_
Bootstrapping	_	_
I	_	_
Bootstrapping	_	_
II	_	_
MONETSupervised	_	_
learning	_	_
Figure	_	_
9	_	_
:	_	_
We	_	_
qualitatively	_	_
compare	_	_
our	_	_
MONET	_	_
with	_	_
5	_	_
baseline	_	_
algorithms	_	_
on	_	_
humans	_	_
,	_	_
monkeys	_	_
,	_	_
and	_	_
dogs	_	_
.	_	_

#200
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
MONET	_	_
Rhodin	_	_
et	_	_
al	_	_
(	_	_
a	_	_
)	_	_
Monkey	_	_
subject	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
MONET	_	_
Rhodin	_	_
et	_	_
al	_	_
(	_	_
b	_	_
)	_	_
Human3.6M	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
Normalized	_	_
Distance	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
D	_	_
et	_	_
ec	_	_
tio	_	_
n	_	_
R	_	_
at	_	_
e	_	_
MONET	_	_
Rhodin	_	_
et	_	_
al	_	_
(	_	_
c	_	_
)	_	_
Panoptic	_	_
Studio	_	_
Figure	_	_
10	_	_
:	_	_
Comparison	_	_
with	_	_
Rhodin	_	_
et	_	_
al.	_	_
[	_	_
54	_	_
]	_	_
that	_	_
predict	_	_
3D	_	_
points	_	_
for	_	_
cross-view	_	_
supervision	_	_
on	_	_
monkey	_	_
,	_	_
Human3.6M	_	_
,	_	_
and	_	_
Panoptic	_	_
Studio	_	_
datasets	_	_
.	_	_

#201
data	_	_
.	_	_

#202
Table	_	_
2	_	_
summarizes	_	_
the	_	_
mean	_	_
pixel	_	_
error	_	_
as	_	_
varying	_	_
the	_	_
labeled	_	_
and	_	_
unlabeled	_	_
subjects	_	_
.	_	_

#203
As	_	_
expected	_	_
,	_	_
as	_	_
the	_	_
labeled	_	_
data	_	_
increases	_	_
,	_	_
the	_	_
error	_	_
decreases	_	_
while	_	_
the	_	_
minimally	_	_
labeled	_	_
S1	_	_
(	_	_
subject	_	_
1	_	_
)	_	_
still	_	_
produces	_	_
less	_	_
than	_	_
15	_	_
max	_	_
pixel	_	_
error	_	_
.	_	_

#204
We	_	_
also	_	_
compare	_	_
to	_	_
a	_	_
3D	_	_
prediction	_	_
approach	_	_
[	_	_
54	_	_
]	_	_
,	_	_
which	_	_
showed	_	_
strong	_	_
performance	_	_
on	_	_
Human3.6M	_	_
dataset	_	_
.	_	_

#205
Similar	_	_
to	_	_
their	_	_
experimental	_	_
setup	_	_
,	_	_
we	_	_
use	_	_
S1	_	_
,	_	_
S5	_	_
,	_	_
and	_	_
S6	_	_
as	_	_
the	_	_
labeled	_	_
data	_	_
,	_	_
and	_	_
S7	_	_
and	_	_
S8	_	_
as	_	_
the	_	_
unlabeled	_	_
data	_	_
for	_	_
training	_	_
.	_	_

#206
In	_	_
addition	_	_
to	_	_
Human3.6M	_	_
dataset	_	_
,	_	_
we	_	_
also	_	_
conduct	_	_
the	_	_
comparison	_	_
on	_	_
the	_	_
Monkey	_	_
and	_	_
CMU	_	_
Panoptic	_	_
dataset	_	_
[	_	_
28	_	_
]	_	_
.	_	_

#207
Figure	_	_
10	_	_
illustrates	_	_
the	_	_
PCK	_	_
measure	_	_
on	_	_
the	_	_
unlabeled	_	_
data	_	_
.	_	_

#208
Our	_	_
approach	_	_
outperforms	_	_
the	_	_
baseline	_	_
on	_	_
all	_	_
the	_	_
datasets	_	_
.	_	_

#209
The	_	_
advantage	_	_
of	_	_
our	_	_
approach	_	_
is	_	_
especially	_	_
reflected	_	_
on	_	_
the	_	_
CMU	_	_
Panoptic	_	_
dataset	_	_
.	_	_

#210
Full	_	_
body	_	_
is	_	_
not	_	_
often	_	_
visible	_	_
due	_	_
to	_	_
the	_	_
narrow	_	_
FOV	_	_
cameras	_	_
,	_	_
which	_	_
makes	_	_
the	_	_
explicit	_	_
3D	_	_
reconstruction	_	_
in	_	_
[	_	_
54	_	_
]	_	_
of	_	_
body	_	_
less	_	_
efficient	_	_
.	_	_

#211
Qualitative	_	_
Comparison	_	_
A	_	_
qualitative	_	_
comparison	_	_
can	_	_
be	_	_
found	_	_
in	_	_
Figure	_	_
9	_	_
.	_	_

#212
MONET	_	_
can	_	_
precisely	_	_
localize	_	_
keypoints	_	_
by	_	_
leveraging	_	_
multiview	_	_
images	_	_
jointly	_	_
.	_	_

#213
This	_	_
becomes	_	_
more	_	_
evident	_	_
when	_	_
disambiguating	_	_
symmetric	_	_
keypoints	_	_
,	_	_
e.g.	_	_
,	_	_
left	_	_
and	_	_
right	_	_
hands	_	_
,	_	_
as	_	_
epipolar	_	_
divergence	_	_
penalizes	_	_
geometric	_	_
inconsistency	_	_
(	_	_
reprojection	_	_
error	_	_
)	_	_
.	_	_

#214
It	_	_
also	_	_
shows	_	_
stronger	_	_
performance	_	_
under	_	_
occlusion	_	_
(	_	_
the	_	_
bottom	_	_
figure	_	_
)	_	_
as	_	_
the	_	_
occluded	_	_
keypoints	_	_
can	_	_
be	_	_
visible	_	_
to	_	_
other	_	_
views	_	_
that	_	_
can	_	_
enforce	_	_
to	_	_
the	_	_
correct	_	_
location	_	_
.	_	_

#215
5	_	_
.	_	_

#216
Discussion	_	_
We	_	_
present	_	_
a	_	_
new	_	_
semi-supervised	_	_
framework	_	_
,	_	_
MONET	_	_
,	_	_
to	_	_
train	_	_
keypoint	_	_
detection	_	_
networks	_	_
by	_	_
leveraging	_	_
multi-view	_	_
image	_	_
streams	_	_
.	_	_

#217
The	_	_
key	_	_
innovation	_	_
is	_	_
a	_	_
measure	_	_
of	_	_
geometric	_	_
consistency	_	_
between	_	_
keypoint	_	_
distributions	_	_
called	_	_
epipolar	_	_
divergence	_	_
.	_	_

#218
Similar	_	_
to	_	_
epipolar	_	_
distance	_	_
between	_	_
corresponding	_	_
points	_	_
,	_	_
it	_	_
allows	_	_
us	_	_
to	_	_
directly	_	_
compute	_	_
re-projection	_	_
error	_	_
while	_	_
training	_	_
a	_	_
network	_	_
.	_	_

#219
We	_	_
introduce	_	_
a	_	_
stereo	_	_
rectification	_	_
of	_	_
the	_	_
keypoint	_	_
distribution	_	_
that	_	_
simplifies	_	_
the	_	_
computational	_	_
complexity	_	_
and	_	_
imposes	_	_
geometric	_	_
meaning	_	_
on	_	_
constructing	_	_
1D	_	_
distributions	_	_
.	_	_

#220
A	_	_
twin	_	_
network	_	_
is	_	_
used	_	_
to	_	_
embed	_	_
computation	_	_
of	_	_
epipolar	_	_
divergence	_	_
.	_	_

#221
We	_	_
also	_	_
use	_	_
multiview	_	_
image	_	_
streams	_	_
to	_	_
augment	_	_
the	_	_
data	_	_
in	_	_
space	_	_
and	_	_
time	_	_
,	_	_
which	_	_
bootstraps	_	_
unlabeled	_	_
data	_	_
.	_	_

#222
We	_	_
demonstrate	_	_
that	_	_
our	_	_
framework	_	_
outperforms	_	_
existing	_	_
approaches	_	_
,	_	_
e.g.	_	_
,	_	_
multiview	_	_
bootstrapping	_	_
,	_	_
in	_	_
terms	_	_
of	_	_
accuracy	_	_
(	_	_
PCK	_	_
)	_	_
and	_	_
precision	_	_
(	_	_
reprojection	_	_
error	_	_
)	_	_
,	_	_
and	_	_
apply	_	_
it	_	_
to	_	_
non-human	_	_
species	_	_
such	_	_
as	_	_
dogs	_	_
and	_	_
monkeys	_	_
.	_	_

#223
We	_	_
anticipate	_	_
that	_	_
this	_	_
framework	_	_
will	_	_
provide	_	_
a	_	_
fundamental	_	_
basis	_	_
for	_	_
enabling	_	_
flexible	_	_
marker-less	_	_
motion	_	_
capture	_	_
that	_	_
requires	_	_
exploiting	_	_
a	_	_
large	_	_
(	_	_
potentially	_	_
unbounded	_	_
)	_	_
number	_	_
of	_	_
unlabeled	_	_
data	_	_
.	_	_

#224
6	_	_
.	_	_

#225
Acknowledgments	_	_
We	_	_
thank	_	_
David	_	_
Crandall	_	_
for	_	_
his	_	_
support	_	_
and	_	_
feedback	_	_
.	_	_

#226
This	_	_
work	_	_
is	_	_
supported	_	_
by	_	_
NSF	_	_
IIS	_	_
1846031	_	_
.	_	_