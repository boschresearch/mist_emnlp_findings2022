#0
DEEP	_	_
SPECTRAL	_	_
CONVOLUTION	_	_
NETWORK	_	_
FOR	_	_
HYPERSPECTRAL	_	_
UNMIXING	_	_
Savas	_	_
Ozkan	_	_
,	_	_
Gozde	_	_
Bozdagi	_	_
Akar	_	_
Middle	_	_
East	_	_
Technical	_	_
University	_	_
Department	_	_
of	_	_
Electrical/Electronics	_	_
Engineering	_	_
Ankara	_	_
,	_	_
Turkey	_	_
ABSTRACT	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
hyperspectral	_	_
unmixing	_	_
technique	_	_
based	_	_
on	_	_
deep	_	_
spectral	_	_
convolution	_	_
networks	_	_
(	_	_
DSCN	_	_
)	_	_
.	_	_

#1
Particularly	_	_
,	_	_
three	_	_
important	_	_
contributions	_	_
are	_	_
presented	_	_
throughout	_	_
this	_	_
paper	_	_
.	_	_

#2
First	_	_
,	_	_
fully-connected	_	_
linear	_	_
operation	_	_
is	_	_
replaced	_	_
with	_	_
spectral	_	_
convolutions	_	_
to	_	_
extract	_	_
local	_	_
spectral	_	_
characteristics	_	_
from	_	_
hyperspectral	_	_
signatures	_	_
with	_	_
a	_	_
deeper	_	_
network	_	_
architecture	_	_
.	_	_

#3
Second	_	_
,	_	_
instead	_	_
of	_	_
batch	_	_
normalization	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
spectral	_	_
normalization	_	_
layer	_	_
which	_	_
improves	_	_
the	_	_
selectivity	_	_
of	_	_
filters	_	_
by	_	_
normalizing	_	_
their	_	_
spectral	_	_
responses	_	_
.	_	_

#4
Third	_	_
,	_	_
we	_	_
introduce	_	_
two	_	_
fusion	_	_
configurations	_	_
that	_	_
produce	_	_
ideal	_	_
abundance	_	_
maps	_	_
by	_	_
using	_	_
the	_	_
abstract	_	_
representations	_	_
computed	_	_
from	_	_
previous	_	_
layers	_	_
.	_	_

#5
In	_	_
experiments	_	_
,	_	_
we	_	_
use	_	_
two	_	_
real	_	_
datasets	_	_
to	_	_
evaluate	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
method	_	_
with	_	_
other	_	_
baseline	_	_
techniques	_	_
.	_	_

#6
The	_	_
experimental	_	_
results	_	_
validate	_	_
that	_	_
the	_	_
proposed	_	_
method	_	_
outperforms	_	_
baselines	_	_
based	_	_
on	_	_
Root	_	_
Mean	_	_
Square	_	_
Error	_	_
(	_	_
RMSE	_	_
)	_	_
.	_	_

#7
Index	_	_
Terms—	_	_
Hyperspectral	_	_
Unmixing	_	_
,	_	_
Deep	_	_
Spectral	_	_
Convolution	_	_
Networks	_	_
1	_	_
.	_	_

#8
INTRODUCTION	_	_
Even	_	_
though	_	_
hyperspectral	_	_
data	_	_
provides	_	_
rich	_	_
content	_	_
information	_	_
about	_	_
the	_	_
Earth	_	_
surface	_	_
and	_	_
it	_	_
has	_	_
been	_	_
used	_	_
in	_	_
a	_	_
variety	_	_
of	_	_
remote	_	_
sensing	_	_
applications	_	_
,	_	_
the	_	_
materials	_	_
exhibited	_	_
from	_	_
the	_	_
surface	_	_
can	_	_
be	_	_
mixed	_	_
per	_	_
pixel	_	_
in	_	_
different	_	_
fractions	_	_
due	_	_
to	_	_
the	_	_
low-spatial	_	_
resolution	_	_
of	_	_
the	_	_
sensors	_	_
.	_	_

#9
Therefore	_	_
,	_	_
high	_	_
dimensional	_	_
material	_	_
signatures	_	_
E	_	_
=	_	_
{	_	_
e1	_	_
,	_	_
e2	_	_
,	_	_
...	_	_
,	_	_
ek	_	_
}	_	_
(	_	_
i.e.	_	_
,	_	_
endmembers	_	_
)	_	_
and	_	_
their	_	_
fractions	_	_
y	_	_
=	_	_
{	_	_
y1	_	_
,	_	_
y2	_	_
,	_	_
...	_	_
,	_	_
yk	_	_
}	_	_
(	_	_
i.e.	_	_
,	_	_
abundance	_	_
maps	_	_
)	_	_
for	_	_
each	_	_
pixel	_	_
x	_	_
need	_	_
to	_	_
be	_	_
extracted	_	_
blindly	_	_
from	_	_
data	_	_
.	_	_

#10
The	_	_
mixture	_	_
of	_	_
materials	_	_
can	_	_
be	_	_
formulated	_	_
with	_	_
a	_	_
linear	_	_
model	_	_
which	_	_
intuitively	_	_
defines	_	_
the	_	_
necessary	_	_
parameters	_	_
for	_	_
the	_	_
problem	_	_
:	_	_
x	_	_
=	_	_
K∑	_	_
k=1	_	_
ek	_	_
·	_	_
yk	_	_
+	_	_
η	_	_
,	_	_
s.t	_	_
.	_	_

#11
yk	_	_
≥	_	_
0	_	_
,	_	_
K∑	_	_
k=1	_	_
yk	_	_
=	_	_
1	_	_
(	_	_
1	_	_
)	_	_
where	_	_
K	_	_
is	_	_
the	_	_
number	_	_
of	_	_
materials	_	_
in	_	_
the	_	_
scene	_	_
and	_	_
η	_	_
is	_	_
the	_	_
random	_	_
noise	_	_
to	_	_
approximate	_	_
the	_	_
problem	_	_
to	_	_
nonlinearity	_	_
.	_	_

#12
Email	_	_
:	_	_
ozkan.savas	_	_
@	_	_
metu.edu.tr	_	_
Moreover	_	_
,	_	_
there	_	_
are	_	_
two	_	_
more	_	_
constraints	_	_
which	_	_
bound	_	_
the	_	_
physical	_	_
properties	_	_
of	_	_
the	_	_
solution	_	_
.	_	_

#13
Primarily	_	_
,	_	_
the	_	_
solutions	_	_
in	_	_
literature	_	_
are	_	_
highly	_	_
influenced	_	_
by	_	_
the	_	_
geometrical	_	_
volume-based	_	_
assumption	_	_
where	_	_
the	_	_
vertices	_	_
of	_	_
data	_	_
distribution	_	_
correspond	_	_
to	_	_
endmembers	_	_
,	_	_
since	_	_
all	_	_
data	_	_
can	_	_
be	_	_
reconstructed	_	_
by	_	_
the	_	_
combination	_	_
of	_	_
these	_	_
vertices	_	_
with	_	_
different	_	_
fractions	_	_
.	_	_

#14
This	_	_
derivation	_	_
is	_	_
exploited	_	_
in	_	_
several	_	_
approaches	_	_
as	_	_
the	_	_
presence	_	_
of	_	_
pure-pixel	_	_
[	_	_
4	_	_
,	_	_
20	_	_
]	_	_
,	_	_
projection-based	_	_
[	_	_
11	_	_
,	_	_
3	_	_
]	_	_
,	_	_
kernel-based	_	_
[	_	_
2	_	_
,	_	_
19	_	_
]	_	_
in	_	_
literature	_	_
.	_	_

#15
However	_	_
,	_	_
even	_	_
if	_	_
these	_	_
linear	_	_
methods	_	_
work	_	_
seamlessly	_	_
to	_	_
some	_	_
extent	_	_
,	_	_
e.g.	_	_
,	_	_
on	_	_
synthetic	_	_
data	_	_
/	_	_
controlled	_	_
environment	_	_
,	_	_
they	_	_
strive	_	_
to	_	_
cope	_	_
with	_	_
some	_	_
cases	_	_
such	_	_
as	_	_
multiple	_	_
scattering	_	_
effects	_	_
,	_	_
microscopic-level	_	_
material	_	_
mixtures	_	_
and	_	_
water-absorbed	_	_
environment	_	_
on	_	_
real	_	_
data	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#16
Similarly	_	_
,	_	_
various	_	_
solutions	_	_
based	_	_
on	_	_
nonlinear	_	_
projection	_	_
[	_	_
17	_	_
,	_	_
1	_	_
]	_	_
and	_	_
nonlinear	_	_
kernel	_	_
function	_	_
[	_	_
9	_	_
,	_	_
5	_	_
]	_	_
are	_	_
derived	_	_
to	_	_
mitigate	_	_
these	_	_
issues	_	_
.	_	_

#17
Very	_	_
recently	_	_
,	_	_
sparse	_	_
neural	_	_
networks	_	_
[	_	_
13	_	_
,	_	_
14	_	_
]	_	_
introduce	_	_
significant	_	_
performance	_	_
improvements	_	_
compared	_	_
to	_	_
the	_	_
traditional	_	_
blind	_	_
linear/nonlinear	_	_
approaches	_	_
and	_	_
supervised	_	_
neural	_	_
network	_	_
methods	_	_
[	_	_
16	_	_
,	_	_
15	_	_
]	_	_
.	_	_

#18
Modifications	_	_
on	_	_
the	_	_
network	_	_
architecture	_	_
and	_	_
loss	_	_
function	_	_
constitute	_	_
the	_	_
mainstream	_	_
of	_	_
these	_	_
methods	_	_
.	_	_

#19
Both	_	_
methods	_	_
explain	_	_
that	_	_
combination	_	_
of	_	_
ReLU	_	_
activation	_	_
with	_	_
batch	_	_
normalization	_	_
ultimately	_	_
improves	_	_
the	_	_
sparsity	_	_
of	_	_
abundance	_	_
maps	_	_
while	_	_
endmember	_	_
estimates	_	_
lead	_	_
to	_	_
near-optimum	_	_
solutions	_	_
.	_	_

#20
In	_	_
addition	_	_
,	_	_
[	_	_
13	_	_
]	_	_
introduces	_	_
a	_	_
novel	_	_
loss	_	_
function	_	_
which	_	_
is	_	_
in	_	_
accordance	_	_
with	_	_
the	_	_
problem	_	_
by	_	_
exploiting	_	_
spectral	_	_
angle	_	_
similarities	_	_
,	_	_
regularization	_	_
layers	_	_
and	_	_
additional	_	_
constrains	_	_
that	_	_
boost	_	_
the	_	_
sparsity	_	_
of	_	_
abundance	_	_
maps	_	_
and	_	_
parameter	_	_
convergence	_	_
.	_	_

#21
However	_	_
,	_	_
as	_	_
explained	_	_
[	_	_
13	_	_
]	_	_
,	_	_
these	_	_
methods	_	_
should	_	_
be	_	_
combined	_	_
with	_	_
additional	_	_
hyperspectral	_	_
unmixing	_	_
methods	_	_
(	_	_
i.e.	_	_
even	_	_
if	_	_
they	_	_
outperform	_	_
the	_	_
state-of-the-art	_	_
methods	_	_
)	_	_
in	_	_
order	_	_
to	_	_
improve	_	_
the	_	_
performance	_	_
even	_	_
further	_	_
.	_	_

#22
This	_	_
limitation	_	_
stems	_	_
by	_	_
the	_	_
fact	_	_
that	_	_
high-dimensionality	_	_
of	_	_
signatures	_	_
,	_	_
shallow	_	_
network	_	_
structure	_	_
and	_	_
averaging	_	_
operations	_	_
in	_	_
batch	_	_
normalization	_	_
.	_	_

#23
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
deep	_	_
spectral	_	_
convolutional	_	_
network	_	_
(	_	_
DSCN	_	_
)	_	_
to	_	_
unmix	_	_
hyperspectral	_	_
data	_	_
with	_	_
pre-computed	_	_
endmembers	_	_
.	_	_

#24
To	_	_
address	_	_
these	_	_
current	_	_
limitations	_	_
,	_	_
we	_	_
present	_	_
several	_	_
contributions	_	_
to	_	_
the	_	_
architecture	_	_
as	_	_
follows	_	_
:	_	_
Project	_	_
Website	_	_
:	_	_
https	_	_
:	_	_
//github.com/savasozkan/dscn	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
6	_	_
.	_	_

#25
2v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
2	_	_
Ju	_	_
n	_	_
•	_	_
To	_	_
reduce	_	_
the	_	_
adverse	_	_
effects	_	_
of	_	_
high-dimensionality	_	_
(	_	_
we	_	_
will	_	_
discuss	_	_
theoretical	_	_
explanations/observations	_	_
in	_	_
detail	_	_
in	_	_
Section	_	_
2	_	_
)	_	_
,	_	_
we	_	_
replace	_	_
the	_	_
fully-connected	_	_
linear	_	_
operation	_	_
with	_	_
convolutions	_	_
which	_	_
enables	_	_
to	_	_
extract	_	_
representative	_	_
local	_	_
spectral	_	_
information	_	_
from	_	_
a	_	_
signature	_	_
rather	_	_
than	_	_
its	_	_
full	_	_
version	_	_
.	_	_

#26
•	_	_
Furthermore	_	_
,	_	_
use	_	_
of	_	_
convolutions	_	_
allows	_	_
us	_	_
to	_	_
promote	_	_
a	_	_
deeper	_	_
architecture	_	_
,	_	_
in	_	_
other	_	_
words	_	_
,	_	_
a	_	_
sequence	_	_
of	_	_
convolutions	_	_
which	_	_
improves	_	_
the	_	_
sparsity	_	_
as	_	_
well	_	_
as	_	_
high-level	_	_
abstract	_	_
representation	_	_
of	_	_
signatures	_	_
.	_	_

#27
•	_	_
For	_	_
convolution	_	_
layers	_	_
,	_	_
we	_	_
replace	_	_
batch	_	_
normalization	_	_
with	_	_
spectral	_	_
normalization	_	_
which	_	_
aims	_	_
to	_	_
improve	_	_
the	_	_
spectral	_	_
selectivity	_	_
of	_	_
layers	_	_
.	_	_

#28
By	_	_
this	_	_
way	_	_
,	_	_
more	_	_
beneficial	_	_
spectral	_	_
characteristics	_	_
can	_	_
be	_	_
extracted	_	_
from	_	_
hyperspectral	_	_
data	_	_
to	_	_
unmix	_	_
the	_	_
fractions	_	_
of	_	_
materials	_	_
.	_	_

#29
•	_	_
Lastly	_	_
,	_	_
two	_	_
different	_	_
fusion	_	_
configurations	_	_
that	_	_
estimate	_	_
ideal	_	_
abundance	_	_
map	_	_
for	_	_
each	_	_
pixel	_	_
are	_	_
proposed	_	_
as	_	_
DSCN-S	_	_
and	_	_
DSCN-P	_	_
which	_	_
use	_	_
the	_	_
representations	_	_
computed	_	_
from	_	_
previous	_	_
layers	_	_
.	_	_

#30
The	_	_
main	_	_
difference	_	_
is	_	_
that	_	_
DSCN-S	_	_
configuration	_	_
estimates	_	_
more	_	_
sparse	_	_
abundance	_	_
maps	_	_
while	_	_
DSCN-P	_	_
yields	_	_
more	_	_
probabilistic	_	_
results	_	_
due	_	_
to	_	_
their	_	_
architecture	_	_
variations	_	_
.	_	_

#31
2	_	_
.	_	_

#32
HYPERSPECTRAL	_	_
UNMIXING	_	_
WITH	_	_
SPECTRAL	_	_
CONVOLUTIONS	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
initially	_	_
formulate	_	_
the	_	_
problem	_	_
to	_	_
clarify	_	_
the	_	_
understandability	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
.	_	_

#33
Later	_	_
,	_	_
we	_	_
provide	_	_
theoretical	_	_
explanations/observations	_	_
of	_	_
the	_	_
modifications	_	_
that	_	_
are	_	_
introduced	_	_
throughout	_	_
this	_	_
paper	_	_
.	_	_

#34
Lastly	_	_
,	_	_
the	_	_
details	_	_
of	_	_
the	_	_
method	_	_
and	_	_
related	_	_
information	_	_
about	_	_
the	_	_
architecture	_	_
are	_	_
explained	_	_
.	_	_

#35
2.1	_	_
.	_	_

#36
Preliminary	_	_
Formulation	_	_
.	_	_

#37
Let	_	_
x	_	_
be	_	_
a	_	_
pixel	_	_
of	_	_
hyperspectral	_	_
data	_	_
that	_	_
is	_	_
mixed	_	_
by	_	_
constituent	_	_
materials	_	_
E	_	_
with	_	_
various	_	_
fractions	_	_
y	_	_
.	_	_

#38
Generally	_	_
,	_	_
two	_	_
steps	_	_
should	_	_
be	_	_
defined	_	_
as	_	_
quantifying	_	_
of	_	_
abundance	_	_
maps	_	_
Enc	_	_
(	_	_
.	_	_
)	_	_

#39
and	_	_
reconstruction	_	_
of	_	_
a	_	_
pixel	_	_
Dec	_	_
(	_	_
.	_	_
)	_	_

#40
in	_	_
order	_	_
to	_	_
extract	_	_
abundance	_	_
maps	_	_
ŷ	_	_
and	_	_
endmembers	_	_
Wd	_	_
respectively	_	_
from	_	_
data	_	_
.	_	_

#41
Here	_	_
,	_	_
x̂	_	_
=	_	_
Dec	_	_
(	_	_
ŷ	_	_
;	_	_
Wd	_	_
)	_	_
is	_	_
basically	_	_
equal	_	_
to	_	_
vector	_	_
multiplications	_	_
as	_	_
in	_	_
Eq.	_	_
1	_	_
.	_	_

#42
On	_	_
the	_	_
otherhand	_	_
,	_	_
abundance	_	_
maps	_	_
ŷ	_	_
are	_	_
estimated	_	_
by	_	_
Enc	_	_
(	_	_
.	_	_
)	_	_

#43
as	_	_
follows	_	_
:	_	_
ŷ	_	_
=	_	_
Enc	_	_
(	_	_
x	_	_
;	_	_
θe	_	_
)	_	_
s.t	_	_
.	_	_

#44
ŷk	_	_
≥	_	_
0	_	_
,	_	_
K∑	_	_
k=1	_	_
ŷk	_	_
=	_	_
1	_	_
(	_	_
2	_	_
)	_	_
where	_	_
θe	_	_
is	_	_
the	_	_
set	_	_
of	_	_
trainable	_	_
parameters	_	_
to	_	_
obtain	_	_
optimum	_	_
abundance	_	_
map	_	_
estimates	_	_
.	_	_

#45
Remark	_	_
that	_	_
we	_	_
singly	_	_
focus	_	_
on	_	_
to	_	_
improve	_	_
Enc	_	_
(	_	_
.	_	_
)	_	_

#46
step	_	_
throughout	_	_
this	_	_
paper	_	_
by	_	_
using	_	_
pre-computed	_	_
endmembers	_	_
Wd	_	_
.	_	_

#47
Impact	_	_
of	_	_
Spectral	_	_
Convolution	_	_
.	_	_

#48
As	_	_
explained	_	_
in	_	_
detail	_	_
[	_	_
13	_	_
]	_	_
,	_	_
after	_	_
the	_	_
elimination	_	_
of	_	_
bias	_	_
terms	_	_
,	_	_
fully-connected	_	_
linear	_	_
layer	_	_
is	_	_
a	_	_
simple	_	_
affine	_	_
transformation	_	_
which	_	_
projects	_	_
data	_	_
to	_	_
a	_	_
more	_	_
separable	_	_
space	_	_
to	_	_
ease	_	_
the	_	_
estimation	_	_
process	_	_
.	_	_

#49
However	_	_
,	_	_
as	_	_
previously	_	_
discussed	_	_
for	_	_
feature	_	_
hashing/indexing	_	_
[	_	_
12	_	_
,	_	_
7	_	_
]	_	_
,	_	_
when	_	_
the	_	_
dimensionality	_	_
of	_	_
data	_	_
increases	_	_
,	_	_
irregularity	_	_
of	_	_
data	_	_
leads	_	_
to	_	_
holes	_	_
which	_	_
hardens	_	_
to	_	_
realize	_	_
an	_	_
unsupervised	_	_
method	_	_
for	_	_
the	_	_
problem	_	_
.	_	_

#50
A	_	_
straightforward	_	_
solution	_	_
is	_	_
to	_	_
use	_	_
supervised	_	_
data	_	_
to	_	_
learn	_	_
a	_	_
more	_	_
robust	_	_
projection	_	_
to	_	_
fill	_	_
these	_	_
holes	_	_
[	_	_
12	_	_
]	_	_
.	_	_

#51
Another	_	_
solution	_	_
is	_	_
to	_	_
divide	_	_
data	_	_
into	_	_
several	_	_
overlapping/non-overlapping	_	_
parts	_	_
to	_	_
increase	_	_
the	_	_
representation	_	_
capacity	_	_
per	_	_
element	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#52
In	_	_
particular	_	_
,	_	_
convolution	_	_
layer	_	_
shares	_	_
similar	_	_
objective	_	_
as	_	_
in	_	_
the	_	_
second	_	_
approach	_	_
(	_	_
i.e.	_	_
small/local	_	_
parts	_	_
)	_	_
[	_	_
10	_	_
]	_	_
and	_	_
it	_	_
extracts	_	_
discriminative	_	_
responses	_	_
which	_	_
indicate	_	_
the	_	_
local	_	_
characteristics	_	_
of	_	_
data	_	_
.	_	_

#53
Note	_	_
that	_	_
we	_	_
use	_	_
only	_	_
1D	_	_
convolutions	_	_
per	_	_
pixel	_	_
to	_	_
identify	_	_
the	_	_
spectral	_	_
characteristics	_	_
of	_	_
data	_	_
,	_	_
not	_	_
their	_	_
spatial	_	_
information	_	_
.	_	_

#54
Moreover	_	_
,	_	_
since	_	_
respectively	_	_
less	_	_
number	_	_
of	_	_
trainable	_	_
parameters	_	_
are	_	_
learned	_	_
in	_	_
the	_	_
convolution	_	_
layers	_	_
compared	_	_
to	_	_
fully	_	_
linear	_	_
operations	_	_
,	_	_
a	_	_
deeper	_	_
architecture	_	_
can	_	_
be	_	_
promoted	_	_
in	_	_
our	_	_
method	_	_
.	_	_

#55
Ultimately	_	_
,	_	_
it	_	_
boosts	_	_
the	_	_
discriminative	_	_
power	_	_
of	_	_
the	_	_
representation	_	_
by	_	_
extracting	_	_
a	_	_
sequence	_	_
of	_	_
abstracts	_	_
from	_	_
lower-level	_	_
to	_	_
higher	_	_
ones	_	_
as	_	_
explained	_	_
[	_	_
10	_	_
,	_	_
6	_	_
]	_	_
.	_	_

#56
Spectral	_	_
Normalization	_	_
.	_	_

#57
Practically	_	_
,	_	_
combination	_	_
of	_	_
ReLU	_	_
with	_	_
batch	_	_
normalization	_	_
enables	_	_
a	_	_
network	_	_
to	_	_
select	_	_
the	_	_
sparse	_	_
outputs	_	_
of	_	_
an	_	_
affine	_	_
transformation	_	_
(	_	_
i.e.	_	_
the	_	_
responses	_	_
of	_	_
fully	_	_
linear	_	_
/	_	_
convolution	_	_
)	_	_
based	_	_
on	_	_
batch	_	_
characteristics	_	_
of	_	_
data	_	_
.	_	_

#58
However	_	_
,	_	_
this	_	_
is	_	_
not	_	_
completely	_	_
practical	_	_
to	_	_
reveal	_	_
the	_	_
latent	_	_
spectral	_	_
characteristics	_	_
of	_	_
a	_	_
pixel	_	_
.	_	_

#59
For	_	_
this	_	_
purpose	_	_
,	_	_
we	_	_
utilize	_	_
spectral	_	_
normalization	_	_
with	_	_
ReLU	_	_
activation	_	_
after	_	_
each	_	_
convolution	_	_
layer	_	_
.	_	_

#60
Intuitively	_	_
,	_	_
spectral	_	_
normalization	_	_
normalizes	_	_
the	_	_
responses	_	_
of	_	_
convolution	_	_
by	_	_
regarding	_	_
their	_	_
responses	_	_
for	_	_
spectral	_	_
values	_	_
along	_	_
with	_	_
the	_	_
batch	_	_
characteristics	_	_
.	_	_

#61
By	_	_
this	_	_
way	_	_
,	_	_
each	_	_
layer	_	_
(	_	_
combination	_	_
of	_	_
convolution	_	_
,	_	_
spectral	_	_
norm	_	_
and	_	_
ReLU	_	_
)	_	_
computes	_	_
the	_	_
most	_	_
representative	_	_
spectral	_	_
responses	_	_
about	_	_
data	_	_
while	_	_
preserving	_	_
batch	_	_
characteristic	_	_
of	_	_
data	_	_
.	_	_

#62
Most	_	_
relevant	_	_
normalization	_	_
type	_	_
in	_	_
literature	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
[	_	_
18	_	_
]	_	_
used	_	_
for	_	_
style	_	_
transfer	_	_
.	_	_

#63
Fusion	_	_
Layer	_	_
.	_	_

#64
As	_	_
indicated	_	_
[	_	_
13	_	_
,	_	_
14	_	_
]	_	_
,	_	_
combination	_	_
of	_	_
ReLU	_	_
and	_	_
batch	_	_
normalization	_	_
yields	_	_
robustness	_	_
for	_	_
abundance	_	_
estimates	_	_
to	_	_
obtain	_	_
sparse	_	_
abundance	_	_
maps	_	_
and	_	_
endmembers	_	_
.	_	_

#65
However	_	_
,	_	_
for	_	_
finer	_	_
abundance	_	_
values	_	_
,	_	_
probabilistic/distancebased	_	_
approaches	_	_
might	speculation	_
be	_	_
practical	_	_
for	_	_
several	_	_
datasets	_	_
as	_	_
in	_	_
[	_	_
4	_	_
,	_	_
5	_	_
]	_	_
.	_	_

#66
For	_	_
this	_	_
purpose	_	_
,	_	_
throughout	_	_
the	_	_
paper	_	_
,	_	_
we	_	_
introduce	_	_
two	_	_
difference	_	_
fusion	_	_
configurations	_	_
.	_	_

#67
If	_	_
we	_	_
explain	_	_
the	_	_
purpose	_	_
of	_	_
fusion	_	_
layer	_	_
in	_	_
detail	_	_
,	_	_
the	_	_
aim	_	_
is	_	_
to	_	_
fuse	_	_
the	_	_
responses	_	_
of	_	_
spectral	_	_
convolutions	_	_
from	_	_
the	_	_
previous	_	_
layers	_	_
to	_	_
estimate	_	_
the	_	_
true	_	_
abundance	_	_
maps	_	_
per-pixel	_	_
at	_	_
the	_	_
final	_	_
layer	_	_
.	_	_

#68
Note	_	_
that	_	_
instead	_	_
of	_	_
feeding	_	_
high-dimensional	_	_
data	_	_
directly	_	_
to	_	_
a	_	_
fully-connected	_	_
linear	_	_
layer	_	_
as	_	_
in	_	_
[	_	_
13	_	_
,	_	_
14	_	_
]	_	_
,	_	_
the	_	_
proposed	_	_
method	_	_
reduces	_	_
the	_	_
dimension	_	_
of	_	_
data	_	_
iteratively	_	_
while	_	_
enriching	_	_
the	_	_
representation	_	_
capacity	_	_
of	_	_
the	_	_
input	_	_
with	_	_
sparse	_	_
transformations	_	_
(	_	_
i.e.	_	_
convolutions	_	_
)	_	_
for	_	_
both	_	_
configurations	_	_
.	_	_

#69
Table	_	_
1	_	_
.	_	_

#70
RMSE	_	_
results	_	_
on	_	_
Jasper	_	_
Ridge	_	_
dataset	_	_
.	_	_

#71
Mean	_	_
and	_	_
standard	_	_
deviation	_	_
are	_	_
reported	_	_
.	_	_

#72
Best	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
bold	_	_
.	_	_

#73
Root	_	_
Mean	_	_
Square	_	_
Error	_	_
(	_	_
RMSE	_	_
)	_	_
(	_	_
×10−2	_	_
)	_	_
VCA	_	_
DMaxD	_	_
SCM	_	_
DgS-NMF	_	_
EndNet	_	_
EndNet-SPU	_	_
EndNetDSCNS	_	_
EndNetDSCNP	_	_
#	_	_
1	_	_
17.68±6.2	_	_
17.03±0.0	_	_
23.87±0.0	_	_
11.66±0.2	_	_
10.12±0.6	_	_
8.24±0.4	_	_
6.04±1.2	_	_
5.77±0.2	_	_
#	_	_
2	_	_
13.45±1.9	_	_
21.34±0.0	_	_
13.30±0.0	_	_
4.13±0.0	_	_
11.48±0.8	_	_
6.17±0.3	_	_
3.96±0.7	_	_
4.52±0.3	_	_
#	_	_
3	_	_
38.93±7.9	_	_
14.34±0.0	_	_
28.47±0.0	_	_
11.13±0.3	_	_
9.53±0.3	_	_
8.98±0.2	_	_
8.93±1.5	_	_
13.07±0.4	_	_
#	_	_
4	_	_
29.13±4.2	_	_
11.21±0.0	_	_
19.87±0.0	_	_
5.68±0.1	_	_
12.29±0.4	_	_
8.55±0.1	_	_
9.31±1.3	_	_
14.36±0.4	_	_
Avg	_	_
.	_	_

#74
24.80±5.1	_	_
15.98±0.0	_	_
23.04±0.0	_	_
8.15±0.2	_	_
10.85±0.6	_	_
7.96±0.3	_	_
7.06±0.9	_	_
9.43±0.3	_	_
Fig.	_	_
1	_	_
.	_	_

#75
Visualization	_	_
of	_	_
the	_	_
results	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
on	_	_
Jasper	_	_
Ridge	_	_
dataset	_	_
for	_	_
each	_	_
material	_	_
.	_	_

#76
From	_	_
left	_	_
to	_	_
right	_	_
,	_	_
ground	_	_
truth	_	_
,	_	_
estimated	_	_
abundance	_	_
map	_	_
,	_	_
absolute	_	_
difference	_	_
respectively	_	_
.	_	_

#77
First	_	_
configuration	_	_
,	_	_
i.e.	_	_
DSCN-S	_	_
,	_	_
computes	_	_
the	_	_
ideal	_	_
abundance	_	_
values	_	_
for	_	_
each	_	_
pixel	_	_
with	_	_
the	_	_
combination	_	_
of	_	_
fully-connected	_	_
linear	_	_
,	_	_
batch	_	_
norm	_	_
,	_	_
ReLU	_	_
and	_	_
l1-norm	_	_
layers	_	_
by	_	_
taking	_	_
the	_	_
hidden	_	_
representation	_	_
computed	_	_
from	_	_
the	_	_
spectral	_	_
convolutions	_	_
as	_	_
an	_	_
input	_	_
.	_	_

#78
Note	_	_
that	_	_
due	_	_
to	_	_
joint	_	_
usage	_	_
of	_	_
ReLU	_	_
and	_	_
batch	_	_
normalization	_	_
,	_	_
it	_	_
is	_	_
expected	_	_
that	_	_
the	_	_
estimated	_	_
abundance	_	_
maps	_	_
are	_	_
quite	_	_
sparse	_	_
.	_	_

#79
Second	_	_
configuration	_	_
,	_	_
i.e.	_	_
DSCN-P	_	_
,	_	_
consists	_	_
of	_	_
fully-connected	_	_
linear	_	_
and	_	_
softmax	_	_
activation	_	_
layers	_	_
.	_	_

#80
The	_	_
softmax	_	_
activation	_	_
function	_	_
response	_	_
is	_	_
as	_	_
follows	_	_
:	_	_
softmax	_	_
(	_	_
h	_	_
)	_	_
=	_	_
eh∑K	_	_
k=1	_	_
e	_	_
hk	_	_
(	_	_
3	_	_
)	_	_
where	_	_
h	_	_
is	_	_
the	_	_
outputs	_	_
of	_	_
fully-connected	_	_
linear	_	_
layer	_	_
.	_	_

#81
Due	_	_
to	_	_
the	_	_
architecture	_	_
,	_	_
this	_	_
configuration	_	_
yields	_	_
probabilistic	_	_
results	_	_
and	_	_
finer	_	_
abundance	_	_
maps	_	_
for	_	_
highly-mixtured	_	_
scenes	_	_
.	_	_

#82
Lastly	_	_
,	_	_
both	_	_
of	_	_
these	_	_
configuration	_	_
layers	_	_
is	_	_
the	_	_
final	_	_
layer	_	_
of	_	_
Enc	_	_
(	_	_
.	_	_
)	_	_
.	_	_

#83
2.2	_	_
.	_	_

#84
Deep	_	_
Spectral	_	_
Convolution	_	_
Network	_	_
Architecture	_	_
.	_	_

#85
First	_	_
,	_	_
a	_	_
pixel	_	_
is	_	_
filtered	_	_
by	_	_
two	_	_
consecutive	_	_
spectral	_	_
convolution	_	_
blocks	_	_
.	_	_

#86
Note	_	_
that	_	_
the	_	_
number	_	_
of	_	_
blocks	_	_
and	_	_
inner	_	_
structure	_	_
can	_	_
still	_	_
be	_	_
tuned	_	_
for	_	_
different	_	_
datasets	_	_
(	_	_
i.e.	_	_
deeper	_	_
networks	_	_
)	_	_
.	_	_

#87
Each	_	_
block	_	_
consists	_	_
of	_	_
spectral	_	_
normalization	_	_
and	_	_
ReLU	_	_
activation	_	_
layers	_	_
after	_	_
a	_	_
spectral	_	_
convolution	_	_
.	_	_

#88
To	_	_
reduce	_	_
the	_	_
dimensionality	_	_
of	_	_
responses	_	_
,	_	_
maxpool	_	_
layer	_	_
is	_	_
exploited	_	_
at	_	_
each	_	_
block	_	_
.	_	_

#89
To	_	_
this	_	_
end	_	_
,	_	_
these	_	_
blocks	_	_
ultimately	_	_
behave	_	_
like	_	_
a	_	_
feature	_	_
extractor	_	_
.	_	_

#90
At	_	_
the	_	_
third	_	_
block	_	_
,	_	_
batch	_	_
normalization	_	_
and	_	_
ReLU	_	_
are	_	_
utilized	_	_
with	_	_
spectral	_	_
convolution	_	_
to	_	_
determine	_	_
the	_	_
responses	_	_
based	_	_
on	_	_
only	_	_
their	_	_
batch	_	_
characteristics	_	_
.	_	_

#91
This	_	_
implicitly	_	_
corresponds	_	_
to	_	_
the	_	_
mutual	_	_
distribution	_	_
of	_	_
data	_	_
as	_	_
in	_	_
[	_	_
4	_	_
,	_	_
5	_	_
]	_	_
.	_	_

#92
This	_	_
is	_	_
critical	_	_
since	_	_
the	_	_
final	_	_
convolution	_	_
block	_	_
reduces	_	_
the	_	_
depth	_	_
size	_	_
regarding	_	_
to	_	_
the	_	_
overall	_	_
data	_	_
batch	_	_
characteristic	_	_
.	_	_

#93
Lastly	_	_
,	_	_
fusion	_	_
layer	_	_
(	_	_
i.e.	_	_
either	_	_
DSCN-S	_	_
or	_	_
DSCN-P	_	_
)	_	_
is	_	_
used	_	_
to	_	_
compute	_	_
abundance	_	_
maps	_	_
for	_	_
a	_	_
pixel	_	_
.	_	_

#94
Learning	_	_
.	_	_

#95
For	_	_
parameter	_	_
optimization	_	_
,	_	_
we	_	_
use	_	_
the	_	_
loss	_	_
function	_	_
that	_	_
is	_	_
proposed	_	_
in	_	_
[	_	_
13	_	_
]	_	_
and	_	_
the	_	_
parameters	_	_
θe	_	_
are	_	_
updated	_	_
by	_	_
back-propagation	_	_
scheme	_	_
.	_	_

#96
This	_	_
full	_	_
loss	_	_
function	_	_
is	_	_
written	_	_
as	_	_
:	_	_
L	_	_
=	_	_
−λ1DKL	_	_
(	_	_
1.0||C	_	_
(	_	_
x	_	_
,	_	_
x̂	_	_
)	_	_
)	_	_
+	_	_
λ2‖ŷ‖1	_	_
+	_	_
λ3‖θe‖2	_	_
(	_	_
4	_	_
)	_	_
where	_	_
λ1	_	_
,	_	_
λ2	_	_
and	_	_
λ3	_	_
are	_	_
set	_	_
to	_	_
10	_	_
,	_	_
0.4	_	_
and	_	_
10−5	_	_
respectively	_	_
.	_	_

#97
DKL	_	_
(	_	_
.	_	_
)	_	_

#98
is	_	_
the	_	_
Kullback-Leibler	_	_
divergence	_	_
term	_	_
and	_	_
C	_	_
(	_	_
.	_	_
,	_	_
.	_	_
)	_	_

#99
is	_	_
the	_	_
normalized	_	_
SAD	_	_
score	_	_
between	_	_
the	_	_
original	_	_
and	_	_
reconstructed	_	_
version	_	_
of	_	_
signatures	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#100
Note	_	_
that	_	_
finetuning	_	_
of	_	_
pre-computed	_	_
endmember	_	_
Wd	_	_
is	_	_
not	_	_
allowed	_	_
during	_	_
the	_	_
training	_	_
.	_	_

#101
Moreover	_	_
,	_	_
unlike	_	_
[	_	_
13	_	_
]	_	_
,	_	_
denoising	_	_
autoencoder	_	_
scheme	_	_
is	_	_
not	_	_
used	_	_
for	_	_
the	_	_
method	_	_
,	_	_
since	_	_
our	_	_
aim	_	_
is	_	_
to	_	_
obtain	_	_
actual/finer	_	_
abundance	_	_
values	_	_
rather	_	_
than	_	_
coarse	_	_
estimation	_	_
of	_	_
constituent	_	_
endmembers	_	_
from	_	_
data	_	_
.	_	_

#102
Adam	_	_
stochastic	_	_
optimizer	_	_
[	_	_
8	_	_
]	_	_
is	_	_
used	_	_
with	_	_
the	_	_
previously	_	_
explained	_	_
parameter	_	_
settings	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#103
The	_	_
number	_	_
of	_	_
iteration	_	_
is	_	_
set	_	_
to	_	_
5K	_	_
and	_	_
the	_	_
parameters	_	_
are	_	_
randomly	_	_
initialized	_	_
.	_	_

#104
Codes	_	_
are	_	_
implemented	_	_
on	_	_
Python	_	_
by	_	_
extensively	_	_
leveraging	_	_
Tensorflow	_	_
framework	_	_
.	_	_

#105
3	_	_
.	_	_

#106
EXPERIMENTS	_	_
3.1	_	_
.	_	_

#107
Datasets	_	_
,	_	_
Evaluation	_	_
Metric	_	_
and	_	_
Baselines	_	_
To	_	_
make	_	_
fair	_	_
and	_	_
realistic	_	_
comparisons	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
proposed	_	_
method	_	_
on	_	_
two	_	_
real	_	_
datasets	_	_
,	_	_
namely	_	_
Jasper	_	_
Ridge	_	_
[	_	_
22	_	_
]	_	_
and	_	_
Urban	_	_
[	_	_
22	_	_
]	_	_
,	_	_
which	_	_
are	_	_
extensively	_	_
used	_	_
in	_	_
literature	_	_
.	_	_

#108
Briefly	_	_
,	_	_
for	_	_
Jasper	_	_
Ridge	_	_
dataset	_	_
,	_	_
the	_	_
spectral	_	_
and	_	_
spatial	_	_
Table	_	_
2	_	_
.	_	_

#109
RMSE	_	_
results	_	_
on	_	_
Urban	_	_
dataset	_	_
.	_	_

#110
Mean	_	_
and	_	_
standard	_	_
deviation	_	_
are	_	_
reported	_	_
.	_	_

#111
Best	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
bold	_	_
.	_	_

#112
Root	_	_
Mean	_	_
Square	_	_
Error	_	_
(	_	_
RMSE	_	_
)	_	_
(	_	_
×10−2	_	_
)	_	_
VCA	_	_
DMaxD	_	_
SCM	_	_
DgS-NMF	_	_
EndNet	_	_
EndNet-SPU	_	_
EndNetDSCNS	_	_
EndNetDSCNP	_	_
#	_	_
1	_	_
42.14±7.2	_	_
30.68±0.0	_	_
32.79±0.0	_	_
13.18±0.1	_	_
13.04±0.3	_	_
10.41±0.2	_	_
12.85±1.3	_	_
9.87±0.2	_	_
#	_	_
2	_	_
48.46±5.6	_	_
47.26±0.0	_	_
36.25±0.0	_	_
12.95±0.0	_	_
14.43±0.3	_	_
12.24±0.3	_	_
13.67±1.5	_	_
12.08±0.3	_	_
#	_	_
3	_	_
17.18±3.7	_	_
26.71±0.0	_	_
32.61±0.0	_	_
9.57±0.1	_	_
8.71±0.5	_	_
8.35±0.3	_	_
8.53±0.7	_	_
7.54±0.1	_	_
#	_	_
4	_	_
16.94±2.1	_	_
19.49±0.0	_	_
32.86±0.0	_	_
6.27±0.0	_	_
7.59±0.2	_	_
5.92±0.1	_	_
7.03±0.9	_	_
6.65±0.1	_	_
Avg	_	_
.	_	_

#113
31.18±4.7	_	_
31.04±0.0	_	_
33.59±0.0	_	_
10.49±0.1	_	_
10.94±0.4	_	_
9.23±0.2	_	_
10.52±1.1	_	_
9.04±0.2	_	_
resolutions	_	_
are	_	_
198	_	_
and	_	_
100	_	_
×	_	_
100	_	_
,	_	_
respectively	_	_
.	_	_

#114
There	_	_
are	_	_
four	_	_
main	_	_
materials	_	_
:	_	_
Tree	_	_
(	_	_
#	_	_
1	_	_
)	_	_
,	_	_
Water	_	_
(	_	_
#	_	_
2	_	_
)	_	_
,	_	_
Soil	_	_
(	_	_
#	_	_
3	_	_
)	_	_
and	_	_
Road	_	_
(	_	_
#	_	_
4	_	_
)	_	_
.	_	_

#115
The	_	_
spatial	_	_
resolution	_	_
of	_	_
Urban	_	_
dataset	_	_
is	_	_
307	_	_
×	_	_
307	_	_
and	_	_
its	_	_
spectral	_	_
resolution	_	_
is	_	_
162	_	_
.	_	_

#116
Similarly	_	_
,	_	_
it	_	_
has	_	_
four	_	_
constituent	_	_
materials	_	_
in	_	_
the	_	_
scene	_	_
:	_	_
Asphalt	_	_
(	_	_
#	_	_
1	_	_
)	_	_
,	_	_
Grass	_	_
(	_	_
#	_	_
2	_	_
)	_	_
,	_	_
Tree	_	_
(	_	_
#	_	_
3	_	_
)	_	_
and	_	_
Roof	_	_
(	_	_
#	_	_
4	_	_
)	_	_
.	_	_

#117
For	_	_
reliable	_	_
assessments	_	_
,	_	_
tests	_	_
are	_	_
repeated	_	_
20	_	_
times	_	_
for	_	_
each	_	_
method	_	_
,	_	_
thus	_	_
mean	_	_
and	_	_
standard	_	_
deviation	_	_
of	_	_
the	_	_
results	_	_
are	_	_
reported	_	_
.	_	_

#118
Furthermore	_	_
,	_	_
we	_	_
compare	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
method	_	_
with	_	_
several	_	_
baseline	_	_
algorithms	_	_
such	_	_
as	_	_
VCA	_	_
[	_	_
11	_	_
]	_	_
,	_	_
DMaxD	_	_
[	_	_
4	_	_
]	_	_
,	_	_
SCM	_	_
[	_	_
21	_	_
]	_	_
,	_	_
DgS-NMF	_	_
[	_	_
22	_	_
]	_	_
,	_	_
EndNet	_	_
[	_	_
13	_	_
]	_	_
and	_	_
EndNet-SPU	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#119
For	_	_
a	_	_
performance	_	_
metric	_	_
,	_	_
we	_	_
utilize	_	_
Root	_	_
Mean	_	_
Square	_	_
Error	_	_
(	_	_
RMSE	_	_
)	_	_
to	_	_
measure	_	_
the	_	_
error	_	_
between	_	_
estimated	_	_
abundance	_	_
maps	_	_
and	_	_
ground	_	_
truth	_	_
.	_	_

#120
To	_	_
preserve	_	_
non-linearity	_	_
for	_	_
VCA	_	_
and	_	_
DMaxD	_	_
abundance	_	_
estimates	_	_
,	_	_
we	_	_
compute	_	_
their	_	_
endmember	_	_
estimates	_	_
with	_	_
Multilinear	_	_
Mixing	_	_
Model	_	_
(	_	_
MLM	_	_
)	_	_
[	_	_
5	_	_
]	_	_
throughout	_	_
the	_	_
experiments	_	_
.	_	_

#121
Lastly	_	_
,	_	_
the	_	_
proposed	_	_
unmixing	_	_
method	_	_
(	_	_
i.e.	_	_
either	_	_
DSCN-S	_	_
or	_	_
DSCN-P	_	_
)	_	_
aims	_	_
to	_	_
improve	_	_
the	_	_
abundance	_	_
map	_	_
results	_	_
of	_	_
endmembers	_	_
estimated	_	_
by	_	_
EndNet	_	_
[	_	_
13	_	_
]	_	_
which	_	_
recently	_	_
achieves	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
in	_	_
literature	_	_
.	_	_

#122
You	_	_
can	_	_
find	_	_
further	_	_
detail	_	_
about	_	_
EndNet	_	_
from	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#123
3.2	_	_
.	_	_

#124
Experimental	_	_
Results	_	_
Jasper	_	_
Ridge	_	_
.	_	_

#125
Experimental	_	_
results	_	_
for	_	_
this	_	_
dataset	_	_
are	_	_
illustrated	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#126
From	_	_
these	_	_
results	_	_
,	_	_
the	_	_
sparse	_	_
version	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
(	_	_
i.e.	_	_
DSCN-S	_	_
)	_	_
achieves	_	_
the	_	_
best	_	_
overall	_	_
accuracy	_	_
.	_	_

#127
It	_	_
approximately	_	_
introduces	_	_
1	_	_
%	_	_
improvements	_	_
to	_	_
the	_	_
second	_	_
best	_	_
result	_	_
which	_	_
is	_	_
obtained	_	_
by	_	_
EndNet-SPU	_	_
combination	_	_
.	_	_

#128
As	_	_
identified	_	_
[	_	_
13	_	_
]	_	_
,	_	_
Soil	_	_
(	_	_
#	_	_
3	_	_
)	_	_
and	_	_
Road	_	_
(	_	_
#	_	_
4	_	_
)	_	_
materials	_	_
are	_	_
highly	_	_
correlated	_	_
and	_	_
it	_	_
is	_	_
only	_	_
practical	_	_
to	_	_
quantify	_	_
the	_	_
fractions	_	_
with	_	_
supervised	_	_
data	_	_
or	_	_
spatial	_	_
reasoning	_	_
as	_	_
in	_	_
DgS-NMF	_	_
method	_	_
.	_	_

#129
For	_	_
Tree	_	_
(	_	_
#	_	_
1	_	_
)	_	_
and	_	_
Water	_	_
(	_	_
#	_	_
2	_	_
)	_	_
materials	_	_
in	_	_
particular	_	_
,	_	_
the	_	_
proposed	_	_
method	_	_
nearly	_	_
attains	_	_
ideal	_	_
abundance	_	_
performance	_	_
for	_	_
the	_	_
materials	_	_
.	_	_

#130
In	_	_
addition	_	_
,	_	_
Fig.	_	_
1	_	_
shows	_	_
the	_	_
quantitative	_	_
results	_	_
of	_	_
the	_	_
method	_	_
(	_	_
DSCN-S	_	_
)	_	_
for	_	_
each	_	_
materials	_	_
(	_	_
i.e.	_	_
each	_	_
row	_	_
)	_	_
.	_	_

#131
Perceptually	_	_
impressive	_	_
results	_	_
are	_	_
obtained	_	_
especially	_	_
for	_	_
Soil	_	_
and	_	_
Water	_	_
.	_	_

#132
Similarly	_	_
,	_	_
the	_	_
error	_	_
concentrates	_	_
at	_	_
the	_	_
boundaries	_	_
of	_	_
water-ground	_	_
as	_	_
well	_	_
as	_	_
road-soil	_	_
intersections	_	_
.	_	_

#133
However	_	_
,	_	_
there	_	_
is	_	_
an	_	_
issue	_	_
for	_	_
DSCN-S	_	_
that	_	_
the	_	_
variances	_	_
in	_	_
the	_	_
accuracies	_	_
are	_	_
a	_	_
bit	_	_
high	_	_
while	_	_
DSCN-P	_	_
obtains	_	_
more	_	_
stable	_	_
results	_	_
.	_	_

#134
The	_	_
main	_	_
drawback	_	_
arises	_	_
primarily	_	_
due	_	_
to	_	_
the	_	_
lack	_	_
of	_	_
parameter	_	_
convergence	_	_
to	_	_
a	_	_
global	_	_
solution	_	_
for	_	_
every	_	_
initialization	_	_
.	_	_

#135
We	_	_
believe	_	_
that	_	_
this	_	_
issue	_	_
can	_	_
be	_	_
reduced	_	_
by	_	_
the	_	_
detail	_	_
experiments	_	_
on	_	_
the	_	_
network	_	_
architecture	_	_
.	_	_

#136
Urban	_	_
.	_	_

#137
Table	_	_
2	_	_
shows	_	_
the	_	_
experimental	_	_
results	_	_
on	_	_
Urban	_	_
dataset	_	_
.	_	_

#138
The	_	_
probabilistic	_	_
configuration	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
,	_	_
DSCN-P	_	_
,	_	_
obtains	_	_
the	_	_
best	_	_
overall	_	_
results	_	_
with	_	_
small	_	_
improvements	_	_
over	_	_
EndNet-SPU	_	_
.	_	_

#139
Note	_	_
that	_	_
the	_	_
actual	_	_
abundance	_	_
maps	_	_
of	_	_
data	_	_
is	_	_
quite	_	_
dense	_	_
(	_	_
i.e.	_	_
highly-mixtured	_	_
)	_	_
,	_	_
thus	_	_
the	_	_
probabilistic	_	_
version	_	_
can	_	_
be	_	_
more	_	_
appropriate	_	_
for	_	_
this	_	_
case	_	_
.	_	_

#140
The	_	_
experiment	_	_
results	_	_
also	_	_
support	_	_
this	_	_
assumption	_	_
by	_	_
yielding	_	_
better	_	_
performance	_	_
.	_	_

#141
Lastly	_	_
,	_	_
the	_	_
variation	_	_
in	_	_
the	_	_
scores	_	_
is	_	_
still	_	_
an	_	_
issue	_	_
for	_	_
DSCN-S	_	_
while	_	_
DSCN-P	_	_
generates	_	_
more	_	_
consistent	_	_
results	_	_
.	_	_

#142
4	_	_
.	_	_

#143
CONCLUSION	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
deep	_	_
spectral	_	_
convolution	_	_
network	_	_
to	_	_
unmix	_	_
hyperspectral	_	_
data	_	_
with	_	_
pre-computed	_	_
endmembers	_	_
.	_	_

#144
Throughout	_	_
the	_	_
paper	_	_
,	_	_
we	_	_
introduce	_	_
three	_	_
critical	_	_
contributions	_	_
for	_	_
the	_	_
unmixing	_	_
problem	_	_
.	_	_

#145
First	_	_
,	_	_
instead	_	_
of	_	_
a	_	_
single	_	_
layer	_	_
fully-connected	_	_
linear	_	_
operation	_	_
,	_	_
a	_	_
network	_	_
that	_	_
is	_	_
composed	_	_
of	_	_
several	_	_
spectral	_	_
convolution	_	_
layers	_	_
with	_	_
a	_	_
deeper	_	_
architecture	_	_
is	_	_
proposed	_	_
.	_	_

#146
Later	_	_
,	_	_
we	_	_
present	_	_
a	_	_
novel	_	_
spectral	_	_
normalization	_	_
layer	_	_
that	_	_
is	_	_
able	_	_
to	_	_
normalize	_	_
responses	_	_
of	_	_
filters	_	_
to	_	_
improve	_	_
the	_	_
selectivity	_	_
of	_	_
layers	_	_
.	_	_

#147
Lastly	_	_
,	_	_
we	_	_
introduce	_	_
two	_	_
configurations	_	_
for	_	_
the	_	_
fusion	_	_
of	_	_
the	_	_
responses	_	_
of	_	_
previous	_	_
layers	_	_
and	_	_
the	_	_
computation	_	_
of	_	_
abundance	_	_
maps	_	_
.	_	_

#148
The	_	_
experimental	_	_
results	_	_
validate	_	_
that	_	_
the	_	_
proposed	_	_
method	_	_
in	_	_
this	_	_
paper	_	_
obtains	_	_
the	_	_
new	_	_
state-of-the-art	_	_
performance	_	_
on	_	_
two	_	_
real	_	_
datasets	_	_
.	_	_

#149
5	_	_
.	_	_

#150
ACKNOWLEDGMENTS	_	_
We	_	_
gratefully	_	_
acknowledge	_	_
the	_	_
support	_	_
of	_	_
NVIDIA	_	_
Corporation	_	_
with	_	_
the	_	_
donation	_	_
of	_	_
Quadro	_	_
P5000	_	_
used	_	_
for	_	_
this	_	_
research	_	_
.	_	_

#151
6	_	_
.	_	_

#152
REFERENCES	_	_
[	_	_
1	_	_
]	_	_
C.	_	_
M.	_	_
Bachmann	_	_
,	_	_
T.	_	_
L.	_	_
Ainsworth	_	_
,	_	_
and	_	_
R.	_	_
A.	_	_
Fusina	_	_
,	_	_
“Improved	_	_
manifold	_	_
coordinate	_	_
representations	_	_
of	_	_
large-scale	_	_
hyperspectral	_	_
scenes	_	_
,	_	_
”	_	_
IEEE	_	_
TGRS	_	_
,	_	_
pp	_	_
.	_	_

#153
2786–	_	_
2803	_	_
,	_	_
2006	_	_
.	_	_

#154
[	_	_
2	_	_
]	_	_
V.	_	_
F.	_	_
Haertel	_	_
and	_	_
Y.	_	_
E.	_	_
Shimabukuro	_	_
,	_	_
“Spectral	_	_
linear	_	_
mixing	_	_
model	_	_
in	_	_
low	_	_
spatial	_	_
resolution	_	_
image	_	_
data	_	_
,	_	_
”	_	_
IEEE	_	_
TGRS	_	_
,	_	_
pp	_	_
.	_	_

#155
2555–2562	_	_
,	_	_
2005	_	_
.	_	_

#156
[	_	_
3	_	_
]	_	_
J.	_	_
C.	_	_
Harsanyi	_	_
and	_	_
C.-I	_	_
.	_	_

#157
Chang	_	_
,	_	_
“Hyperspectral	_	_
image	_	_
classification	_	_
and	_	_
dimensionality	_	_
reduction	_	_
:	_	_
An	_	_
orthogonal	_	_
subspace	_	_
projection	_	_
approach	_	_
,	_	_
”	_	_
IEEE	_	_
TGRS	_	_
,	_	_
pp	_	_
.	_	_

#158
779–785	_	_
,	_	_
1994	_	_
.	_	_

#159
[	_	_
4	_	_
]	_	_
R.	_	_
Heylen	_	_
,	_	_
D.	_	_
Burazerovic	_	_
,	_	_
and	_	_
P.	_	_
Scheunders	_	_
,	_	_
“Nonlinear	_	_
spectral	_	_
unmixing	_	_
by	_	_
geodesic	_	_
simplex	_	_
volume	_	_
maximization	_	_
,	_	_
”	_	_
IEEE	_	_
J-STSP	_	_
,	_	_
pp	_	_
.	_	_

#160
534–542	_	_
,	_	_
2011	_	_
.	_	_

#161
[	_	_
5	_	_
]	_	_
R.	_	_
Heylen	_	_
,	_	_
P.	_	_
Scheunders	_	_
,	_	_
A.	_	_
Rangarajan	_	_
,	_	_
and	_	_
P.	_	_
Gader	_	_
,	_	_
“Nonlinear	_	_
unmixing	_	_
by	_	_
using	_	_
different	_	_
metrics	_	_
in	_	_
a	_	_
linear	_	_
unmixing	_	_
chain	_	_
,	_	_
”	_	_
IEEE	_	_
JSTARS	_	_
,	_	_
pp	_	_
.	_	_

#162
2655–2664	_	_
,	_	_
2015	_	_
.	_	_

#163
[	_	_
6	_	_
]	_	_
G.	_	_
Hinton	_	_
and	_	_
R.	_	_
Salakhutdinov	_	_
,	_	_
“Reducing	_	_
the	_	_
dimensionality	_	_
of	_	_
data	_	_
with	_	_
neural	_	_
networks.”	_	_
in	_	_
Science	_	_
,	_	_
2006	_	_
,	_	_
pp	_	_
.	_	_

#164
504–507	_	_
.	_	_

#165
[	_	_
7	_	_
]	_	_
H.	_	_
Jegou	_	_
,	_	_
M.	_	_
Douze	_	_
,	_	_
and	_	_
C.	_	_
Schmid	_	_
,	_	_
“Product	_	_
quantization	_	_
for	_	_
nearest	_	_
neighbor	_	_
search.”	_	_
IEEE	_	_
PAMI	_	_
,	_	_
pp	_	_
.	_	_

#166
117–128	_	_
,	_	_
2011	_	_
.	_	_

#167
[	_	_
8	_	_
]	_	_
D.	_	_
Kingma	_	_
and	_	_
J.	_	_
Ba	_	_
,	_	_
“Adam	_	_
:	_	_
A	_	_
method	_	_
for	_	_
stochastic	_	_
optimization	_	_
,	_	_
”	_	_
arXiv	_	_
preprint	_	_
arXiv:1412.6980	_	_
,	_	_
2014	_	_
.	_	_

#168
[	_	_
9	_	_
]	_	_
F.	_	_
Kizel	_	_
,	_	_
M.	_	_
Shoshany	_	_
,	_	_
N.	_	_
S.	_	_
Netanyahu	_	_
,	_	_
G.	_	_
Even-Tzur	_	_
,	_	_
and	_	_
J	_	_
.	_	_

#169
A.	_	_
Benediktsson	_	_
,	_	_
“A	_	_
stepwise	_	_
analytical	_	_
projected	_	_
gradient	_	_
descent	_	_
search	_	_
for	_	_
hyperspectral	_	_
unmixing	_	_
and	_	_
its	_	_
code	_	_
vectorization	_	_
,	_	_
”	_	_
IEEE	_	_
TGRS	_	_
,	_	_
2017	_	_
.	_	_

#170
[	_	_
10	_	_
]	_	_
A.	_	_
Krizhevsky	_	_
,	_	_
I.	_	_
Sutskever	_	_
,	_	_
and	_	_
G.	_	_
E.	_	_
Hinton	_	_
,	_	_
“Imagenet	_	_
classification	_	_
with	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
”	_	_
in	_	_
NIPS	_	_
,	_	_
2012	_	_
,	_	_
pp	_	_
.	_	_

#171
1097–1105	_	_
.	_	_

#172
[	_	_
11	_	_
]	_	_
J.	_	_
M.	_	_
Nascimento	_	_
and	_	_
J.	_	_
M.	_	_
Dias	_	_
,	_	_
“Vertex	_	_
component	_	_
analysis	_	_
:	_	_
A	_	_
fast	_	_
algorithm	_	_
to	_	_
unmix	_	_
hyperspectral	_	_
data	_	_
,	_	_
”	_	_
IEEE	_	_
TGRS	_	_
,	_	_
pp	_	_
.	_	_

#173
898–910	_	_
,	_	_
2005	_	_
.	_	_

#174
[	_	_
12	_	_
]	_	_
M.	_	_
Norouzi	_	_
and	_	_
D.	_	_
Blei	_	_
,	_	_
“Minimal	_	_
loss	_	_
hashing	_	_
for	_	_
compact	_	_
binary	_	_
codes.”	_	_
ICML	_	_
,	_	_
pp	_	_
.	_	_

#175
353–360	_	_
,	_	_
2011	_	_
.	_	_

#176
[	_	_
13	_	_
]	_	_
S.	_	_
Ozkan	_	_
,	_	_
B.	_	_
Kaya	_	_
,	_	_
and	_	_
G.	_	_
B.	_	_
Akar	_	_
,	_	_
“Endnet	_	_
:	_	_
Sparse	_	_
autoencoder	_	_
network	_	_
for	_	_
endmember	_	_
extraction	_	_
and	_	_
hyperspectral	_	_
unmixing.”	_	_
arXiv	_	_
preprint	_	_
arXiv:1708.01894	_	_
,	_	_
2017	_	_
.	_	_

#177
[	_	_
14	_	_
]	_	_
F.	_	_
Palsson	_	_
,	_	_
J.	_	_
Sigurdsson	_	_
,	_	_
J.	_	_
Sveinsson	_	_
,	_	_
and	_	_
M.	_	_
Ulfarsson	_	_
,	_	_
“Neural	_	_
network	_	_
hyperspectral	_	_
unmixing	_	_
with	_	_
spectral	_	_
information	_	_
divergence	_	_
objective	_	_
,	_	_
”	_	_
IGARSS	_	_
,	_	_
2017	_	_
.	_	_

#178
[	_	_
15	_	_
]	_	_
B.	_	_
Pan	_	_
,	_	_
Z.	_	_
Shi	_	_
,	_	_
and	_	_
X.	_	_
Xu	_	_
,	_	_
“R-vcanet	_	_
:	_	_
A	_	_
new	_	_
deep-learning-based	_	_
hyperspectral	_	_
image	_	_
classification	_	_
method.”	_	_
IEEE	_	_
JSTAR	_	_
,	_	_
pp	_	_
.	_	_

#179
1975–1986	_	_
.	_	_

#180
[	_	_
16	_	_
]	_	_
J.	_	_
Plaza	_	_
,	_	_
A.	_	_
Plaza	_	_
,	_	_
R.	_	_
Pérez	_	_
,	_	_
and	_	_
P.	_	_
Martinez	_	_
,	_	_
“Joint	_	_
linear/nonlinear	_	_
spectral	_	_
unmixing	_	_
of	_	_
hyperspectral	_	_
image	_	_
data	_	_
,	_	_
”	_	_
in	_	_
IGARSS	_	_
.	_	_

#181
IEEE	_	_
,	_	_
2007	_	_
,	_	_
pp	_	_
.	_	_

#182
4037–4040	_	_
.	_	_

#183
[	_	_
17	_	_
]	_	_
S.	_	_
T.	_	_
Roweis	_	_
and	_	_
L.	_	_
K.	_	_
Saul	_	_
,	_	_
“Nonlinear	_	_
dimensionality	_	_
reduction	_	_
by	_	_
locally	_	_
linear	_	_
embedding	_	_
,	_	_
”	_	_
Science	_	_
,	_	_
pp	_	_
.	_	_

#184
2323–2326	_	_
,	_	_
2000	_	_
.	_	_

#185
[	_	_
18	_	_
]	_	_
D.	_	_
Ulyanov	_	_
,	_	_
A.	_	_
Vedaldi	_	_
,	_	_
and	_	_
V.	_	_
Lempitsky	_	_
,	_	_
“Instance	_	_
normalization	_	_
:	_	_
The	_	_
missing	_	_
ingredient	_	_
for	_	_
fast	_	_
stylization.”	_	_
in	_	_
arXiv	_	_
preprint	_	_
arXiv:1701.02096	_	_
,	_	_
2016	_	_
.	_	_

#186
[	_	_
19	_	_
]	_	_
F.-Y	_	_
.	_	_

#187
Wang	_	_
,	_	_
C.-Y	_	_
.	_	_

#188
Chi	_	_
,	_	_
T.-H.	_	_
Chan	_	_
,	_	_
and	_	_
Y.	_	_
Wang	_	_
,	_	_
“Nonnegative	_	_
least-correlated	_	_
component	_	_
analysis	_	_
for	_	_
separation	_	_
of	_	_
dependent	_	_
sources	_	_
by	_	_
volume	_	_
maximization	_	_
,	_	_
”	_	_
IEEE	_	_
PAMI	_	_
,	_	_
pp	_	_
.	_	_

#189
875–888	_	_
,	_	_
2010	_	_
.	_	_

#190
[	_	_
20	_	_
]	_	_
M.	_	_
E.	_	_
Winter	_	_
,	_	_
“N-findr	_	_
:	_	_
An	_	_
algorithm	_	_
for	_	_
fast	_	_
autonomous	_	_
spectral	_	_
end-member	_	_
determination	_	_
in	_	_
hyperspectral	_	_
data	_	_
,	_	_
”	_	_
in	_	_
SPIE	_	_
,	_	_
1999	_	_
,	_	_
pp	_	_
.	_	_

#191
266–275	_	_
.	_	_

#192
[	_	_
21	_	_
]	_	_
Y.	_	_
Zhou	_	_
,	_	_
A.	_	_
Rangarajan	_	_
,	_	_
and	_	_
P.	_	_
D.	_	_
Gader	_	_
,	_	_
“A	_	_
spatial	_	_
compositional	_	_
model	_	_
for	_	_
linear	_	_
unmixing	_	_
and	_	_
endmember	_	_
uncertainty	_	_
estimation	_	_
,	_	_
”	_	_
IEEE	_	_
TIP	_	_
,	_	_
pp	_	_
.	_	_

#193
5987–6002	_	_
,	_	_
2016	_	_
.	_	_

#194
[	_	_
22	_	_
]	_	_
F.	_	_
Zhu	_	_
,	_	_
Y.	_	_
Wang	_	_
,	_	_
B	_	_
.	_	_

#195
Fan	_	_
,	_	_
S.	_	_
Xiang	_	_
,	_	_
G.	_	_
Meng	_	_
,	_	_
and	_	_
C.	_	_
Pan	_	_
,	_	_
“Spectral	_	_
unmixing	_	_
via	_	_
data-guided	_	_
sparsity	_	_
,	_	_
”	_	_
IEEE	_	_
TIP	_	_
,	_	_
pp	_	_
.	_	_

#196
5412–5427	_	_
,	_	_
2014	_	_
.	_	_