#0
Dynamic	_	_
Vision	_	_
Sensors	_	_
for	_	_
Human	_	_
Activity	_	_
Recognition	_	_
Stefanie	_	_
Anna	_	_
Baby1	_	_
,	_	_
Bimal	_	_
Vinod2	_	_
,	_	_
Chaitanya	_	_
Chinni3	_	_
,	_	_
Kaushik	_	_
Mitra4	_	_
Computational	_	_
Imaging	_	_
Lab	_	_
IIT	_	_
Madras	_	_
,	_	_
Chennai	_	_
,	_	_
India	_	_
{	_	_
1ee13b120,2ee15m005,3ee13b072,4kmitra	_	_
}	_	_
@	_	_
ee.iitm.ac.in	_	_

#1
Abstract	_	_

#2
Unlike	_	_
conventional	_	_
cameras	_	_
which	_	_
capture	_	_
video	_	_
at	_	_
a	_	_
fixed	_	_
frame	_	_
rate	_	_
,	_	_
Dynamic	_	_
Vision	_	_
Sensors	_	_
(	_	_
DVS	_	_
)	_	_
record	_	_
only	_	_
changes	_	_
in	_	_
pixel	_	_
intensity	_	_
values	_	_
.	_	_

#3
The	_	_
output	_	_
of	_	_
DVS	_	_
is	_	_
simply	_	_
a	_	_
stream	_	_
of	_	_
discrete	_	_
ON/OFF	_	_
events	_	_
based	_	_
on	_	_
the	_	_
polarity	_	_
of	_	_
change	_	_
in	_	_
its	_	_
pixel	_	_
values	_	_
.	_	_

#4
DVS	_	_
has	_	_
many	_	_
attractive	_	_
features	_	_
such	_	_
as	_	_
low	_	_
power	_	_
consumption	_	_
,	_	_
high	_	_
temporal	_	_
resolution	_	_
,	_	_
high	_	_
dynamic	_	_
range	_	_
and	_	_
less	_	_
storage	_	_
requirements	_	_
.	_	_

#5
All	_	_
these	_	_
make	_	_
DVS	_	_
a	_	_
very	_	_
promising	_	_
camera	_	_
for	_	_
potential	_	_
applications	_	_
in	_	_
wearable	_	_
platforms	_	_
where	_	_
power	_	_
consumption	_	_
is	_	_
a	_	_
major	_	_
concern	_	_
.	_	_

#6
In	_	_
this	_	_
paper	_	_
we	_	_
explore	_	_
the	_	_
feasibility	_	_
of	_	_
using	_	_
DVS	_	_
for	_	_
Human	_	_
Activity	_	_
Recognition	_	_
(	_	_
HAR	_	_
)	_	_
.	_	_

#7
We	_	_
propose	_	_
to	_	_
use	_	_
the	_	_
various	_	_
slices	_	_
(	_	_
such	_	_
as	_	_
x	_	_
−	_	_
y	_	_
,	_	_
x	_	_
−	_	_
t	_	_
and	_	_
y	_	_
−	_	_
t	_	_
)	_	_
of	_	_
the	_	_
DVS	_	_
video	_	_
as	_	_
a	_	_
feature	_	_
map	_	_
for	_	_
HAR	_	_
and	_	_
denote	_	_
them	_	_
as	_	_
Motion	_	_
Maps	_	_
.	_	_

#8
We	_	_
show	_	_
that	_	_
fusing	_	_
motion	_	_
maps	_	_
with	_	_
Motion	_	_
Boundary	_	_
Histogram	_	_
(	_	_
MBH	_	_
)	_	_
gives	_	_
good	_	_
performance	_	_
on	_	_
the	_	_
benchmark	_	_
DVS	_	_
dataset	_	_
as	_	_
well	_	_
as	_	_
on	_	_
a	_	_
real	_	_
DVS	_	_
gesture	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
.	_	_

#9
Interestingly	_	_
,	_	_
the	_	_
performance	_	_
of	_	_
DVS	_	_
is	_	_
comparable	_	_
to	_	_
that	_	_
of	_	_
conventional	_	_
videos	_	_
although	_	_
DVS	_	_
captures	_	_
only	_	_
sparse	_	_
motion	_	_
information	_	_
.	_	_

#10
1	_	_
.	_	_

#11
Introduction	_	_
Conventional	_	_
video	_	_
camera	_	_
uses	_	_
frame	_	_
based	_	_
visual	_	_
acquisition	_	_
where	_	_
each	_	_
pixel	_	_
is	_	_
sampled	_	_
at	_	_
a	_	_
fixed	_	_
frame	_	_
rate	_	_
irrespective	_	_
of	_	_
whether	_	_
or	_	_
not	_	_
their	_	_
value	_	_
changed	_	_
.	_	_

#12
This	_	_
leads	_	_
to	_	_
data	_	_
redundancy	_	_
and	_	_
hence	_	_
increased	_	_
bandwidth	_	_
and	_	_
memory	_	_
requirements	_	_
.	_	_

#13
Dynamic	_	_
Vision	_	_
Sensor	_	_
(	_	_
DVS	_	_
)	_	_
[	_	_
9	_	_
]	_	_
is	_	_
a	_	_
recent	_	_
innovation	_	_
in	_	_
machine	_	_
vision	_	_
that	_	_
mimics	_	_
some	_	_
of	_	_
the	_	_
functionalities	_	_
of	_	_
the	_	_
human	_	_
retinal	_	_
vision	_	_
.	_	_

#14
Instead	_	_
of	_	_
capturing	_	_
the	_	_
whole	_	_
frame	_	_
,	_	_
it	_	_
records	_	_
only	_	_
those	_	_
pixels	_	_
that	_	_
see	_	_
a	_	_
change	_	_
in	_	_
intensity	_	_
values	_	_
.	_	_

#15
If	_	_
the	_	_
magnitude	_	_
of	_	_
change	_	_
in	_	_
log	_	_
intensity	_	_
value	_	_
at	_	_
a	_	_
pixel	_	_
is	_	_
beyond	_	_
a	_	_
threshold	_	_
an	_	_
ON	_	_
or	_	_
OFF	_	_
event	_	_
is	_	_
generated	_	_
.	_	_

#16
A	_	_
major	_	_
advantage	_	_
of	_	_
DVS	_	_
is	_	_
its	_	_
ultra	_	_
low	_	_
power	_	_
consumption	_	_
.	_	_

#17
This	_	_
is	_	_
because	_	_
it	_	_
only	_	_
generates	_	_
ON/OFF	_	_
events	_	_
and	_	_
avoids	_	_
the	_	_
use	_	_
of	_	_
ADCs	_	_
which	_	_
consumes	_	_
the	_	_
most	_	_
power	_	_
in	_	_
conventional	_	_
cameras	_	_
.	_	_

#18
Hence	_	_
DVS	_	_
could	capability-feasibility	_
be	_	_
used	_	_
to	_	_
boost	_	_
the	_	_
battery	_	_
life	_	_
in	_	_
wearable	_	_
or	_	_
portable	_	_
devices	_	_
like	_	_
untethered	_	_
Augmented	_	_
Reality	_	_
(	_	_
AR	_	_
)	_	_
devices	_	_
,	_	_
which	_	_
currently	_	_
use	_	_
conventional	_	_
cameras	_	_
for	_	_
various	_	_
purposes	_	_
such	_	_
as	_	_
gesture/activity	_	_
recognition	_	_
and	_	_
building	_	_
3−D	_	_
maps	_	_
.	_	_

#19
With	_	_
this	_	_
idea	_	_
in	_	_
mind	_	_
,	_	_
we	_	_
explore	_	_
performing	_	_
activity/gesture	_	_
recognition	_	_
using	_	_
DVS	_	_
.	_	_

#20
DVS	_	_
is	_	_
intrinsically	_	_
suitable	_	_
for	_	_
gesture/activity	_	_
recognition	_	_
since	_	_
it	_	_
does	_	_
not	_	_
record	_	_
any	_	_
static	_	_
information	_	_
about	_	_
the	_	_
scene	_	_
.	_	_

#21
Thus	_	_
,	_	_
we	_	_
can	_	_
avoid	_	_
the	_	_
overhead	_	_
of	_	_
preprocessing	_	_
algorithms	_	_
such	_	_
as	_	_
background	_	_
subtraction	_	_
and	_	_
contour	_	_
extraction	_	_
used	_	_
in	_	_
conventional	_	_
image	_	_
processing	_	_
.	_	_

#22
For	_	_
the	_	_
task	_	_
of	_	_
human	_	_
activity	_	_
recognition	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
simple	_	_
method	_	_
of	_	_
using	_	_
various	_	_
slices	_	_
(	_	_
x	_	_
−	_	_
y	_	_
,	_	_
x	_	_
−	_	_
t	_	_
and	_	_
y	_	_
−	_	_
t	_	_
)	_	_
of	_	_
the	_	_
DVS	_	_
video	_	_
as	_	_
feature	_	_
maps	_	_
.	_	_

#23
We	_	_
denote	_	_
these	_	_
maps	_	_
as	_	_
motion	_	_
maps	_	_
and	_	_
employ	_	_
Bag	_	_
of	_	_
Visual	_	_
Words	_	_
framework	_	_
to	_	_
extract	_	_
critical	_	_
features	_	_
.	_	_

#24
Recognition	_	_
rates	_	_
obtained	_	_
were	_	_
similar	_	_
to	_	_
that	_	_
of	_	_
existing	_	_
descriptors	_	_
under	_	_
this	_	_
setting	_	_
.	_	_

#25
We	_	_
also	_	_
combined	_	_
the	_	_
motion	_	_
maps’	_	_
features	_	_
with	_	_
state-of-the-art	_	_
motion	_	_
descriptor	_	_
Motion	_	_
Boundary	_	_
Histogram	_	_
(	_	_
MBH	_	_
)	_	_
to	_	_
obtain	_	_
the	_	_
best	_	_
recognition	_	_
rates	_	_
,	_	_
much	_	_
higher	_	_
than	_	_
the	_	_
HAR	_	_
performance	_	_
of	_	_
individual	_	_
descriptors	_	_
.	_	_

#26
The	_	_
results	_	_
on	_	_
DVS	_	_
data	_	_
are	_	_
even	_	_
comparable	_	_
with	_	_
the	_	_
recognition	_	_
rates	_	_
seen	_	_
in	_	_
conventional	_	_
videos	_	_
.	_	_

#27
This	_	_
is	_	_
quite	_	_
surprising	_	_
given	_	_
that	_	_
DVS	_	_
data	_	_
is	_	_
a	_	_
very	_	_
compressed	_	_
version	_	_
of	_	_
the	_	_
original	_	_
video	_	_
data	_	_
with	_	_
a	_	_
remarkably	_	_
sparse	_	_
encoding	_	_
.	_	_

#28
We	_	_
experimented	_	_
on	_	_
two	_	_
datasets	_	_
:	_	_
the	_	_
DVS	_	_
recordings	_	_
of	_	_
UCF11	_	_
[	_	_
7	_	_
]	_	_
and	_	_
a	_	_
hand	_	_
gesture	_	_
DVS	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
.	_	_

#29
In	_	_
both	_	_
the	_	_
datasets	_	_
our	_	_
results	_	_
have	_	_
been	_	_
promising	_	_
for	_	_
DVS	_	_
.	_	_

#30
1.1	_	_
.	_	_

#31
Related	_	_
Work	_	_
There	_	_
are	_	_
several	_	_
works	_	_
in	_	_
literature	_	_
for	_	_
human	_	_
activity	_	_
recognition	_	_
,	_	_
of	_	_
which	_	_
we	_	_
mention	_	_
here	_	_
a	_	_
few	_	_
relevant	_	_
ones	_	_
.	_	_

#32
For	_	_
a	_	_
typical	_	_
activity	_	_
recognition	_	_
task	_	_
,	_	_
two	_	_
types	_	_
of	_	_
features	_	_
are	_	_
classically	_	_
extracted	_	_
-	_	_
descriptors	_	_
based	_	_
on	_	_
motion	_	_
and	_	_
those	_	_
based	_	_
on	_	_
shape	_	_
.	_	_

#33
Motion	_	_
History	_	_
Images	_	_
(	_	_
MHI	_	_
)	_	_
from	_	_
videos	_	_
accumulate	_	_
foreground	_	_
regions	_	_
of	_	_
a	_	_
person	_	_
and	_	_
accounts	_	_
for	_	_
its	_	_
shape	_	_
and	_	_
stance	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#34
Several	_	_
more	_	_
contourbased	_	_
approaches	_	_
such	_	_
as	_	_
Cartesian	_	_
Coordinate	_	_
Features	_	_
,	_	_
Fourier	_	_
Descriptors	_	_
Features	_	_
[	_	_
8	_	_
,	_	_
6	_	_
]	_	_
,	_	_
Centroid-Distance	_	_
Features	_	_
and	_	_
Chord-Length	_	_
Features	_	_
provide	_	_
shape	_	_
description	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#35
7v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
3	_	_
M	_	_
ar	_	_
2	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#36
For	_	_
motion	_	_
based	_	_
descriptors	_	_
,	_	_
Histogram	_	_
of	_	_
Optical	_	_
Flow	_	_
(	_	_
HOF	_	_
)	_	_
computes	_	_
optical	_	_
flow	_	_
of	_	_
pixels	_	_
between	_	_
consecutive	_	_
frames	_	_
using	_	_
brightness	_	_
constancy	_	_
assumption	_	_
[	_	_
11	_	_
,	_	_
4	_	_
]	_	_
.	_	_

#37
Motion	_	_
boundary	_	_
histograms	_	_
take	_	_
one	_	_
step	_	_
further	_	_
by	_	_
performing	_	_
derivative	_	_
operation	_	_
on	_	_
the	_	_
optical	_	_
flow	_	_
field	_	_
.	_	_

#38
This	_	_
makes	_	_
the	_	_
feature	_	_
invariant	_	_
to	_	_
local	_	_
translation	_	_
motion	_	_
of	_	_
the	_	_
camera	_	_
and	_	_
captures	_	_
only	_	_
relative	_	_
motion	_	_
in	_	_
the	_	_
video	_	_
[	_	_
5	_	_
,	_	_
17	_	_
]	_	_
.	_	_

#39
Several	_	_
other	_	_
descriptors	_	_
work	_	_
by	_	_
extracting	_	_
the	_	_
scene	_	_
(	_	_
background	_	_
)	_	_
,	_	_
color/hue	_	_
and	_	_
texture	_	_
based	_	_
features	_	_
in	_	_
a	_	_
video	_	_
.	_	_

#40
But	_	_
texture	_	_
and	_	_
hue	_	_
information	_	_
is	_	_
unavailable	_	_
in	_	_
DVS	_	_
data	_	_
because	_	_
of	_	_
its	_	_
binary	_	_
encoding	_	_
scheme	_	_
.	_	_

#41
The	_	_
scene	_	_
context	_	_
based	_	_
descriptors	_	_
can	_	_
also	_	_
not	_	_
be	_	_
used	_	_
with	_	_
DVS	_	_
videos	_	_
since	_	_
scenes	_	_
usually	_	_
are	_	_
static	_	_
in	_	_
a	_	_
video	_	_
,	_	_
unless	_	_
there	_	_
is	_	_
significant	_	_
camera	_	_
motion	_	_
.	_	_

#42
Nevertheless	_	_
,	_	_
volume	_	_
based	_	_
features	_	_
like	_	_
motion	_	_
and	_	_
shape	_	_
often	_	_
provide	_	_
sufficient	_	_
information	_	_
required	_	_
to	_	_
perform	_	_
decent	_	_
recognition	_	_
and	_	_
are	_	_
more	_	_
popular	_	_
than	_	_
surface	_	_
features	_	_
like	_	_
color	_	_
and	_	_
texture	_	_
.	_	_

#43
Human	_	_
activity	_	_
recognition	_	_
has	_	_
been	_	_
popularly	_	_
solved	_	_
by	_	_
extracting	_	_
local	_	_
features	_	_
from	_	_
videos	_	_
on	_	_
which	_	_
Bag	_	_
of	_	_
Visual	_	_
Words	_	_
model	_	_
(	_	_
BoVW	_	_
)	_	_
is	_	_
learnt	_	_
and	_	_
a	_	_
classifier	_	_
,	_	_
typically	_	_
SVM	_	_
is	_	_
trained	_	_
[	_	_
18	_	_
,	_	_
14	_	_
]	_	_
.	_	_

#44
As	_	_
against	_	_
this	_	_
,	_	_
recent	_	_
works	_	_
on	_	_
HAR	_	_
has	_	_
focussed	_	_
on	_	_
deep	_	_
learning	_	_
techniques	_	_
for	_	_
improving	_	_
recognition	_	_
rates	_	_
.	_	_

#45
Deep	_	_
Convolutional	_	_
and	_	_
LSTM	_	_
Recurrent	_	_
Neural	_	_
network	_	_
units	_	_
can	_	_
be	_	_
trained	_	_
to	_	_
automate	_	_
feature	_	_
extraction	_	_
and	_	_
directly	_	_
perform	_	_
natural	_	_
sensor	_	_
fusion	_	_
[	_	_
13	_	_
]	_	_
on	_	_
human	_	_
videos	_	_
.	_	_

#46
Two-stream	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
learn	_	_
the	_	_
spatial	_	_
and	_	_
temporal	_	_
information	_	_
extracted	_	_
from	_	_
RGB	_	_
and	_	_
optical	_	_
flow	_	_
images	_	_
of	_	_
videos	_	_
and	_	_
are	_	_
also	_	_
becoming	_	_
common	_	_
for	_	_
activity	_	_
recognition	_	_
[	_	_
12	_	_
,	_	_
15	_	_
]	_	_
.	_	_

#47
However	_	_
our	_	_
method	_	_
is	_	_
simple	_	_
and	_	_
easy	_	_
to	_	_
implement	_	_
,	_	_
providing	_	_
an	_	_
intuitive	_	_
framework	_	_
for	_	_
activity	_	_
recognition	_	_
.	_	_

#48
Since	_	_
DVS	_	_
recording	_	_
provide	_	_
us	_	_
with	_	_
both	_	_
motion	_	_
and	_	_
shape	_	_
cues	_	_
,	_	_
we	_	_
exploit	_	_
these	_	_
critical	_	_
information	_	_
by	_	_
proposing	_	_
a	_	_
fusion	_	_
of	_	_
simple	_	_
shape	_	_
and	_	_
motion	_	_
based	_	_
feature	_	_
descriptors	_	_
.	_	_

#49
2	_	_
.	_	_

#50
DVS	_	_
Based	_	_
Activity	_	_
Recognition	_	_
Unlike	_	_
conventional	_	_
camera	_	_
,	_	_
DVS	_	_
captures	_	_
only	_	_
motion	_	_
and	_	_
thus	_	_
avoids	_	_
the	_	_
need	_	_
to	_	_
perform	_	_
background	_	_
subtraction	_	_
.	_	_

#51
Also	_	_
DVS	_	_
output	_	_
is	_	_
usually	_	_
sparse	_	_
because	_	_
change	_	_
in	_	_
pixel	_	_
intensity	_	_
occurs	_	_
only	_	_
at	_	_
texture	_	_
edges	_	_
.	_	_

#52
To	_	_
exploit	_	_
this	_	_
sparsity	_	_
and	_	_
motion	_	_
cues	_	_
captured	_	_
by	_	_
DVS	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
extract	_	_
various	_	_
projections	_	_
of	_	_
the	_	_
DVS	_	_
data	_	_
(	_	_
the	_	_
motion	_	_
maps	_	_
)	_	_
and	_	_
use	_	_
them	_	_
as	_	_
feature	_	_
descriptor	_	_
for	_	_
activity	_	_
recognition	_	_
.	_	_

#53
Finally	_	_
we	_	_
fuse	_	_
the	_	_
motion	_	_
maps	_	_
with	_	_
a	_	_
state-of-the-art	_	_
motion	_	_
descriptor	_	_
MBH	_	_
[	_	_
5	_	_
]	_	_
to	_	_
further	_	_
improve	_	_
the	_	_
recognition	_	_
accuracy	_	_
.	_	_

#54
The	_	_
overall	_	_
architecture	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#55
We	_	_
first	_	_
convert	_	_
DVS	_	_
event	_	_
streams	_	_
into	_	_
a	_	_
video	_	_
by	_	_
accumulating	_	_
events	_	_
over	_	_
a	_	_
time	_	_
window	_	_
.	_	_

#56
For	_	_
our	_	_
experiments	_	_
we	_	_
made	_	_
videos	_	_
at	_	_
30fps	_	_
framerate	_	_
.	_	_

#57
From	_	_
this	_	_
,	_	_
we	_	_
obtain	_	_
different	_	_
2-D	_	_
projections	_	_
:	_	_
x	_	_
−	_	_
y	_	_
,	_	_
x	_	_
−	_	_
t	_	_
and	_	_
y	_	_
−	_	_
t	_	_
by	_	_
averaging	_	_
over	_	_
each	_	_
left-out	_	_
dimension	_	_
.	_	_

#58
Thus	_	_
,	_	_
x−	_	_
y	_	_
projection	_	_
is	_	_
obtained	_	_
by	_	_
averaging	_	_
over	_	_
the	_	_
time	_	_
axis	_	_
,	_	_
x−	_	_
t	_	_
averages	_	_
y−axis	_	_
and	_	_
y−	_	_
t	_	_
by	_	_
averages	_	_
x−axis	_	_
.	_	_

#59
We	_	_
call	_	_
these	_	_
2-D	_	_
projections	_	_
as	_	_
motion	_	_
maps	_	_
since	_	_
DVS	_	_
captures	_	_
the	_	_
direction	_	_
of	_	_
motion	_	_
of	_	_
the	_	_
foreground	_	_
object	_	_
.	_	_

#60
The	_	_
x−y	_	_
motion	_	_
map	_	_
gives	_	_
us	_	_
the	_	_
mean	_	_
pose	_	_
and	_	_
stance	_	_
of	_	_
an	_	_
object	_	_
in	_	_
the	_	_
scene	_	_
whereas	_	_
the	_	_
x	_	_
−	_	_
t	_	_
and	_	_
the	_	_
y	_	_
−	_	_
t	_	_
motion	_	_
maps	_	_
record	_	_
the	_	_
manner	_	_
in	_	_
which	_	_
the	_	_
object	_	_
had	_	_
moved	_	_
over	_	_
the	_	_
video’s	_	_
duration	_	_
.	_	_

#61
Our	_	_
proposed	_	_
x−y	_	_
motion	_	_
map	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
idea	_	_
of	_	_
motion	_	_
history	_	_
images	_	_
[	_	_
1	_	_
]	_	_
but	_	_
we	_	_
have	_	_
two	_	_
additional	_	_
maps	_	_
that	_	_
account	_	_
for	_	_
the	_	_
movement	_	_
of	_	_
the	_	_
object	_	_
along	_	_
the	_	_
horizontal	_	_
and	_	_
vertical	_	_
directions	_	_
.	_	_

#62
From	_	_
the	_	_
motion	_	_
maps	_	_
,	_	_
we	_	_
extract	_	_
Bag	_	_
of	_	_
Features	_	_
(	_	_
BoF	_	_
)	_	_
,	_	_
where	_	_
we	_	_
use	_	_
Speeded	_	_
Up	_	_
Robust	_	_
Features	_	_
(	_	_
SURF	_	_
)	_	_
[	_	_
2	_	_
]	_	_
extracted	_	_
through	_	_
grid	_	_
search	_	_
on	_	_
the	_	_
maps	_	_
.	_	_

#63
This	_	_
is	_	_
followed	_	_
by	_	_
k−means	_	_
clustering	_	_
of	_	_
the	_	_
train	_	_
datas	_	_
features	_	_
to	_	_
create	_	_
a	_	_
visual	_	_
vocabulary	_	_
of	_	_
k	_	_
words	_	_
.	_	_

#64
Then	_	_
features	_	_
from	_	_
each	_	_
video	_	_
are	_	_
binned	_	_
to	_	_
these	_	_
k	_	_
clusters	_	_
and	_	_
are	_	_
L2	_	_
normalized	_	_
.	_	_

#65
Finally	_	_
,	_	_
a	_	_
linear	_	_
SVM	_	_
classifier	_	_
under	_	_
one-vs-all	_	_
encoding	_	_
scheme	_	_
is	_	_
trained	_	_
on	_	_
the	_	_
encoded	_	_
features	_	_
to	_	_
predict	_	_
the	_	_
performed	_	_
activity	_	_
.	_	_

#66
Since	_	_
the	_	_
motion	_	_
maps	_	_
inherently	_	_
complement	_	_
each	_	_
another	_	_
with	_	_
the	_	_
x	_	_
−	_	_
y	_	_
map	_	_
encoding	_	_
the	_	_
shape	_	_
and	_	_
pose	_	_
of	_	_
the	_	_
object	_	_
while	_	_
the	_	_
x−	_	_
t	_	_
and	_	_
the	_	_
y	_	_
−	_	_
t	_	_
motion	_	_
maps	_	_
describing	_	_
its	_	_
motion	_	_
,	_	_
we	_	_
combine	_	_
all	_	_
the	_	_
three	_	_
motion	_	_
maps’	_	_
descriptors	_	_
to	_	_
obtain	_	_
better	_	_
classification	_	_
accuracy	_	_
.	_	_

#67
We	_	_
tried	_	_
fusion	_	_
of	_	_
features	_	_
before	_	_
as	_	_
well	_	_
as	_	_
after	_	_
performing	_	_
BoF	_	_
and	_	_
observed	_	_
that	_	_
fusion	_	_
after	_	_
BoF	_	_
performs	_	_
better	_	_
.	_	_

#68
This	_	_
result	_	_
is	_	_
in	_	_
line	_	_
with	_	_
that	_	_
of	_	_
[	_	_
14	_	_
]	_	_
where	_	_
the	_	_
authors	_	_
give	_	_
a	_	_
comprehensive	_	_
study	_	_
of	_	_
fusing	_	_
feature	_	_
descriptors	_	_
at	_	_
several	_	_
stages	_	_
of	_	_
BoF	_	_
.	_	_

#69
For	_	_
the	_	_
final	_	_
recognition	_	_
task	_	_
,	_	_
the	_	_
motion	_	_
maps	_	_
descriptors	_	_
are	_	_
further	_	_
combined	_	_
with	_	_
the	_	_
MBH	_	_
descriptor	_	_
since	_	_
MBH	_	_
also	_	_
encodes	_	_
the	_	_
appearance	_	_
of	_	_
objects	_	_
and	_	_
local	_	_
motion	_	_
in	_	_
video	_	_
but	_	_
in	_	_
a	_	_
method	_	_
that	_	_
is	_	_
distinctly	_	_
different	_	_
.	_	_

#70
MBH	_	_
takes	_	_
derivative	_	_
of	_	_
optical	_	_
flow	_	_
which	_	_
in	_	_
turn	_	_
is	_	_
computed	_	_
using	_	_
derivative	_	_
of	_	_
the	_	_
video	_	_
frames	_	_
with	_	_
respect	_	_
to	_	_
its	_	_
spatial	_	_
and	_	_
temporal	_	_
coordinates	_	_
.	_	_

#71
Hence	_	_
MBH	_	_
employs	_	_
second	_	_
order	_	_
statistics	_	_
for	_	_
its	_	_
feature	_	_
extraction	_	_
while	_	_
the	_	_
motion	_	_
maps	_	_
use	_	_
simple	_	_
zero	_	_
order	_	_
statistics	_	_
.	_	_

#72
Thus	_	_
these	_	_
two	_	_
descriptors	_	_
supplement	_	_
one	_	_
another	_	_
and	_	_
their	_	_
combined	_	_
feature	_	_
set	_	_
outperforms	_	_
the	_	_
individual	_	_
recognition	_	_
rates	_	_
.	_	_

#73
In	_	_
the	_	_
next	_	_
section	_	_
,	_	_
we	_	_
evaluate	_	_
these	_	_
descriptors	_	_
and	_	_
the	_	_
loss	_	_
in	_	_
performance	_	_
of	_	_
HAR	_	_
on	_	_
using	_	_
DVS	_	_
data	_	_
compared	_	_
to	_	_
conventional	_	_
videos	_	_
on	_	_
a	_	_
benchmark	_	_
dataset	_	_
.	_	_

#74
We	_	_
also	_	_
report	_	_
the	_	_
performance	_	_
on	_	_
the	_	_
DVS	_	_
gesture	_	_
dataset	_	_
we	_	_
have	_	_
collected	_	_
.	_	_

#75
From	_	_
the	_	_
results	_	_
,	_	_
we	_	_
assess	_	_
the	_	_
usability	_	_
of	_	_
DVS	_	_
for	_	_
activity	_	_
recognition	_	_
and	_	_
conclude	_	_
with	_	_
its	_	_
shortcomings	_	_
.	_	_

#76
3	_	_
.	_	_

#77
Datasets	_	_
We	_	_
performed	_	_
our	_	_
experiments	_	_
on	_	_
two	_	_
datasets	_	_
-	_	_
the	_	_
UCF	_	_
YouTube	_	_
Action	_	_
Data	_	_
Set	_	_
or	_	_
UCF11	_	_
[	_	_
10	_	_
]	_	_
and	_	_
a	_	_
DVS	_	_
gesture	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
using	_	_
DVS128	_	_
.	_	_

#78
The	_	_
UCF11	_	_
data	_	_
was	_	_
chosen	_	_
because	_	_
it	_	_
is	_	_
one	_	_
of	_	_
the	_	_
few	_	_
Figure	_	_
1	_	_
:	_	_
Our	_	_
proposed	_	_
method	_	_
:	_	_
The	_	_
event	_	_
stream	_	_
from	_	_
DVS	_	_
is	_	_
converted	_	_
into	_	_
video	_	_
at	_	_
30fps	_	_
.	_	_

#79
Motion	_	_
maps	_	_
are	_	_
generated	_	_
through	_	_
various	_	_
projections	_	_
of	_	_
this	_	_
event	_	_
video	_	_
and	_	_
SURF	_	_
features	_	_
are	_	_
extracted	_	_
.	_	_

#80
MBH	_	_
features	_	_
using	_	_
dense	_	_
trajectory	_	_
are	_	_
also	_	_
extracted	_	_
.	_	_

#81
Bag	_	_
of	_	_
features	_	_
encoding	_	_
from	_	_
both	_	_
these	_	_
descriptors	_	_
are	_	_
combined	_	_
and	_	_
given	_	_
to	_	_
linear	_	_
SVM	_	_
classifier	_	_
(	_	_
one-vs-all	_	_
)	_	_
.	_	_

#82
human	_	_
action	_	_
datasets	_	_
whose	_	_
benchmark	_	_
DVS	_	_
counterpart	_	_
is	_	_
publicly	_	_
available	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#83
The	_	_
DVS	_	_
data	_	_
was	_	_
created	_	_
by	_	_
the	_	_
authors	_	_
[	_	_
7	_	_
]	_	_
by	_	_
re-recording	_	_
the	_	_
existing	_	_
benchmark	_	_
UCF11	_	_
videos	_	_
played	_	_
on	_	_
a	_	_
monitor	_	_
using	_	_
a	_	_
DAViS240C	_	_
vision	_	_
sensor	_	_
.	_	_

#84
Since	_	_
the	_	_
data	_	_
was	_	_
not	_	_
directly	_	_
recorded	_	_
from	_	_
the	_	_
wild	_	_
,	_	_
this	_	_
would	_	_
mean	_	_
that	_	_
time	_	_
resolution	_	_
greater	_	_
than	_	_
that	_	_
provided	_	_
by	_	_
the	_	_
UCF11	_	_
video	_	_
is	_	_
not	_	_
available	_	_
in	_	_
DVS	_	_
under	_	_
this	_	_
simulated	_	_
setting	_	_
.	_	_

#85
Nonetheless	_	_
,	_	_
the	_	_
dataset	_	_
is	_	_
sufficient	_	_
for	_	_
our	_	_
experiments	_	_
since	_	_
it	_	_
captures	_	_
the	_	_
sensor	_	_
noise	_	_
in	_	_
DVS	_	_
and	_	_
is	_	_
used	_	_
on	_	_
action	_	_
videos	_	_
that	_	_
by	_	_
themselves	_	_
are	_	_
not	_	_
very	_	_
fast	_	_
paced	_	_
.	_	_

#86
The	_	_
UCF11	_	_
dataset	_	_
contains	_	_
eleven	_	_
action	_	_
classes	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
,	_	_
viz	_	_
.	_	_

#87
basketball	_	_
shooting	_	_
,	_	_
biking	_	_
,	_	_
diving	_	_
,	_	_
golf	_	_
swinging	_	_
,	_	_
horse	_	_
riding	_	_
,	_	_
soccer	_	_
juggling	_	_
,	_	_
swinging	_	_
,	_	_
tennis	_	_
swinging	_	_
,	_	_
trampoline	_	_
jumping	_	_
,	_	_
volleyball	_	_
spiking	_	_
and	_	_
walking	_	_
dog	_	_
.	_	_

#88
Each	_	_
class	_	_
is	_	_
further	_	_
subdivided	_	_
in	_	_
to	_	_
25	_	_
groups	_	_
that	_	_
allow	_	_
us	_	_
to	_	_
perform	_	_
Leave	_	_
One	_	_
Out	_	_
(	_	_
LOO	_	_
)	_	_
cross	_	_
validation	_	_
twenty	_	_
five	_	_
times	_	_
on	_	_
the	_	_
actions	_	_
as	_	_
suggested	_	_
by	_	_
the	_	_
creators	_	_
of	_	_
the	_	_
data	_	_
.	_	_

#89
Figure	_	_
2	_	_
:	_	_
YouTube	_	_
Action	_	_
Data	_	_
Set	_	_
1	_	_
For	_	_
a	_	_
second	_	_
round	_	_
of	_	_
experiments	_	_
,	_	_
we	_	_
used	_	_
the	_	_
DVS	_	_
hand	_	_
gesture	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
using	_	_
DVS128	_	_
and	_	_
we	_	_
refer	_	_
them	_	_
as	_	_
the	_	_
DVS	_	_
Gesture	_	_
data	_	_
(	_	_
see	_	_
Figure	_	_
3	_	_
)	_	_
.	_	_

#90
The	_	_
dataset	_	_
contains	_	_
10	_	_
different	_	_
hand	_	_
gestures	_	_
,	_	_
each	_	_
performed	_	_
1Image	_	_
source	_	_
:	_	_
http	_	_
:	_	_
//crcv.ucf.edu/data/UCF_	_	_
YouTube_Action.php	_	_
Figure	_	_
3	_	_
:	_	_
Gestures	_	_
from	_	_
the	_	_
DVS	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
.	_	_

#91
Ground	_	_
truth	_	_
from	_	_
an	_	_
RGB	_	_
camera	_	_
is	_	_
also	_	_
shown	_	_
.	_	_

#92
10	_	_
times	_	_
by	_	_
12	_	_
subjects	_	_
constituting	_	_
a	_	_
total	_	_
of	_	_
1200	_	_
gestures	_	_
.	_	_

#93
The	_	_
hand	_	_
gestures	_	_
used	_	_
are	_	_
left	_	_
swipe	_	_
,	_	_
right	_	_
swipe	_	_
,	_	_
beckon	_	_
,	_	_
counter-clock	_	_
wise	_	_
rotation	_	_
,	_	_
clock	_	_
wise	_	_
rotation	_	_
,	_	_
swipe	_	_
down	_	_
,	_	_
swipe	_	_
up	_	_
,	_	_
swipe	_	_
V	_	_
,	_	_
wave	_	_
X	_	_
and	_	_
wave	_	_
Z	_	_
.	_	_

#94
We	_	_
performed	_	_
12-fold	_	_
cross-validation	_	_
for	_	_
all	_	_
experiments	_	_
on	_	_
this	_	_
dataset	_	_
leaving	_	_
out	_	_
one	_	_
subject	_	_
each	_	_
time	_	_
.	_	_

#95
Figure	_	_
4	_	_
shows	_	_
the	_	_
motion	_	_
maps	_	_
created	_	_
from	_	_
randomly	_	_
picked	_	_
videos	_	_
of	_	_
the	_	_
eleven	_	_
classes	_	_
of	_	_
UCF11	_	_
data	_	_
.	_	_

#96
Note	_	_
that	_	_
in	_	_
the	_	_
x	_	_
−	_	_
y	_	_
map	_	_
,	_	_
much	_	_
of	_	_
the	_	_
shape	_	_
and	_	_
pose	_	_
of	_	_
the	_	_
object	_	_
is	_	_
captured	_	_
.	_	_

#97
Similarly	_	_
,	_	_
the	_	_
x	_	_
−	_	_
t	_	_
and	_	_
y	_	_
−	_	_
t	_	_
slices	_	_
show	_	_
rhythmic	_	_
patterns	_	_
based	_	_
on	_	_
the	_	_
movement	_	_
involved	_	_
typical	_	_
for	_	_
a	_	_
given	_	_
action	_	_
category	_	_
.	_	_

#98
Notable	_	_
ones	_	_
among	_	_
these	_	_
are	_	_
winding	_	_
river-like	_	_
y	_	_
−	_	_
t	_	_
motion	_	_
map	_	_
for	_	_
action	_	_
class	_	_
swinging	_	_
and	_	_
the	_	_
rhythmic	_	_
up	_	_
and	_	_
down	_	_
spikes	_	_
in	_	_
the	_	_
x−	_	_
t	_	_
motion	_	_
map	_	_
for	_	_
trampoline	_	_
class	_	_
.	_	_

#99
4	_	_
.	_	_

#100
Feature	_	_
Extraction	_	_
and	_	_
classification	_	_
This	_	_
section	_	_
describes	_	_
the	_	_
steps	_	_
that	_	_
we	_	_
used	_	_
for	_	_
feature	_	_
extraction	_	_
and	_	_
classification	_	_
.	_	_

#101
To	_	_
evaluate	_	_
existing	_	_
motion	_	_
descriptors	_	_
like	_	_
HoG	_	_
,	_	_
HOF	_	_
and	_	_
MBH	_	_
,	_	_
we	_	_
extracted	_	_
local	_	_
spatio-temporal	_	_
features	_	_
using	_	_
dense	_	_
trajectories	_	_
from	_	_
(	_	_
a	_	_
)	_	_
x−	_	_
y	_	_
Motion	_	_
Map	_	_
(	_	_
b	_	_
)	_	_
x−	_	_
t	_	_
Motion	_	_
Map	_	_
(	_	_
c	_	_
)	_	_
y	_	_
−	_	_
t	_	_
Motion	_	_
Map	_	_
Figure	_	_
4	_	_
:	_	_
The	_	_
three	_	_
Motion	_	_
maps	_	_
for	_	_
11	_	_
randomly	_	_
picked	_	_
UCF11	_	_
videos	_	_
.	_	_

#102
the	_	_
videos	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#103
Dense	_	_
trajectories	_	_
are	_	_
created	_	_
by	_	_
dense	_	_
sampling	_	_
of	_	_
images	_	_
frame	_	_
wise	_	_
.	_	_

#104
The	_	_
sampled	_	_
points	_	_
are	_	_
tracked	_	_
along	_	_
frames	_	_
using	_	_
dense	_	_
optical	_	_
flow	_	_
field	_	_
and	_	_
the	_	_
trajectory	_	_
length	_	_
is	_	_
limited	_	_
to	_	_
15	_	_
frames	_	_
to	_	_
avoid	_	_
drifting	_	_
of	_	_
tracked	_	_
points	_	_
.	_	_

#105
Along	_	_
the	_	_
path	_	_
tracked	_	_
,	_	_
descriptors	_	_
like	_	_
HoG	_	_
,	_	_
HOF	_	_
or	_	_
MBH	_	_
are	_	_
computed	_	_
using	_	_
the	_	_
neighborhood	_	_
volume	_	_
of	_	_
32	_	_
×	_	_
32	_	_
×	_	_
15	_	_
pixels	_	_
.	_	_

#106
This	_	_
volume	_	_
is	_	_
further	_	_
divided	_	_
into	_	_
cells	_	_
of	_	_
size	_	_
16	_	_
×	_	_
16	_	_
pixels	_	_
×5	_	_
frames	_	_
.	_	_

#107
So	_	_
each	_	_
tracked	_	_
tube	_	_
gives	_	_
a	_	_
2	_	_
×	_	_
2	_	_
×	_	_
3	_	_
cells	_	_
.	_	_

#108
Within	_	_
each	_	_
cell	_	_
,	_	_
the	_	_
histograms	_	_
of	_	_
descriptors	_	_
are	_	_
found	_	_
.	_	_

#109
For	_	_
HoG	_	_
and	_	_
MBH	_	_
,	_	_
we	_	_
used	_	_
8	_	_
orientation	_	_
bins	_	_
per	_	_
cell	_	_
and	_	_
the	_	_
magnitude	_	_
of	_	_
the	_	_
feature	_	_
values	_	_
were	_	_
used	_	_
for	_	_
weighting	_	_
.	_	_

#110
For	_	_
HOF	_	_
,	_	_
an	_	_
additional	_	_
bin	_	_
was	_	_
added	_	_
to	_	_
account	_	_
for	_	_
pixels	_	_
whose	_	_
flow	_	_
value	_	_
was	_	_
smaller	_	_
than	_	_
a	_	_
threshold	_	_
.	_	_

#111
All	_	_
the	_	_
descriptors	_	_
were	_	_
also	_	_
L2	_	_
normalized	_	_
before	_	_
performing	_	_
bag	_	_
of	_	_
features	_	_
.	_	_

#112
In	_	_
total	_	_
,	_	_
HoG	_	_
gave	_	_
feature	_	_
descriptors	_	_
of	_	_
size	_	_
96	_	_
per	_	_
tracked	_	_
volume	_	_
(	_	_
2	_	_
×	_	_
2	_	_
×	_	_
3	_	_
cells	_	_
per	_	_
tracked	_	_
path	_	_
times	_	_
8	_	_
bins	_	_
)	_	_
while	_	_
HOF	_	_
produced	_	_
108	_	_
features	_	_
(	_	_
2×	_	_
2×	_	_
3	_	_
cells	_	_
times	_	_
9	_	_
bins	_	_
)	_	_
.	_	_

#113
MBH	_	_
also	_	_
gave	_	_
96	_	_
features	_	_
similar	_	_
to	_	_
HoG	_	_
,	_	_
but	_	_
in	_	_
both	_	_
horizontal	_	_
and	_	_
vertical	_	_
directions	_	_
.	_	_

#114
Thus	_	_
,	_	_
overall	_	_
it	_	_
had	_	_
twice	_	_
the	_	_
number	_	_
of	_	_
features	_	_
for	_	_
a	_	_
chosen	_	_
trajectory	_	_
.	_	_

#115
Bag	_	_
of	_	_
features	_	_
was	_	_
individually	_	_
performed	_	_
on	_	_
each	_	_
of	_	_
these	_	_
descriptors	_	_
.	_	_

#116
Since	_	_
each	_	_
video	_	_
produced	_	_
about	_	_
≈	_	_
500	_	_
,	_	_
000	_	_
dense	_	_
tracks	_	_
,	_	_
most	_	_
of	_	_
them	_	_
in	_	_
close	_	_
proximity	_	_
to	_	_
one	_	_
another	_	_
,	_	_
BoF	_	_
was	_	_
done	_	_
on	_	_
a	_	_
subset	_	_
of	_	_
training	_	_
features	_	_
on	_	_
100	_	_
,	_	_
000	_	_
trajectories	_	_
randomly	_	_
selected	_	_
.	_	_

#117
To	_	_
ensure	_	_
that	_	_
every	_	_
video	_	_
in	_	_
the	_	_
train	_	_
set	_	_
contributes	_	_
to	_	_
the	_	_
codebook	_	_
,	_	_
we	_	_
selected	_	_
features	_	_
randomly	_	_
from	_	_
each	_	_
video	_	_
instead	_	_
of	_	_
pooling	_	_
all	_	_
extracted	_	_
features	_	_
first	_	_
and	_	_
performing	_	_
random	_	_
selection	_	_
.	_	_

#118
The	_	_
codebook	_	_
dimension	_	_
in	_	_
the	_	_
clustering	_	_
step	_	_
was	_	_
maintained	_	_
at	_	_
500	_	_
.	_	_

#119
After	_	_
learning	_	_
the	_	_
cluster	_	_
centers	_	_
,	_	_
all	_	_
features	_	_
of	_	_
the	_	_
video	_	_
were	_	_
used	_	_
to	_	_
generate	_	_
the	_	_
histograms	_	_
of	_	_
the	_	_
same	_	_
500	_	_
bins	_	_
.	_	_

#120
Finally	_	_
the	_	_
segregated	_	_
features	_	_
were	_	_
L2	_	_
normalized	_	_
and	_	_
SVM	_	_
classifier	_	_
was	_	_
trained	_	_
.	_	_

#121
On	_	_
each	_	_
motion	_	_
map	_	_
also	_	_
we	_	_
individually	_	_
performed	_	_
bag	_	_
of	_	_
features	_	_
with	_	_
a	_	_
codebook	_	_
of	_	_
dimension	_	_
500	_	_
.	_	_

#122
We	_	_
have	_	_
used	_	_
Matlab’s	_	_
built-in	_	_
function	_	_
bagOfFeatures	_	_
for	_	_
this	_	_
step	_	_
and	_	_
trained	_	_
one-vs-all	_	_
linear	_	_
SVM	_	_
for	_	_
the	_	_
multi-class	_	_
recognition	_	_
.	_	_

#123
The	_	_
results	_	_
under	_	_
Leave	_	_
One	_	_
Out	_	_
cross-validation	_	_
method	_	_
for	_	_
all	_	_
these	_	_
descriptors	_	_
are	_	_
given	_	_
in	_	_
the	_	_
next	_	_
section	_	_
.	_	_

#124
5	_	_
.	_	_

#125
Experimental	_	_
Results	_	_
We	_	_
have	_	_
conducted	_	_
our	_	_
experiments	_	_
on	_	_
two	_	_
datasets	_	_
as	_	_
explained	_	_
in	_	_
the	_	_
following	_	_
section2	_	_
.	_	_

#126
5.1	_	_
.	_	_

#127
HAR	_	_
on	_	_
UCF11	_	_
and	_	_
its	_	_
DVS	_	_
counterpart	_	_
In	_	_
this	_	_
experiment	_	_
,	_	_
HAR	_	_
was	_	_
performed	_	_
on	_	_
the	_	_
original	_	_
UCF11	_	_
dataset	_	_
(	_	_
RGB	_	_
)	_	_
and	_	_
its	_	_
corresponding	_	_
DVS	_	_
recordings	_	_
.	_	_

#128
Table	_	_
1a	_	_
provides	_	_
the	_	_
recognition	_	_
rates	_	_
obtained	_	_
with	_	_
25	_	_
fold	_	_
Leave	_	_
One	_	_
Out	_	_
cross-validation	_	_
method	_	_
.	_	_

#129
Figure	_	_
5	_	_
:	_	_
Confusion	_	_
matrix	_	_
for	_	_
UCF11-DVS	_	_
dataset	_	_
on	_	_
combining	_	_
motion	_	_
maps	_	_
and	_	_
MBH	_	_
The	_	_
results	_	_
show	_	_
that	_	_
fusion	_	_
of	_	_
motion	_	_
maps	_	_
from	_	_
the	_	_
DVS	_	_
data	_	_
gave	_	_
a	_	_
HAR	_	_
rate	_	_
of	_	_
67.27	_	_
%	_	_
,	_	_
comparable	_	_
to	_	_
the	_	_
rates	_	_
for	_	_
HOF	_	_
and	_	_
HoG	_	_
on	_	_
the	_	_
original	_	_
UCF11	_	_
data	_	_
.	_	_

#130
Interestingly	_	_
,	_	_
with	_	_
the	_	_
individual	_	_
motion	_	_
map	_	_
descriptors	_	_
the	_	_
DVS	_	_
recordings	_	_
of	_	_
UCF11	_	_
gave	_	_
higher	_	_
recognition	_	_
rates	_	_
while	_	_
descriptors	_	_
like	_	_
HoG	_	_
and	_	_
HOF	_	_
performed	_	_
better	_	_
on	_	_
the	_	_
original	_	_
videos	_	_
.	_	_

#131
This	_	_
is	_	_
because	_	_
there	_	_
is	_	_
no	_	_
background	_	_
2For	_	_
our	_	_
code	_	_
and	_	_
DVS	_	_
gesture	_	_
dataset	_	_
refer	_	_
-	_	_
https	_	_
:	_	_
//github	_	_
.	_	_

#132
com/Computational-Imaging-Lab-IITM/HAR-DVS	_	_
Dataset	_	_
HoG	_	_
HOF	_	_
MBH	_	_
x-y	_	_
Motion	_	_
Map	_	_
x-t	_	_
Motion	_	_
Map	_	_
y-t	_	_
Motion	_	_
Map	_	_
Combined	_	_
Motion	_	_
Maps	_	_
Motion	_	_
Maps	_	_
+	_	_
HOF	_	_
Motion	_	_
Maps	_	_
+	_	_
MBH	_	_
Original	_	_
UCF11	_	_
0.6319	_	_
0.5754	_	_
0.7707	_	_
0.4397	_	_
0.4567	_	_
0.4077	_	_
0.5867	_	_
0.6922	_	_
0.7933	_	_
DVS	_	_
recordings	_	_
of	_	_
UCF11	_	_
0.5358	_	_
0.6043	_	_
0.7016	_	_
0.4943	_	_
0.451	_	_
0.4629	_	_
0.6727	_	_
0.7299	_	_
0.7513	_	_
(	_	_
a	_	_
)	_	_
Results	_	_
on	_	_
UCF11	_	_
and	_	_
its	_	_
DVS	_	_
counterpart	_	_
Dataset	_	_
HoG	_	_
HOF	_	_
MBH	_	_
x-y	_	_
Motion	_	_
Map	_	_
x-t	_	_
Motion	_	_
Map	_	_
y-t	_	_
Motion	_	_
Map	_	_
Combined	_	_
Motion	_	_
Maps	_	_
Motion	_	_
Maps	_	_
+	_	_
HOF	_	_
Motion	_	_
Maps	_	_
+	_	_
MBH	_	_
DVS	_	_
gesture	_	_
dataset	_	_
0.8768	_	_
0.9689	_	_
0.9468	_	_
0.7748	_	_
0.8349	_	_
0.7899	_	_
0.9529	_	_
0.9809	_	_
0.9880	_	_
(	_	_
b	_	_
)	_	_
Results	_	_
on	_	_
the	_	_
DVS	_	_
gesture	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
Table	_	_
1	_	_
:	_	_
Recognition	_	_
rates	_	_
for	_	_
various	_	_
motion	_	_
and	_	_
shape	_	_
descriptors	_	_
on	_	_
the	_	_
UCF11	_	_
dataset	_	_
,	_	_
its	_	_
corresponding	_	_
DVS	_	_
data	_	_
and	_	_
the	_	_
DVS	_	_
gesture	_	_
dataset	_	_
collected	_	_
by	_	_
us	_	_
.	_	_

#133
Note	_	_
that	_	_
MBH	_	_
features	_	_
give	_	_
70	_	_
%	_	_
accuracy	_	_
while	_	_
addition	_	_
of	_	_
motion	_	_
maps	_	_
give	_	_
75	_	_
%	_	_
accuracy	_	_
on	_	_
the	_	_
DVS	_	_
recordings	_	_
of	_	_
UCF11	_	_
data	_	_
clutter	_	_
and	_	_
scene	_	_
information	_	_
in	_	_
DVS	_	_
recording	_	_
for	_	_
distracting	_	_
its	_	_
bag	_	_
of	_	_
features	_	_
encoding	_	_
.	_	_

#134
KNN	_	_
classifier	_	_
was	_	_
also	_	_
used	_	_
for	_	_
the	_	_
final	_	_
predictions	_	_
,	_	_
but	_	_
it	_	_
gave	_	_
consistently	_	_
about	_	_
5	_	_
%	_	_
lower	_	_
HAR	_	_
rates	_	_
.	_	_

#135
Similarly	_	_
,	_	_
it	_	_
was	_	_
observed	_	_
that	_	_
simply	_	_
using	_	_
a	_	_
larger	_	_
dimension	_	_
codebook	_	_
of	_	_
size	_	_
4000	_	_
improves	_	_
recognition	_	_
rates	_	_
by	_	_
2	_	_
−	_	_
3	_	_
%	_	_
.	_	_

#136
Because	_	_
our	_	_
aim	_	_
is	_	_
to	_	_
study	_	_
the	_	_
performance	_	_
of	_	_
DVS	_	_
data	_	_
for	_	_
HAR	_	_
compared	_	_
to	_	_
original	_	_
data	_	_
,	_	_
we	_	_
limited	_	_
our	_	_
codebook	_	_
size	_	_
to	_	_
500	_	_
words	_	_
since	_	_
using	_	_
higher	_	_
sized	_	_
codebook	_	_
simply	_	_
improved	_	_
both	_	_
the	_	_
results	_	_
.	_	_

#137
To	_	_
further	_	_
boost	_	_
HAR	_	_
rates	_	_
,	_	_
we	_	_
separately	_	_
included	_	_
the	_	_
MBH	_	_
and	_	_
HOF	_	_
descriptors	_	_
along	_	_
with	_	_
the	_	_
motion	_	_
maps	_	_
and	_	_
trained	_	_
SVM	_	_
classifier	_	_
in	_	_
light	_	_
of	_	_
the	_	_
complementarity	_	_
they	_	_
offer	_	_
.	_	_

#138
The	_	_
HAR	_	_
values	_	_
in	_	_
Table	_	_
1a	_	_
show	_	_
that	_	_
the	_	_
features	_	_
from	_	_
motion	_	_
maps	_	_
better	_	_
complement	_	_
the	_	_
second	_	_
order	_	_
statistics	_	_
of	_	_
MBH	_	_
than	_	_
the	_	_
first	_	_
order	_	_
HOF	_	_
features	_	_
on	_	_
both	_	_
the	_	_
UCF11	_	_
datasets	_	_
.	_	_

#139
The	_	_
results	_	_
also	_	_
show	_	_
that	_	_
the	_	_
fusion	_	_
of	_	_
MBH	_	_
and	_	_
motion	_	_
maps	_	_
gave	_	_
the	_	_
highest	_	_
recognition	_	_
rate	_	_
among	_	_
all	_	_
descriptors	_	_
and	_	_
nearly	_	_
bridged	_	_
the	_	_
performance	_	_
gap	_	_
between	_	_
DVS	_	_
and	_	_
conventional	_	_
videos	_	_
on	_	_
the	_	_
benchmark	_	_
UCF11	_	_
data	_	_
.	_	_

#140
Given	_	_
the	_	_
sparsity	_	_
of	_	_
DVS	_	_
data	_	_
,	_	_
it	_	_
is	_	_
remarkable	_	_
that	_	_
the	_	_
final	_	_
descriptor	_	_
has	_	_
provided	_	_
a	_	_
nearequivalent	_	_
performance	_	_
on	_	_
DVS	_	_
when	_	_
compared	_	_
to	_	_
the	_	_
results	_	_
on	_	_
conventional	_	_
videos	_	_
.	_	_

#141
5.2	_	_
.	_	_

#142
Recognition	_	_
on	_	_
our	_	_
DVS	_	_
gesture	_	_
dataset	_	_
With	_	_
our	_	_
DVS	_	_
gesture	_	_
dataset	_	_
also	_	_
,	_	_
we	_	_
obtained	_	_
decent	_	_
recognition	_	_
rates	_	_
by	_	_
combining	_	_
the	_	_
motion	_	_
maps	_	_
alone	_	_
,	_	_
nearly	_	_
same	_	_
as	_	_
that	_	_
given	_	_
by	_	_
existing	_	_
feature	_	_
descriptors	_	_
.	_	_

#143
Combining	_	_
motion	_	_
maps	_	_
with	_	_
the	_	_
MBH	_	_
descriptor	_	_
again	_	_
augmented	_	_
HAR	_	_
performance	_	_
to	_	_
give	_	_
the	_	_
highest	_	_
recognition	_	_
rates	_	_
as	_	_
seen	_	_
in	_	_
Table	_	_
1b	_	_
.	_	_

#144
6	_	_
.	_	_

#145
Conclusion	_	_
and	_	_
Future	_	_
Work	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
have	_	_
analyzed	_	_
the	_	_
performance	_	_
of	_	_
DVS	_	_
data	_	_
in	_	_
human	_	_
activity	_	_
recognition	_	_
and	_	_
compared	_	_
it	_	_
with	_	_
its	_	_
conventional	_	_
frame-based	_	_
counterpart	_	_
using	_	_
traditional	_	_
feature	_	_
extraction	_	_
techniques	_	_
.	_	_

#146
We	_	_
also	_	_
proposed	_	_
a	_	_
new	_	_
encoding	_	_
technique	_	_
(	_	_
motion	_	_
maps	_	_
)	_	_
that	_	_
is	_	_
suited	_	_
especially	_	_
for	_	_
DVS	_	_
data	_	_
in	_	_
light	_	_
of	_	_
its	_	_
sparse	_	_
and	_	_
concise	_	_
recording	_	_
scheme	_	_
.	_	_

#147
Combining	_	_
the	_	_
existing	_	_
MBH	_	_
descriptor	_	_
with	_	_
motion	_	_
maps	_	_
gave	_	_
the	_	_
best	_	_
recognition	_	_
results	_	_
.	_	_

#148
Based	_	_
on	_	_
the	_	_
feature	_	_
descriptors	_	_
available	_	_
for	_	_
its	_	_
encoding	_	_
,	_	_
HAR	_	_
results	_	_
from	_	_
DVS	_	_
recordings	_	_
have	_	_
been	_	_
nearly	_	_
equal	_	_
to	_	_
that	_	_
of	_	_
RGB	_	_
videos	_	_
on	_	_
the	_	_
benchmark	_	_
UCF11	_	_
data	_	_
.	_	_

#149
Additional	_	_
features	_	_
based	_	_
on	_	_
the	_	_
scene	_	_
,	_	_
texture	_	_
and	_	_
hue	_	_
have	_	_
enabled	_	_
better	_	_
recognition	_	_
rates	_	_
with	_	_
actual	_	_
videos	_	_
.	_	_

#150
But	_	_
these	_	_
are	_	_
more	_	_
complex	_	_
and	_	_
unavailable	_	_
for	_	_
use	_	_
with	_	_
the	_	_
DVS	_	_
data	_	_
from	_	_
the	_	_
very	_	_
beginning	_	_
.	_	_

#151
Hence	_	_
respecting	_	_
the	_	_
limitations	_	_
that	_	_
come	_	_
with	_	_
DVS	_	_
,	_	_
we	_	_
conclude	_	_
that	_	_
within	_	_
the	_	_
framework	_	_
of	_	_
its	_	_
possible	_	_
descriptors	_	_
it	_	_
is	_	_
just	_	_
as	_	_
useful	_	_
for	_	_
HAR	_	_
as	_	_
conventional	_	_
videos	_	_
and	_	_
could	capability-feasibility	_
efficiently	_	_
be	_	_
used	_	_
in	_	_
place	_	_
of	_	_
the	_	_
latter	_	_
,	_	_
especially	_	_
in	_	_
low	_	_
power	_	_
and	_	_
high	_	_
speed	_	_
applications	_	_
.	_	_

#152
As	_	_
future	_	_
work	_	_
,	_	_
we	_	_
can	_	_
look	_	_
at	_	_
improving	_	_
performance	_	_
of	_	_
simple	_	_
bag-of-features	_	_
where	_	_
location	_	_
based	_	_
relations	_	_
are	_	_
not	_	_
preserved	_	_
due	_	_
to	_	_
its	_	_
pooling	_	_
step	_	_
.	_	_

#153
Rather	_	_
than	_	_
destroying	_	_
spatial	_	_
information	_	_
between	_	_
the	_	_
extracted	_	_
features	_	_
in	_	_
the	_	_
image	_	_
,	_	_
methods	_	_
like	_	_
Spatial	_	_
Correlogram	_	_
and	_	_
matching	_	_
can	_	_
be	_	_
employed	_	_
on	_	_
the	_	_
DVS	_	_
data	_	_
.	_	_

#154
Also	_	_
,	_	_
we	_	_
noted	_	_
that	_	_
similar	_	_
to	_	_
recognition	_	_
rates	_	_
in	_	_
conventional	_	_
videos	_	_
,	_	_
dense	_	_
trajectories	_	_
with	_	_
MBH	_	_
gave	_	_
the	_	_
best	_	_
results	_	_
on	_	_
using	_	_
traditional	_	_
features	_	_
in	_	_
DVS	_	_
as	_	_
well	_	_
.	_	_

#155
Much	_	_
of	_	_
the	_	_
success	_	_
of	_	_
dense	_	_
tracking	_	_
comes	_	_
from	_	_
the	_	_
fact	_	_
that	_	_
it	_	_
generates	_	_
too	_	_
many	_	_
interest	_	_
points	_	_
given	_	_
any	_	_
video	_	_
sample	_	_
.	_	_

#156
Visualization	_	_
of	_	_
the	_	_
interest	_	_
points	_	_
found	_	_
by	_	_
dense	_	_
sampling	_	_
showed	_	_
that	_	_
some	_	_
of	_	_
these	_	_
are	_	_
randomly	_	_
fired	_	_
noisy	_	_
events	_	_
in	_	_
DVS	_	_
unrelated	_	_
to	_	_
the	_	_
object	_	_
in	_	_
foreground	_	_
.	_	_

#157
A	_	_
simple	_	_
median	_	_
filtering	_	_
pre-processing	_	_
before	_	_
finding	_	_
dense	_	_
interest	_	_
points	_	_
however	_	_
did	_	_
not	_	_
improve	_	_
recognition	_	_
rate	_	_
.	_	_

#158
In	_	_
order	_	_
to	_	_
truly	_	_
address	_	_
the	_	_
problem	_	_
,	_	_
a	_	_
new	_	_
method	_	_
specifically	_	_
for	_	_
finding	_	_
and	_	_
tracking	_	_
DVS	_	_
events	_	_
should	deontic	_
itself	_	_
be	_	_
invented	_	_
.	_	_

#159
This	_	_
would	_	_
act	_	_
as	_	_
the	_	_
true	_	_
initial	_	_
step	_	_
for	_	_
improving	_	_
the	_	_
performance	_	_
of	_	_
HAR	_	_
on	_	_
using	_	_
optical	_	_
flow	_	_
,	_	_
MBH	_	_
as	_	_
well	_	_
as	_	_
dense	_	_
trajectories	_	_
with	_	_
dynamic	_	_
vision	_	_
sensors	_	_
.	_	_