#0
An	_	_
Introduction	_	_
to	_	_
Image	_	_
Synthesis	_	_
with	_	_
Generative	_	_
Adversarial	_	_
Nets	_	_
He	_	_
Huang	_	_
,	_	_
Philip	_	_
S.	_	_
Yu	_	_
and	_	_
Changhu	_	_
Wang	_	_
Abstract—There	_	_
has	_	_
been	_	_
a	_	_
drastic	_	_
growth	_	_
of	_	_
research	_	_
in	_	_
Generative	_	_
Adversarial	_	_
Nets	_	_
(	_	_
GANs	_	_
)	_	_
in	_	_
the	_	_
past	_	_
few	_	_
years	_	_
.	_	_

#1
Proposed	_	_
in	_	_
2014	_	_
,	_	_
GAN	_	_
has	_	_
been	_	_
applied	_	_
to	_	_
various	_	_
applications	_	_
such	_	_
as	_	_
computer	_	_
vision	_	_
and	_	_
natural	_	_
language	_	_
processing	_	_
,	_	_
and	_	_
achieves	_	_
impressive	_	_
performance	_	_
.	_	_

#2
Among	_	_
the	_	_
many	_	_
applications	_	_
of	_	_
GAN	_	_
,	_	_
image	_	_
synthesis	_	_
is	_	_
the	_	_
most	_	_
well-studied	_	_
one	_	_
,	_	_
and	_	_
research	_	_
in	_	_
this	_	_
area	_	_
has	_	_
already	_	_
demonstrated	_	_
the	_	_
great	_	_
potential	_	_
of	_	_
using	_	_
GAN	_	_
in	_	_
image	_	_
synthesis	_	_
.	_	_

#3
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
provide	_	_
a	_	_
taxonomy	_	_
of	_	_
methods	_	_
used	_	_
in	_	_
image	_	_
synthesis	_	_
,	_	_
review	_	_
different	_	_
models	_	_
for	_	_
text-to-image	_	_
synthesis	_	_
and	_	_
image-to-image	_	_
translation	_	_
,	_	_
and	_	_
discuss	_	_
some	_	_
evaluation	_	_
metrics	_	_
as	_	_
well	_	_
as	_	_
possible	_	_
future	_	_
research	_	_
directions	_	_
in	_	_
image	_	_
synthesis	_	_
with	_	_
GAN	_	_
.	_	_

#4
Index	_	_
Terms—Deep	_	_
Learning	_	_
,	_	_
Generative	_	_
Adversarial	_	_
Nets	_	_
,	_	_
Image	_	_
Synthesis	_	_
,	_	_
Computer	_	_
Vision	_	_
.	_	_

#5
F	_	_

#6
1	_	_
INTRODUCTION	_	_

#7
W	_	_
ITH	_	_
recent	_	_
advances	_	_
in	_	_
deep	_	_
learning	_	_
,	_	_
machine	_	_
learning	_	_
algorithms	_	_
have	_	_
evolved	_	_
to	_	_
such	_	_
an	_	_
extent	_	_
that	_	_
they	_	_
can	_	_
compete	_	_
and	_	_
even	_	_
defeat	_	_
humans	_	_
in	_	_
some	_	_
tasks	_	_
,	_	_
such	_	_
as	_	_
image	_	_
classification	_	_
on	_	_
ImageNet	_	_
[	_	_
1	_	_
]	_	_
,	_	_
playing	_	_
Go	_	_
[	_	_
2	_	_
]	_	_
and	_	_
Texas	_	_
Hold’em	_	_
poker	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#8
However	_	_
,	_	_
we	_	_
still	_	_
can	_	_
not	_	_
conclude	_	_
that	_	_
those	_	_
algorithms	_	_
have	_	_
true	_	_
“intelligence”	_	_
,	_	_
since	_	_
knowing	_	_
how	_	_
to	_	_
do	_	_
something	_	_
does	_	_
not	_	_
necessarily	_	_
mean	_	_
understanding	_	_
something	_	_
,	_	_
and	_	_
it	_	_
is	_	_
critical	_	_
for	_	_
a	_	_
truly	_	_
intelligent	_	_
agent	_	_
to	_	_
understand	_	_
its	_	_
tasks	_	_
.	_	_

#9
“What	_	_
I	_	_
can	_	_
not	_	_
create	_	_
,	_	_
I	_	_
do	_	_
not	_	_
understand”	_	_
,	_	_
said	_	_
the	_	_
famous	_	_
physicist	_	_
Richard	_	_
Feynman	_	_
.	_	_

#10
To	_	_
put	_	_
this	_	_
quote	_	_
in	_	_
the	_	_
case	_	_
of	_	_
machine	_	_
learning	_	_
,	_	_
we	_	_
can	_	_
say	_	_
that	_	_
,	_	_
for	_	_
machines	_	_
to	_	_
understand	_	_
their	_	_
input	_	_
data	_	_
,	_	_
they	_	_
need	_	_
to	_	_
learn	_	_
to	_	_
create	_	_
the	_	_
data	_	_
.	_	_

#11
The	_	_
most	_	_
promising	_	_
approach	_	_
is	_	_
to	_	_
use	_	_
generative	_	_
models	_	_
that	_	_
learn	_	_
to	_	_
discover	_	_
the	_	_
essence	_	_
of	_	_
data	_	_
and	_	_
find	_	_
a	_	_
best	_	_
distribution	_	_
to	_	_
represent	_	_
it	_	_
.	_	_

#12
Also	_	_
,	_	_
with	_	_
a	_	_
learned	_	_
generative	_	_
model	_	_
,	_	_
we	_	_
can	_	_
even	_	_
draw	_	_
samples	_	_
which	_	_
are	_	_
not	_	_
in	_	_
the	_	_
training	_	_
set	_	_
but	_	_
follow	_	_
the	_	_
same	_	_
distribution	_	_
.	_	_

#13
As	_	_
a	_	_
new	_	_
framework	_	_
of	_	_
generative	_	_
model	_	_
,	_	_
Generative	_	_
Adversarial	_	_
Net	_	_
(	_	_
GAN	_	_
)	_	_
[	_	_
4	_	_
]	_	_
,	_	_
proposed	_	_
in	_	_
2014	_	_
,	_	_
is	_	_
able	_	_
to	_	_
generate	_	_
better	_	_
synthetic	_	_
images	_	_
than	_	_
previous	_	_
generative	_	_
models	_	_
,	_	_
and	_	_
since	_	_
then	_	_
it	_	_
has	_	_
become	_	_
one	_	_
of	_	_
the	_	_
most	_	_
popular	_	_
research	_	_
areas	_	_
.	_	_

#14
A	_	_
Generative	_	_
Adversarial	_	_
Net	_	_
consists	_	_
of	_	_
two	_	_
neural	_	_
networks	_	_
,	_	_
a	_	_
generator	_	_
and	_	_
a	_	_
discriminator	_	_
,	_	_
where	_	_
the	_	_
generator	_	_
tries	_	_
to	_	_
produce	_	_
realistic	_	_
samples	_	_
that	_	_
fool	_	_
the	_	_
discriminator	_	_
,	_	_
while	_	_
the	_	_
discriminator	_	_
tries	_	_
to	_	_
distinguish	_	_
real	_	_
samples	_	_
from	_	_
generated	_	_
ones	_	_
.	_	_

#15
There	_	_
are	_	_
two	_	_
main	_	_
threads	_	_
of	_	_
research	_	_
on	_	_
GAN	_	_
.	_	_

#16
One	_	_
is	_	_
the	_	_
theoretical	_	_
thread	_	_
that	_	_
tries	_	_
to	_	_
alleviate	_	_
the	_	_
instability	_	_
and	_	_
mode	_	_
collapse	_	_
problems	_	_
of	_	_
GAN	_	_
[	_	_
5	_	_
]	_	_
[	_	_
6	_	_
]	_	_
[	_	_
7	_	_
]	_	_
[	_	_
8	_	_
]	_	_
[	_	_
9	_	_
]	_	_
[	_	_
10	_	_
]	_	_
,	_	_
or	_	_
reformulate	_	_
it	_	_
from	_	_
different	_	_
angles	_	_
like	_	_
information	_	_
theory	_	_
[	_	_
11	_	_
]	_	_
and	_	_
energy-based	_	_
models	_	_
[	_	_
12	_	_
]	_	_
.	_	_

#17
The	_	_
other	_	_
thread	_	_
focuses	_	_
on	_	_
the	_	_
applications	_	_
of	_	_
GAN	_	_
in	_	_
computer	_	_
vision	_	_
(	_	_
CV	_	_
)	_	_
[	_	_
5	_	_
]	_	_
,	_	_
natural	_	_
language	_	_
processing	_	_
(	_	_
NLP	_	_
)	_	_
[	_	_
13	_	_
]	_	_
and	_	_
other	_	_
areas	_	_
.	_	_

#18
•	_	_
He	_	_
Huang	_	_
and	_	_
Philip	_	_
S.	_	_
Yu	_	_
are	_	_
with	_	_
the	_	_
Department	_	_
of	_	_
Computer	_	_
Science	_	_
,	_	_
University	_	_
of	_	_
Illinois	_	_
at	_	_
Chicago	_	_
,	_	_
USA	_	_
.	_	_

#19
Emails	_	_
:	_	_
{	_	_
hehuang	_	_
,	_	_
psyu	_	_
}	_	_
@	_	_
uic.edu	_	_
•	_	_
Changhu	_	_
Wang	_	_
is	_	_
with	_	_
ByteDance	_	_
AI	_	_
Lab	_	_
,	_	_
China	_	_
.	_	_

#20
Email	_	_
:	_	_
wangchanghu	_	_
@	_	_
bytedance.com	_	_
There	_	_
is	_	_
a	_	_
great	_	_
tutorial	_	_
given	_	_
by	_	_
Goodfellow	_	_
in	_	_
NIPS	_	_
2016	_	_
on	_	_
GAN	_	_
[	_	_
14	_	_
]	_	_
where	_	_
he	_	_
describes	_	_
the	_	_
importance	_	_
of	_	_
generative	_	_
models	_	_
,	_	_
explains	_	_
how	_	_
GAN	_	_
works	_	_
,	_	_
compares	_	_
GAN	_	_
with	_	_
other	_	_
generative	_	_
model	_	_
and	_	_
discusses	_	_
frontier	_	_
research	_	_
topics	_	_
in	_	_
GAN	_	_
.	_	_

#21
Also	_	_
,	_	_
there	_	_
is	_	_
a	_	_
recent	_	_
review	_	_
paper	_	_
on	_	_
GAN	_	_
[	_	_
15	_	_
]	_	_
which	_	_
reviews	_	_
several	_	_
GAN	_	_
architectures	_	_
and	_	_
training	_	_
techniques	_	_
,	_	_
and	_	_
introduces	_	_
some	_	_
applications	_	_
of	_	_
GAN	_	_
.	_	_

#22
However	_	_
,	_	_
both	_	_
of	_	_
these	_	_
papers	_	_
are	_	_
in	_	_
a	_	_
general	_	_
scope	_	_
,	_	_
without	_	_
going	_	_
into	_	_
details	_	_
of	_	_
specific	_	_
applications	_	_
.	_	_

#23
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
specifically	_	_
focus	_	_
on	_	_
image	_	_
synthesis	_	_
,	_	_
whose	_	_
goal	_	_
is	_	_
to	_	_
generate	_	_
images	_	_
,	_	_
since	_	_
it	_	_
is	_	_
by	_	_
far	_	_
the	_	_
most	_	_
studied	_	_
area	_	_
where	_	_
GAN	_	_
has	_	_
been	_	_
applied	_	_
.	_	_

#24
Besides	_	_
image	_	_
synthesis	_	_
,	_	_
there	_	_
are	_	_
many	_	_
other	_	_
applications	_	_
of	_	_
GAN	_	_
in	_	_
computer	_	_
vision	_	_
,	_	_
such	_	_
as	_	_
image	_	_
in-painting	_	_
[	_	_
16	_	_
]	_	_
,	_	_
image	_	_
captioning	_	_
[	_	_
17	_	_
]	_	_
[	_	_
18	_	_
]	_	_
[	_	_
19	_	_
]	_	_
,	_	_
object	_	_
detection	_	_
[	_	_
20	_	_
]	_	_
and	_	_
semantic	_	_
segmentation	_	_
[	_	_
21	_	_
]	_	_
.	_	_

#25
Research	_	_
of	_	_
applying	_	_
GAN	_	_
in	_	_
natural	_	_
language	_	_
processing	_	_
is	_	_
also	_	_
a	_	_
growing	_	_
trend	_	_
,	_	_
such	_	_
as	_	_
text	_	_
modeling	_	_
[	_	_
13	_	_
]	_	_
[	_	_
22	_	_
]	_	_
[	_	_
23	_	_
]	_	_
,	_	_
dialogue	_	_
generation	_	_
[	_	_
24	_	_
]	_	_
,	_	_
question	_	_
answering	_	_
[	_	_
25	_	_
]	_	_
and	_	_
neural	_	_
machine	_	_
translation	_	_
[	_	_
26	_	_
]	_	_
.	_	_

#26
However	_	_
,	_	_
training	_	_
GAN	_	_
in	_	_
NLP	_	_
tasks	_	_
is	_	_
more	_	_
difficult	_	_
and	_	_
requires	_	_
more	_	_
techniques	_	_
[	_	_
13	_	_
]	_	_
,	_	_
which	_	_
also	_	_
makes	_	_
it	_	_
a	_	_
challenging	_	_
but	_	_
intriguing	_	_
research	_	_
area	_	_
.	_	_

#27
The	_	_
main	_	_
goal	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
to	_	_
provide	_	_
an	_	_
overview	_	_
of	_	_
the	_	_
methods	_	_
used	_	_
in	_	_
image	_	_
synthesis	_	_
with	_	_
GAN	_	_
and	_	_
point	_	_
out	_	_
strengths	_	_
and	_	_
weaknesses	_	_
of	_	_
current	_	_
methods	_	_
.	_	_

#28
We	_	_
classify	_	_
the	_	_
main	_	_
approaches	_	_
in	_	_
image	_	_
synthesis	_	_
into	_	_
three	_	_
methods	_	_
,	_	_
i.e.	_	_
direct	_	_
methods	_	_
,	_	_
hierarchical	_	_
methods	_	_
and	_	_
iterative	_	_
methods	_	_
.	_	_

#29
Besides	_	_
these	_	_
most	_	_
commonly	_	_
used	_	_
methods	_	_
,	_	_
there	_	_
are	_	_
also	_	_
other	_	_
methods	_	_
which	_	_
we	_	_
will	_	_
briefly	_	_
mention	_	_
.	_	_

#30
We	_	_
then	_	_
give	_	_
a	_	_
detailed	_	_
discussion	_	_
in	_	_
two	_	_
of	_	_
the	_	_
most	_	_
important	_	_
tasks	_	_
in	_	_
image	_	_
synthesis	_	_
,	_	_
i.e.	_	_
text-to-image	_	_
synthesis	_	_
and	_	_
image-to-image	_	_
translation	_	_
.	_	_

#31
We	_	_
also	_	_
discuss	_	_
the	_	_
possible	_	_
reasons	_	_
why	_	_
GAN	_	_
performs	_	_
so	_	_
well	_	_
in	_	_
certain	_	_
tasks	_	_
and	_	_
its	_	_
role	_	_
in	_	_
our	_	_
goal	_	_
to	_	_
artificial	_	_
intelligence	_	_
.	_	_

#32
The	_	_
goal	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
to	_	_
provide	_	_
a	_	_
simple	_	_
guideline	_	_
for	_	_
those	_	_
who	_	_
want	_	_
to	_	_
apply	_	_
GAN	_	_
to	_	_
their	_	_
problems	_	_
and	_	_
help	_	_
further	_	_
research	_	_
in	_	_
GAN	_	_
.	_	_

#33
The	_	_
rest	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
organized	_	_
as	_	_
follows	_	_
.	_	_

#34
In	_	_
Section	_	_
2	_	_
we	_	_
first	_	_
review	_	_
some	_	_
core	_	_
concepts	_	_
of	_	_
GAN	_	_
,	_	_
as	_	_
well	_	_
as	_	_
some	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#35
9v	_	_
2	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
7	_	_
N	_	_
ov	_	_
2	_	_
variants	_	_
and	_	_
training	_	_
issues	_	_
.	_	_

#36
Then	_	_
in	_	_
Section	_	_
3	_	_
we	_	_
introduce	_	_
three	_	_
main	_	_
approaches	_	_
and	_	_
some	_	_
other	_	_
approaches	_	_
used	_	_
in	_	_
image	_	_
synthesis	_	_
.	_	_

#37
In	_	_
Section	_	_
4	_	_
,	_	_
we	_	_
discuss	_	_
several	_	_
methods	_	_
in	_	_
text-to-image	_	_
synthesis	_	_
and	_	_
some	_	_
possible	_	_
research	_	_
directions	_	_
for	_	_
improvements	_	_
.	_	_

#38
In	_	_
Section	_	_
5	_	_
,	_	_
we	_	_
first	_	_
introduce	_	_
supervised	_	_
and	_	_
unsupervised	_	_
methods	_	_
for	_	_
image-to-image	_	_
translation	_	_
,	_	_
and	_	_
then	_	_
turn	_	_
to	_	_
more	_	_
specific	_	_
applications	_	_
like	_	_
face	_	_
editing	_	_
,	_	_
video	_	_
prediction	_	_
and	_	_
image	_	_
super-resolution	_	_
.	_	_

#39
In	_	_
Section	_	_
6	_	_
we	_	_
review	_	_
some	_	_
evaluation	_	_
metrics	_	_
for	_	_
synthetic	_	_
images	_	_
,	_	_
while	_	_
in	_	_
Section	_	_
7	_	_
we	_	_
discuss	_	_
the	_	_
discriminator’s	_	_
role	_	_
as	_	_
a	_	_
learned	_	_
loss	_	_
function	_	_
.	_	_

#40
Conclusions	_	_
are	_	_
given	_	_
in	_	_
Section	_	_
8	_	_
.	_	_

#41
2	_	_
GAN	_	_
PRELIMINARIES	_	_

#42
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
review	_	_
some	_	_
core	_	_
concepts	_	_
of	_	_
Generative	_	_
Adversarial	_	_
Nets	_	_
(	_	_
GANs	_	_
)	_	_
and	_	_
some	_	_
improvements	_	_
.	_	_

#43
A	_	_
generative	_	_
model	_	_
G	_	_
parameterized	_	_
by	_	_
θ	_	_
takes	_	_
as	_	_
input	_	_
a	_	_
random	_	_
noise	_	_
z	_	_
and	_	_
output	_	_
a	_	_
sampleG	_	_
(	_	_
z	_	_
;	_	_
θ	_	_
)	_	_
,	_	_
so	_	_
the	_	_
output	_	_
can	_	_
be	_	_
regarded	_	_
as	_	_
a	_	_
sample	_	_
drawn	_	_
from	_	_
a	_	_
distribution	_	_
:	_	_
G	_	_
(	_	_
z	_	_
;	_	_
θ	_	_
)	_	_
∼	_	_
pg	_	_
.	_	_

#44
Meanwhile	_	_
,	_	_
we	_	_
have	_	_
a	_	_
lot	_	_
of	_	_
training	_	_
data	_	_
x	_	_
drawn	_	_
from	_	_
pdata	_	_
,	_	_
and	_	_
the	_	_
training	_	_
objective	_	_
for	_	_
the	_	_
generative	_	_
model	_	_
G	_	_
is	_	_
to	_	_
approximate	_	_
pdata	_	_
using	_	_
pg	_	_
.	_	_

#45
True/FakeG	_	_
D	_	_
z	_	_
G	_	_
(	_	_
z	_	_
)	_	_
x	_	_
Generator	_	_
Discriminator	_	_
Random	_	_
noise	_	_
True	_	_
data	_	_
Synthetic	_	_
data	_	_
Fig.	_	_
1	_	_
.	_	_

#46
General	_	_
structure	_	_
of	_	_
a	_	_
Generative	_	_
Adversarial	_	_
Network	_	_
,	_	_
where	_	_
the	_	_
generator	_	_
G	_	_
takes	_	_
a	_	_
noise	_	_
vector	_	_
z	_	_
as	_	_
input	_	_
and	_	_
output	_	_
a	_	_
synthetic	_	_
sample	_	_
G	_	_
(	_	_
z	_	_
)	_	_
,	_	_
and	_	_
the	_	_
discriminator	_	_
takes	_	_
both	_	_
the	_	_
synthetic	_	_
input	_	_
G	_	_
(	_	_
z	_	_
)	_	_
and	_	_
true	_	_
sample	_	_
x	_	_
as	_	_
inputs	_	_
and	_	_
predict	_	_
whether	_	_
they	_	_
are	_	_
real	_	_
or	_	_
fake	_	_
.	_	_

#47
Generative	_	_
Adversarial	_	_
Net	_	_
(	_	_
GAN	_	_
)	_	_
[	_	_
4	_	_
]	_	_
consists	_	_
of	_	_
two	_	_
separate	_	_
neural	_	_
networks	_	_
:	_	_
a	_	_
generatorG	_	_
that	_	_
takes	_	_
a	_	_
random	_	_
noise	_	_
vector	_	_
z	_	_
,	_	_
and	_	_
outputs	_	_
synthetic	_	_
data	_	_
G	_	_
(	_	_
z	_	_
)	_	_
;	_	_
a	_	_
discriminator	_	_
D	_	_
that	_	_
takes	_	_
an	_	_
input	_	_
x	_	_
or	_	_
G	_	_
(	_	_
z	_	_
)	_	_
and	_	_
output	_	_
a	_	_
probability	_	_
D	_	_
(	_	_
x	_	_
)	_	_
or	_	_
D	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
to	_	_
indicate	_	_
whether	_	_
it	_	_
is	_	_
synthetic	_	_
or	_	_
from	_	_
the	_	_
true	_	_
data	_	_
distribution	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#48
Both	_	_
of	_	_
the	_	_
generator	_	_
and	_	_
discriminator	_	_
can	_	_
be	_	_
arbitrary	_	_
neural	_	_
networks	_	_
.	_	_

#49
The	_	_
first	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
uses	_	_
fully	_	_
connected	_	_
layer	_	_
as	_	_
its	_	_
building	_	_
block	_	_
.	_	_

#50
Later	_	_
,	_	_
DCGAN	_	_
[	_	_
5	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
fully	_	_
convolutional	_	_
neural	_	_
networks	_	_
which	_	_
achieves	_	_
better	_	_
performance	_	_
,	_	_
and	_	_
since	_	_
then	_	_
convolution	_	_
and	_	_
transposed	_	_
convolution	_	_
layers	_	_
have	_	_
become	_	_
the	_	_
core	_	_
components	_	_
in	_	_
many	_	_
GAN	_	_
models	_	_
.	_	_

#51
For	_	_
more	_	_
details	_	_
on	_	_
(	_	_
transposed	_	_
)	_	_
convolution	_	_
arithmetic	_	_
,	_	_
please	_	_
refer	_	_
to	_	_
this	_	_
report	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#52
The	_	_
original	_	_
way	_	_
to	_	_
train	_	_
the	_	_
generator	_	_
and	_	_
discriminator	_	_
is	_	_
to	_	_
form	_	_
a	_	_
two-player	_	_
min-max	_	_
game	_	_
where	_	_
the	_	_
generator	_	_
G	_	_
tries	_	_
to	_	_
generate	_	_
realistic	_	_
data	_	_
to	_	_
fool	_	_
the	_	_
discriminator	_	_
while	_	_
discriminator	_	_
D	_	_
tries	_	_
to	_	_
distinguish	_	_
between	_	_
real	_	_
and	_	_
synthetic	_	_
data	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#53
The	_	_
value	_	_
function	_	_
to	_	_
be	_	_
optimized	_	_
is	_	_
shown	_	_
in	_	_
Equation	_	_
1	_	_
,	_	_
where	_	_
pdata	_	_
(	_	_
x	_	_
)	_	_
denotes	_	_
the	_	_
true	_	_
data	_	_
distribution	_	_
and	_	_
pz	_	_
(	_	_
z	_	_
)	_	_
denote	_	_
the	_	_
noise	_	_
distribution	_	_
.	_	_

#54
min	_	_
G	_	_
max	_	_
D	_	_
V	_	_
(	_	_
D	_	_
,	_	_
G	_	_
)	_	_
=	_	_
Ex∼pdata	_	_
(	_	_
x	_	_
)	_	_
[	_	_
logD	_	_
(	_	_
x	_	_
)	_	_
]	_	_
+	_	_
Ez∼pz	_	_
(	_	_
z	_	_
)	_	_
[	_	_
log	_	_
(	_	_
1−D	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
)	_	_
]	_	_
(	_	_
1	_	_
)	_	_
However	_	_
,	_	_
when	_	_
the	_	_
discriminator	_	_
is	_	_
trained	_	_
much	_	_
better	_	_
than	_	_
the	_	_
generator	_	_
,	_	_
D	_	_
can	_	_
reject	_	_
the	_	_
samples	_	_
from	_	_
G	_	_
with	_	_
confidence	_	_
close	_	_
to	_	_
1	_	_
,	_	_
and	_	_
thus	_	_
the	_	_
loss	_	_
log	_	_
(	_	_
1	_	_
−	_	_
D	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
)	_	_
saturates	_	_
and	_	_
G	_	_
can	_	_
not	_	_
learn	_	_
anything	_	_
from	_	_
zero	_	_
gradient	_	_
.	_	_

#55
To	_	_
prevent	_	_
this	_	_
,	_	_
instead	_	_
of	_	_
training	_	_
G	_	_
to	_	_
minimize	_	_
log	_	_
(	_	_
1	_	_
−D	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
)	_	_
,	_	_
we	_	_
can	_	_
train	_	_
it	_	_
to	_	_
maximize	_	_
logD	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#56
Although	_	_
the	_	_
new	_	_
loss	_	_
function	_	_
for	_	_
G	_	_
gives	_	_
a	_	_
different	_	_
scale	_	_
of	_	_
gradient	_	_
than	_	_
the	_	_
original	_	_
one	_	_
,	_	_
it	_	_
still	_	_
provides	_	_
the	_	_
same	_	_
direction	_	_
of	_	_
gradient	_	_
and	_	_
does	_	_
not	_	_
saturate	_	_
.	_	_

#57
2.1	_	_
Conditional	_	_
GAN	_	_

#58
In	_	_
the	_	_
original	_	_
GAN	_	_
,	_	_
we	_	_
have	_	_
no	_	_
control	_	_
of	_	_
what	_	_
to	_	_
be	_	_
generated	_	_
,	_	_
since	_	_
the	_	_
output	_	_
is	_	_
only	_	_
dependent	_	_
on	_	_
random	_	_
noise	_	_
.	_	_

#59
However	_	_
,	_	_
we	_	_
can	_	_
add	_	_
a	_	_
conditional	_	_
input	_	_
c	_	_
to	_	_
the	_	_
random	_	_
noise	_	_
z	_	_
so	_	_
that	_	_
the	_	_
generated	_	_
image	_	_
is	_	_
defined	_	_
by	_	_
G	_	_
(	_	_
c	_	_
,	_	_
z	_	_
)	_	_
[	_	_
28	_	_
]	_	_
.	_	_

#60
Typically	_	_
,	_	_
the	_	_
conditional	_	_
input	_	_
vector	_	_
c	_	_
is	_	_
concatenated	_	_
with	_	_
the	_	_
noise	_	_
vector	_	_
z	_	_
,	_	_
and	_	_
the	_	_
resulting	_	_
vector	_	_
is	_	_
put	_	_
into	_	_
the	_	_
generator	_	_
as	_	_
it	_	_
is	_	_
in	_	_
the	_	_
original	_	_
GAN	_	_
.	_	_

#61
Besides	_	_
,	_	_
we	_	_
can	_	_
perform	_	_
other	_	_
data	_	_
augmentation	_	_
on	_	_
c	_	_
and	_	_
z	_	_
,	_	_
as	_	_
in	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#62
The	_	_
meaning	_	_
of	_	_
conditional	_	_
input	_	_
c	_	_
is	_	_
arbitrary	_	_
,	_	_
for	_	_
example	_	_
,	_	_
it	_	_
can	_	_
be	_	_
the	_	_
class	_	_
of	_	_
image	_	_
,	_	_
attributes	_	_
of	_	_
object	_	_
[	_	_
28	_	_
]	_	_
or	_	_
an	_	_
embedding	_	_
of	_	_
text	_	_
descriptions	_	_
of	_	_
the	_	_
image	_	_
we	_	_
want	_	_
to	_	_
generate	_	_
[	_	_
30	_	_
]	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#63
2.2	_	_
GAN	_	_
with	_	_
Auxiliary	_	_
Classifier	_	_

#64
In	_	_
order	_	_
to	_	_
feed	_	_
more	_	_
side-information	_	_
and	_	_
to	_	_
allow	_	_
for	_	_
semi-supervised	_	_
learning	_	_
,	_	_
one	_	_
can	_	_
add	_	_
an	_	_
additional	_	_
task-specific	_	_
auxiliary	_	_
classifier	_	_
to	_	_
the	_	_
discriminator	_	_
,	_	_
so	_	_
that	_	_
the	_	_
model	_	_
is	_	_
optimized	_	_
on	_	_
the	_	_
original	_	_
tasks	_	_
as	_	_
well	_	_
as	_	_
the	_	_
additional	_	_
task	_	_
[	_	_
32	_	_
]	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#65
The	_	_
architecture	_	_
of	_	_
such	_	_
method	_	_
is	_	_
illustrated	_	_
in	_	_
Figure	_	_
2	_	_
,	_	_
where	_	_
C	_	_
is	_	_
the	_	_
auxiliary	_	_
classifier	_	_
.	_	_

#66
Adding	_	_
auxiliary	_	_
classifiers	_	_
allows	_	_
us	_	_
to	_	_
use	_	_
pre-trained	_	_
models	_	_
(	_	_
e.g.	_	_
image	_	_
classifiers	_	_
trained	_	_
on	_	_
ImageNet	_	_
)	_	_
,	_	_
and	_	_
experiments	_	_
in	_	_
AC-GAN	_	_
[	_	_
33	_	_
]	_	_
demonstrate	_	_
that	_	_
such	_	_
method	_	_
can	_	_
help	_	_
generating	_	_
sharper	_	_
images	_	_
as	_	_
well	_	_
as	_	_
alleviate	_	_
the	_	_
mode	_	_
collapse	_	_
problem	_	_
.	_	_

#67
Using	_	_
auxiliary	_	_
classifiers	_	_
can	_	_
also	_	_
help	_	_
in	_	_
applications	_	_
such	_	_
as	_	_
text-to-image	_	_
synthesis	_	_
[	_	_
34	_	_
]	_	_
and	_	_
image-to-image	_	_
translation	_	_
[	_	_
35	_	_
]	_	_
.	_	_

#68
G	_	_
D	_	_
C	_	_
z	_	_
G	_	_
(	_	_
y	_	_
,	_	_
z	_	_
)	_	_
x	_	_
ŷ	_	_
True/Fakey	_	_
Fig.	_	_
2	_	_
.	_	_

#69
Architecture	_	_
of	_	_
GAN	_	_
with	_	_
auxiliary	_	_
classifier	_	_
,	_	_
where	_	_
y	_	_
is	_	_
the	_	_
conditional	_	_
input	_	_
label	_	_
and	_	_
C	_	_
is	_	_
the	_	_
classifier	_	_
that	_	_
takes	_	_
the	_	_
synthetic	_	_
image	_	_
G	_	_
(	_	_
y	_	_
,	_	_
z	_	_
)	_	_
as	_	_
input	_	_
and	_	_
predict	_	_
its	_	_
label	_	_
ŷ	_	_

#70
2.3	_	_
GAN	_	_
with	_	_
Encoder	_	_

#71
Although	_	_
GAN	_	_
can	_	_
transform	_	_
a	_	_
noise	_	_
vector	_	_
z	_	_
into	_	_
a	_	_
synthetic	_	_
data	_	_
sample	_	_
G	_	_
(	_	_
z	_	_
)	_	_
,	_	_
it	_	_
does	_	_
not	_	_
allow	_	_
inverse	_	_
transformation	_	_
.	_	_

#72
If	_	_
we	_	_
treat	_	_
the	_	_
noise	_	_
distribution	_	_
as	_	_
a	_	_
latent	_	_
feature	_	_
space	_	_
for	_	_
data	_	_
samples	_	_
,	_	_
GAN	_	_
lacks	_	_
the	_	_
ability	_	_
to	_	_
map	_	_
data	_	_
sample	_	_
x	_	_
into	_	_
latent	_	_
feature	_	_
z	_	_
.	_	_

#73
In	_	_
order	_	_
to	_	_
allow	_	_
such	_	_
mapping	_	_
,	_	_
two	_	_
concurrent	_	_
works	_	_
BiGAN	_	_
[	_	_
36	_	_
]	_	_
and	_	_
ALI	_	_
[	_	_
37	_	_
]	_	_
propose	_	_
to	_	_
add	_	_
an	_	_
encoder	_	_
E	_	_
in	_	_
the	_	_
original	_	_
GAN	_	_
framework	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
3	_	_
.	_	_

#74
Let	_	_
Ωx	_	_
be	_	_
the	_	_
data	_	_
space	_	_
and	_	_
Ωz	_	_
be	_	_
the	_	_
latent	_	_
feature	_	_
space	_	_
,	_	_
the	_	_
encoder	_	_
E	_	_
takes	_	_
x	_	_
∈	_	_
Ωx	_	_
as	_	_
input	_	_
and	_	_
produce	_	_
a	_	_
feature	_	_
vector	_	_
E	_	_
(	_	_
x	_	_
)	_	_
∈	_	_
Ωz	_	_
as	_	_
output	_	_
.	_	_

#75
The	_	_
discriminator	_	_
D	_	_
is	_	_
modified	_	_
to	_	_
take	_	_
both	_	_
a	_	_
data	_	_
sample	_	_
and	_	_
a	_	_
feature	_	_
vector	_	_
as	_	_
input	_	_
to	_	_
calculate	_	_
P	_	_
(	_	_
Y	_	_
|x	_	_
,	_	_
z	_	_
)	_	_
,	_	_
where	_	_
Y	_	_
=	_	_
1	_	_
indicates	_	_
the	_	_
sample	_	_
is	_	_
real	_	_
and	_	_
Y	_	_
=	_	_
0	_	_
means	_	_
the	_	_
data	_	_
is	_	_
generated	_	_
by	_	_
G.	_	_
G	_	_
D	_	_
E	_	_
z	_	_
x	_	_
G	_	_
(	_	_
z	_	_
)	_	_
E	_	_
(	_	_
x	_	_
)	_	_
True/Fake	_	_
Features	_	_
Data	_	_
x	_	_
,	_	_
E	_	_
(	_	_
x	_	_
)	_	_
G	_	_
(	_	_
z	_	_
)	_	_
,	_	_
z	_	_
Fig.	_	_
3	_	_
.	_	_

#76
Architecture	_	_
of	_	_
BiGAN/ALI	_	_
The	_	_
objective	_	_
is	_	_
thus	_	_
defined	_	_
as	_	_
:	_	_
min	_	_
G	_	_
,	_	_
E	_	_
max	_	_
D	_	_
V	_	_
(	_	_
G	_	_
,	_	_
E	_	_
,	_	_
D	_	_
)	_	_
=	_	_
Ex∼pdata	_	_
(	_	_
x	_	_
)	_	_
logD	_	_
(	_	_
x	_	_
,	_	_
E	_	_
(	_	_
x	_	_
)	_	_
)	_	_
+	_	_
Ez∼pz	_	_
(	_	_
z	_	_
)	_	_
log	_	_
(	_	_
1−D	_	_
(	_	_
G	_	_
(	_	_
z	_	_
)	_	_
,	_	_
z	_	_
)	_	_
)	_	_
(	_	_
2	_	_
)	_	_

#77
2.4	_	_
GAN	_	_
with	_	_
Variational	_	_
Auto-Encoder	_	_

#78
E	_	_
G	_	_
x	_	_
z	_	_
⇠	_	_
E	_	_
(	_	_
x	_	_
)	_	_
G	_	_
(	_	_
z	_	_
)	_	_
True/Fake	_	_
VAE	_	_
GAN	_	_
Fig.	_	_
4	_	_
.	_	_

#79
Architecture	_	_
of	_	_
VAE-GAN	_	_
VAE-GAN	_	_
[	_	_
38	_	_
]	_	_
proposes	_	_
to	_	_
combine	_	_
Variational	_	_
Auto-Encoder	_	_
(	_	_
VAE	_	_
)	_	_
[	_	_
39	_	_
]	_	_
with	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
to	_	_
exploit	_	_
both	_	_
of	_	_
their	_	_
benefits	_	_
,	_	_
as	_	_
GAN	_	_
can	_	_
generate	_	_
sharp	_	_
images	_	_
but	_	_
often	_	_
miss	_	_
some	_	_
modes	_	_
while	_	_
images	_	_
produced	_	_
by	_	_
VAE	_	_
[	_	_
39	_	_
]	_	_
are	_	_
blurry	_	_
but	_	_
have	_	_
large	_	_
variety	_	_
.	_	_

#80
The	_	_
architecture	_	_
of	_	_
VAE-GAN	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
4	_	_
.	_	_

#81
The	_	_
VAE	_	_
part	_	_
regularize	_	_
the	_	_
encoder	_	_
E	_	_
by	_	_
imposing	_	_
a	_	_
prior	_	_
of	_	_
normal	_	_
distribution	_	_
(	_	_
e.g.	_	_
z	_	_
∼	_	_
N	_	_
(	_	_
0	_	_
,	_	_
1	_	_
)	_	_
)	_	_
,	_	_
and	_	_
the	_	_
VAE	_	_
loss	_	_
term	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LV	_	_
AE	_	_
=	_	_
−Ez∼q	_	_
(	_	_
z|x	_	_
)	_	_
log	_	_
[	_	_
p	_	_
(	_	_
x|z	_	_
)	_	_
]	_	_
+DKL	_	_
(	_	_
q	_	_
(	_	_
z|x	_	_
)	_	_
||p	_	_
(	_	_
x	_	_
)	_	_
)	_	_
,	_	_
(	_	_
3	_	_
)	_	_
where	_	_
z	_	_
∼	_	_
E	_	_
(	_	_
x	_	_
)	_	_
=	_	_
q	_	_
(	_	_
z|x	_	_
)	_	_
,	_	_
x	_	_
∼	_	_
G	_	_
(	_	_
z	_	_
)	_	_
=	_	_
p	_	_
(	_	_
x|z	_	_
)	_	_
and	_	_
DKL	_	_
is	_	_
the	_	_
Kullback-Leibler	_	_
divergence	_	_
.	_	_

#82
Also	_	_
,	_	_
VAE-GAN	_	_
[	_	_
38	_	_
]	_	_
proposes	_	_
to	_	_
represent	_	_
the	_	_
reconstruction	_	_
loss	_	_
of	_	_
VAE	_	_
in	_	_
terms	_	_
of	_	_
the	_	_
discriminator	_	_
D.	_	_
Let	_	_
Dl	_	_
(	_	_
x	_	_
)	_	_
denotes	_	_
the	_	_
representation	_	_
of	_	_
the	_	_
l-th	_	_
layer	_	_
of	_	_
the	_	_
discriminator	_	_
,	_	_
and	_	_
a	_	_
Gaussian	_	_
observation	_	_
model	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
:	_	_
p	_	_
(	_	_
D	_	_
(	_	_
x	_	_
)	_	_
|z	_	_
)	_	_
=	_	_
N	_	_
(	_	_
D	_	_
(	_	_
x	_	_
)	_	_
|D	_	_
(	_	_
x̃	_	_
)	_	_
,	_	_
I	_	_
)	_	_
,	_	_
(	_	_
4	_	_
)	_	_
where	_	_
x̃	_	_
∼	_	_
G	_	_
(	_	_
z	_	_
)	_	_
is	_	_
a	_	_
sample	_	_
from	_	_
the	_	_
generator	_	_
,	_	_
and	_	_
I	_	_
is	_	_
the	_	_
identity	_	_
matrix	_	_
.	_	_

#83
So	_	_
the	_	_
new	_	_
VAE	_	_
loss	_	_
is	_	_
:	_	_
LV	_	_
AE	_	_
=	_	_
−Ez∼q	_	_
(	_	_
z|x	_	_
)	_	_
log	_	_
[	_	_
p	_	_
(	_	_
D	_	_
(	_	_
x	_	_
)	_	_
|z	_	_
)	_	_
]	_	_
+DKL	_	_
(	_	_
q	_	_
(	_	_
z|x	_	_
)	_	_
||p	_	_
(	_	_
x	_	_
)	_	_
)	_	_
,	_	_
(	_	_
5	_	_
)	_	_
which	_	_
is	_	_
then	_	_
combined	_	_
with	_	_
the	_	_
GAN	_	_
loss	_	_
defined	_	_
in	_	_
Equation	_	_
1	_	_
.	_	_

#84
Experiments	_	_
demonstrate	_	_
that	_	_
VAE-GAN	_	_
can	_	_
generate	_	_
better	_	_
images	_	_
than	_	_
VAE	_	_
or	_	_
GAN	_	_
alone	_	_
.	_	_

#85
2.5	_	_
Handling	_	_
Mode	_	_
Collapse	_	_

#86
Although	_	_
GAN	_	_
is	_	_
very	_	_
effective	_	_
in	_	_
image	_	_
synthesis	_	_
,	_	_
its	_	_
training	_	_
process	_	_
is	_	_
very	_	_
unstable	_	_
and	_	_
requires	_	_
a	_	_
lot	_	_
of	_	_
tricks	_	_
to	_	_
get	_	_
a	_	_
good	_	_
result	_	_
,	_	_
as	_	_
pointed	_	_
out	_	_
in	_	_
[	_	_
4	_	_
]	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#87
Despite	_	_
its	_	_
instability	_	_
in	_	_
training	_	_
,	_	_
GAN	_	_
also	_	_
suffers	_	_
from	_	_
the	_	_
mode	_	_
collapse	_	_
problem	_	_
,	_	_
as	_	_
discussed	_	_
in	_	_
[	_	_
4	_	_
]	_	_
[	_	_
5	_	_
]	_	_
[	_	_
40	_	_
]	_	_
.	_	_

#88
In	_	_
the	_	_
original	_	_
GAN	_	_
formulation	_	_
[	_	_
4	_	_
]	_	_
,	_	_
the	_	_
discriminator	_	_
does	_	_
not	_	_
need	_	_
to	_	_
consider	_	_
the	_	_
variety	_	_
of	_	_
synthetic	_	_
samples	_	_
,	_	_
but	_	_
only	_	_
focuses	_	_
on	_	_
telling	_	_
whether	_	_
each	_	_
sample	_	_
is	_	_
realistic	_	_
or	_	_
not	_	_
,	_	_
which	_	_
makes	_	_
it	_	_
possible	_	_
for	_	_
the	_	_
generator	_	_
to	_	_
spend	_	_
efforts	_	_
in	_	_
generating	_	_
a	_	_
few	_	_
samples	_	_
that	_	_
are	_	_
good	_	_
enough	_	_
to	_	_
fool	_	_
the	_	_
discriminator	_	_
.	_	_

#89
For	_	_
example	_	_
,	_	_
although	_	_
the	_	_
MNIST	_	_
[	_	_
41	_	_
]	_	_
dataset	_	_
contains	_	_
images	_	_
of	_	_
digits	_	_
from	_	_
0	_	_
to	_	_
9	_	_
,	_	_
in	_	_
an	_	_
extreme	_	_
case	_	_
,	_	_
a	_	_
generator	_	_
only	_	_
needs	_	_
to	_	_
learn	_	_
to	_	_
generate	_	_
one	_	_
of	_	_
the	_	_
ten	_	_
digits	_	_
perfectly	_	_
to	_	_
completely	_	_
fool	_	_
the	_	_
discriminator	_	_
,	_	_
and	_	_
then	_	_
the	_	_
generator	_	_
stops	_	_
trying	_	_
to	_	_
generate	_	_
the	_	_
other	_	_
nine	_	_
digits	_	_
.	_	_

#90
The	_	_
absence	_	_
of	_	_
the	_	_
other	_	_
nine	_	_
digits	_	_
is	_	_
an	_	_
example	_	_
of	_	_
inter-class	_	_
mode	_	_
collapse	_	_
.	_	_

#91
An	_	_
example	_	_
of	_	_
intra-class	_	_
mode	_	_
collapse	_	_
is	_	_
,	_	_
there	_	_
are	_	_
many	_	_
writing	_	_
styles	_	_
for	_	_
each	_	_
of	_	_
the	_	_
digits	_	_
,	_	_
but	_	_
the	_	_
generator	_	_
only	_	_
learns	_	_
to	_	_
generate	_	_
one	_	_
perfect	_	_
sample	_	_
for	_	_
each	_	_
digit	_	_
to	_	_
successfully	_	_
fool	_	_
the	_	_
discriminator	_	_
.	_	_

#92
Many	_	_
methods	_	_
have	_	_
been	_	_
proposed	_	_
to	_	_
address	_	_
the	_	_
model	_	_
collapse	_	_
problem	_	_
.	_	_

#93
One	_	_
technique	_	_
is	_	_
called	_	_
minibatch	_	_
features	_	_
[	_	_
6	_	_
]	_	_
,	_	_
whose	_	_
idea	_	_
is	_	_
to	_	_
make	_	_
the	_	_
discriminator	_	_
compare	_	_
an	_	_
example	_	_
to	_	_
a	_	_
minibatch	_	_
of	_	_
true	_	_
samples	_	_
as	_	_
well	_	_
as	_	_
a	_	_
mini-batch	_	_
of	_	_
generated	_	_
samples	_	_
.	_	_

#94
In	_	_
this	_	_
way	_	_
,	_	_
the	_	_
discriminator	_	_
can	_	_
learn	_	_
to	_	_
tell	_	_
if	_	_
a	_	_
generated	_	_
sample	_	_
is	_	_
too	_	_
similar	_	_
to	_	_
some	_	_
other	_	_
generated	_	_
samples	_	_
by	_	_
measuring	_	_
samples’	_	_
distances	_	_
in	_	_
latent	_	_
space	_	_
.	_	_

#95
Although	_	_
this	_	_
method	_	_
works	_	_
well	_	_
,	_	_
as	_	_
discussed	_	_
in	_	_
[	_	_
14	_	_
]	_	_
,	_	_
the	_	_
performance	_	_
largely	_	_
depends	_	_
on	_	_
what	_	_
features	_	_
are	_	_
used	_	_
in	_	_
distance	_	_
calculation	_	_
.	_	_

#96
MRGAN	_	_
[	_	_
7	_	_
]	_	_
proposes	_	_
to	_	_
add	_	_
an	_	_
encoder	_	_
which	_	_
transforms	_	_
a	_	_
sample	_	_
in	_	_
data	_	_
space	_	_
back	_	_
to	_	_
latent	_	_
space	_	_
,	_	_
as	_	_
in	_	_
BiGAN	_	_
[	_	_
36	_	_
]	_	_
.	_	_

#97
The	_	_
combination	_	_
of	_	_
encoder	_	_
and	_	_
generator	_	_
acts	_	_
as	_	_
an	_	_
auto-encoder	_	_
,	_	_
whose	_	_
reconstruction	_	_
loss	_	_
is	_	_
added	_	_
to	_	_
the	_	_
adversarial	_	_
loss	_	_
to	_	_
act	_	_
as	_	_
a	_	_
mode	_	_
regularizer	_	_
.	_	_

#98
Meanwhile	_	_
,	_	_
the	_	_
discriminator	_	_
is	_	_
also	_	_
trained	_	_
to	_	_
discriminate	_	_
reconstructed	_	_
samples	_	_
,	_	_
which	_	_
acts	_	_
as	_	_
another	_	_
mode	_	_
regularizer	_	_
.	_	_

#99
WGAN	_	_
[	_	_
8	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
Wasserstein	_	_
distance	_	_
to	_	_
measure	_	_
the	_	_
similarity	_	_
between	_	_
true	_	_
data	_	_
distribution	_	_
and	_	_
the	_	_
learned	_	_
distribution	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
Jensen-Shannon	_	_
divergence	_	_
as	_	_
in	_	_
the	_	_
original	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#100
Although	_	_
it	_	_
theoretically	_	_
avoids	_	_
mode	_	_
collapse	_	_
,	_	_
it	_	_
takes	_	_
a	_	_
longer	_	_
time	_	_
for	_	_
the	_	_
model	_	_
to	_	_
converge	_	_
than	_	_
previous	_	_
GANs	_	_
.	_	_

#101
To	_	_
alleviate	_	_
this	_	_
problem	_	_
,	_	_
WGAN-GP	_	_
[	_	_
9	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
gradient	_	_
penalty	_	_
,	_	_
instead	_	_
of	_	_
weight	_	_
clipping	_	_
in	_	_
WGAN	_	_
.	_	_

#102
WGAN-GP	_	_
generally	_	_
produces	_	_
good	_	_
images	_	_
and	_	_
greatly	_	_
avoid	_	_
mode	_	_
collapse	_	_
,	_	_
and	_	_
it	_	_
is	_	_
easy	_	_
to	_	_
apply	_	_
this	_	_
training	_	_
framework	_	_
to	_	_
other	_	_
GAN	_	_
models	_	_
.	_	_

#103
More	_	_
tips	_	_
for	_	_
training	_	_
GANs	_	_
can	_	_
be	_	_
found	_	_
in	_	_
Soumith’s	_	_
NIPS	_	_
2016	_	_
tutorial	_	_
“How	_	_
to	_	_
train	_	_
a	_	_
GAN”1	_	_
.	_	_

#104
1.	_	_
https	_	_
:	_	_
//github.com/soumith/ganhacks	_	_
Generator	_	_
Generator	_	_
1Input	_	_
Output	_	_
Input	_	_
Generator	_	_
2	_	_
Output	_	_
Input	_	_
Generator	_	_
1	_	_
Generator	_	_
2	_	_
Generator	_	_
k	_	_
Output	_	_
Discriminator	_	_
Discriminator	_	_
2Discriminator	_	_
1	_	_
Discriminator	_	_
2Discriminator	_	_
1	_	_
Discriminator	_	_
k	_	_
(	_	_
a	_	_
)	_	_
Direct	_	_
method	_	_
(	_	_
b	_	_
)	_	_
Hierarchical	_	_
method	_	_
(	_	_
c	_	_
)	_	_
Iterative	_	_
method	_	_
Structure/Background	_	_
Texture/Foreground	_	_
(	_	_
different	_	_
structures	_	_
,	_	_
not	_	_
share	_	_
weights	_	_
)	_	_
(	_	_
same	_	_
or	_	_
similar	_	_
structures	_	_
,	_	_
may	_	_
share	_	_
weights	_	_
)	_	_
low	_	_
resolution	_	_
/	_	_
rough	_	_
details	_	_
high	_	_
resolution	_	_
/	_	_
sharp	_	_
details	_	_
Discriminator	_	_
k-1	_	_
Generator	_	_
k-1	_	_
Fig.	_	_
5	_	_
.	_	_

#105
Three	_	_
approaches	_	_
of	_	_
image	_	_
synthesis	_	_
using	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
.	_	_

#106
The	_	_
direct	_	_
method	_	_
does	_	_
everything	_	_
with	_	_
only	_	_
one	_	_
generator	_	_
and	_	_
one	_	_
discriminator	_	_
,	_	_
while	_	_
the	_	_
other	_	_
two	_	_
methods	_	_
have	_	_
multiple	_	_
generators	_	_
and	_	_
discriminators	_	_
.	_	_

#107
Hierarchical	_	_
method	_	_
usually	_	_
use	_	_
two	_	_
layers	_	_
of	_	_
GANs	_	_
,	_	_
where	_	_
each	_	_
GAN	_	_
plays	_	_
a	_	_
fundamentally	_	_
different	_	_
role	_	_
than	_	_
the	_	_
other	_	_
one	_	_
.	_	_

#108
Iterative	_	_
method	_	_
,	_	_
however	_	_
,	_	_
contains	_	_
multiple	_	_
GANs	_	_
that	_	_
perform	_	_
the	_	_
same	_	_
task	_	_
but	_	_
operate	_	_
at	_	_
different	_	_
resolution	_	_
.	_	_

#109
Transposed	_	_
convolution	_	_
Batch	_	_
normalization	_	_
ReLU	_	_
activation	_	_
Convolution	_	_
Batch	_	_
normalization	_	_
Leaky	_	_
ReLU	_	_
activation	_	_
Generator	_	_
Discriminator	_	_
Fig.	_	_
6	_	_
.	_	_

#110
Building	_	_
blocks	_	_
of	_	_
DCGAN	_	_
,	_	_
where	_	_
the	_	_
generator	_	_
uses	_	_
transposed	_	_
convolution	_	_
,	_	_
batch-normalization	_	_
and	_	_
ReLU	_	_
activation	_	_
,	_	_
while	_	_
the	_	_
discriminator	_	_
uses	_	_
convolution	_	_
,	_	_
batch-normalization	_	_
and	_	_
LeakyReLU	_	_
activation	_	_

#111
3	_	_
GENERAL	_	_
APPROACHES	_	_
OF	_	_
IMAGE	_	_
SYNTHESIS	_	_

#112
WITH	_	_
GAN	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
summarize	_	_
the	_	_
three	_	_
main	_	_
approaches	_	_
used	_	_
in	_	_
generating	_	_
images	_	_
,	_	_
i.e.	_	_
direct	_	_
methods	_	_
,	_	_
iterative	_	_
methods	_	_
and	_	_
hierarchical	_	_
methods	_	_
respectively	_	_
,	_	_
which	_	_
form	_	_
the	_	_
basis	_	_
of	_	_
all	_	_
applications	_	_
mentioned	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#113
The	_	_
overall	_	_
structures	_	_
of	_	_
these	_	_
three	_	_
methods	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
5	_	_
.	_	_

#114
3.1	_	_
Direct	_	_
Methods	_	_

#115
All	_	_
methods	_	_
under	_	_
this	_	_
category	_	_
follows	_	_
the	_	_
philosophy	_	_
of	_	_
using	_	_
one	_	_
generator	_	_
and	_	_
one	_	_
discriminator	_	_
in	_	_
their	_	_
models	_	_
,	_	_
and	_	_
the	_	_
structures	_	_
of	_	_
the	_	_
generator	_	_
and	_	_
the	_	_
discriminator	_	_
are	_	_
straight-forward	_	_
without	_	_
branches	_	_
.	_	_

#116
Many	_	_
of	_	_
the	_	_
earliest	_	_
GAN	_	_
models	_	_
fall	_	_
into	_	_
this	_	_
category	_	_
,	_	_
like	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
,	_	_
DCGAN	_	_
[	_	_
5	_	_
]	_	_
,	_	_
ImprovedGAN	_	_
[	_	_
6	_	_
]	_	_
,	_	_
InfoGAN	_	_
[	_	_
11	_	_
]	_	_
,	_	_
f-GAN	_	_
[	_	_
42	_	_
]	_	_
and	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#117
Among	_	_
them	_	_
,	_	_
DCGAN	_	_
is	_	_
one	_	_
of	_	_
the	_	_
most	_	_
classic	_	_
ones	_	_
whose	_	_
structure	_	_
is	_	_
used	_	_
by	_	_
many	_	_
later	_	_
models	_	_
such	_	_
as	_	_
[	_	_
11	_	_
]	_	_
[	_	_
30	_	_
]	_	_
[	_	_
43	_	_
]	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#118
The	_	_
general	_	_
building	_	_
blocks	_	_
used	_	_
in	_	_
DCGAN	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
6	_	_
,	_	_
where	_	_
the	_	_
generator	_	_
uses	_	_
transposed	_	_
convolution	_	_
,	_	_
batch-normalization	_	_
and	_	_
ReLU	_	_
activation	_	_
,	_	_
while	_	_
the	_	_
discriminator	_	_
uses	_	_
convolution	_	_
,	_	_
batch-normalization	_	_
and	_	_
LeakyReLU	_	_
activation	_	_
.	_	_

#119
This	_	_
kind	_	_
of	_	_
method	_	_
is	_	_
relatively	_	_
more	_	_
straight-forward	_	_
to	_	_
design	_	_
and	_	_
implement	_	_
when	_	_
compared	_	_
with	_	_
hierarchical	_	_
and	_	_
iterative	_	_
methods	_	_
,	_	_
and	_	_
it	_	_
usually	_	_
achieves	_	_
good	_	_
results	_	_
.	_	_

#120
3.2	_	_
Hierarchical	_	_
Methods	_	_

#121
Contrary	_	_
to	_	_
the	_	_
Direct	_	_
Method	_	_
,	_	_
algorithms	_	_
under	_	_
the	_	_
Hierarchical	_	_
Method	_	_
use	_	_
two	_	_
generators	_	_
and	_	_
two	_	_
discriminators	_	_
in	_	_
their	_	_
models	_	_
,	_	_
where	_	_
different	_	_
generators	_	_
have	_	_
different	_	_
purposes	_	_
.	_	_

#122
The	_	_
idea	_	_
behind	_	_
those	_	_
methods	_	_
is	_	_
to	_	_
separate	_	_
an	_	_
image	_	_
into	_	_
two	_	_
parts	_	_
,	_	_
like	_	_
“styles	_	_
&	_	_
structure”	_	_
and	_	_
“foreground	_	_
&	_	_
background”	_	_
.	_	_

#123
The	_	_
relation	_	_
between	_	_
the	_	_
two	_	_
generators	_	_
may	_	_
be	_	_
either	_	_
parallel	_	_
or	_	_
sequential	_	_
.	_	_

#124
SS-GAN	_	_
[	_	_
44	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
two	_	_
GANs	_	_
,	_	_
a	_	_
Structure-GAN	_	_
for	_	_
generating	_	_
a	_	_
surface	_	_
normal	_	_
map	_	_
from	_	_
random	_	_
noise	_	_
ẑ	_	_
,	_	_
and	_	_
another	_	_
Style-GAN	_	_
that	_	_
takes	_	_
both	_	_
the	_	_
generated	_	_
surface	_	_
normal	_	_
map	_	_
as	_	_
well	_	_
as	_	_
a	_	_
noise	_	_
z̃	_	_
as	_	_
input	_	_
and	_	_
outputs	_	_
an	_	_
image	_	_
.	_	_

#125
The	_	_
Structure-GAN	_	_
uses	_	_
the	_	_
same	_	_
building	_	_
blocks	_	_
as	_	_
DCGAN	_	_
[	_	_
5	_	_
]	_	_
,	_	_
while	_	_
the	_	_
Style-GAN	_	_
is	_	_
slightly	_	_
different	_	_
.	_	_

#126
For	_	_
Style-Generator	_	_
,	_	_
the	_	_
generated	_	_
surface	_	_
normal	_	_
map	_	_
and	_	_
the	_	_
noise	_	_
vector	_	_
go	_	_
through	_	_
several	_	_
convolutional	_	_
and	_	_
transposed	_	_
convolutional	_	_
layers	_	_
respectively	_	_
,	_	_
and	_	_
then	_	_
the	_	_
results	_	_
are	_	_
concatenated	_	_
into	_	_
a	_	_
single	_	_
tensor	_	_
which	_	_
will	_	_
go	_	_
through	_	_
the	_	_
remaining	_	_
layers	_	_
in	_	_
Style-Generator	_	_
.	_	_

#127
As	_	_
for	_	_
the	_	_
Style-Discriminator	_	_
,	_	_
each	_	_
surface	_	_
normal	_	_
map	_	_
and	_	_
its	_	_
corresponding	_	_
image	_	_
are	_	_
concatenated	_	_
at	_	_
the	_	_
channel	_	_
dimension	_	_
to	_	_
form	_	_
a	_	_
single	_	_
input	_	_
to	_	_
the	_	_
discriminator	_	_
.	_	_

#128
Besides	_	_
,	_	_
SS-GAN	_	_
assumes	_	_
that	_	_
,	_	_
a	_	_
good	_	_
synthetic	_	_
image	_	_
should	deontic	_
also	_	_
be	_	_
used	_	_
to	_	_
reconstruct	_	_
a	_	_
good	_	_
surface	_	_
normal	_	_
map	_	_
.	_	_

#129
Under	_	_
this	_	_
assumption	_	_
,	_	_
SS-GAN	_	_
designs	_	_
a	_	_
fully-connected	_	_
network	_	_
that	_	_
transforms	_	_
an	_	_
image	_	_
back	_	_
to	_	_
its	_	_
surface	_	_
normal	_	_
map	_	_
,	_	_
and	_	_
uses	_	_
a	_	_
pixel-wise	_	_
loss	_	_
that	_	_
enforces	_	_
the	_	_
reconstructed	_	_
surface	_	_
normal	_	_
to	_	_
approximate	_	_
the	_	_
true	_	_
one	_	_
.	_	_

#130
A	_	_
main	_	_
limitation	_	_
of	_	_
SS-GAN	_	_
is	_	_
that	_	_
it	_	_
requires	_	_
to	_	_
use	_	_
Kinect	_	_
to	_	_
obtain	_	_
groundtruth	_	_
for	_	_
surface	_	_
normal	_	_
maps	_	_
.	_	_

#131
As	_	_
a	_	_
special	_	_
example	_	_
,	_	_
LR-GAN	_	_
[	_	_
45	_	_
]	_	_
chooses	_	_
to	_	_
generate	_	_
the	_	_
foreground	_	_
and	_	_
background	_	_
content	_	_
using	_	_
different	_	_
generator	_	_
,	_	_
but	_	_
only	_	_
one	_	_
discriminator	_	_
is	_	_
used	_	_
to	_	_
judge	_	_
the	_	_
images	_	_
while	_	_
the	_	_
recurrent	_	_
image	_	_
generation	_	_
process	_	_
is	_	_
related	_	_
to	_	_
the	_	_
iterative	_	_
method	_	_
.	_	_

#132
Nonetheless	_	_
,	_	_
experiments	_	_
of	_	_
LR-GAN	_	_
demonstrate	_	_
that	_	_
it	_	_
is	_	_
possible	_	_
to	_	_
separate	_	_
the	_	_
generation	_	_
of	_	_
foreground	_	_
and	_	_
background	_	_
content	_	_
and	_	_
produce	_	_
sharper	_	_
images	_	_
.	_	_

#133
3.3	_	_
Iterative	_	_
Methods	_	_

#134
This	_	_
method	_	_
differentiates	_	_
itself	_	_
from	_	_
Hierarchical	_	_
Methods	_	_
in	_	_
two	_	_
ways	_	_
.	_	_

#135
First	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
two	_	_
different	_	_
generators	_	_
that	_	_
perform	_	_
different	_	_
roles	_	_
,	_	_
the	_	_
models	_	_
in	_	_
this	_	_
category	_	_
use	_	_
multiple	_	_
generators	_	_
that	_	_
have	_	_
similar	_	_
or	_	_
even	_	_
the	_	_
same	_	_
structures	_	_
,	_	_
and	_	_
they	_	_
generate	_	_
images	_	_
from	_	_
coarse	_	_
to	_	_
fine	_	_
,	_	_
with	_	_
each	_	_
generator	_	_
refining	_	_
the	_	_
details	_	_
of	_	_
the	_	_
results	_	_
from	_	_
the	_	_
previous	_	_
generator	_	_
.	_	_

#136
Second	_	_
,	_	_
when	_	_
using	_	_
the	_	_
same	_	_
structures	_	_
in	_	_
the	_	_
generators	_	_
,	_	_
Iterative	_	_
Methods	_	_
can	_	_
use	_	_
weight-sharing	_	_
among	_	_
the	_	_
generators	_	_
[	_	_
45	_	_
]	_	_
,	_	_
while	_	_
Hierarchical	_	_
Methods	_	_
usually	_	_
can	_	_
not	_	_
.	_	_

#137
LAPGAN	_	_
[	_	_
40	_	_
]	_	_
is	_	_
the	_	_
first	_	_
GAN	_	_
that	_	_
uses	_	_
an	_	_
iterative	_	_
method	_	_
to	_	_
generate	_	_
images	_	_
from	_	_
coarse	_	_
to	_	_
fine	_	_
using	_	_
Laplacian	_	_
pyramid	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#138
The	_	_
multiple	_	_
generators	_	_
in	_	_
LAPGAN	_	_
perform	_	_
the	_	_
same	_	_
task	_	_
:	_	_
takes	_	_
an	_	_
image	_	_
from	_	_
previous	_	_
generator	_	_
and	_	_
a	_	_
noise	_	_
vector	_	_
as	_	_
input	_	_
,	_	_
and	_	_
then	_	_
outputs	_	_
the	_	_
details	_	_
(	_	_
a	_	_
residual	_	_
image	_	_
)	_	_
that	_	_
can	_	_
make	_	_
the	_	_
image	_	_
sharper	_	_
when	_	_
added	_	_
to	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#139
The	_	_
only	_	_
difference	_	_
in	_	_
the	_	_
structures	_	_
of	_	_
those	_	_
generators	_	_
is	_	_
the	_	_
size	_	_
of	_	_
input/output	_	_
dimension	_	_
,	_	_
while	_	_
an	_	_
exception	_	_
is	_	_
that	_	_
the	_	_
generator	_	_
at	_	_
the	_	_
lowest	_	_
level	_	_
only	_	_
takes	_	_
a	_	_
noise	_	_
vector	_	_
as	_	_
input	_	_
and	_	_
outputs	_	_
an	_	_
image	_	_
.	_	_

#140
LAPGAN	_	_
outperforms	_	_
the	_	_
original	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
and	_	_
shows	_	_
that	_	_
iterative	_	_
method	_	_
can	_	_
generate	_	_
sharper	_	_
images	_	_
than	_	_
direct	_	_
method	_	_
.	_	_

#141
StackGAN	_	_
[	_	_
29	_	_
]	_	_
,	_	_
as	_	_
an	_	_
iterative	_	_
method	_	_
,	_	_
has	_	_
only	_	_
two	_	_
layers	_	_
of	_	_
generators	_	_
.	_	_

#142
First	_	_
generator	_	_
takes	_	_
an	_	_
input	_	_
(	_	_
z	_	_
,	_	_
c	_	_
)	_	_
and	_	_
then	_	_
outputs	_	_
a	_	_
blurry	_	_
image	_	_
that	_	_
can	_	_
show	_	_
a	_	_
rough	_	_
shape	_	_
and	_	_
blurry	_	_
details	_	_
of	_	_
the	_	_
objects	_	_
,	_	_
while	_	_
the	_	_
second	_	_
generator	_	_
takes	_	_
(	_	_
z	_	_
,	_	_
c	_	_
)	_	_
and	_	_
the	_	_
image	_	_
generated	_	_
by	_	_
the	_	_
previous	_	_
generator	_	_
and	_	_
then	_	_
output	_	_
a	_	_
larger	_	_
image	_	_
with	_	_
more	_	_
photo-realistic	_	_
details	_	_
.	_	_

#143
Another	_	_
example	_	_
of	_	_
Iterative	_	_
Methods	_	_
is	_	_
SGAN	_	_
[	_	_
47	_	_
]	_	_
which	_	_
stacks	_	_
generators	_	_
that	_	_
takes	_	_
lower	_	_
level	_	_
feature	_	_
as	_	_
input	_	_
and	_	_
outputs	_	_
higher	_	_
level	_	_
features	_	_
,	_	_
while	_	_
the	_	_
bottom	_	_
generator	_	_
takes	_	_
a	_	_
noise	_	_
vector	_	_
as	_	_
input	_	_
and	_	_
the	_	_
top	_	_
generator	_	_
outputs	_	_
an	_	_
image	_	_
.	_	_

#144
The	_	_
necessity	_	_
of	_	_
using	_	_
separate	_	_
generators	_	_
for	_	_
different	_	_
levels	_	_
of	_	_
features	_	_
is	_	_
that	_	_
SGAN	_	_
associates	_	_
an	_	_
encoder	_	_
,	_	_
a	_	_
discriminator	_	_
and	_	_
a	_	_
Q-network	_	_
[	_	_
47	_	_
]	_	_
(	_	_
which	_	_
is	_	_
used	_	_
to	_	_
predict	_	_
the	_	_
posterior	_	_
probability	_	_
P	_	_
(	_	_
zi|hi	_	_
)	_	_
for	_	_
entropy	_	_
maximization	_	_
,	_	_
where	_	_
hi	_	_
is	_	_
the	_	_
output	_	_
feature	_	_
of	_	_
the	_	_
i-th	_	_
layer	_	_
)	_	_
for	_	_
each	_	_
generator	_	_
,	_	_
so	_	_
as	_	_
to	_	_
constrain	_	_
and	_	_
improve	_	_
the	_	_
quality	_	_
of	_	_
those	_	_
features	_	_
.	_	_

#145
An	_	_
example	_	_
of	_	_
using	_	_
weight-sharing	_	_
is	_	_
the	_	_
GRAN	_	_
[	_	_
48	_	_
]	_	_
model	_	_
,	_	_
which	_	_
is	_	_
an	_	_
extension	_	_
to	_	_
the	_	_
DRAW	_	_
[	_	_
49	_	_
]	_	_
model	_	_
which	_	_
is	_	_
based	_	_
on	_	_
variational	_	_
autoencoder	_	_
[	_	_
39	_	_
]	_	_
.	_	_

#146
As	_	_
in	_	_
DRAW	_	_
,	_	_
GRAN	_	_
generates	_	_
an	_	_
image	_	_
in	_	_
a	_	_
recurrent	_	_
way	_	_
that	_	_
feeds	_	_
the	_	_
output	_	_
of	_	_
the	_	_
previous	_	_
step	_	_
into	_	_
the	_	_
model	_	_
and	_	_
the	_	_
output	_	_
of	_	_
the	_	_
current	_	_
step	_	_
will	_	_
be	_	_
fed	_	_
back	_	_
as	_	_
the	_	_
input	_	_
in	_	_
the	_	_
next	_	_
step	_	_
.	_	_

#147
All	_	_
steps	_	_
use	_	_
the	_	_
same	_	_
generator	_	_
,	_	_
so	_	_
the	_	_
weights	_	_
are	_	_
shared	_	_
among	_	_
them	_	_
,	_	_
just	_	_
like	_	_
classic	_	_
Recurrent	_	_
Neural	_	_
Network	_	_
(	_	_
RNN	_	_
)	_	_
.	_	_

#148
3.4	_	_
Other	_	_
Methods	_	_

#149
PPGN	_	_
[	_	_
50	_	_
]	_	_
produces	_	_
impressive	_	_
images	_	_
in	_	_
several	_	_
tasks	_	_
,	_	_
such	_	_
as	_	_
class-conditioned	_	_
image	_	_
synthesis	_	_
[	_	_
28	_	_
]	_	_
,	_	_
text-to-image	_	_
synthesis	_	_
[	_	_
30	_	_
]	_	_
and	_	_
image	_	_
inpainting	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#150
Different	_	_
from	_	_
other	_	_
methods	_	_
mentioned	_	_
earlier	_	_
,	_	_
PPGN	_	_
uses	_	_
activation	_	_
maximization	_	_
[	_	_
51	_	_
]	_	_
to	_	_
generate	_	_
images	_	_
,	_	_
and	_	_
it	_	_
is	_	_
based	_	_
on	_	_
sampling	_	_
with	_	_
a	_	_
prior	_	_
learned	_	_
with	_	_
denoising	_	_
autoencoder	_	_
(	_	_
DAE	_	_
)	_	_
[	_	_
52	_	_
]	_	_
.	_	_

#151
To	_	_
generate	_	_
an	_	_
image	_	_
conditioned	_	_
on	_	_
a	_	_
certain	_	_
class	_	_
label	_	_
y	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
a	_	_
feed-forward	_	_
way	_	_
(	_	_
e.g.	_	_
recurrent	_	_
methods	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
feed-forward	_	_
if	_	_
unfolded	_	_
through	_	_
time	_	_
)	_	_
,	_	_
PPGN	_	_
runs	_	_
an	_	_
optimization	_	_
process	_	_
that	_	_
finds	_	_
an	_	_
input	_	_
z	_	_
to	_	_
the	_	_
generator	_	_
that	_	_
makes	_	_
the	_	_
output	_	_
image	_	_
highly	_	_
activate	_	_
a	_	_
certain	_	_
neuron	_	_
in	_	_
another	_	_
pretrained	_	_
classifier	_	_
(	_	_
in	_	_
this	_	_
case	_	_
,	_	_
the	_	_
neuron	_	_
in	_	_
the	_	_
output	_	_
layer	_	_
that	_	_
corresponds	_	_
to	_	_
its	_	_
class	_	_
label	_	_
y	_	_
)	_	_
.	_	_

#152
In	_	_
order	_	_
to	_	_
generate	_	_
better	_	_
higher	_	_
resolution	_	_
images	_	_
,	_	_
ProgressiveGAN	_	_
[	_	_
53	_	_
]	_	_
proposes	_	_
to	_	_
start	_	_
with	_	_
training	_	_
a	_	_
generator	_	_
and	_	_
discriminator	_	_
of	_	_
4×4	_	_
pixels	_	_
,	_	_
after	_	_
which	_	_
it	_	_
incrementally	_	_
adds	_	_
extra	_	_
layers	_	_
that	_	_
doubles	_	_
the	_	_
output	_	_
resolution	_	_
up	_	_
to	_	_
1024	_	_
×	_	_
1024	_	_
.	_	_

#153
This	_	_
approach	_	_
allows	_	_
the	_	_
model	_	_
to	_	_
learn	_	_
coarse	_	_
structure	_	_
first	_	_
and	_	_
then	_	_
focus	_	_
on	_	_
refining	_	_
details	_	_
later	_	_
,	_	_
instead	_	_
of	_	_
having	_	_
to	_	_
deal	_	_
with	_	_
all	_	_
details	_	_
at	_	_
different	_	_
scale	_	_
simultaneously	_	_
.	_	_

#154
4	_	_
TEXT-TO-IMAGE	_	_
SYNTHESIS	_	_

#155
When	_	_
we	_	_
apply	_	_
GAN	_	_
to	_	_
image	_	_
synthesis	_	_
,	_	_
it	_	_
is	_	_
desired	_	_
to	_	_
control	_	_
the	_	_
content	_	_
of	_	_
generated	_	_
images	_	_
.	_	_

#156
Although	_	_
there	_	_
are	_	_
label-conditioned	_	_
GAN	_	_
models	_	_
like	_	_
cGAN	_	_
[	_	_
28	_	_
]	_	_
which	_	_
can	_	_
generate	_	_
images	_	_
belong	_	_
to	_	_
a	_	_
specific	_	_
class	_	_
,	_	_
it	_	_
remains	_	_
a	_	_
great	_	_
challenge	_	_
to	_	_
generate	_	_
images	_	_
based	_	_
on	_	_
text	_	_
descriptions	_	_
.	_	_

#157
Text-to-image	_	_
synthesis	_	_
is	_	_
kind	_	_
of	_	_
the	_	_
holy	_	_
grail	_	_
of	_	_
computer	_	_
vision	_	_
,	_	_
since	_	_
if	_	_
an	_	_
algorithm	_	_
can	_	_
generate	_	_
truly	_	_
realistic	_	_
images	_	_
from	_	_
mere	_	_
text	_	_
descriptions	_	_
,	_	_
we	_	_
can	_	_
have	_	_
a	_	_
high	_	_
confidence	_	_
that	_	_
the	_	_
algorithm	_	_
actually	_	_
understands	_	_
what	_	_
is	_	_
in	_	_
the	_	_
images	_	_
,	_	_
where	_	_
computer	_	_
vision	_	_
is	_	_
about	_	_
teaching	_	_
computers	_	_
to	_	_
see	_	_
and	_	_
understand	_	_
visual	_	_
contents	_	_
in	_	_
the	_	_
real	_	_
world	_	_
.	_	_

#158
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
provides	_	_
the	_	_
first	_	_
attempt	_	_
of	_	_
using	_	_
GAN	_	_
to	_	_
generate	_	_
images	_	_
from	_	_
text	_	_
descriptions	_	_
.	_	_

#159
The	_	_
idea	_	_
is	_	_
similar	_	_
to	_	_
conditional	_	_
GAN	_	_
that	_	_
concatenates	_	_
the	_	_
condition	_	_
vector	_	_
with	_	_
the	_	_
noise	_	_
vector	_	_
,	_	_
but	_	_
with	_	_
the	_	_
difference	_	_
of	_	_
using	_	_
the	_	_
embedding	_	_
of	_	_
text	_	_
sentences	_	_
instead	_	_
of	_	_
class	_	_
labels	_	_
or	_	_
attributes	_	_
.	_	_

#160
The	_	_
embedding	_	_
method	_	_
used	_	_
in	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
is	_	_
from	_	_
another	_	_
paper	_	_
[	_	_
54	_	_
]	_	_
that	_	_
tries	_	_
to	_	_
learn	_	_
robust	_	_
embeddings	_	_
of	_	_
images	_	_
and	_	_
sentences	_	_
given	_	_
the	_	_
imagesentence	_	_
pairs	_	_
.	_	_

#161
Except	_	_
for	_	_
the	_	_
sentence	_	_
embedding	_	_
method	_	_
,	_	_
the	_	_
generator	_	_
of	_	_
GAN-INT-CLS	_	_
follows	_	_
the	_	_
same	_	_
architecture	_	_
of	_	_
DCGAN	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#162
As	_	_
for	_	_
the	_	_
discriminator	_	_
,	_	_
in	_	_
order	_	_
to	_	_
take	_	_
into	_	_
account	_	_
the	_	_
text	_	_
description	_	_
,	_	_
the	_	_
text	_	_
embedding	_	_
vector	_	_
of	_	_
length	_	_
K	_	_
is	_	_
spatially	_	_
replicated	_	_
to	_	_
become	_	_
a	_	_
text	_	_
embedding	_	_
tensor	_	_
of	_	_
shape	_	_
[	_	_
W	_	_
×H	_	_
×K	_	_
]	_	_
,	_	_
where	_	_
W	_	_
and	_	_
H	_	_
are	_	_
the	_	_
width	_	_
and	_	_
height	_	_
of	_	_
the	_	_
generated	_	_
image’s	_	_
feature	_	_
tensor	_	_
after	_	_
going	_	_
through	_	_
several	_	_
convolutional	_	_
layers	_	_
in	_	_
the	_	_
discriminator	_	_
.	_	_

#163
Then	_	_
the	_	_
text	_	_
embedding	_	_
tensor	_	_
is	_	_
combined	_	_
with	_	_
the	_	_
image	_	_
feature	_	_
tensor	_	_
of	_	_
shape	_	_
[	_	_
W	_	_
×H×C	_	_
]	_	_
at	_	_
the	_	_
channel	_	_
dimension	_	_
C	_	_
and	_	_
forms	_	_
a	_	_
new	_	_
tensor	_	_
of	_	_
shape	_	_
[	_	_
W×H×	_	_
(	_	_
C+K	_	_
)	_	_
]	_	_
,	_	_
which	_	_
then	_	_
goes	_	_
through	_	_
the	_	_
rest	_	_
layers	_	_
of	_	_
the	_	_
discriminator	_	_
.	_	_

#164
The	_	_
intuition	_	_
behind	_	_
this	_	_
approach	_	_
is	_	_
that	_	_
,	_	_
by	_	_
spatial	_	_
replication	_	_
and	_	_
depth	_	_
concatenation	_	_
,	_	_
each	_	_
“pixel”	_	_
(	_	_
of	_	_
shape	_	_
[	_	_
1	_	_
×	_	_
1	_	_
×	_	_
(	_	_
C	_	_
+	_	_
K	_	_
)	_	_
]	_	_
)	_	_
of	_	_
the	_	_
image’s	_	_
feature	_	_
tensor	_	_
contains	_	_
all	_	_
the	_	_
information	_	_
of	_	_
text	_	_
description	_	_
,	_	_
and	_	_
then	_	_
the	_	_
convolutional	_	_
layers	_	_
can	_	_
learn	_	_
to	_	_
align	_	_
the	_	_
image’s	_	_
content	_	_
with	_	_
certain	_	_
parts	_	_
of	_	_
the	_	_
text	_	_
feature	_	_
by	_	_
using	_	_
multiple	_	_
convolution	_	_
kernels	_	_
.	_	_

#165
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
also	_	_
proposes	_	_
to	_	_
distinguish	_	_
between	_	_
two	_	_
sources	_	_
of	_	_
errors	_	_
:	_	_
unrealistic	_	_
image	_	_
with	_	_
any	_	_
text	_	_
,	_	_
and	_	_
realistic	_	_
image	_	_
with	_	_
mismatched	_	_
text	_	_
.	_	_

#166
To	_	_
train	_	_
the	_	_
discriminator	_	_
to	_	_
distinguish	_	_
these	_	_
two	_	_
kinds	_	_
of	_	_
errors	_	_
,	_	_
three	_	_
types	_	_
of	_	_
input	_	_
are	_	_
fed	_	_
into	_	_
the	_	_
discriminator	_	_
at	_	_
every	_	_
training	_	_
step	_	_
:	_	_
{	_	_
real	_	_
image	_	_
,	_	_
right	_	_
text	_	_
}	_	_
,	_	_
{	_	_
real	_	_
image	_	_
,	_	_
wrong	_	_
text	_	_
}	_	_
and	_	_
{	_	_
fake	_	_
image	_	_
,	_	_
right	_	_
text	_	_
}	_	_
.	_	_

#167
The	_	_
experimental	_	_
results	_	_
in	_	_
[	_	_
30	_	_
]	_	_
show	_	_
that	_	_
such	_	_
training	_	_
technique	_	_
is	_	_
important	_	_
in	_	_
generating	_	_
high	_	_
quality	_	_
images	_	_
,	_	_
since	_	_
it	_	_
tells	_	_
the	_	_
model	_	_
not	_	_
only	_	_
how	_	_
to	_	_
generate	_	_
realistic	_	_
images	_	_
,	_	_
but	_	_
also	_	_
the	_	_
correspondence	_	_
between	_	_
text	_	_
and	_	_
images	_	_
.	_	_

#168
In	_	_
addition	_	_
,	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
manifold	_	_
interpolation	_	_
to	_	_
obtain	_	_
more	_	_
text	_	_
embeddings	_	_
,	_	_
by	_	_
simply	_	_
interpolating	_	_
between	_	_
captions	_	_
in	_	_
the	_	_
training	_	_
set	_	_
.	_	_

#169
Since	_	_
the	_	_
number	_	_
of	_	_
captions	_	_
for	_	_
each	_	_
image	_	_
is	_	_
usually	_	_
no	_	_
more	_	_
than	_	_
five	_	_
and	_	_
an	_	_
image	_	_
can	_	_
be	_	_
described	_	_
in	_	_
many	_	_
ways	_	_
,	_	_
doing	_	_
such	_	_
interpolation	_	_
allows	_	_
the	_	_
model	_	_
to	_	_
learn	_	_
from	_	_
possible	_	_
text	_	_
descriptions	_	_
that	_	_
are	_	_
not	_	_
in	_	_
the	_	_
training	_	_
set	_	_
.	_	_

#170
According	_	_
to	_	_
the	_	_
authors	_	_
,	_	_
the	_	_
interpolated	_	_
text	_	_
embeddings	_	_
need	_	_
not	_	_
correspond	_	_
to	_	_
actual	_	_
human-written	_	_
text	_	_
,	_	_
so	_	_
there	_	_
is	_	_
no	_	_
extra	_	_
labeling	_	_
required	_	_
.	_	_

#171
TAC-GAN	_	_
[	_	_
34	_	_
]	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
and	_	_
AC-GAN	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#172
With	_	_
the	_	_
auxiliary	_	_
classifier	_	_
,	_	_
TAC-GAN	_	_
is	_	_
able	_	_
to	_	_
achieve	_	_
higher	_	_
Inception	_	_
Score	_	_
[	_	_
6	_	_
]	_	_
than	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
and	_	_
StackGAN	_	_
[	_	_
29	_	_
]	_	_
on	_	_
the	_	_
Oxford-102	_	_
[	_	_
55	_	_
]	_	_
daataset	_	_
.	_	_

#173
4.1	_	_
Text-to-Image	_	_
with	_	_
Location	_	_
Constraints	_	_

#174
Although	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
and	_	_
StackGAN	_	_
[	_	_
29	_	_
]	_	_
can	_	_
generate	_	_
images	_	_
based	_	_
on	_	_
text	_	_
description	_	_
,	_	_
they	_	_
fail	_	_
to	_	_
capture	_	_
the	_	_
localization	_	_
constraints	_	_
of	_	_
the	_	_
objects	_	_
in	_	_
images	_	_
.	_	_

#175
To	_	_
allow	_	_
encoding	_	_
spatial	_	_
constraints	_	_
,	_	_
Reed	_	_
et	_	_
al.	_	_
propose	_	_
GAWWN	_	_
[	_	_
31	_	_
]	_	_
that	_	_
presents	_	_
two	_	_
possible	_	_
solutions	_	_
.	_	_

#176
The	_	_
first	_	_
approach	_	_
proposed	_	_
in	_	_
GAWWN	_	_
[	_	_
31	_	_
]	_	_
is	_	_
to	_	_
learn	_	_
a	_	_
bounding	_	_
box	_	_
for	_	_
an	_	_
object	_	_
by	_	_
putting	_	_
a	_	_
spatially	_	_
replicated	_	_
text	_	_
embedding	_	_
tensor	_	_
through	_	_
a	_	_
Spatial	_	_
Transformer	_	_
Network	_	_
[	_	_
56	_	_
]	_	_
.	_	_

#177
Here	_	_
the	_	_
spatial	_	_
replication	_	_
is	_	_
the	_	_
same	_	_
process	_	_
mentioned	_	_
in	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#178
The	_	_
output	_	_
of	_	_
spatial	_	_
transformer	_	_
network	_	_
is	_	_
a	_	_
tensor	_	_
with	_	_
the	_	_
same	_	_
dimension	_	_
as	_	_
input	_	_
,	_	_
but	_	_
values	_	_
outside	_	_
of	_	_
the	_	_
bounding	_	_
are	_	_
all	_	_
zeros	_	_
.	_	_

#179
The	_	_
output	_	_
tensor	_	_
of	_	_
the	_	_
spatial	_	_
transformer	_	_
goes	_	_
through	_	_
several	_	_
convolutional	_	_
layers	_	_
to	_	_
reduce	_	_
its	_	_
size	_	_
back	_	_
to	_	_
a	_	_
1dimensional	_	_
vector	_	_
,	_	_
which	_	_
not	_	_
only	_	_
preserves	_	_
the	_	_
information	_	_
of	_	_
text	_	_
but	_	_
also	_	_
provides	_	_
a	_	_
constraint	_	_
on	_	_
object’s	_	_
location	_	_
by	_	_
the	_	_
bounding	_	_
box	_	_
.	_	_

#180
A	_	_
benefit	_	_
of	_	_
this	_	_
approach	_	_
is	_	_
that	_	_
it	_	_
is	_	_
end-to-end	_	_
,	_	_
without	_	_
requiring	_	_
additional	_	_
input	_	_
.	_	_

#181
The	_	_
second	_	_
approach	_	_
proposed	_	_
in	_	_
GAWWN	_	_
[	_	_
31	_	_
]	_	_
is	_	_
to	_	_
use	_	_
user-specified	_	_
keypoints	_	_
to	_	_
constrain	_	_
the	_	_
different	_	_
parts	_	_
(	_	_
e.g.	_	_
head	_	_
,	_	_
leg	_	_
,	_	_
arm	_	_
,	_	_
tail	_	_
,	_	_
etc	_	_
.	_	_
)	_	_

#182
of	_	_
the	_	_
object	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#183
For	_	_
each	_	_
keypoint	_	_
,	_	_
a	_	_
mask	_	_
matrix	_	_
is	_	_
generated	_	_
where	_	_
the	_	_
keypoint	_	_
position	_	_
is	_	_
1	_	_
and	_	_
others	_	_
0	_	_
,	_	_
and	_	_
all	_	_
the	_	_
matrices	_	_
are	_	_
combined	_	_
through	_	_
depth	_	_
concatenation	_	_
to	_	_
form	_	_
a	_	_
mask	_	_
tensor	_	_
of	_	_
shape	_	_
[	_	_
M	_	_
×	_	_
M	_	_
×	_	_
K	_	_
]	_	_
,	_	_
where	_	_
M	_	_
is	_	_
the	_	_
size	_	_
of	_	_
the	_	_
mask	_	_
and	_	_
K	_	_
is	_	_
the	_	_
number	_	_
of	_	_
keypoints	_	_
.	_	_

#184
The	_	_
tensor	_	_
is	_	_
then	_	_
flattened	_	_
into	_	_
a	_	_
binary	_	_
matrix	_	_
with	_	_
1	_	_
indicating	_	_
the	_	_
presence	_	_
of	_	_
a	_	_
keypoint	_	_
and	_	_
0	_	_
otherwise	_	_
,	_	_
and	_	_
then	_	_
replicated	_	_
depth-wise	_	_
to	_	_
become	_	_
a	_	_
tensor	_	_
to	_	_
be	_	_
fed	_	_
into	_	_
remaining	_	_
layers	_	_
.	_	_

#185
Although	_	_
this	_	_
approach	_	_
allows	_	_
more	_	_
detailed	_	_
constraints	_	_
on	_	_
the	_	_
object	_	_
,	_	_
it	_	_
requires	_	_
extra	_	_
user	_	_
input	_	_
to	_	_
specify	_	_
the	_	_
keypoints	_	_
.	_	_

#186
Even	_	_
if	_	_
GAWWN	_	_
can	_	_
infer	_	_
unobserved	_	_
keypoints	_	_
from	_	_
a	_	_
few	_	_
user-specified	_	_
keypoints	_	_
so	_	_
that	_	_
the	_	_
user	_	_
does	_	_
not	_	_
need	_	_
to	_	_
specify	_	_
all	_	_
keypoints	_	_
,	_	_
the	_	_
cost	_	_
of	_	_
extra	_	_
user	_	_
input	_	_
is	_	_
still	_	_
non-trivial	_	_
.	_	_

#187
The	_	_
remaining	_	_
part	_	_
of	_	_
GAWWN	_	_
[	_	_
31	_	_
]	_	_
is	_	_
similar	_	_
to	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
,	_	_
with	_	_
the	_	_
difference	_	_
of	_	_
using	_	_
a	_	_
separate	_	_
pathway	_	_
to	_	_
process	_	_
bounding	_	_
box	_	_
tensor	_	_
or	_	_
keypoint	_	_
tensor	_	_
.	_	_

#188
Although	_	_
GAWWN	_	_
provides	_	_
two	_	_
approaches	_	_
that	_	_
can	_	_
enforce	_	_
location	_	_
constraints	_	_
on	_	_
the	_	_
generated	_	_
images	_	_
,	_	_
it	_	_
only	_	_
works	_	_
on	_	_
images	_	_
with	_	_
single	_	_
objects	_	_
,	_	_
since	_	_
neither	_	_
of	_	_
the	_	_
proposed	_	_
methods	_	_
is	_	_
able	_	_
to	_	_
handle	_	_
several	_	_
different	_	_
objects	_	_
in	_	_
an	_	_
image	_	_
.	_	_

#189
From	_	_
the	_	_
result	_	_
shown	_	_
in	_	_
[	_	_
31	_	_
]	_	_
,	_	_
GAWWN	_	_
works	_	_
well	_	_
on	_	_
the	_	_
CUB	_	_
dataset	_	_
[	_	_
57	_	_
]	_	_
,	_	_
while	_	_
the	_	_
synthetic	_	_
images	_	_
generated	_	_
from	_	_
the	_	_
model	_	_
trained	_	_
on	_	_
MPII	_	_
Human	_	_
Pose	_	_
(	_	_
MHP	_	_
)	_	_
dataset	_	_
[	_	_
58	_	_
]	_	_
are	_	_
very	_	_
blurry	_	_
and	_	_
it	_	_
is	_	_
hard	_	_
to	_	_
tell	_	_
what	_	_
the	_	_
content	_	_
is	_	_
.	_	_

#190
This	_	_
may	_	_
be	_	_
due	_	_
to	_	_
the	_	_
fact	_	_
that	_	_
the	_	_
poses	_	_
of	_	_
a	_	_
standing	_	_
bird	_	_
are	_	_
very	_	_
similar	_	_
to	_	_
each	_	_
other	_	_
(	_	_
note	_	_
that	_	_
birds	_	_
in	_	_
the	_	_
CUB	_	_
dataset	_	_
are	_	_
at	_	_
standing	_	_
poses	_	_
)	_	_
,	_	_
while	_	_
a	_	_
human’s	_	_
poses	_	_
can	_	_
be	_	_
of	_	_
uncountable	_	_
types	_	_
.	_	_

#191
The	_	_
main	_	_
benefit	_	_
of	_	_
specifying	_	_
the	_	_
locations	_	_
of	_	_
each	_	_
part	_	_
of	_	_
the	_	_
objects	_	_
is	_	_
that	_	_
it	_	_
yields	_	_
more	_	_
interpretable	_	_
results	_	_
,	_	_
and	_	_
that	_	_
it	_	_
is	_	_
desirable	_	_
that	_	_
the	_	_
model	_	_
can	_	_
understand	_	_
the	_	_
concepts	_	_
of	_	_
different	_	_
parts	_	_
of	_	_
objects	_	_
,	_	_
which	_	_
is	_	_
one	_	_
of	_	_
the	_	_
ultimate	_	_
goals	_	_
of	_	_
computer	_	_
vision	_	_
.	_	_

#192
4.2	_	_
Text-to-Image	_	_
with	_	_
Stacked	_	_
GANs	_	_

#193
Instead	_	_
of	_	_
using	_	_
only	_	_
one	_	_
generator	_	_
,	_	_
StackGAN	_	_
[	_	_
29	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
two	_	_
different	_	_
generators	_	_
for	_	_
text-to-image	_	_
synthesis	_	_
.	_	_

#194
The	_	_
first	_	_
generator	_	_
is	_	_
responsible	_	_
for	_	_
generating	_	_
low-resolution	_	_
images	_	_
that	_	_
contain	_	_
rough	_	_
shapes	_	_
and	_	_
colors	_	_
of	_	_
objects	_	_
,	_	_
while	_	_
the	_	_
second	_	_
generator	_	_
takes	_	_
the	_	_
output	_	_
of	_	_
the	_	_
first	_	_
generator	_	_
and	_	_
produces	_	_
images	_	_
with	_	_
higher	_	_
resolution	_	_
and	_	_
sharper	_	_
details	_	_
.	_	_

#195
Each	_	_
generator	_	_
is	_	_
associated	_	_
with	_	_
its	_	_
own	_	_
discriminator	_	_
.	_	_

#196
Besides	_	_
,	_	_
StackGAN	_	_
also	_	_
proposes	_	_
a	_	_
conditional	_	_
data	_	_
augmentation	_	_
technique	_	_
to	_	_
produce	_	_
more	_	_
text	_	_
embeddings	_	_
for	_	_
the	_	_
generator	_	_
.	_	_

#197
StackGAN	_	_
randomly	_	_
samples	_	_
from	_	_
a	_	_
Gaussian	_	_
distribution	_	_
N	_	_
(	_	_
µ	_	_
(	_	_
φt	_	_
)	_	_
,	_	_
Σ	_	_
(	_	_
φt	_	_
)	_	_
)	_	_
,	_	_
where	_	_
the	_	_
mean	_	_
vector	_	_
µ	_	_
(	_	_
φt	_	_
)	_	_
and	_	_
diagonal	_	_
variance	_	_
matrix	_	_
Σ	_	_
(	_	_
φt	_	_
)	_	_
are	_	_
functions	_	_
of	_	_
text	_	_
embedding	_	_
φt	_	_
.	_	_

#198
To	_	_
further	_	_
enforce	_	_
the	_	_
smoothness	_	_
over	_	_
conditional	_	_
manifold	_	_
,	_	_
N	_	_
(	_	_
µ	_	_
(	_	_
φt	_	_
)	_	_
,	_	_
Σ	_	_
(	_	_
φt	_	_
)	_	_
)	_	_
is	_	_
constrained	_	_
to	_	_
approximate	_	_
a	_	_
standard	_	_
Gaussian	_	_
distribution	_	_
N	_	_
(	_	_
0	_	_
,	_	_
1	_	_
)	_	_
by	_	_
adding	_	_
a	_	_
Kullback-Leibler	_	_
divergence	_	_
regularization	_	_
term	_	_
.	_	_

#199
As	_	_
an	_	_
improved	_	_
version	_	_
of	_	_
StackGAN	_	_
,	_	_
StackGAN++	_	_
[	_	_
59	_	_
]	_	_
proposes	_	_
to	_	_
have	_	_
more	_	_
pairs	_	_
of	_	_
generators	_	_
and	_	_
discriminators	_	_
instead	_	_
of	_	_
just	_	_
two	_	_
,	_	_
adds	_	_
an	_	_
unconditional	_	_
image	_	_
synthesis	_	_
loss	_	_
to	_	_
the	_	_
discriminators	_	_
,	_	_
and	_	_
uses	_	_
a	_	_
colorconsistency	_	_
regularization	_	_
term	_	_
calculated	_	_
by	_	_
mean-square	_	_
loss	_	_
of	_	_
the	_	_
means	_	_
and	_	_
variances	_	_
between	_	_
real	_	_
and	_	_
fake	_	_
images	_	_
.	_	_

#200
AttnGAN	_	_
[	_	_
60	_	_
]	_	_
further	_	_
extends	_	_
the	_	_
architecture	_	_
of	_	_
StackGAN++	_	_
[	_	_
59	_	_
]	_	_
by	_	_
using	_	_
attention	_	_
mechanism	_	_
over	_	_
image	_	_
and	_	_
text	_	_
features	_	_
.	_	_

#201
In	_	_
AttnGAM	_	_
,	_	_
each	_	_
sentence	_	_
is	_	_
embedded	_	_
into	_	_
a	_	_
global	_	_
sentence	_	_
vector	_	_
and	_	_
each	_	_
word	_	_
of	_	_
the	_	_
sentence	_	_
is	_	_
also	_	_
embedded	_	_
into	_	_
a	_	_
word	_	_
vector	_	_
.	_	_

#202
The	_	_
global	_	_
sentence	_	_
vector	_	_
is	_	_
used	_	_
to	_	_
generate	_	_
a	_	_
low	_	_
resolution	_	_
image	_	_
at	_	_
the	_	_
first	_	_
stage	_	_
,	_	_
and	_	_
then	_	_
the	_	_
following	_	_
stages	_	_
use	_	_
the	_	_
input	_	_
image	_	_
features	_	_
from	_	_
the	_	_
previous	_	_
stage	_	_
and	_	_
the	_	_
word	_	_
vectors	_	_
as	_	_
input	_	_
to	_	_
the	_	_
attention	_	_
layer	_	_
and	_	_
calculate	_	_
a	_	_
word-context	_	_
vector	_	_
which	_	_
will	_	_
be	_	_
combined	_	_
with	_	_
the	_	_
image	_	_
features	_	_
and	_	_
form	_	_
the	_	_
input	_	_
to	_	_
the	_	_
generator	_	_
that	_	_
will	_	_
generate	_	_
new	_	_
image	_	_
features	_	_
.	_	_

#203
Besides	_	_
,	_	_
AttnGAN	_	_
also	_	_
proposes	_	_
a	_	_
Deep	_	_
Attentional	_	_
Multimodal	_	_
Similarity	_	_
Model	_	_
(	_	_
DAMSM	_	_
)	_	_
that	_	_
uses	_	_
attention	_	_
layers	_	_
to	_	_
compute	_	_
the	_	_
similarity	_	_
between	_	_
images	_	_
and	_	_
text	_	_
using	_	_
both	_	_
global	_	_
sentence	_	_
vectors	_	_
as	_	_
well	_	_
as	_	_
fine-grained	_	_
word	_	_
vectors	_	_
,	_	_
which	_	_
provides	_	_
an	_	_
additional	_	_
fine-grained	_	_
imagetext	_	_
matching	_	_
loss	_	_
for	_	_
training	_	_
the	_	_
generator	_	_
.	_	_

#204
Experiments	_	_
of	_	_
AttnGAN	_	_
not	_	_
only	_	_
show	_	_
the	_	_
effectiveness	_	_
of	_	_
using	_	_
attention	_	_
layers	_	_
in	_	_
image	_	_
synthesis	_	_
,	_	_
but	_	_
also	_	_
make	_	_
the	_	_
model	_	_
more	_	_
interpretable	_	_
.	_	_

#205
With	_	_
stacked	_	_
generators	_	_
,	_	_
StackGAN	_	_
[	_	_
29	_	_
]	_	_
,	_	_
StackGAN++	_	_
[	_	_
59	_	_
]	_	_
and	_	_
AttnGAN	_	_
[	_	_
60	_	_
]	_	_
produce	_	_
sharper	_	_
images	_	_
than	_	_
GAN-INT-CLS	_	_
[	_	_
30	_	_
]	_	_
and	_	_
GAWWN	_	_
[	_	_
31	_	_
]	_	_
on	_	_
the	_	_
CUB	_	_
[	_	_
57	_	_
]	_	_
and	_	_
Oxford-102	_	_
[	_	_
55	_	_
]	_	_
datasets	_	_
.	_	_

#206
Although	_	_
AttnGAN	_	_
[	_	_
60	_	_
]	_	_
claims	_	_
to	_	_
have	_	_
significantly	_	_
higher	_	_
Inception	_	_
Score	_	_
[	_	_
6	_	_
]	_	_
than	_	_
PPGN	_	_
[	_	_
50	_	_
]	_	_
on	_	_
the	_	_
COCO	_	_
[	_	_
61	_	_
]	_	_
dataset	_	_
,	_	_
the	_	_
examples	_	_
it	_	_
provides	_	_
do	_	_
not	_	_
visually	_	_
look	_	_
apparently	_	_
better	_	_
.	_	_

#207
4.3	_	_
Text-to-Image	_	_
by	_	_
Iterative	_	_
Sampling	_	_

#208
Different	_	_
from	_	_
previous	_	_
approaches	_	_
that	_	_
directly	_	_
incorporate	_	_
the	_	_
text	_	_
information	_	_
in	_	_
the	_	_
generation	_	_
process	_	_
,	_	_
PPGN	_	_
[	_	_
50	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
activation	_	_
maximization	_	_
[	_	_
51	_	_
]	_	_
method	_	_
to	_	_
generate	_	_
images	_	_
in	_	_
an	_	_
iterative	_	_
sampling	_	_
way	_	_
.	_	_

#209
The	_	_
model	_	_
contains	_	_
two	_	_
separate	_	_
parts	_	_
:	_	_
a	_	_
pretrained	_	_
image	_	_
captioning	_	_
model	_	_
,	_	_
and	_	_
an	_	_
image	_	_
generator	_	_
.	_	_

#210
The	_	_
image	_	_
generator	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
denoising	_	_
auto-encoder	_	_
and	_	_
GAN	_	_
,	_	_
trained	_	_
independent	_	_
of	_	_
the	_	_
image	_	_
captioning	_	_
model	_	_
.	_	_

#211
Let	_	_
p	_	_
(	_	_
x	_	_
)	_	_
be	_	_
the	_	_
distribution	_	_
of	_	_
images	_	_
,	_	_
and	_	_
p	_	_
(	_	_
y	_	_
)	_	_
be	_	_
the	_	_
distribution	_	_
of	_	_
text	_	_
descriptions	_	_
,	_	_
we	_	_
want	_	_
to	_	_
sample	_	_
image	_	_
from	_	_
the	_	_
joint	_	_
distribution	_	_
p	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
.	_	_

#212
PPGN	_	_
factorizes	_	_
the	_	_
joint	_	_
distribution	_	_
into	_	_
two	_	_
factors	_	_
:	_	_
p	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
=	_	_
p	_	_
(	_	_
x	_	_
)	_	_
p	_	_
(	_	_
y|x	_	_
)	_	_
,	_	_
where	_	_
p	_	_
(	_	_
x	_	_
)	_	_
is	_	_
modeled	_	_
by	_	_
the	_	_
generator	_	_
,	_	_
and	_	_
p	_	_
(	_	_
y|x	_	_
)	_	_
is	_	_
modeled	_	_
by	_	_
the	_	_
image	_	_
captioning	_	_
model	_	_
.	_	_

#213
According	_	_
to	_	_
the	_	_
paper	_	_
[	_	_
50	_	_
]	_	_
,	_	_
given	_	_
a	_	_
trained	_	_
image	_	_
generator	_	_
and	_	_
image	_	_
captioning	_	_
model	_	_
,	_	_
the	_	_
following	_	_
iterative	_	_
sampling	_	_
process	_	_
is	_	_
performed	_	_
to	_	_
obtain	_	_
an	_	_
image	_	_
x	_	_
based	_	_
on	_	_
the	_	_
description	_	_
y∗	_	_
:	_	_
xt+1	_	_
=	_	_
xt	_	_
+	_	_
ε1	_	_
∂	_	_
log	_	_
p	_	_
(	_	_
xt	_	_
)	_	_
∂xt	_	_
+	_	_
ε2	_	_
∂	_	_
log	_	_
p	_	_
(	_	_
y	_	_
=	_	_
y∗|xt	_	_
)	_	_
∂xt	_	_
+N	_	_
(	_	_
0	_	_
,	_	_
ε23	_	_
)	_	_
,	_	_
(	_	_
6	_	_
)	_	_
where	_	_
the	_	_
ε1	_	_
term	_	_
takes	_	_
a	_	_
step	_	_
from	_	_
current	_	_
image	_	_
xt	_	_
to	_	_
a	_	_
more	_	_
realistic	_	_
image	_	_
(	_	_
regardless	_	_
of	_	_
the	_	_
text	_	_
description	_	_
)	_	_
,	_	_
the	_	_
ε2	_	_
term	_	_
takes	_	_
a	_	_
step	_	_
from	_	_
current	_	_
image	_	_
to	_	_
an	_	_
image	_	_
that	_	_
better	_	_
matches	_	_
the	_	_
description	_	_
y∗	_	_
(	_	_
here	_	_
the	_	_
LRCN	_	_
model	_	_
[	_	_
62	_	_
]	_	_
is	_	_
used	_	_
for	_	_
the	_	_
p	_	_
(	_	_
y	_	_
=	_	_
y∗|x	_	_
)	_	_
term	_	_
)	_	_
,	_	_
and	_	_
the	_	_
ε3	_	_
term	_	_
adds	_	_
a	_	_
small	_	_
noise	_	_
to	_	_
allow	_	_
for	_	_
a	_	_
broader	_	_
search	_	_
in	_	_
the	_	_
latent	_	_
space	_	_
.	_	_

#214
Although	_	_
such	_	_
iterative	_	_
sampling	_	_
method	_	_
takes	_	_
more	_	_
time	_	_
to	_	_
generate	_	_
an	_	_
image	_	_
in	_	_
test	_	_
phrase	_	_
,	_	_
it	_	_
can	_	_
generate	_	_
higher	_	_
resolution	_	_
images	_	_
with	_	_
better	_	_
quality	_	_
than	_	_
previous	_	_
methods	_	_
like	_	_
[	_	_
30	_	_
]	_	_
[	_	_
31	_	_
]	_	_
[	_	_
29	_	_
]	_	_
,	_	_
and	_	_
its	_	_
performance	_	_
is	_	_
among	_	_
the	_	_
best	_	_
in	_	_
both	_	_
class-conditioned	_	_
and	_	_
text-conditioned	_	_
image	_	_
synthesis	_	_
.	_	_

#215
4.4	_	_
Limitations	_	_
of	_	_
Current	_	_
Text-to-Image	_	_
Models	_	_

#216
Present	_	_
text-to-image	_	_
models	_	_
perform	_	_
well	_	_
on	_	_
datasets	_	_
with	_	_
single	_	_
object	_	_
per	_	_
image	_	_
,	_	_
such	_	_
as	_	_
human	_	_
faces	_	_
in	_	_
CelebA	_	_
[	_	_
63	_	_
]	_	_
,	_	_
birds	_	_
in	_	_
CUB	_	_
[	_	_
57	_	_
]	_	_
,	_	_
flowers	_	_
in	_	_
Oxford-102	_	_
[	_	_
55	_	_
]	_	_
,	_	_
and	_	_
some	_	_
objects	_	_
in	_	_
ImageNet	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#217
Also	_	_
,	_	_
they	_	_
can	_	_
synthesize	_	_
reasonable	_	_
images	_	_
for	_	_
scenes	_	_
like	_	_
bedrooms	_	_
and	_	_
living	_	_
rooms	_	_
in	_	_
the	_	_
LSUN	_	_
[	_	_
64	_	_
]	_	_
,	_	_
even	_	_
though	_	_
the	_	_
objects	_	_
in	_	_
the	_	_
scenes	_	_
lack	_	_
sharp	_	_
details	_	_
.	_	_

#218
However	_	_
,	_	_
all	_	_
present	_	_
models	_	_
work	_	_
badly	_	_
in	_	_
situation	_	_
where	_	_
multiple	_	_
complicated	_	_
objects	_	_
are	_	_
involved	_	_
in	_	_
one	_	_
image	_	_
,	_	_
as	_	_
it	_	_
is	_	_
in	_	_
the	_	_
MS-COCO	_	_
dataset	_	_
[	_	_
61	_	_
]	_	_
.	_	_

#219
A	_	_
plausible	_	_
reason	_	_
why	_	_
current	_	_
models	_	_
fail	_	_
to	_	_
work	_	_
well	_	_
on	_	_
complicated	_	_
images	_	_
is	_	_
that	_	_
the	_	_
models	_	_
only	_	_
learn	_	_
the	_	_
overall	_	_
features	_	_
of	_	_
an	_	_
image	_	_
,	_	_
instead	_	_
of	_	_
learning	_	_
the	_	_
concept	_	_
of	_	_
each	_	_
kind	_	_
of	_	_
objects	_	_
in	_	_
it	_	_
.	_	_

#220
This	_	_
gives	_	_
an	_	_
explanation	_	_
why	_	_
synthetic	_	_
scenes	_	_
of	_	_
bedrooms	_	_
and	_	_
living	_	_
rooms	_	_
lack	_	_
sharp	_	_
details	_	_
,	_	_
since	_	_
the	_	_
model	_	_
do	_	_
not	_	_
distinguish	_	_
between	_	_
a	_	_
bed	_	_
and	_	_
a	_	_
desk	_	_
,	_	_
all	_	_
it	_	_
sees	_	_
is	_	_
that	_	_
some	_	_
patterns	_	_
of	_	_
shapes	_	_
and	_	_
colors	_	_
should	deontic	_
be	_	_
put	_	_
somewhere	_	_
in	_	_
the	_	_
synthetic	_	_
image	_	_
.	_	_

#221
In	_	_
other	_	_
words	_	_
,	_	_
the	_	_
model	_	_
does	_	_
not	_	_
really	_	_
understand	_	_
the	_	_
image	_	_
,	_	_
but	_	_
just	_	_
remembers	_	_
where	_	_
to	_	_
put	_	_
some	_	_
shapes	_	_
and	_	_
colors	_	_
.	_	_

#222
Generative	_	_
Adversarial	_	_
Network	_	_
certainly	_	_
provides	_	_
us	_	_
a	_	_
promising	_	_
way	_	_
to	_	_
do	_	_
text-to-image	_	_
synthesis	_	_
,	_	_
since	_	_
it	_	_
produces	_	_
sharper	_	_
images	_	_
than	_	_
any	_	_
other	_	_
generative	_	_
methods	_	_
so	_	_
far	_	_
.	_	_

#223
To	_	_
take	_	_
a	_	_
further	_	_
step	_	_
in	_	_
text-to-image	_	_
synthesis	_	_
,	_	_
we	_	_
need	_	_
to	_	_
figure	_	_
out	_	_
novel	_	_
ways	_	_
to	_	_
teach	_	_
the	_	_
algorithms	_	_
the	_	_
concepts	_	_
of	_	_
things	_	_
.	_	_

#224
One	_	_
possible	_	_
way	_	_
is	_	_
to	_	_
train	_	_
separate	_	_
models	_	_
that	_	_
can	_	_
generate	_	_
different	_	_
kinds	_	_
of	_	_
objects	_	_
,	_	_
and	_	_
then	_	_
train	_	_
another	_	_
model	_	_
that	_	_
learns	_	_
how	_	_
to	_	_
combine	_	_
different	_	_
objects	_	_
(	_	_
i.e.	_	_
the	_	_
reasonable	_	_
relations	_	_
between	_	_
objects	_	_
)	_	_
into	_	_
one	_	_
image	_	_
based	_	_
on	_	_
the	_	_
text	_	_
descriptions	_	_
.	_	_

#225
However	_	_
,	_	_
such	_	_
method	_	_
requires	_	_
large	_	_
training	_	_
sets	_	_
for	_	_
different	_	_
objects	_	_
,	_	_
and	_	_
another	_	_
large	_	_
dataset	_	_
that	_	_
contains	_	_
images	_	_
of	_	_
those	_	_
different	_	_
objects	_	_
,	_	_
which	_	_
is	_	_
hard	_	_
to	_	_
acquire	_	_
.	_	_

#226
Another	_	_
possible	_	_
direction	_	_
may	_	_
be	_	_
to	_	_
make	_	_
use	_	_
of	_	_
the	_	_
Capsule	_	_
idea	_	_
proposed	_	_
by	_	_
Hinton	_	_
et	_	_
al.	_	_
[	_	_
65	_	_
]	_	_
[	_	_
66	_	_
]	_	_
since	_	_
capsules	_	_
are	_	_
designed	_	_
to	_	_
capture	_	_
the	_	_
concepts	_	_
of	_	_
objects	_	_
,	_	_
but	_	_
how	_	_
to	_	_
efficiently	_	_
train	_	_
such	_	_
capsulebased	_	_
network	_	_
is	_	_
still	_	_
a	_	_
problem	_	_
to	_	_
be	_	_
solved	_	_
.	_	_

#227
5	_	_
IMAGE-TO-IMAGE	_	_
TRANSLATION	_	_

#228
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
discuss	_	_
another	_	_
type	_	_
of	_	_
image	_	_
synthesis	_	_
,	_	_
image-to-image	_	_
translation	_	_
,	_	_
which	_	_
takes	_	_
images	_	_
as	_	_
conditional	_	_
input	_	_
.	_	_

#229
Image-to-image	_	_
translation	_	_
is	_	_
defined	_	_
as	_	_
the	_	_
problem	_	_
of	_	_
translating	_	_
a	_	_
possible	_	_
representation	_	_
of	_	_
one	_	_
scene	_	_
into	_	_
another	_	_
,	_	_
such	_	_
as	_	_
mapping	_	_
BW	_	_
images	_	_
into	_	_
RGB	_	_
images	_	_
,	_	_
or	_	_
the	_	_
other	_	_
way	_	_
around	_	_
[	_	_
67	_	_
]	_	_
.	_	_

#230
This	_	_
problem	_	_
is	_	_
related	_	_
to	_	_
style	_	_
transfer	_	_
[	_	_
68	_	_
]	_	_
[	_	_
69	_	_
]	_	_
,	_	_
which	_	_
takes	_	_
a	_	_
content	_	_
image	_	_
and	_	_
a	_	_
style	_	_
image	_	_
and	_	_
output	_	_
an	_	_
image	_	_
with	_	_
the	_	_
content	_	_
of	_	_
the	_	_
content	_	_
image	_	_
and	_	_
the	_	_
style	_	_
of	_	_
the	_	_
style	_	_
image	_	_
.	_	_

#231
Image-to-image	_	_
translation	_	_
can	_	_
be	_	_
viewed	_	_
as	_	_
a	_	_
generalization	_	_
of	_	_
style	_	_
transfer	_	_
,	_	_
since	_	_
it	_	_
is	_	_
not	_	_
limited	_	_
to	_	_
transferring	_	_
the	_	_
styles	_	_
of	_	_
images	_	_
,	_	_
but	_	_
can	_	_
also	_	_
manipulate	_	_
attributes	_	_
of	_	_
objects	_	_
(	_	_
as	_	_
in	_	_
applications	_	_
of	_	_
face	_	_
editing	_	_
)	_	_
.	_	_

#232
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
introduce	_	_
several	_	_
models	_	_
that	_	_
work	_	_
for	_	_
general	_	_
image-to-image	_	_
translation	_	_
,	_	_
from	_	_
supervised	_	_
methods	_	_
to	_	_
unsupervised	_	_
ones	_	_
.	_	_

#233
The	_	_
“supervision”	_	_
here	_	_
means	_	_
that	_	_
for	_	_
each	_	_
image	_	_
in	_	_
the	_	_
source	_	_
domain	_	_
,	_	_
there	_	_
is	_	_
a	_	_
corresponding	_	_
ground-truth	_	_
image	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
.	_	_

#234
Later	_	_
we	_	_
will	_	_
turn	_	_
into	_	_
different	_	_
specific	_	_
applications	_	_
such	_	_
as	_	_
face	_	_
editing	_	_
,	_	_
image	_	_
super	_	_
resolution	_	_
,	_	_
image	_	_
in-painting	_	_
and	_	_
video	_	_
prediction	_	_
.	_	_

#235
5.1	_	_
Supervised	_	_
Image-to-Image	_	_
Translation	_	_

#236
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
proposes	_	_
to	_	_
combine	_	_
the	_	_
loss	_	_
of	_	_
a	_	_
conditional	_	_
Generative	_	_
Adversarial	_	_
Network	_	_
(	_	_
cGAN	_	_
)	_	_
with	_	_
L1	_	_
regularization	_	_
loss	_	_
,	_	_
so	_	_
that	_	_
the	_	_
generator	_	_
is	_	_
not	_	_
only	_	_
trained	_	_
to	_	_
fool	_	_
the	_	_
discriminator	_	_
but	_	_
also	_	_
to	_	_
generate	_	_
images	_	_
as	_	_
close	_	_
to	_	_
ground-truth	_	_
as	_	_
possible	_	_
.	_	_

#237
The	_	_
reason	_	_
for	_	_
using	_	_
L1	_	_
instead	_	_
of	_	_
L2	_	_
is	_	_
that	_	_
L1	_	_
produces	_	_
less	_	_
blurry	_	_
images	_	_
[	_	_
70	_	_
]	_	_
.	_	_

#238
The	_	_
conditional	_	_
GAN	_	_
loss	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LcGAN	_	_
(	_	_
G	_	_
,	_	_
D	_	_
)	_	_
=	_	_
Ex	_	_
,	_	_
y∼pdata	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
[	_	_
logD	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
]	_	_
+	_	_
Ex∼pdata	_	_
(	_	_
x	_	_
)	_	_
,	_	_
z∼pz	_	_
(	_	_
z	_	_
)	_	_
[	_	_
log	_	_
(	_	_
1−D	_	_
(	_	_
x	_	_
,	_	_
G	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
)	_	_
]	_	_
,	_	_
(	_	_
7	_	_
)	_	_
where	_	_
x	_	_
,	_	_
y	_	_
∼	_	_
p	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
are	_	_
images	_	_
of	_	_
the	_	_
same	_	_
scene	_	_
with	_	_
different	_	_
styles	_	_
,	_	_
z	_	_
∼	_	_
p	_	_
(	_	_
z	_	_
)	_	_
is	_	_
a	_	_
random	_	_
noise	_	_
as	_	_
in	_	_
the	_	_
regular	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#239
The	_	_
L1	_	_
loss	_	_
for	_	_
constraining	_	_
self-similarity	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LL1	_	_
(	_	_
G	_	_
)	_	_
=	_	_
Ex	_	_
,	_	_
y∼pdata	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
,	_	_
z∼pz	_	_
(	_	_
z	_	_
)	_	_
[	_	_
||y	_	_
−G	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
||1	_	_
]	_	_
,	_	_
(	_	_
8	_	_
)	_	_
The	_	_
overall	_	_
objective	_	_
is	_	_
thus	_	_
given	_	_
by	_	_
:	_	_
G∗	_	_
,	_	_
D∗	_	_
=	_	_
arg	_	_
min	_	_
G	_	_
max	_	_
D	_	_
LcGAN	_	_
(	_	_
G	_	_
,	_	_
D	_	_
)	_	_
+	_	_
λLL1	_	_
(	_	_
G	_	_
)	_	_
,	_	_
(	_	_
9	_	_
)	_	_
where	_	_
λ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
to	_	_
balance	_	_
the	_	_
two	_	_
loss	_	_
terms	_	_
.	_	_

#240
The	_	_
authors	_	_
of	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
find	_	_
that	_	_
the	_	_
noise	_	_
z	_	_
does	_	_
not	_	_
have	_	_
obvious	_	_
effect	_	_
on	_	_
the	_	_
output	_	_
,	_	_
so	_	_
they	_	_
provide	_	_
the	_	_
noise	_	_
in	_	_
the	_	_
form	_	_
of	_	_
dropout	_	_
at	_	_
training	_	_
and	_	_
test	_	_
time	_	_
instead	_	_
of	_	_
drawing	_	_
samples	_	_
from	_	_
a	_	_
random	_	_
distribution	_	_
.	_	_

#241
The	_	_
generator	_	_
structure	_	_
for	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
is	_	_
based	_	_
on	_	_
U-Net	_	_
[	_	_
71	_	_
]	_	_
,	_	_
which	_	_
belongs	_	_
to	_	_
the	_	_
encoder-decoder	_	_
framework	_	_
but	_	_
adds	_	_
skip	_	_
connections	_	_
from	_	_
encoder	_	_
to	_	_
decoder	_	_
so	_	_
as	_	_
to	_	_
allow	_	_
circumventing	_	_
the	_	_
bottleneck	_	_
for	_	_
sharing	_	_
low-level	_	_
information	_	_
like	_	_
edges	_	_
of	_	_
objects	_	_
.	_	_

#242
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
proposes	_	_
PatchGAN	_	_
(	_	_
the	_	_
patch-based	_	_
idea	_	_
was	_	_
previously	_	_
explored	_	_
in	_	_
MGAN	_	_
[	_	_
72	_	_
]	_	_
)	_	_
as	_	_
the	_	_
discriminator	_	_
,	_	_
which	_	_
,	_	_
instead	_	_
of	_	_
classifying	_	_
the	_	_
whole	_	_
image	_	_
,	_	_
tries	_	_
to	_	_
classify	_	_
each	_	_
N	_	_
×	_	_
N	_	_
path	_	_
of	_	_
the	_	_
image	_	_
and	_	_
average	_	_
all	_	_
the	_	_
scores	_	_
of	_	_
patches	_	_
to	_	_
get	_	_
the	_	_
final	_	_
score	_	_
for	_	_
the	_	_
image	_	_
.	_	_

#243
The	_	_
motivation	_	_
of	_	_
this	_	_
method	_	_
is	_	_
that	_	_
,	_	_
although	_	_
L1	_	_
and	_	_
L2	_	_
losses	_	_
produce	_	_
blurry	_	_
images	_	_
and	_	_
fail	_	_
to	_	_
capture	_	_
high	_	_
frequency	_	_
details	_	_
,	_	_
in	_	_
many	_	_
cases	_	_
they	_	_
can	_	_
capture	_	_
the	_	_
low	_	_
frequencies	_	_
quite	_	_
well	_	_
.	_	_

#244
In	_	_
order	_	_
to	_	_
capture	_	_
the	_	_
high	_	_
frequency	_	_
details	_	_
,	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
argues	_	_
that	_	_
it	_	_
is	_	_
sufficient	_	_
to	_	_
restrict	_	_
the	_	_
discriminator	_	_
to	_	_
focus	_	_
only	_	_
on	_	_
local	_	_
patches	_	_
.	_	_

#245
From	_	_
the	_	_
experiments	_	_
,	_	_
it	_	_
is	_	_
found	_	_
that	_	_
,	_	_
for	_	_
an	_	_
256×256	_	_
image	_	_
,	_	_
a	_	_
patch-size	_	_
of	_	_
70×70	_	_
works	_	_
best	_	_
.	_	_

#246
Although	_	_
Pix2Pix	_	_
produces	_	_
very	_	_
impressive	_	_
synthetic	_	_
images	_	_
,	_	_
the	_	_
major	_	_
limitation	_	_
is	_	_
that	_	_
it	_	_
must	deontic	_
use	_	_
paired	_	_
images	_	_
as	_	_
supervision	_	_
,	_	_
as	_	_
is	_	_
shown	_	_
in	_	_
Equation	_	_
8	_	_
that	_	_
data	_	_
pair	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
is	_	_
drawn	_	_
from	_	_
the	_	_
joint	_	_
distribution	_	_
p	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
.	_	_

#247
5.2	_	_
Supervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_
Pair-wise	_	_
Discrimination	_	_

#248
PLDT	_	_
[	_	_
35	_	_
]	_	_
proposes	_	_
another	_	_
method	_	_
to	_	_
do	_	_
supervised	_	_
image-to-image	_	_
translation	_	_
,	_	_
by	_	_
adding	_	_
another	_	_
discriminator	_	_
Dpair	_	_
that	_	_
learns	_	_
to	_	_
tell	_	_
whether	_	_
a	_	_
pair	_	_
of	_	_
images	_	_
from	_	_
different	_	_
domains	_	_
is	_	_
associated	_	_
with	_	_
each	_	_
other	_	_
.	_	_

#249
The	_	_
architecture	_	_
of	_	_
PLDT	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
7	_	_
.	_	_

#250
Given	_	_
an	_	_
input	_	_
image	_	_
xs	_	_
from	_	_
source	_	_
domain	_	_
,	_	_
its	_	_
ground-truth	_	_
image	_	_
xt	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
an	_	_
irrelevant	_	_
image	_	_
x−t	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
and	_	_
the	_	_
generator	_	_
G	_	_
transfers	_	_
xs	_	_
into	_	_
an	_	_
image	_	_
x̂t	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
the	_	_
loss	_	_
for	_	_
Dpair	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
:	_	_
Lpair	_	_
=	_	_
−	_	_
t	_	_
·	_	_
log	_	_
[	_	_
Dpair	_	_
(	_	_
xs	_	_
,	_	_
x	_	_
)	_	_
]	_	_
+	_	_
(	_	_
t−	_	_
1	_	_
)	_	_
·	_	_
log	_	_
[	_	_
1−Dpair	_	_
(	_	_
xs	_	_
,	_	_
x	_	_
)	_	_
]	_	_
,	_	_
s.t	_	_
.	_	_

#251
t	_	_
=	_	_
	_	_
0	_	_
if	_	_
x	_	_
=	_	_
xt	_	_
0	_	_
if	_	_
x	_	_
=	_	_
x̂t	_	_
1	_	_
if	_	_
x	_	_
=	_	_
x−t	_	_
.	_	_

#252
(	_	_
10	_	_
)	_	_
The	_	_
generator	_	_
of	_	_
PLDT	_	_
[	_	_
35	_	_
]	_	_
is	_	_
implemented	_	_
in	_	_
an	_	_
encoder-decoder	_	_
fashion	_	_
using	_	_
(	_	_
transposed	_	_
)	_	_
convolutions	_	_
,	_	_
while	_	_
the	_	_
two	_	_
discriminators	_	_
are	_	_
implemented	_	_
as	_	_
fully	_	_
convolutional	_	_
networks	_	_
.	_	_

#253
As	_	_
shown	_	_
in	_	_
the	_	_
experimental	_	_
results	_	_
,	_	_
PLDT	_	_
[	_	_
35	_	_
]	_	_
performs	_	_
domain	_	_
transfer	_	_
that	_	_
modifies	_	_
the	_	_
geometric	_	_
shapes	_	_
of	_	_
objects	_	_
while	_	_
trying	_	_
to	_	_
keep	_	_
the	_	_
texture	_	_
consistent	_	_
among	_	_
all	_	_
associated	_	_
images	_	_
.	_	_

#254
G	_	_
D	_	_
Dpair	_	_
xs	_	_
xtx̂t	_	_
x 	_	_
t	_	_
True/Fake	_	_
Channel	_	_
concatenation	_	_
Select	_	_
Associated/	_	_
Unassociated	_	_
Fig.	_	_
7	_	_
.	_	_

#255
Architecture	_	_
of	_	_
Pixel-Level	_	_
Domain	_	_
Transfer	_	_
(	_	_
PLDT	_	_
)	_	_
[	_	_
35	_	_
]	_	_
.	_	_

#256
In	_	_
this	_	_
example	_	_
,	_	_
the	_	_
source	_	_
domain	_	_
is	_	_
BW	_	_
color	_	_
space	_	_
,	_	_
while	_	_
the	_	_
target	_	_
domain	_	_
is	_	_
RGB	_	_
space	_	_
.	_	_

#257
As	_	_
explained	_	_
in	_	_
[	_	_
35	_	_
]	_	_
,	_	_
this	_	_
model	_	_
can	_	_
performs	_	_
geometric	_	_
modifications	_	_
on	_	_
the	_	_
input	_	_
image	_	_
,	_	_
while	_	_
keeping	_	_
the	_	_
object	_	_
in	_	_
the	_	_
output	_	_
image	_	_
the	_	_
same	_	_
as	_	_
input	_	_
.	_	_

#258
5.3	_	_
Unsupervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_

#259
Cyclic	_	_
Loss	_	_
Two	_	_
concurrent	_	_
works	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
and	_	_
DualGAN	_	_
[	_	_
74	_	_
]	_	_
propose	_	_
to	_	_
add	_	_
a	_	_
self-consistency	_	_
(	_	_
reconstruction	_	_
)	_	_
loss	_	_
that	_	_
tries	_	_
to	_	_
preserve	_	_
the	_	_
input	_	_
image	_	_
after	_	_
a	_	_
cycle	_	_
of	_	_
transformation	_	_
.	_	_

#260
CycleGAN	_	_
and	_	_
DualGAN	_	_
share	_	_
the	_	_
same	_	_
framework	_	_
,	_	_
which	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
8	_	_
.	_	_

#261
As	_	_
we	_	_
can	_	_
see	_	_
,	_	_
the	_	_
two	_	_
generators	_	_
GAB	_	_
and	_	_
GBA	_	_
are	_	_
doing	_	_
opposite	_	_
transformations	_	_
,	_	_
which	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
a	_	_
kind	_	_
of	_	_
dual	_	_
learning	_	_
[	_	_
75	_	_
]	_	_
.	_	_

#262
Besides	_	_
,	_	_
DiscoGAN	_	_
[	_	_
76	_	_
]	_	_
is	_	_
another	_	_
model	_	_
that	_	_
utilizes	_	_
the	_	_
same	_	_
cyclic	_	_
framework	_	_
as	_	_
Figure	_	_
8	_	_
.	_	_

#263
Here	_	_
we	_	_
use	_	_
CycleGAN	_	_
as	_	_
an	_	_
example	_	_
.	_	_

#264
In	_	_
CycleGAN	_	_
,	_	_
there	_	_
are	_	_
two	_	_
generators	_	_
,	_	_
GAB	_	_
that	_	_
transfer	_	_
an	_	_
image	_	_
from	_	_
domain	_	_
A	_	_
to	_	_
B	_	_
and	_	_
GBA	_	_
that	_	_
performs	_	_
the	_	_
opposite	_	_
transformation	_	_
.	_	_

#265
Also	_	_
,	_	_
there	_	_
are	_	_
also	_	_
two	_	_
discriminators	_	_
DA	_	_
and	_	_
DB	_	_
that	_	_
predicts	_	_
whether	_	_
an	_	_
image	_	_
belongs	_	_
to	_	_
that	_	_
domain	_	_
.	_	_

#266
For	_	_
a	_	_
pair	_	_
of	_	_
GAB	_	_
and	_	_
DB	_	_
,	_	_
the	_	_
adversarial	_	_
loss	_	_
function	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LGAN	_	_
(	_	_
GAB	_	_
,	_	_
DB	_	_
)	_	_
=	_	_
Eb∼pB	_	_
(	_	_
b	_	_
)	_	_
[	_	_
logDB	_	_
(	_	_
b	_	_
)	_	_
]	_	_
+	_	_
Ea∼pA	_	_
(	_	_
a	_	_
)	_	_
[	_	_
1−	_	_
log	_	_
(	_	_
DB	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
]	_	_
,	_	_
(	_	_
11	_	_
)	_	_
and	_	_
similarly	_	_
for	_	_
the	_	_
pair	_	_
GBA	_	_
and	_	_
DA	_	_
,	_	_
we	_	_
can	_	_
define	_	_
the	_	_
adversarial	_	_
loss	_	_
as	_	_
LGAN	_	_
(	_	_
GBA	_	_
,	_	_
DA	_	_
)	_	_
.	_	_

#267
Besides	_	_
the	_	_
adversarial	_	_
loss	_	_
,	_	_
a	_	_
cycle-consistency	_	_
loss	_	_
is	_	_
designed	_	_
to	_	_
minimize	_	_
the	_	_
reconstruction	_	_
error	_	_
after	_	_
we	_	_
translate	_	_
an	_	_
image	_	_
of	_	_
one	_	_
domain	_	_
to	_	_
another	_	_
and	_	_
then	_	_
translate	_	_
it	_	_
back	_	_
to	_	_
the	_	_
original	_	_
domain	_	_
,	_	_
i.e.	_	_
a→	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
→	_	_
GBA	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
≈	_	_
a	_	_
.	_	_

#268
Since	_	_
this	_	_
cycle	_	_
can	_	_
be	_	_
defined	_	_
from	_	_
two	_	_
directions	_	_
,	_	_
the	_	_
cycle-consistency	_	_
loss	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
Lcyc	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
)	_	_
=	_	_
Ea∼pA	_	_
(	_	_
a	_	_
)	_	_
[	_	_
||a−GBA	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
||1	_	_
]	_	_
+	_	_
Eb∼pB	_	_
(	_	_
b	_	_
)	_	_
[	_	_
||b−GAB	_	_
(	_	_
GBA	_	_
(	_	_
b	_	_
)	_	_
)	_	_
||1	_	_
]	_	_
.	_	_

#269
(	_	_
12	_	_
)	_	_
Then	_	_
the	_	_
overall	_	_
loss	_	_
function	_	_
is	_	_
:	_	_
L	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
,	_	_
DA	_	_
,	_	_
DB	_	_
)	_	_
=	_	_
LGAN	_	_
(	_	_
GAB	_	_
,	_	_
DB	_	_
)	_	_
+	_	_
LGAN	_	_
(	_	_
GBA	_	_
,	_	_
DA	_	_
)	_	_
+	_	_
λLcyc	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
)	_	_
,	_	_
(	_	_
13	_	_
)	_	_
GAB	_	_
GBA	_	_
DA	_	_
DB	_	_
a	_	_
b	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
GBA	_	_
(	_	_
b	_	_
)	_	_
GAB	_	_
(	_	_
GBA	_	_
(	_	_
b	_	_
)	_	_
)	_	_
GBA	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
||a	_	_
 	_	_
GBA	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
||1	_	_
||b	_	_
 	_	_
GAB	_	_
(	_	_
GBA	_	_
(	_	_
b	_	_
)	_	_
)	_	_
||1	_	_
True	_	_
Fake	_	_
True	_	_
Fake	_	_
Domain	_	_
A	_	_
Domain	_	_
B	_	_
Reconstruction	_	_
Reconstruction	_	_
Fig.	_	_
8	_	_
.	_	_

#270
Framework	_	_
of	_	_
CycleGAN	_	_
and	_	_
DualGAN	_	_
.	_	_

#271
A	_	_
and	_	_
B	_	_
are	_	_
two	_	_
different	_	_
domains	_	_
.	_	_

#272
There	_	_
is	_	_
a	_	_
discriminator	_	_
for	_	_
each	_	_
domain	_	_
that	_	_
judges	_	_
if	_	_
an	_	_
image	_	_
belong	_	_
to	_	_
that	_	_
domain	_	_
.	_	_

#273
Two	_	_
generators	_	_
are	_	_
designed	_	_
to	_	_
translate	_	_
an	_	_
image	_	_
from	_	_
one	_	_
domain	_	_
to	_	_
another	_	_
.	_	_

#274
There	_	_
are	_	_
two	_	_
cycles	_	_
of	_	_
data	_	_
flow	_	_
,	_	_
the	_	_
red	_	_
one	_	_
performs	_	_
a	_	_
sequence	_	_
of	_	_
domain	_	_
transfer	_	_
A	_	_
→	_	_
B	_	_
→	_	_
A	_	_
,	_	_
while	_	_
the	_	_
blue	_	_
one	_	_
is	_	_
B	_	_
→	_	_
A	_	_
→	_	_
B.	_	_
L1	_	_
loss	_	_
is	_	_
applied	_	_
on	_	_
the	_	_
input	_	_
a	_	_
(	_	_
or	_	_
b	_	_
)	_	_
and	_	_
the	_	_
reconstructed	_	_
input	_	_
GBA	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
(	_	_
or	_	_
GAB	_	_
(	_	_
GBA	_	_
(	_	_
b	_	_
)	_	_
)	_	_
)	_	_
to	_	_
enforce	_	_
self-consistency	_	_
.	_	_

#275
where	_	_
λ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
to	_	_
balance	_	_
the	_	_
losses	_	_
.	_	_

#276
Then	_	_
the	_	_
objective	_	_
is	_	_
to	_	_
solve	_	_
for	_	_
:	_	_
G∗AB	_	_
,	_	_
G	_	_
∗	_	_
BA	_	_
=	_	_
arg	_	_
min	_	_
GAB	_	_
,	_	_
GBA	_	_
max	_	_
DB	_	_
,	_	_
DA	_	_
L	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
,	_	_
DA	_	_
,	_	_
DB	_	_
)	_	_
.	_	_

#277
(	_	_
14	_	_
)	_	_
Although	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
and	_	_
DualGAN	_	_
[	_	_
74	_	_
]	_	_
have	_	_
the	_	_
same	_	_
objective	_	_
,	_	_
they	_	_
use	_	_
different	_	_
implementations	_	_
for	_	_
generators	_	_
.	_	_

#278
CycleGAN	_	_
uses	_	_
the	_	_
generator	_	_
structure	_	_
as	_	_
proposed	_	_
in	_	_
[	_	_
68	_	_
]	_	_
,	_	_
while	_	_
DualGAN	_	_
follows	_	_
the	_	_
U-Net	_	_
[	_	_
71	_	_
]	_	_
structure	_	_
as	_	_
in	_	_
[	_	_
67	_	_
]	_	_
[	_	_
71	_	_
]	_	_
.	_	_

#279
Both	_	_
CycleGAN	_	_
and	_	_
DualGAN	_	_
use	_	_
the	_	_
PatchGAN	_	_
with	_	_
size	_	_
70×	_	_
70	_	_
as	_	_
in	_	_
[	_	_
67	_	_
]	_	_
.	_	_

#280
Besides	_	_
different	_	_
generator	_	_
architectures	_	_
,	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
and	_	_
DualGAN	_	_
[	_	_
74	_	_
]	_	_
also	_	_
use	_	_
different	_	_
techniques	_	_
to	_	_
stabilize	_	_
the	_	_
training	_	_
process	_	_
.	_	_

#281
DualGAN	_	_
follows	_	_
the	_	_
training	_	_
procedure	_	_
proposed	_	_
in	_	_
WGAN	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#282
CycleGAN	_	_
applies	_	_
two	_	_
techniques	_	_
.	_	_

#283
First	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
the	_	_
log	_	_
loss	_	_
[	_	_
77	_	_
]	_	_
for	_	_
LGAN	_	_
in	_	_
Equation	_	_
11	_	_
with	_	_
a	_	_
least	_	_
square	_	_
loss	_	_
that	_	_
in	_	_
practice	_	_
performs	_	_
more	_	_
stably	_	_
and	_	_
produces	_	_
higher	_	_
quality	_	_
images	_	_
:	_	_
LLSGAN	_	_
(	_	_
GAB	_	_
,	_	_
DB	_	_
)	_	_
=	_	_
Eb∼pB	_	_
(	_	_
b	_	_
)	_	_
[	_	_
(	_	_
DB	_	_
(	_	_
b	_	_
)	_	_
−	_	_
1	_	_
)	_	_
2	_	_
]	_	_
+	_	_
Ea∼pA	_	_
(	_	_
a	_	_
)	_	_
[	_	_
(	_	_
DB	_	_
(	_	_
GAB	_	_
(	_	_
a	_	_
)	_	_
)	_	_
2	_	_
]	_	_
.	_	_

#284
(	_	_
15	_	_
)	_	_
The	_	_
second	_	_
technique	_	_
used	_	_
in	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
is	_	_
that	_	_
,	_	_
in	_	_
order	_	_
to	_	_
reduce	_	_
model	_	_
oscillation	_	_
[	_	_
4	_	_
]	_	_
,	_	_
CycleGAN	_	_
follows	_	_
SimGAN’s	_	_
[	_	_
78	_	_
]	_	_
strategy	_	_
and	_	_
updates	_	_
discriminators	_	_
DA	_	_
and	_	_
DB	_	_
using	_	_
a	_	_
history	_	_
of	_	_
50	_	_
previously	_	_
generated	_	_
images	_	_
instead	_	_
of	_	_
the	_	_
ones	_	_
produced	_	_
by	_	_
latest	_	_
generators	_	_
.	_	_

#285
Experiments	_	_
of	_	_
CycleGAN	_	_
demonstrate	_	_
the	_	_
potential	_	_
of	_	_
performing	_	_
high-quality	_	_
image-to-image	_	_
translation	_	_
using	_	_
unpaired	_	_
data	_	_
only	_	_
,	_	_
even	_	_
though	_	_
supervised	_	_
method	_	_
like	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
still	_	_
outperforms	_	_
CycleGAN	_	_
by	_	_
a	_	_
noticeable	_	_
margin	_	_
.	_	_

#286
CycleGAN	_	_
also	_	_
conducts	_	_
experiments	_	_
that	_	_
show	_	_
the	_	_
importance	_	_
of	_	_
using	_	_
both	_	_
circles	_	_
in	_	_
the	_	_
circle-consistency	_	_
loss	_	_
defined	_	_
in	_	_
Equation	_	_
12	_	_
.	_	_

#287
However	_	_
,	_	_
failure	_	_
cases	_	_
provided	_	_
in	_	_
[	_	_
73	_	_
]	_	_
show	_	_
that	_	_
,	_	_
just	_	_
as	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
,	_	_
CycleGAN	_	_
does	_	_
not	_	_
work	_	_
in	_	_
cases	_	_
that	_	_
necessitate	_	_
geometric	_	_
transformations	_	_
,	_	_
such	_	_
as	_	_
apple	_	_
↔	_	_
orange	_	_
and	_	_
cat	_	_
↔	_	_
dog	_	_
.	_	_

#288
More	_	_
examples	_	_
are	_	_
available	_	_
on	_	_
CycleGAN’s	_	_
project	_	_
website2	_	_
.	_	_

#289
2.	_	_
https	_	_
:	_	_
//junyanz.github.io/CycleGAN/	_	_

#290
5.4	_	_
Unsupervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_
Distance	_	_

#291
Constraint	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
discovers	_	_
that	_	_
,	_	_
the	_	_
distance	_	_
||xi	_	_
−	_	_
xj	_	_
||	_	_
between	_	_
two	_	_
images	_	_
in	_	_
the	_	_
source	_	_
domain	_	_
A	_	_
is	_	_
highly	_	_
positively	_	_
correlated	_	_
to	_	_
the	_	_
distance	_	_
of	_	_
their	_	_
counterparts	_	_
||GAB	_	_
(	_	_
xi	_	_
)	_	_
−GAB	_	_
(	_	_
xj	_	_
)	_	_
||	_	_
in	_	_
the	_	_
target	_	_
domainB	_	_
,	_	_
which	_	_
can	_	_
be	_	_
seen	_	_
from	_	_
Figure	_	_
1	_	_
of	_	_
the	_	_
paper	_	_
[	_	_
79	_	_
]	_	_
.	_	_

#292
According	_	_
to	_	_
[	_	_
79	_	_
]	_	_
,	_	_
let	_	_
dk	_	_
be	_	_
the	_	_
distance	_	_
||xi−xj	_	_
||	_	_
,	_	_
and	_	_
d′k	_	_
be	_	_
||GAB	_	_
(	_	_
xi	_	_
)	_	_
−GAB	_	_
(	_	_
xj	_	_
)	_	_
||	_	_
,	_	_
a	_	_
high	_	_
correlation	_	_
indicates	_	_
that	_	_
∑	_	_
dkd	_	_
′	_	_
k	_	_
should	inference	_
also	_	_
be	_	_
high	_	_
.	_	_

#293
The	_	_
pair-wise	_	_
distances	_	_
dk	_	_
in	_	_
source	_	_
domain	_	_
are	_	_
fixed	_	_
,	_	_
and	_	_
maximizing	_	_
∑	_	_
dkd	_	_
′	_	_
k	_	_
causes	_	_
dk	_	_
with	_	_
large	_	_
value	_	_
to	_	_
dominate	_	_
the	_	_
loss	_	_
,	_	_
which	_	_
is	_	_
undesirable	_	_
.	_	_

#294
So	_	_
the	_	_
authors	_	_
propose	_	_
to	_	_
minimize	_	_
∑	_	_
|dk	_	_
−	_	_
d′k|	_	_
instead	_	_
.	_	_

#295
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
a	_	_
pair-wise	_	_
distance	_	_
loss	_	_
defined	_	_
as	_	_
:	_	_
Ldist	_	_
(	_	_
GAB	_	_
,	_	_
pA	_	_
)	_	_
=	_	_
Exi	_	_
,	_	_
xj∼pA	_	_
|	_	_
σA	_	_
(	_	_
||xi	_	_
−	_	_
xj	_	_
||1	_	_
−	_	_
µA	_	_
)	_	_
−	_	_
1	_	_
σB	_	_
(	_	_
||GAB	_	_
(	_	_
xi	_	_
)	_	_
−GAB	_	_
(	_	_
xj	_	_
)	_	_
||1	_	_
−	_	_
µB	_	_
)	_	_
|	_	_
,	_	_
(	_	_
16	_	_
)	_	_
where	_	_
µA	_	_
,	_	_
µB	_	_
(	_	_
σA	_	_
,	_	_
σB	_	_
)	_	_
are	_	_
the	_	_
pre-computed	_	_
means	_	_
(	_	_
standard	_	_
deviations	_	_
)	_	_
of	_	_
pair-wise	_	_
distances	_	_
in	_	_
training	_	_
sets	_	_
of	_	_
domain	_	_
A	_	_
,	_	_
B	_	_
respectively	_	_
.	_	_

#296
In	_	_
order	_	_
to	_	_
support	_	_
stochastic	_	_
gradient	_	_
descent	_	_
where	_	_
only	_	_
one	_	_
data	_	_
sample	_	_
is	_	_
fed	_	_
into	_	_
the	_	_
model	_	_
at	_	_
a	_	_
time	_	_
,	_	_
DistanceGAN	_	_
proposes	_	_
another	_	_
self-distance	_	_
constraint	_	_
:	_	_
Lself	_	_
-dist	_	_
(	_	_
GAB	_	_
,	_	_
pA	_	_
)	_	_
=	_	_
Ex∼pA	_	_
|	_	_
σA	_	_
(	_	_
||L	_	_
(	_	_
x	_	_
)	_	_
−R	_	_
(	_	_
x	_	_
)	_	_
||1	_	_
−	_	_
µA	_	_
)	_	_
−	_	_
1	_	_
σB	_	_
(	_	_
||GAB	_	_
(	_	_
L	_	_
(	_	_
x	_	_
)	_	_
)	_	_
−GAB	_	_
(	_	_
R	_	_
(	_	_
x	_	_
)	_	_
)	_	_
||1	_	_
−	_	_
µB	_	_
)	_	_
|	_	_
,	_	_
(	_	_
17	_	_
)	_	_
where	_	_
L	_	_
(	_	_
x	_	_
)	_	_
and	_	_
R	_	_
(	_	_
x	_	_
)	_	_
indicate	_	_
the	_	_
left	_	_
and	_	_
right	_	_
half	_	_
of	_	_
the	_	_
image	_	_
x	_	_
,	_	_
and	_	_
only	_	_
the	_	_
left	_	_
(	_	_
right	_	_
)	_	_
parts	_	_
of	_	_
images	_	_
are	_	_
taken	_	_
into	_	_
account	_	_
when	_	_
calculating	_	_
µA	_	_
,	_	_
σA	_	_
(	_	_
µB	_	_
,	_	_
σB	_	_
)	_	_
.	_	_

#297
Thus	_	_
the	_	_
overall	_	_
loss	_	_
of	_	_
DistanceGAN	_	_
is	_	_
given	_	_
by	_	_
:	_	_
L	_	_
=	_	_
α1ALGAN	_	_
(	_	_
GAB	_	_
,	_	_
DB	_	_
)	_	_
+	_	_
α1BLGAN	_	_
(	_	_
GBA	_	_
,	_	_
DA	_	_
)	_	_
+	_	_
α2ALdist	_	_
(	_	_
GAB	_	_
,	_	_
pA	_	_
)	_	_
+	_	_
α2BLdist	_	_
(	_	_
GBA	_	_
,	_	_
pB	_	_
)	_	_
+	_	_
α3ALself	_	_
-dist	_	_
(	_	_
GAB	_	_
,	_	_
pA	_	_
)	_	_
+	_	_
α3BLself	_	_
-dist	_	_
(	_	_
GBA	_	_
,	_	_
pB	_	_
)	_	_
+	_	_
α4Lcyc	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
)	_	_
,	_	_
(	_	_
18	_	_
)	_	_
where	_	_
Lcyc	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
)	_	_
is	_	_
defined	_	_
in	_	_
Equation	_	_
12	_	_
and	_	_
LGAN	_	_
(	_	_
·	_	_
)	_	_
is	_	_
defined	_	_
in	_	_
Equation	_	_
11	_	_
.	_	_

#298
DistanceGAN	_	_
conducts	_	_
extensive	_	_
experiments	_	_
using	_	_
different	_	_
losses	_	_
:	_	_
Lcyc	_	_
alone	_	_
(	_	_
DiscoGAN	_	_
[	_	_
76	_	_
]	_	_
and	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
)	_	_
,	_	_
one-sided	_	_
distance	_	_
loss	_	_
Ldist	_	_
(	_	_
A	_	_
→	_	_
B	_	_
→	_	_
A	_	_
or	_	_
B	_	_
→	_	_
A	_	_
→	_	_
B	_	_
)	_	_
,	_	_
the	_	_
combination	_	_
of	_	_
Lcyc	_	_
and	_	_
one-sided	_	_
Ldist	_	_
,	_	_
and	_	_
one-sided	_	_
self-distance	_	_
loss	_	_
Lself	_	_
-dist	_	_
alone	_	_
.	_	_

#299
The	_	_
implementation	_	_
of	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
is	_	_
the	_	_
same	_	_
as	_	_
baseline	_	_
(	_	_
DiscoGAN	_	_
[	_	_
76	_	_
]	_	_
or	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
)	_	_
,	_	_
dependent	_	_
on	_	_
which	_	_
one	_	_
is	_	_
to	_	_
be	_	_
compared	_	_
with	_	_
.	_	_

#300
Results	_	_
show	_	_
that	_	_
the	_	_
one-sided	_	_
distance	_	_
loss	_	_
or	_	_
self-distance	_	_
loss	_	_
outperforms	_	_
DiscoGAN	_	_
and	_	_
CycleGAN	_	_
in	_	_
several	_	_
tasks	_	_
,	_	_
and	_	_
that	_	_
the	_	_
combination	_	_
of	_	_
cyclic	_	_
loss	_	_
and	_	_
distance	_	_
loss	_	_
achieves	_	_
the	_	_
best	_	_
results	_	_
in	_	_
some	_	_
cases	_	_
.	_	_

#301
However	_	_
,	_	_
the	_	_
paper	_	_
does	_	_
not	_	_
explore	_	_
other	_	_
possible	_	_
combinations	_	_
,	_	_
such	_	_
as	_	_
using	_	_
both	_	_
distance	_	_
losses	_	_
or	_	_
even	_	_
the	_	_
full	_	_
loss	_	_
function	_	_
defined	_	_
in	_	_
Equation	_	_
18	_	_
.	_	_

#302
One	_	_
interesting	_	_
thing	_	_
is	_	_
,	_	_
as	_	_
stated	_	_
in	_	_
[	_	_
79	_	_
]	_	_
,	_	_
that	_	_
DistanceGAN	_	_
computes	_	_
the	_	_
distances	_	_
in	_	_
raw	_	_
RGB	_	_
space	_	_
and	_	_
still	_	_
achieves	_	_
better	_	_
performance	_	_
than	_	_
baselines	_	_
,	_	_
but	_	_
it	_	_
may	_	_
help	_	_
if	_	_
the	_	_
distances	_	_
are	_	_
calculated	_	_
in	_	_
images’	_	_
latent	_	_
feature	_	_
space	_	_
where	_	_
the	_	_
features	_	_
can	_	_
be	_	_
extracted	_	_
using	_	_
pre-trained	_	_
image	_	_
classifiers	_	_
.	_	_

#303
In	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
,	_	_
the	_	_
authors	_	_
argue	_	_
that	_	_
the	_	_
high	_	_
positive	_	_
correlation	_	_
between	_	_
dk	_	_
and	_	_
d′k	_	_
implies	_	_
that	_	_
∑	_	_
dkd	_	_
′	_	_
k	_	_
should	deontic	_
be	_	_
high	_	_
.	_	_

#304
However	_	_
,	_	_
it	_	_
is	_	_
unclear	_	_
how	_	_
high	_	_
d′k	_	_
should	deontic	_
be	_	_
.	_	_

#305
For	_	_
example	_	_
,	_	_
if	_	_
dk	_	_
=	_	_
1	_	_
and	_	_
we	_	_
know	_	_
that	_	_
d′k	_	_
should	deontic	_
be	_	_
high	_	_
,	_	_
we	_	_
still	_	_
can	_	_
not	_	_
definitely	_	_
say	_	_
that	_	_
d′k	_	_
=	_	_
3	_	_
is	_	_
more	_	_
desirable	_	_
than	_	_
d′k	_	_
=	_	_
2	_	_
.	_	_

#306
The	_	_
concept	_	_
of	_	_
“high”	_	_
is	_	_
blurry	_	_
,	_	_
so	_	_
it	_	_
may	_	_
be	_	_
better	_	_
to	_	_
use	_	_
the	_	_
concept	_	_
“higher”	_	_
.	_	_

#307
An	_	_
alternative	_	_
statement	_	_
could	options	_
be	_	_
,	_	_
let	_	_
x	_	_
,	_	_
y	_	_
,	_	_
z	_	_
be	_	_
images	_	_
from	_	_
domain	_	_
A	_	_
,	_	_
if	_	_
d	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
>	_	_
d	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
,	_	_
then	_	_
d	_	_
(	_	_
GAB	_	_
(	_	_
x	_	_
)	_	_
,	_	_
GAB	_	_
(	_	_
y	_	_
)	_	_
)	_	_
>	_	_
d	_	_
(	_	_
GAB	_	_
(	_	_
x	_	_
)	_	_
,	_	_
GAB	_	_
(	_	_
z	_	_
)	_	_
)	_	_
.	_	_

#308
And	_	_
thus	_	_
we	_	_
can	_	_
design	_	_
a	_	_
loss	_	_
like	_	_
max	_	_
(	_	_
δ	_	_
,	_	_
d	_	_
(	_	_
GAB	_	_
(	_	_
x	_	_
)	_	_
,	_	_
GAB	_	_
(	_	_
y	_	_
)	_	_
)	_	_
−	_	_
d	_	_
(	_	_
GAB	_	_
(	_	_
x	_	_
)	_	_
,	_	_
GAB	_	_
(	_	_
z	_	_
)	_	_
)	_	_
)	_	_
for	_	_
all	_	_
triplets	_	_
x	_	_
,	_	_
y	_	_
,	_	_
z	_	_
∈	_	_
A	_	_
such	_	_
that	_	_
d	_	_
(	_	_
x	_	_
,	_	_
y	_	_
)	_	_
>	_	_
d	_	_
(	_	_
x	_	_
,	_	_
z	_	_
)	_	_
.	_	_

#309
Regardless	_	_
of	_	_
the	_	_
objective	_	_
of	_	_
maximizing	_	_
∑	_	_
dkd	_	_
′	_	_
k	_	_
,	_	_
the	_	_
actual	_	_
loss	_	_
used	_	_
in	_	_
DistanceGAN	_	_
,	_	_
i.e.	_	_
∑	_	_
|dk	_	_
−	_	_
d′k|	_	_
,	_	_
is	_	_
forcing	_	_
d′k	_	_
to	_	_
be	_	_
as	_	_
close	_	_
to	_	_
dk	_	_
as	_	_
possible	_	_
.	_	_

#310
In	_	_
other	_	_
words	_	_
,	_	_
how	_	_
two	_	_
images	_	_
of	_	_
a	_	_
domain	_	_
differ	_	_
from	_	_
each	_	_
other	_	_
should	deontic	_
be	_	_
reflected	_	_
in	_	_
the	_	_
same	_	_
way	_	_
when	_	_
they	_	_
are	_	_
translated	_	_
into	_	_
another	_	_
domain	_	_
,	_	_
which	_	_
can	_	_
be	_	_
called	_	_
“equivariance”	_	_
.	_	_

#311
The	_	_
distance	_	_
loss	_	_
and	_	_
self-distance	_	_
loss	_	_
proposed	_	_
in	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
is	_	_
essentially	_	_
capturing	_	_
this	_	_
“equivariance”	_	_
property	_	_
.	_	_

#312
5.5	_	_
Unsupervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_
Feature	_	_
Constancy	_	_

#313
As	_	_
we	_	_
mention	_	_
previously	_	_
,	_	_
besides	_	_
minimizing	_	_
the	_	_
reconstruction	_	_
error	_	_
at	_	_
raw	_	_
pixel	_	_
level	_	_
,	_	_
we	_	_
can	_	_
also	_	_
do	_	_
this	_	_
at	_	_
higher	_	_
feature	_	_
level	_	_
,	_	_
which	_	_
is	_	_
explored	_	_
in	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
.	_	_

#314
The	_	_
architecture	_	_
of	_	_
DTN	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
9	_	_
,	_	_
where	_	_
the	_	_
generator	_	_
G	_	_
is	_	_
composed	_	_
of	_	_
two	_	_
neural	_	_
networks	_	_
,	_	_
a	_	_
convolutional	_	_
network	_	_
f	_	_
and	_	_
an	_	_
transposed	_	_
convolutional	_	_
network	_	_
g	_	_
such	_	_
that	_	_
G	_	_
=	_	_
g	_	_
◦	_	_
f	_	_
.	_	_

#315
f	_	_
g	_	_
G	_	_
D	_	_
f	_	_
LTID	_	_
LGANG	_	_
LD	_	_
f	_	_
(	_	_
x	_	_
)	_	_
f	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
LCONST	_	_
Fig.	_	_
9	_	_
.	_	_

#316
Architecture	_	_
of	_	_
domain	_	_
transfer	_	_
network	_	_
(	_	_
DTN	_	_
)	_	_
.	_	_

#317
As	_	_
an	_	_
example	_	_
here	_	_
,	_	_
the	_	_
model	_	_
transfers	_	_
images	_	_
from	_	_
BW	_	_
color	_	_
space	_	_
to	_	_
RGB	_	_
color	_	_
space	_	_
.	_	_

#318
The	_	_
generator	_	_
is	_	_
expected	_	_
to	_	_
be	_	_
an	_	_
identity	_	_
matrix	_	_
to	_	_
images	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
so	_	_
that	_	_
in	_	_
this	_	_
example	_	_
,	_	_
an	_	_
RGB	_	_
image	_	_
remains	_	_
unchanged	_	_
when	_	_
put	_	_
into	_	_
the	_	_
generator	_	_
,	_	_
while	_	_
a	_	_
BW	_	_
image	_	_
is	_	_
transformed	_	_
into	_	_
RGB	_	_
by	_	_
the	_	_
generator	_	_
.	_	_

#319
Here	_	_
f	_	_
acts	_	_
as	_	_
a	_	_
feature	_	_
extractor	_	_
,	_	_
and	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
tries	_	_
to	_	_
preserve	_	_
high	_	_
level	_	_
features	_	_
of	_	_
an	_	_
input	_	_
image	_	_
from	_	_
source	_	_
domain	_	_
after	_	_
it	_	_
is	_	_
transferred	_	_
into	_	_
target	_	_
domain	_	_
.	_	_

#320
Let	_	_
Xs	_	_
denote	_	_
source	_	_
domain	_	_
and	_	_
Xt	_	_
be	_	_
target	_	_
domain	_	_
.	_	_

#321
Given	_	_
an	_	_
input	_	_
image	_	_
x	_	_
∈	_	_
Xs	_	_
,	_	_
the	_	_
output	_	_
of	_	_
generator	_	_
is	_	_
G	_	_
(	_	_
x	_	_
)	_	_
=	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
,	_	_
then	_	_
the	_	_
feature	_	_
reconstruction	_	_
error	_	_
can	_	_
be	_	_
defined	_	_
with	_	_
a	_	_
distance	_	_
measure	_	_
d	_	_
(	_	_
DTN	_	_
uses	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
)	_	_
:	_	_
LCONST	_	_
=	_	_
∑	_	_
x∈Xs	_	_
d	_	_
(	_	_
f	_	_
(	_	_
x	_	_
,	_	_
f	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
)	_	_
)	_	_
.	_	_

#322
(	_	_
19	_	_
)	_	_
Besides	_	_
,	_	_
DTN	_	_
also	_	_
expects	_	_
the	_	_
generator	_	_
G	_	_
to	_	_
act	_	_
as	_	_
an	_	_
identity	_	_
matrix	_	_
to	_	_
images	_	_
from	_	_
the	_	_
target	_	_
domain	_	_
.	_	_

#323
Given	_	_
a	_	_
distance	_	_
measure	_	_
d2	_	_
(	_	_
DTN	_	_
uses	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
)	_	_
,	_	_
an	_	_
identity	_	_
mapping	_	_
loss	_	_
for	_	_
target	_	_
domain	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LTID	_	_
=	_	_
∑	_	_
x∈Xt	_	_
d2	_	_
(	_	_
x	_	_
,	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
.	_	_

#324
(	_	_
20	_	_
)	_	_
In	_	_
addition	_	_
,	_	_
DTN	_	_
use	_	_
a	_	_
multi-class	_	_
discriminator	_	_
instead	_	_
of	_	_
a	_	_
binary	_	_
one	_	_
in	_	_
regular	_	_
GAN	_	_
.	_	_

#325
Here	_	_
D	_	_
is	_	_
a	_	_
ternary	_	_
classification	_	_
function	_	_
that	_	_
maps	_	_
an	_	_
image	_	_
to	_	_
one	_	_
of	_	_
the	_	_
three	_	_
classes	_	_
{	_	_
1,2,3	_	_
}	_	_
,	_	_
where	_	_
class	_	_
1	_	_
means	_	_
that	_	_
the	_	_
image	_	_
is	_	_
from	_	_
source	_	_
domain	_	_
but	_	_
transformed	_	_
by	_	_
G	_	_
,	_	_
class	_	_
2	_	_
means	_	_
that	_	_
the	_	_
input	_	_
is	_	_
an	_	_
image	_	_
from	_	_
target	_	_
domain	_	_
but	_	_
transformed	_	_
by	_	_
G	_	_
,	_	_
and	_	_
class	_	_
3	_	_
means	_	_
that	_	_
the	_	_
input	_	_
image	_	_
is	_	_
from	_	_
target	_	_
domain	_	_
without	_	_
any	_	_
transformation	_	_
.	_	_

#326
Let	_	_
Di	_	_
(	_	_
x	_	_
)	_	_
denotes	_	_
the	_	_
probability	_	_
of	_	_
x	_	_
belongs	_	_
to	_	_
class	_	_
i	_	_
,	_	_
the	_	_
discriminator	_	_
loss	_	_
LD	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LD	_	_
=	_	_
−	_	_
Ex∈Xs	_	_
logD1	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
−	_	_
Ex∈Xt	_	_
logD2	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
−	_	_
Ex∈Xt	_	_
logD3	_	_
(	_	_
x	_	_
)	_	_
.	_	_

#327
(	_	_
21	_	_
)	_	_
Similarly	_	_
,	_	_
the	_	_
generator’s	_	_
adversarial	_	_
loss	_	_
LGANG	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LGANG	_	_
=	_	_
−	_	_
Ex∈Xs	_	_
logD3	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
−	_	_
Ex∈Xt	_	_
logD3	_	_
(	_	_
g	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
.	_	_

#328
(	_	_
22	_	_
)	_	_
In	_	_
order	_	_
to	_	_
slightly	_	_
smoothen	_	_
the	_	_
generated	_	_
images	_	_
,	_	_
DTN	_	_
adds	_	_
an	_	_
anisotropic	_	_
total	_	_
variation	_	_
loss	_	_
[	_	_
81	_	_
]	_	_
LTV	_	_
defined	_	_
on	_	_
generated	_	_
images	_	_
z	_	_
=	_	_
[	_	_
zij	_	_
]	_	_
=	_	_
G	_	_
(	_	_
x	_	_
)	_	_
:	_	_
LTV	_	_
(	_	_
z	_	_
)	_	_
=	_	_
∑	_	_
i	_	_
,	_	_
j	_	_
(	_	_
(	_	_
zi	_	_
,	_	_
j+1	_	_
−	_	_
z2ij	_	_
+	_	_
(	_	_
zi+1	_	_
,	_	_
j	_	_
−	_	_
zij	_	_
)	_	_
2	_	_
)	_	_
2	_	_
.	_	_

#329
(	_	_
23	_	_
)	_	_
Then	_	_
the	_	_
overall	_	_
generator	_	_
loss	_	_
LG	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LG	_	_
=	_	_
LGANG	_	_
+	_	_
αLCONST	_	_
+	_	_
βLTID	_	_
+	_	_
γLTV	_	_
.	_	_

#330
(	_	_
24	_	_
)	_	_
Experiments	_	_
show	_	_
that	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
produces	_	_
impressive	_	_
images	_	_
on	_	_
face-to-emoji	_	_
task	_	_
,	_	_
which	_	_
is	_	_
competitive	_	_
with	_	_
some	_	_
existing	_	_
emoji	_	_
generating	_	_
programs	_	_
.	_	_

#331
5.6	_	_
Unsupervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_

#332
Auxiliary	_	_
Classifier	_	_
Bousmalis	_	_
et	_	_
.	_	_

#333
al	_	_
[	_	_
82	_	_
]	_	_
propose	_	_
to	_	_
use	_	_
a	_	_
task-specific	_	_
auxiliary	_	_
classifier	_	_
to	_	_
help	_	_
unsupervised	_	_
image-to-image	_	_
translation	_	_
.	_	_

#334
We	_	_
denote	_	_
their	_	_
model	_	_
as	_	_
DAAC	_	_
(	_	_
short	_	_
for	_	_
“Domain	_	_
Adaption	_	_
with	_	_
Auxiliary	_	_
Classifier”	_	_
)	_	_
here	_	_
for	_	_
convenience	_	_
.	_	_

#335
DAAC	_	_
[	_	_
82	_	_
]	_	_
contains	_	_
a	_	_
task-specific	_	_
classifier	_	_
C	_	_
(	_	_
x	_	_
)	_	_
→	_	_
y	_	_
that	_	_
assigns	_	_
a	_	_
label	_	_
vector	_	_
y	_	_
to	_	_
an	_	_
image	_	_
in	_	_
either	_	_
source	_	_
domain	_	_
or	_	_
target	_	_
domain	_	_
.	_	_

#336
This	_	_
classifier	_	_
C	_	_
,	_	_
for	_	_
example	_	_
,	_	_
can	_	_
be	_	_
an	_	_
image	_	_
classification	_	_
model	_	_
.	_	_

#337
The	_	_
architecture	_	_
of	_	_
DAAC	_	_
is	_	_
similar	_	_
to	_	_
Figure	_	_
2	_	_
.	_	_

#338
Given	_	_
a	_	_
training	_	_
set	_	_
Xs	_	_
in	_	_
which	_	_
each	_	_
image	_	_
x	_	_
∈	_	_
Xs	_	_
has	_	_
a	_	_
class	_	_
label	_	_
yx	_	_
,	_	_
the	_	_
objective	_	_
of	_	_
C	_	_
is	_	_
to	_	_
minimize	_	_
the	_	_
cross-entropy	_	_
loss	_	_
LC	_	_
:	_	_
Lc	_	_
(	_	_
G	_	_
,	_	_
C	_	_
)	_	_
=	_	_
Ex∈Xs	_	_
[	_	_
−yᵀ	_	_
x	_	_
log	_	_
[	_	_
C	_	_
(	_	_
G	_	_
(	_	_
x	_	_
)	_	_
)	_	_
]	_	_
−	_	_
yᵀ	_	_
x	_	_
log	_	_
[	_	_
C	_	_
(	_	_
x	_	_
)	_	_
]	_	_
]	_	_
.	_	_
(	_	_
25	_	_
)	_	_

#339
Besides	_	_
,	_	_
DAAC	_	_
[	_	_
82	_	_
]	_	_
also	_	_
proposes	_	_
a	_	_
content-similarity	_	_
loss	_	_
in	_	_
cases	_	_
with	_	_
prior	_	_
knowledge	_	_
on	_	_
what	_	_
information	_	_
should	deontic	_
be	_	_
preserved	_	_
after	_	_
the	_	_
domain	_	_
adaption	_	_
process	_	_
.	_	_

#340
For	_	_
example	_	_
,	_	_
we	_	_
may	_	_
expect	_	_
the	_	_
hues	_	_
of	_	_
the	_	_
source	_	_
image	_	_
and	_	_
the	_	_
adapted	_	_
image	_	_
to	_	_
be	_	_
the	_	_
same	_	_
.	_	_

#341
In	_	_
[	_	_
82	_	_
]	_	_
,	_	_
the	_	_
authors	_	_
consider	_	_
the	_	_
case	_	_
where	_	_
they	_	_
render	_	_
objects	_	_
with	_	_
black	_	_
background	_	_
and	_	_
expect	_	_
the	_	_
adapted	_	_
images	_	_
to	_	_
have	_	_
the	_	_
same	_	_
objects	_	_
but	_	_
different	_	_
backgrouds	_	_
.	_	_

#342
In	_	_
this	_	_
case	_	_
,	_	_
each	_	_
image	_	_
x	_	_
is	_	_
associated	_	_
with	_	_
a	_	_
binary	_	_
mask	_	_
mx	_	_
∈	_	_
Rk	_	_
(	_	_
k	_	_
is	_	_
the	_	_
number	_	_
of	_	_
pixels	_	_
in	_	_
x	_	_
)	_	_
to	_	_
separate	_	_
foreground	_	_
and	_	_
background	_	_
,	_	_
and	_	_
the	_	_
content-similarity	_	_
loss	_	_
Ls	_	_
,	_	_
which	_	_
is	_	_
a	_	_
variation	_	_
of	_	_
the	_	_
pairwise	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
PMSE	_	_
)	_	_
[	_	_
83	_	_
]	_	_
,	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
:	_	_
Ls	_	_
(	_	_
G	_	_
)	_	_
=	_	_
Ex∈Xs	_	_
[	_	_
k	_	_
||	_	_
(	_	_
x−G	_	_
(	_	_
x	_	_
)	_	_
)	_	_
◦mx||22	_	_
−	_	_
1	_	_
k2	_	_
(	_	_
(	_	_
x−G	_	_
(	_	_
x	_	_
)	_	_
)	_	_
ᵀmx	_	_
)	_	_
2	_	_
]	_	_
,	_	_
(	_	_
26	_	_
)	_	_
where	_	_
||	_	_
·	_	_
||22	_	_
is	_	_
the	_	_
squared	_	_
L2	_	_
norm	_	_
,	_	_
and	_	_
◦	_	_
is	_	_
the	_	_
Hardamard	_	_
product	_	_
.	_	_

#343
Note	_	_
that	_	_
here	_	_
the	_	_
image	_	_
x	_	_
is	_	_
flattened	_	_
into	_	_
a	_	_
vector	_	_
form	_	_
.	_	_

#344
The	_	_
total	_	_
objective	_	_
is	_	_
thus	_	_
given	_	_
by	_	_
:	_	_
arg	_	_
min	_	_
G	_	_
,	_	_
C	_	_
max	_	_
D	_	_
αLGAN	_	_
(	_	_
G	_	_
,	_	_
D	_	_
)	_	_
+	_	_
βLc	_	_
(	_	_
G	_	_
,	_	_
C	_	_
)	_	_
+	_	_
γLs	_	_
(	_	_
G	_	_
)	_	_
,	_	_
(	_	_
27	_	_
)	_	_
where	_	_
LGAN	_	_
(	_	_
G	_	_
,	_	_
D	_	_
)	_	_
is	_	_
the	_	_
regular	_	_
GAN	_	_
objective	_	_
defined	_	_
in	_	_
Equation	_	_
1	_	_
,	_	_
and	_	_
α	_	_
,	_	_
β	_	_
,	_	_
γ	_	_
are	_	_
hyper-parameters	_	_
to	_	_
balance	_	_
different	_	_
terms	_	_
of	_	_
the	_	_
objective	_	_
.	_	_

#345
Although	_	_
Lc	_	_
(	_	_
G	_	_
,	_	_
C	_	_
)	_	_
and	_	_
Ls	_	_
(	_	_
G	_	_
)	_	_
help	_	_
in	_	_
generating	_	_
better	_	_
adapted	_	_
images	_	_
,	_	_
the	_	_
amount	_	_
of	_	_
labeled	_	_
images	_	_
is	_	_
limited	_	_
in	_	_
real	_	_
world	_	_
,	_	_
and	_	_
fine-grained	_	_
pixel-level	_	_
masks	_	_
are	_	_
even	_	_
harder	_	_
to	_	_
obtain	_	_
.	_	_

#346
Nonetheless	_	_
,	_	_
using	_	_
auxiliary	_	_
classifier	_	_
can	_	_
also	_	_
help	_	_
to	_	_
avoid	_	_
mode	_	_
collapse	_	_
,	_	_
as	_	_
in	_	_
AC-GAN	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#347
EA	_	_
EB	_	_
GB	_	_
GA	_	_
DA	_	_
DB	_	_
zA	_	_
⇠	_	_
EA	_	_
(	_	_
xA	_	_
)	_	_
xA	_	_
zB	_	_
⇠	_	_
EB	_	_
(	_	_
xB	_	_
)	_	_
GA	_	_
(	_	_
zA	_	_
)	_	_
GA	_	_
(	_	_
zB	_	_
)	_	_
GB	_	_
(	_	_
zB	_	_
)	_	_
GB	_	_
(	_	_
zA	_	_
)	_	_
XB	_	_
Shared	_	_
latent	_	_
space	_	_
True/Fake	_	_
True/Fake	_	_
Fig.	_	_
10	_	_
.	_	_

#348
Architecture	_	_
of	_	_
UNIT	_	_
.	_	_

#349
The	_	_
two	_	_
encoders	_	_
share	_	_
weights	_	_
at	_	_
the	_	_
last	_	_
few	_	_
layers	_	_
,	_	_
while	_	_
the	_	_
two	_	_
generators	_	_
share	_	_
weights	_	_
at	_	_
the	_	_
first	_	_
few	_	_
layers	_	_
,	_	_
as	_	_
indicated	_	_
by	_	_
the	_	_
dashed	_	_
lines	_	_
.	_	_

#350
5.7	_	_
Unsupervised	_	_
Image-to-Image	_	_
Translation	_	_
with	_	_

#351
VAE	_	_
and	_	_
Weight	_	_
Sharing	_	_
UNIT	_	_
[	_	_
84	_	_
]	_	_
proposes	_	_
to	_	_
add	_	_
VAE	_	_
to	_	_
CoGAN	_	_
[	_	_
85	_	_
]	_	_
for	_	_
unsupervised	_	_
image-to-image	_	_
translation	_	_
,	_	_
as	_	_
illustrated	_	_
in	_	_
Figure	_	_
10	_	_
.	_	_

#352
In	_	_
addition	_	_
,	_	_
UNIT	_	_
assumes	_	_
that	_	_
both	_	_
encoders	_	_
share	_	_
the	_	_
same	_	_
latent	_	_
space	_	_
,	_	_
which	_	_
means	_	_
,	_	_
let	_	_
xA	_	_
,	_	_
xB	_	_
be	_	_
the	_	_
same	_	_
image	_	_
in	_	_
different	_	_
domains	_	_
,	_	_
and	_	_
then	_	_
the	_	_
shared	_	_
latent	_	_
space	_	_
implies	_	_
that	_	_
EA	_	_
(	_	_
xA	_	_
)	_	_
=	_	_
EB	_	_
(	_	_
BB	_	_
)	_	_
.	_	_

#353
Based	_	_
on	_	_
the	_	_
shared-latent	_	_
space	_	_
assumption	_	_
,	_	_
UNIT	_	_
enforces	_	_
weight	_	_
sharing	_	_
between	_	_
the	_	_
last	_	_
few	_	_
layers	_	_
of	_	_
the	_	_
encoders	_	_
and	_	_
between	_	_
the	_	_
first	_	_
few	_	_
layers	_	_
of	_	_
the	_	_
generators	_	_
.	_	_

#354
The	_	_
objective	_	_
function	_	_
for	_	_
UNIT	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
the	_	_
objectives	_	_
of	_	_
GAN	_	_
and	_	_
VAE	_	_
,	_	_
with	_	_
the	_	_
difference	_	_
of	_	_
using	_	_
two	_	_
sets	_	_
of	_	_
GANs/VAEs	_	_
and	_	_
adding	_	_
hyper-parameters	_	_
λs	_	_
to	_	_
balance	_	_
different	_	_
loss	_	_
terms	_	_
.	_	_

#355
Also	_	_
,	_	_
UNIT	_	_
states	_	_
that	_	_
the	_	_
shared	_	_
latent	_	_
space	_	_
assumption	_	_
implies	_	_
the	_	_
cycle-consistency	_	_
[	_	_
73	_	_
]	_	_
[	_	_
76	_	_
]	_	_
[	_	_
74	_	_
]	_	_
,	_	_
so	_	_
it	_	_
adds	_	_
another	_	_
constraint	_	_
to	_	_
its	_	_
objective	_	_
which	_	_
is	_	_
VAE-like	_	_
:	_	_
Lcc1	_	_
=λ3DKL	_	_
(	_	_
qA	_	_
(	_	_
zA|xA	_	_
)	_	_
||pη	_	_
(	_	_
z	_	_
)	_	_
)	_	_
(	_	_
28	_	_
)	_	_
+	_	_
λ3DKL	_	_
(	_	_
qB	_	_
(	_	_
zB	_	_
|FAB	_	_
(	_	_
xA	_	_
)	_	_
)	_	_
||pη	_	_
(	_	_
z	_	_
)	_	_
)	_	_
−	_	_
λ4EzB∼qB	_	_
(	_	_
zB	_	_
|FAB	_	_
(	_	_
xA	_	_
)	_	_
)	_	_
[	_	_
log	_	_
pGA	_	_
(	_	_
xA|zB	_	_
)	_	_
]	_	_
,	_	_
where	_	_
qA	_	_
(	_	_
zA|xA	_	_
)	_	_
=	_	_
N	_	_
(	_	_
zA|EA	_	_
(	_	_
xA	_	_
)	_	_
,	_	_
I	_	_
)	_	_
,	_	_
qB	_	_
(	_	_
zA|xB	_	_
)	_	_
=	_	_
N	_	_
(	_	_
zB	_	_
|EA	_	_
(	_	_
xB	_	_
)	_	_
,	_	_
I	_	_
)	_	_
,	_	_
pη	_	_
(	_	_
z	_	_
)	_	_
=	_	_
N	_	_
(	_	_
z|0	_	_
,	_	_
I	_	_
)	_	_
,	_	_
and	_	_
FAB	_	_
(	_	_
x	_	_
)	_	_
=	_	_
GB	_	_
(	_	_
EA	_	_
(	_	_
x	_	_
)	_	_
)	_	_
.	_	_

#356
And	_	_
Lcc2	_	_
can	_	_
be	_	_
defined	_	_
similarly	_	_
by	_	_
reversing	_	_
subscripts	_	_
of	_	_
A	_	_
and	_	_
B	_	_
.	_	_

#357
Although	_	_
UNIT	_	_
performs	_	_
better	_	_
than	_	_
models	_	_
like	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
and	_	_
CoGAN	_	_
[	_	_
85	_	_
]	_	_
on	_	_
MNIST	_	_
[	_	_
41	_	_
]	_	_
,	_	_
Street	_	_
View	_	_
House	_	_
Number	_	_
(	_	_
SVHN	_	_
)	_	_
[	_	_
86	_	_
]	_	_
datasets	_	_
in	_	_
terms	_	_
of	_	_
cross-domain	_	_
classification	_	_
accuracy	_	_
,	_	_
it	_	_
does	_	_
not	_	_
compare	_	_
with	_	_
other	_	_
unsupervised	_	_
methods	_	_
such	_	_
as	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
and	_	_
DiscoGAN	_	_
[	_	_
76	_	_
]	_	_
,	_	_
nor	_	_
does	_	_
it	_	_
use	_	_
other	_	_
widely	_	_
adopted	_	_
evaluation	_	_
metrics	_	_
like	_	_
Inception	_	_
Score	_	_
.	_	_

#358
5.8	_	_
Unsupervised	_	_
Multi-domain	_	_
Image-to-Image	_	_
Translation	_	_

#359
Previous	_	_
models	_	_
can	_	_
only	_	_
transform	_	_
images	_	_
between	_	_
two	_	_
domains	_	_
,	_	_
but	_	_
if	_	_
we	_	_
want	_	_
to	_	_
transform	_	_
an	_	_
image	_	_
among	_	_
several	_	_
domains	_	_
,	_	_
we	_	_
need	_	_
to	_	_
train	_	_
a	_	_
separate	_	_
generator	_	_
for	_	_
each	_	_
pair	_	_
of	_	_
domains	_	_
,	_	_
which	_	_
is	_	_
costly	_	_
.	_	_

#360
To	_	_
deal	_	_
with	_	_
this	_	_
problem	_	_
,	_	_
StarGAN	_	_
[	_	_
87	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
one	_	_
generator	_	_
that	_	_
can	_	_
generate	_	_
images	_	_
of	_	_
all	_	_
domains	_	_
.	_	_

#361
Instead	_	_
of	_	_
taking	_	_
only	_	_
an	_	_
image	_	_
as	_	_
conditional	_	_
input	_	_
,	_	_
StarGAN	_	_
also	_	_
takes	_	_
the	_	_
label	_	_
of	_	_
target	_	_
domain	_	_
as	_	_
input	_	_
,	_	_
and	_	_
the	_	_
generator	_	_
is	_	_
designed	_	_
to	_	_
transform	_	_
the	_	_
input	_	_
image	_	_
to	_	_
the	_	_
target	_	_
domain	_	_
indicated	_	_
by	_	_
the	_	_
input	_	_
label	_	_
.	_	_

#362
Similar	_	_
to	_	_
DAAC	_	_
[	_	_
82	_	_
]	_	_
and	_	_
AC-GAN	_	_
[	_	_
33	_	_
]	_	_
,	_	_
StarGAN	_	_
uses	_	_
an	_	_
auxiliary	_	_
domain	_	_
classifier	_	_
which	_	_
classifies	_	_
an	_	_
image	_	_
into	_	_
its	_	_
belonged	_	_
domain	_	_
.	_	_

#363
In	_	_
addition	_	_
,	_	_
a	_	_
cycle-consistency	_	_
loss	_	_
[	_	_
73	_	_
]	_	_
is	_	_
used	_	_
to	_	_
preserve	_	_
the	_	_
content	_	_
similarity	_	_
between	_	_
input	_	_
and	_	_
output	_	_
images	_	_
.	_	_

#364
In	_	_
order	_	_
to	_	_
allow	_	_
StarGAN	_	_
to	_	_
train	_	_
on	_	_
multiple	_	_
datasets	_	_
that	_	_
may	_	_
have	_	_
different	_	_
sets	_	_
of	_	_
labels	_	_
,	_	_
StarGAN	_	_
uses	_	_
an	_	_
additional	_	_
one-hot	_	_
vector	_	_
to	_	_
indicate	_	_
the	_	_
datset	_	_
and	_	_
concatenates	_	_
all	_	_
label	_	_
vectors	_	_
into	_	_
one	_	_
vector	_	_
,	_	_
setting	_	_
the	_	_
unspecified	_	_
labels	_	_
as	_	_
zero	_	_
for	_	_
each	_	_
dataset	_	_
.	_	_

#365
5.9	_	_
Summary	_	_
on	_	_
General	_	_
Image-to-Image	_	_
Translation	_	_

#366
So	_	_
far	_	_
we	_	_
have	_	_
discussed	_	_
some	_	_
general	_	_
image-to-image	_	_
translation	_	_
methods	_	_
,	_	_
the	_	_
different	_	_
losses	_	_
they	_	_
use	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#367
The	_	_
simplest	_	_
loss	_	_
is	_	_
the	_	_
pixel-wise	_	_
L1	_	_
reconstruction	_	_
loss	_	_
operates	_	_
in	_	_
the	_	_
target	_	_
domain	_	_
,	_	_
which	_	_
requires	_	_
paired	_	_
training	_	_
samples	_	_
.	_	_

#368
Both	_	_
one-sided	_	_
and	_	_
bi-directional	_	_
reconstruction	_	_
loss	_	_
can	_	_
treated	_	_
as	_	_
the	_	_
unsupervised	_	_
version	_	_
of	_	_
pixel-wise	_	_
L1	_	_
reconstruction	_	_
loss	_	_
,	_	_
since	_	_
they	_	_
enforce	_	_
cycle-consistency	_	_
and	_	_
do	_	_
not	_	_
require	_	_
paired	_	_
training	_	_
samples	_	_
.	_	_

#369
The	_	_
additional	_	_
VAE	_	_
loss	_	_
is	_	_
based	_	_
on	_	_
the	_	_
assumption	_	_
of	_	_
shared	_	_
latent	_	_
space	_	_
of	_	_
both	_	_
source	_	_
and	_	_
target	_	_
domain	_	_
,	_	_
and	_	_
it	_	_
also	_	_
implies	_	_
the	_	_
bi-directional	_	_
cycle-consistency	_	_
loss	_	_
.	_	_

#370
The	_	_
equivariance	_	_
loss	_	_
,	_	_
however	_	_
,	_	_
does	_	_
not	_	_
try	_	_
to	_	_
reconstruct	_	_
images	_	_
,	_	_
but	_	_
to	_	_
preserve	_	_
the	_	_
difference	_	_
between	_	_
images	_	_
across	_	_
source	_	_
and	_	_
target	_	_
domain	_	_
.	_	_

#371
Different	_	_
from	_	_
previously	_	_
mentioned	_	_
losses	_	_
that	_	_
work	_	_
on	_	_
the	_	_
generators	_	_
directly	_	_
,	_	_
pair-wise	_	_
discriminator	_	_
loss	_	_
,	_	_
ternary	_	_
discriminator	_	_
loss	_	_
and	_	_
auxiliary	_	_
classifier	_	_
loss	_	_
work	_	_
on	_	_
the	_	_
discriminator	_	_
side	_	_
and	_	_
make	_	_
the	_	_
discriminator	_	_
better	_	_
at	_	_
distinguishing	_	_
real	_	_
and	_	_
fake	_	_
samples	_	_
,	_	_
and	_	_
then	_	_
the	_	_
generator	_	_
can	_	_
also	_	_
learn	_	_
better	_	_
with	_	_
the	_	_
enhanced	_	_
discriminator	_	_
.	_	_

#372
Among	_	_
all	_	_
mentioned	_	_
models	_	_
,	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
produces	_	_
sharpest	_	_
images	_	_
,	_	_
even	_	_
though	_	_
the	_	_
L1	_	_
loss	_	_
is	_	_
just	_	_
a	_	_
simple	_	_
add-on	_	_
component	_	_
to	_	_
the	_	_
original	_	_
GAN	_	_
model	_	_
.	_	_

#373
It	_	_
may	_	_
be	_	_
interesting	_	_
to	_	_
combine	_	_
L1	_	_
loss	_	_
with	_	_
the	_	_
pair-wise	_	_
discriminator	_	_
in	_	_
PLDT	_	_
[	_	_
35	_	_
]	_	_
which	_	_
may	_	_
improve	_	_
the	_	_
model’s	_	_
performance	_	_
on	_	_
image-to-image	_	_
translations	_	_
that	_	_
involve	_	_
geometric	_	_
changes	_	_
on	_	_
images	_	_
.	_	_

#374
Also	_	_
,	_	_
Pix2Pix	_	_
may	_	_
benefit	_	_
from	_	_
preserving	_	_
similarity	_	_
information	_	_
between	_	_
images	_	_
in	_	_
source	_	_
and	_	_
target	_	_
domains	_	_
,	_	_
as	_	_
done	_	_
in	_	_
some	_	_
unsupervised	_	_
methods	_	_
like	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
and	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
.	_	_

#375
As	_	_
for	_	_
unsupervised	_	_
methods	_	_
,	_	_
although	_	_
their	_	_
results	_	_
are	_	_
not	_	_
as	_	_
sharp	_	_
as	_	_
supervised	_	_
methods	_	_
like	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
,	_	_
they	_	_
are	_	_
a	_	_
promising	_	_
research	_	_
direction	_	_
,	_	_
since	_	_
they	_	_
do	_	_
not	_	_
require	_	_
paired	_	_
data	_	_
and	_	_
collecting	_	_
labeled	_	_
data	_	_
is	_	_
very	_	_
costly	_	_
in	_	_
real	_	_
world	_	_
.	_	_

#376
TABLE	_	_
1	_	_
Summary	_	_
of	_	_
losses	_	_
used	_	_
in	_	_
image	_	_
image-to-image	_	_
translation	_	_
,	_	_
“sup”	_	_
is	_	_
short	_	_
for	_	_
“supervision”	_	_
.	_	_

#377
“	_	_
√	_	_
”	_	_
in	_	_
both	_	_
“source	_	_
domain”	_	_
and	_	_
“target	_	_
domain”	_	_
columns	_	_
indicates	_	_
that	_	_
the	_	_
loss	_	_
requires	_	_
images	_	_
from	_	_
both	_	_
domains	_	_
(	_	_
but	_	_
not	_	_
necessarily	_	_
paired	_	_
)	_	_
.	_	_

#378
Loss	_	_
Source	_	_
domain	_	_
Target	_	_
domain	_	_
Sup	_	_
Models	_	_
Pixel-level	_	_
L1	_	_
√	_	_
√	_	_
Pix2Pix	_	_
[	_	_
67	_	_
]	_	_
Pair-wise	_	_
discriminator	_	_
√	_	_
√	_	_
PLDT	_	_
[	_	_
35	_	_
]	_	_
Bi-directional	_	_
reconstruction	_	_
√	_	_
√	_	_
CycleGAN	_	_
[	_	_
73	_	_
]	_	_
,	_	_
DualGAN	_	_
[	_	_
74	_	_
]	_	_
,	_	_
DiscoGAN	_	_
[	_	_
76	_	_
]	_	_
,	_	_
StarGAN	_	_
[	_	_
87	_	_
]	_	_
One-sided	_	_
reconstruction	_	_
√	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
Ternary	_	_
discriminator	_	_
√	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
Equivariance	_	_
√	_	_
√	_	_
DistanceGAN	_	_
[	_	_
79	_	_
]	_	_
Auxiliary	_	_
classifier	_	_
√	_	_
√	_	_
DAAC	_	_
[	_	_
82	_	_
]	_	_
,	_	_
StarGAN	_	_
[	_	_
87	_	_
]	_	_
VAE	_	_
√	_	_
√	_	_
UNIT	_	_
[	_	_
84	_	_
]	_	_

#379
5.10	_	_
Task-Specific	_	_
Image-to-Image	_	_
Translation	_	_

#380
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
are	_	_
going	_	_
to	_	_
introduce	_	_
some	_	_
models	_	_
that	_	_
work	_	_
on	_	_
more	_	_
specific	_	_
image-to-image	_	_
translation	_	_
applications	_	_
.	_	_

#381
5.10.1	_	_
Face	_	_
Editing	_	_

#382
Face	_	_
editing	_	_
is	_	_
closely	_	_
related	_	_
to	_	_
image-to-image	_	_
translation	_	_
,	_	_
since	_	_
it	_	_
also	_	_
takes	_	_
images	_	_
as	_	_
input	_	_
and	_	_
produces	_	_
images	_	_
.	_	_

#383
However	_	_
,	_	_
Face	_	_
editing	_	_
focuses	_	_
more	_	_
on	_	_
manipulating	_	_
the	_	_
attributes	_	_
of	_	_
humans’	_	_
faces	_	_
while	_	_
image-to-image	_	_
translation	_	_
is	_	_
a	_	_
more	_	_
general	_	_
scope	_	_
.	_	_

#384
IcGAN	_	_
[	_	_
43	_	_
]	_	_
proposes	_	_
to	_	_
learn	_	_
two	_	_
separate	_	_
encoders	_	_
,	_	_
Ez	_	_
that	_	_
maps	_	_
an	_	_
image	_	_
to	_	_
its	_	_
latent	_	_
vector	_	_
z	_	_
and	_	_
Ey	_	_
that	_	_
learns	_	_
the	_	_
attribute	_	_
information	_	_
vector	_	_
y	_	_
.	_	_

#385
Attribute	_	_
manipulation	_	_
is	_	_
performed	_	_
by	_	_
tuning	_	_
the	_	_
attribute	_	_
vector	_	_
y	_	_
,	_	_
concatenating	_	_
it	_	_
with	_	_
z	_	_
and	_	_
then	_	_
putting	_	_
the	_	_
combined	_	_
vector	_	_
as	_	_
input	_	_
to	_	_
the	_	_
generator	_	_
.	_	_

#386
Instead	_	_
of	_	_
generating	_	_
images	_	_
directly	_	_
,	_	_
Shen	_	_
et	_	_
.	_	_

#387
al	_	_
[	_	_
88	_	_
]	_	_
propose	_	_
to	_	_
learn	_	_
residual	_	_
images	_	_
of	_	_
attributes	_	_
.	_	_

#388
To	_	_
add	_	_
an	_	_
attribute	_	_
to	_	_
an	_	_
image	_	_
x	_	_
,	_	_
a	_	_
residual	_	_
imageG	_	_
(	_	_
x	_	_
)	_	_
is	_	_
obtained	_	_
by	_	_
putting	_	_
it	_	_
through	_	_
a	_	_
generator	_	_
G	_	_
,	_	_
and	_	_
then	_	_
the	_	_
manipulated	_	_
image	_	_
is	_	_
obtained	_	_
by	_	_
x+G	_	_
(	_	_
x	_	_
)	_	_
.	_	_

#389
The	_	_
framework	_	_
of	_	_
this	_	_
model	_	_
follows	_	_
the	_	_
dual	_	_
learning	_	_
approach	_	_
we	_	_
discuss	_	_
earlier	_	_
,	_	_
and	_	_
the	_	_
discriminator	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
ternary	_	_
classifier	_	_
in	_	_
DTN	_	_
[	_	_
80	_	_
]	_	_
,	_	_
which	_	_
distinguishes	_	_
whether	_	_
the	_	_
image	_	_
is	_	_
from	_	_
ground-truth	_	_
or	_	_
is	_	_
manipulated	_	_
by	_	_
either	_	_
of	_	_
the	_	_
two	_	_
generators	_	_
(	_	_
GAB	_	_
,	_	_
GBA	_	_
)	_	_
.	_	_

#390
Recently	_	_
,	_	_
Brock	_	_
et	_	_
.	_	_

#391
al	_	_
propose	_	_
Introspective	_	_
Adversarial	_	_
Network	_	_
(	_	_
IAN	_	_
)	_	_
[	_	_
89	_	_
]	_	_
for	_	_
neural	_	_
photo	_	_
editing	_	_
,	_	_
which	_	_
,	_	_
similar	_	_
to	_	_
VAE-GAN	_	_
[	_	_
38	_	_
]	_	_
,	_	_
is	_	_
also	_	_
a	_	_
combination	_	_
of	_	_
GAN	_	_
[	_	_
4	_	_
]	_	_
and	_	_
VAE	_	_
[	_	_
39	_	_
]	_	_
.	_	_

#392
However	_	_
,	_	_
unlike	_	_
VAE-GAN	_	_
that	_	_
uses	_	_
different	_	_
networks	_	_
for	_	_
discriminator	_	_
and	_	_
encoder	_	_
,	_	_
IAN	_	_
combines	_	_
the	_	_
discriminator	_	_
and	_	_
encoder	_	_
into	_	_
a	_	_
single	_	_
network	_	_
.	_	_

#393
The	_	_
intuition	_	_
behind	_	_
it	_	_
is	_	_
that	_	_
features	_	_
learned	_	_
by	_	_
a	_	_
well	_	_
trained	_	_
discriminator	_	_
tend	_	_
to	_	_
be	_	_
more	_	_
expressive	_	_
than	_	_
those	_	_
learned	_	_
by	_	_
maximum	_	_
likelihood	_	_
,	_	_
which	_	_
is	_	_
the	_	_
reason	_	_
why	_	_
they	_	_
are	_	_
more	_	_
suitable	_	_
for	_	_
inference	_	_
.	_	_

#394
Specifically	_	_
,	_	_
the	_	_
encoder	_	_
is	_	_
implemented	_	_
as	_	_
a	_	_
fully-connected	_	_
layer	_	_
on	_	_
top	_	_
of	_	_
the	_	_
last	_	_
convolution	_	_
layer	_	_
of	_	_
the	_	_
discriminator	_	_
.	_	_

#395
Besides	_	_
manipulating	_	_
face	_	_
attributes	_	_
,	_	_
there	_	_
are	_	_
also	_	_
other	_	_
forms	_	_
of	_	_
“face	_	_
editing”	_	_
such	_	_
as	_	_
generating	_	_
the	_	_
frontal	_	_
view	_	_
of	_	_
a	_	_
person’s	_	_
face	_	_
based	_	_
on	_	_
pictures	_	_
of	_	_
the	_	_
face’s	_	_
side	_	_
view	_	_
.	_	_

#396
Huang	_	_
et	_	_
.	_	_

#397
al	_	_
propose	_	_
Two-Pathway	_	_
Generative	_	_
Adversarial	_	_
Network	_	_
(	_	_
TP-GAN	_	_
)	_	_
[	_	_
90	_	_
]	_	_
that	_	_
performs	_	_
such	_	_
task	_	_
.	_	_

#398
TP-GAN	_	_
consists	_	_
of	_	_
two	_	_
pathways	_	_
,	_	_
a	_	_
global	_	_
structure	_	_
pathway	_	_
and	_	_
a	_	_
local	_	_
texture	_	_
pathway	_	_
,	_	_
where	_	_
each	_	_
pathway	_	_
contains	_	_
a	_	_
pair	_	_
of	_	_
encoder	_	_
and	_	_
decoder	_	_
.	_	_

#399
The	_	_
global	_	_
pathway	_	_
constructs	_	_
a	_	_
blurry	_	_
frontal	_	_
view	_	_
that	_	_
captures	_	_
the	_	_
global	_	_
structure	_	_
of	_	_
the	_	_
face	_	_
,	_	_
while	_	_
the	_	_
local	_	_
pathway	_	_
with	_	_
four	_	_
sub-networks	_	_
attends	_	_
to	_	_
local	_	_
texture	_	_
details	_	_
around	_	_
four	_	_
facial	_	_
landmarks	_	_
,	_	_
which	_	_
are	_	_
left	_	_
eye	_	_
center	_	_
,	_	_
right	_	_
eye	_	_
center	_	_
,	_	_
nose	_	_
tip	_	_
and	_	_
mouth	_	_
center	_	_
.	_	_

#400
Outputs	_	_
of	_	_
four	_	_
sub-networks	_	_
in	_	_
the	_	_
local	_	_
pathway	_	_
are	_	_
concatenated	_	_
with	_	_
the	_	_
output	_	_
of	_	_
global	_	_
pathway	_	_
,	_	_
and	_	_
then	_	_
the	_	_
combined	_	_
tensor	_	_
is	_	_
fed	_	_
into	_	_
successive	_	_
convolution	_	_
layers	_	_
to	_	_
produce	_	_
the	_	_
final	_	_
synthetic	_	_
image	_	_
.	_	_

#401
In	_	_
addition	_	_
to	_	_
L1	_	_
pixel	_	_
loss	_	_
,	_	_
TP-GAN	_	_
proposes	_	_
to	_	_
use	_	_
a	_	_
symmetry	_	_
loss	_	_
to	_	_
constrain	_	_
the	_	_
symmetry	_	_
of	_	_
human	_	_
faces	_	_
,	_	_
and	_	_
an	_	_
identity	_	_
preserving	_	_
loss	_	_
to	_	_
preserve	_	_
the	_	_
identity	_	_
of	_	_
input	_	_
face	_	_
.	_	_

#402
The	_	_
identity	_	_
loss	_	_
is	_	_
implemented	_	_
as	_	_
a	_	_
perceptual	_	_
loss	_	_
[	_	_
68	_	_
]	_	_
.	_	_

#403
5.10.2	_	_
Image	_	_
Super-Resolution	_	_

#404
Another	_	_
application	_	_
of	_	_
GAN	_	_
that	_	_
is	_	_
related	_	_
to	_	_
image-to-image	_	_
translation	_	_
is	_	_
image	_	_
super-resolution	_	_
,	_	_
which	_	_
is	_	_
the	_	_
task	_	_
of	_	_
taking	_	_
a	_	_
low	_	_
resolution	_	_
image	_	_
as	_	_
input	_	_
and	_	_
outputs	_	_
a	_	_
high	_	_
resolution	_	_
one	_	_
with	_	_
sharp	_	_
details	_	_
.	_	_

#405
SRGAN	_	_
[	_	_
91	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
a	_	_
residual	_	_
block	_	_
[	_	_
92	_	_
]	_	_
based	_	_
generator	_	_
and	_	_
a	_	_
fully	_	_
convolutional	_	_
discriminator	_	_
[	_	_
5	_	_
]	_	_
to	_	_
do	_	_
single	_	_
image	_	_
superresolution	_	_
.	_	_

#406
Besides	_	_
adversarial	_	_
loss	_	_
,	_	_
SRGAN	_	_
also	_	_
combines	_	_
pixel-wise	_	_
MSE	_	_
loss	_	_
,	_	_
perceptual	_	_
loss	_	_
[	_	_
68	_	_
]	_	_
and	_	_
regularization	_	_
loss	_	_
[	_	_
68	_	_
]	_	_
.	_	_

#407
SRGAN	_	_
outperforms	_	_
several	_	_
baselines	_	_
on	_	_
some	_	_
metrics	_	_
by	_	_
a	_	_
small	_	_
margin	_	_
,	_	_
but	_	_
the	_	_
difference	_	_
in	_	_
synthetic	_	_
images	_	_
is	_	_
not	_	_
easy	_	_
to	_	_
tell	_	_
without	_	_
zooming	_	_
in	_	_
.	_	_

#408
5.10.3	_	_
Video	_	_
Prediction	_	_

#409
A	_	_
special	_	_
kind	_	_
of	_	_
related	_	_
application	_	_
is	_	_
video	_	_
prediction	_	_
,	_	_
which	_	_
aims	_	_
to	_	_
predict	_	_
the	_	_
next	_	_
frame	_	_
of	_	_
a	_	_
video	_	_
given	_	_
the	_	_
current	_	_
frame	_	_
(	_	_
or	_	_
a	_	_
history	_	_
of	_	_
frames	_	_
)	_	_
.	_	_

#410
VGAN	_	_
[	_	_
93	_	_
]	_	_
proposes	_	_
to	_	_
use	_	_
a	_	_
hierarchical	_	_
model	_	_
for	_	_
video	_	_
prediction	_	_
.	_	_

#411
The	_	_
generator	_	_
has	_	_
two	_	_
data	_	_
streams	_	_
,	_	_
a	_	_
foreground	_	_
stream	_	_
f	_	_
consisting	_	_
of	_	_
3D	_	_
transposed	_	_
convolutions	_	_
,	_	_
and	_	_
a	_	_
background	_	_
stream	_	_
b	_	_
consisting	_	_
of	_	_
2D	_	_
transposed	_	_
convolutions	_	_
.	_	_

#412
The	_	_
foreground	_	_
stream	_	_
is	_	_
also	_	_
responsible	_	_
for	_	_
generating	_	_
a	_	_
mask	_	_
m	_	_
that	_	_
is	_	_
used	_	_
to	_	_
merged	_	_
the	_	_
results	_	_
of	_	_
two	_	_
streams	_	_
:	_	_
m	_	_
f	_	_
+	_	_
(	_	_
1−m	_	_
)	_	_
b	_	_
.	_	_

#413
The	_	_
discriminator	_	_
is	_	_
a	_	_
set	_	_
of	_	_
spatial-temporal	_	_
convolution	_	_
layers	_	_
.	_	_

#414
Mathieu	_	_
et	_	_
.	_	_

#415
al	_	_
propose	_	_
Adv-GDL	_	_
[	_	_
94	_	_
]	_	_
which	_	_
generates	_	_
future	_	_
frames	_	_
in	_	_
an	_	_
iterative	_	_
way	_	_
,	_	_
from	_	_
low	_	_
to	_	_
high	_	_
resolution	_	_
.	_	_

#416
Let	_	_
s1	_	_
,	_	_
s2	_	_
,	_	_
...	_	_
,	_	_
sk	_	_
be	_	_
a	_	_
set	_	_
of	_	_
sizes	_	_
and	_	_
uk	_	_
be	_	_
the	_	_
upsampling	_	_
operator	_	_
from	_	_
size	_	_
sk−1	_	_
to	_	_
sk	_	_
.	_	_

#417
Let	_	_
Xk	_	_
,	_	_
Yk	_	_
be	_	_
ground-truth	_	_
current	_	_
and	_	_
future	_	_
frames	_	_
of	_	_
size	_	_
sk	_	_
and	_	_
Gk	_	_
be	_	_
the	_	_
generator	_	_
that	_	_
produces	_	_
images	_	_
of	_	_
size	_	_
sk	_	_
,	_	_
then	_	_
the	_	_
predicted	_	_
future	_	_
frame	_	_
Ŷk	_	_
is	_	_
calculated	_	_
by	_	_
Ŷk	_	_
=	_	_
uk	_	_
(	_	_
Ŷk−1	_	_
)	_	_
+Gk	_	_
(	_	_
Xk	_	_
,	_	_
uk	_	_
(	_	_
Ŷk−1	_	_
)	_	_
)	_	_
.	_	_

#418
The	_	_
discriminator	_	_
is	_	_
a	_	_
series	_	_
of	_	_
multi-scale	_	_
convolutional	_	_
networks	_	_
with	_	_
scalar	_	_
output	_	_
.	_	_

#419
Although	_	_
both	_	_
methods	_	_
produce	_	_
reasonable	_	_
output	_	_
videos	_	_
when	_	_
the	_	_
time	_	_
interval	_	_
is	_	_
short	_	_
,	_	_
video	_	_
quality	_	_
becomes	_	_
worse	_	_
as	_	_
the	_	_
time	_	_
increases	_	_
.	_	_

#420
Objects	_	_
in	_	_
synthetic	_	_
videos	_	_
lose	_	_
their	_	_
original	_	_
shapes	_	_
and	_	_
get	_	_
morphed	_	_
to	_	_
indistinguishable	_	_
objects	_	_
in	_	_
some	_	_
cases	_	_
,	_	_
which	_	_
may	_	_
be	_	_
due	_	_
to	_	_
the	_	_
models’	_	_
inability	_	_
to	_	_
learn	_	_
legal	_	_
movements	_	_
of	_	_
objects	_	_
.	_	_

#421
6	_	_
EVALUATION	_	_
METRICS	_	_
ON	_	_
SYNTHETIC	_	_
IMAGES	_	_

#422
It	_	_
is	_	_
very	_	_
hard	_	_
to	_	_
quantify	_	_
the	_	_
quality	_	_
of	_	_
synthetic	_	_
images	_	_
,	_	_
and	_	_
metrics	_	_
like	_	_
RMSE	_	_
are	_	_
not	_	_
suitable	_	_
since	_	_
there	_	_
is	_	_
no	_	_
absolute	_	_
one-to-one	_	_
correspondence	_	_
between	_	_
synthetic	_	_
and	_	_
real	_	_
images	_	_
.	_	_

#423
A	_	_
commonly	_	_
used	_	_
subjective	_	_
metric	_	_
is	_	_
to	_	_
use	_	_
the	_	_
Amazon	_	_
Mechanical	_	_
Turk	_	_
(	_	_
AMT	_	_
)	_	_
3	_	_
that	_	_
hires	_	_
humans	_	_
to	_	_
score	_	_
synthetic	_	_
and	_	_
real	_	_
images	_	_
according	_	_
to	_	_
how	_	_
realistic	_	_
they	_	_
think	_	_
the	_	_
images	_	_
are	_	_
.	_	_

#424
However	_	_
,	_	_
people	_	_
often	_	_
have	_	_
different	_	_
opinions	_	_
of	_	_
what	_	_
is	_	_
good	_	_
or	_	_
bad	_	_
,	_	_
so	_	_
we	_	_
also	_	_
need	_	_
objective	_	_
metrics	_	_
to	_	_
evaluate	_	_
the	_	_
quality	_	_
of	_	_
images	_	_
.	_	_

#425
Inception	_	_
score	_	_
(	_	_
IS	_	_
)	_	_
[	_	_
6	_	_
]	_	_
evaluates	_	_
an	_	_
image	_	_
based	_	_
on	_	_
the	_	_
entropy	_	_
in	_	_
class	_	_
probability	_	_
distribution	_	_
when	_	_
it	_	_
is	_	_
put	_	_
into	_	_
a	_	_
pre-trained	_	_
image	_	_
classifier	_	_
.	_	_

#426
One	_	_
intuition	_	_
behind	_	_
Inception	_	_
score	_	_
is	_	_
that	_	_
the	_	_
better	_	_
an	_	_
image	_	_
x	_	_
is	_	_
,	_	_
the	_	_
lower	_	_
the	_	_
entropy	_	_
of	_	_
conditional	_	_
distribution	_	_
p	_	_
(	_	_
y|x	_	_
)	_	_
should	inference	_
be	_	_
,	_	_
which	_	_
means	_	_
the	_	_
classifier	_	_
have	_	_
high	_	_
confidence	_	_
of	_	_
what	_	_
the	_	_
image	_	_
is	_	_
about	_	_
.	_	_

#427
Also	_	_
,	_	_
to	_	_
encourage	_	_
the	_	_
model	_	_
to	_	_
generate	_	_
various	_	_
classes	_	_
of	_	_
images	_	_
,	_	_
the	_	_
marginal	_	_
distribution	_	_
p	_	_
(	_	_
y	_	_
)	_	_
=	_	_
∫	_	_
p	_	_
(	_	_
y|x	_	_
=	_	_
G	_	_
(	_	_
z	_	_
)	_	_
)	_	_
dz	_	_
should	deontic	_
have	_	_
high	_	_
entropy	_	_
.	_	_

#428
Combining	_	_
these	_	_
two	_	_
intuition	_	_
,	_	_
the	_	_
Inception	_	_
score	_	_
is	_	_
calculated	_	_
by	_	_
exp	_	_
(	_	_
Ex∼G	_	_
(	_	_
z	_	_
)	_	_
DKL	_	_
(	_	_
p	_	_
(	_	_
y|x	_	_
)	_	_
||p	_	_
(	_	_
y	_	_
)	_	_
)	_	_
.	_	_

#429
As	_	_
discussed	_	_
in	_	_
[	_	_
95	_	_
]	_	_
,	_	_
Inception	_	_
score	_	_
is	_	_
neither	_	_
sensitive	_	_
to	_	_
prior	_	_
distribution	_	_
of	_	_
labels	_	_
,	_	_
nor	_	_
a	_	_
proper	_	_
distance	_	_
measure	_	_
.	_	_

#430
Also	_	_
,	_	_
Inception	_	_
score	_	_
suffers	_	_
from	_	_
intra-class	_	_
mode	_	_
collapse	_	_
,	_	_
since	_	_
a	_	_
model	_	_
only	_	_
needs	_	_
to	_	_
generate	_	_
one	_	_
perfect	_	_
sample	_	_
for	_	_
each	_	_
class	_	_
to	_	_
get	_	_
a	_	_
perfect	_	_
Inception	_	_
score	_	_
.	_	_

#431
Similar	_	_
to	_	_
Inception	_	_
score	_	_
,	_	_
FCN-score	_	_
[	_	_
67	_	_
]	_	_
adopts	_	_
the	_	_
idea	_	_
that	_	_
if	_	_
the	_	_
synthetic	_	_
images	_	_
are	_	_
realistic	_	_
,	_	_
classifiers	_	_
trained	_	_
on	_	_
real	_	_
images	_	_
will	_	_
be	_	_
able	_	_
to	_	_
classify	_	_
the	_	_
synthetic	_	_
images	_	_
correctly	_	_
.	_	_

#432
However	_	_
,	_	_
an	_	_
image	_	_
classifier	_	_
does	_	_
not	_	_
require	_	_
the	_	_
input	_	_
image	_	_
to	_	_
be	_	_
very	_	_
sharp	_	_
as	_	_
to	_	_
give	_	_
a	_	_
correct	_	_
classification	_	_
,	_	_
which	_	_
means	_	_
that	_	_
metrics	_	_
based	_	_
on	_	_
image	_	_
classifier	_	_
may	_	_
not	_	_
be	_	_
able	_	_
to	_	_
tell	_	_
between	_	_
two	_	_
images	_	_
with	_	_
only	_	_
small	_	_
difference	_	_
in	_	_
details	_	_
.	_	_

#433
Worse	_	_
still	_	_
,	_	_
research	_	_
in	_	_
adversarial	_	_
examples	_	_
[	_	_
96	_	_
]	_	_
shows	_	_
that	_	_
the	_	_
decision	_	_
of	_	_
a	_	_
classifier	_	_
does	_	_
not	_	_
necessarily	_	_
depend	_	_
on	_	_
visual	_	_
content	_	_
of	_	_
images	_	_
but	_	_
can	_	_
be	_	_
highly	_	_
influenced	_	_
by	_	_
noise	_	_
invisible	_	_
to	_	_
humans	_	_
,	_	_
which	_	_
raises	_	_
more	_	_
questions	_	_
on	_	_
this	_	_
metric	_	_
.	_	_

#434
Fréchet	_	_
Inception	_	_
Distance	_	_
(	_	_
FID	_	_
)	_	_
[	_	_
97	_	_
]	_	_
provides	_	_
a	_	_
different	_	_
approach	_	_
.	_	_

#435
First	_	_
,	_	_
generated	_	_
images	_	_
are	_	_
embedded	_	_
into	_	_
a	_	_
latent	_	_
feature	_	_
space	_	_
of	_	_
a	_	_
chosen	_	_
layer	_	_
of	_	_
the	_	_
Inception	_	_
Net	_	_
.	_	_

#436
Second	_	_
,	_	_
embeddings	_	_
of	_	_
generated	_	_
and	_	_
real	_	_
images	_	_
are	_	_
treated	_	_
as	_	_
samples	_	_
from	_	_
two	_	_
continuous	_	_
multivariate	_	_
Gaussians	_	_
so	_	_
that	_	_
their	_	_
means	_	_
and	_	_
covariances	_	_
can	_	_
be	_	_
calculated	_	_
.	_	_

#437
Then	_	_
the	_	_
quality	_	_
of	_	_
generated	_	_
images	_	_
can	_	_
be	_	_
determined	_	_
by	_	_
the	_	_
Fréchet	_	_
Distance	_	_
between	_	_
the	_	_
two	_	_
Gaussians	_	_
:	_	_
FID	_	_
(	_	_
x	_	_
,	_	_
g	_	_
)	_	_
=	_	_
||µx	_	_
−	_	_
µg||22	_	_
+	_	_
Tr	_	_
(	_	_
Σx	_	_
+	_	_
Σg	_	_
−	_	_
2	_	_
(	_	_
ΣxΣg	_	_
)	_	_
2	_	_
)	_	_
,	_	_
(	_	_
29	_	_
)	_	_
where	_	_
(	_	_
µx	_	_
,	_	_
µg	_	_
)	_	_
and	_	_
(	_	_
Σx	_	_
,	_	_
Σg	_	_
)	_	_
are	_	_
the	_	_
means	_	_
and	_	_
covariances	_	_
of	_	_
the	_	_
samples	_	_
from	_	_
the	_	_
true	_	_
data	_	_
distribution	_	_
and	_	_
generator’s	_	_
learned	_	_
distribution	_	_
respectively	_	_
.	_	_

#438
The	_	_
authors	_	_
of	_	_
[	_	_
97	_	_
]	_	_
show	_	_
that	_	_
FID	_	_
is	_	_
consistent	_	_
with	_	_
human	_	_
judgment	_	_
and	_	_
that	_	_
there	_	_
is	_	_
a	_	_
strong	_	_
negative	_	_
correlation	_	_
between	_	_
FID	_	_
and	_	_
the	_	_
quality	_	_
of	_	_
generated	_	_
images	_	_
.	_	_

#439
Furthermore	_	_
,	_	_
FID	_	_
is	_	_
less	_	_
sensitive	_	_
to	_	_
noise	_	_
than	_	_
IS	_	_
and	_	_
can	_	_
detect	_	_
intra-class	_	_
mode	_	_
collapse	_	_
.	_	_

#440
Besides	_	_
Inception	_	_
score	_	_
(	_	_
IS	_	_
)	_	_
,	_	_
FCN-score	_	_
and	_	_
Fréchet	_	_
Inception	_	_
Distance	_	_
(	_	_
FID	_	_
)	_	_
,	_	_
there	_	_
are	_	_
also	_	_
other	_	_
metrics	_	_
like	_	_
Gaussian	_	_
Parzen	_	_
Window	_	_
[	_	_
4	_	_
]	_	_
,	_	_
Generative	_	_
Adversarial	_	_
Metric	_	_
(	_	_
GAM	_	_
)	_	_
[	_	_
48	_	_
]	_	_
3.	_	_
https	_	_
:	_	_
//www.mturk.com/	_	_
and	_	_
MODE	_	_
Score	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#441
Among	_	_
all	_	_
these	_	_
metrics	_	_
,	_	_
Inception	_	_
score	_	_
is	_	_
the	_	_
most	_	_
widely	_	_
adopted	_	_
one	_	_
for	_	_
quantitatively	_	_
evaluating	_	_
synthetic	_	_
images	_	_
,	_	_
and	_	_
there	_	_
is	_	_
a	_	_
recent	_	_
study	_	_
[	_	_
98	_	_
]	_	_
that	_	_
uses	_	_
IS	_	_
to	_	_
compare	_	_
several	_	_
GAN	_	_
models	_	_
.	_	_

#442
Although	_	_
FID	_	_
is	_	_
relatively	_	_
new	_	_
,	_	_
it	_	_
has	_	_
been	_	_
shown	_	_
to	_	_
be	_	_
better	_	_
than	_	_
IS	_	_
[	_	_
97	_	_
]	_	_
[	_	_
95	_	_
]	_	_
.	_	_

#443
7	_	_
DISCRIMINATORS	_	_
AS	_	_
LEARNED	_	_
LOSS	_	_
FUNCTIONS	_	_

#444
Generative	_	_
adversarial	_	_
network	_	_
(	_	_
GAN	_	_
)	_	_
is	_	_
powerful	_	_
and	_	_
effective	_	_
in	_	_
that	_	_
the	_	_
discriminator	_	_
acts	_	_
as	_	_
a	_	_
learned	_	_
loss	_	_
function	_	_
instead	_	_
of	_	_
a	_	_
fixed	_	_
one	_	_
designed	_	_
carefully	_	_
for	_	_
each	_	_
specific	_	_
task	_	_
.	_	_

#445
This	_	_
is	_	_
particularly	_	_
important	_	_
for	_	_
image	_	_
synthesis	_	_
tasks	_	_
whose	_	_
loss	_	_
functions	_	_
are	_	_
hard	_	_
to	_	_
be	_	_
explicitly	_	_
defined	_	_
in	_	_
math	_	_
.	_	_

#446
For	_	_
example	_	_
,	_	_
in	_	_
style	_	_
transfer	_	_
task	_	_
,	_	_
it	_	_
is	_	_
hard	_	_
to	_	_
write	_	_
down	_	_
a	_	_
math	_	_
equation	_	_
that	_	_
evaluates	_	_
how	_	_
well	_	_
an	_	_
image	_	_
matches	_	_
a	_	_
certain	_	_
painting	_	_
style	_	_
.	_	_

#447
For	_	_
image	_	_
synthesis	_	_
tasks	_	_
,	_	_
each	_	_
input	_	_
may	_	_
have	_	_
many	_	_
legal	_	_
outputs	_	_
,	_	_
but	_	_
samples	_	_
in	_	_
training	_	_
set	_	_
can	_	_
not	_	_
cover	_	_
all	_	_
situations	_	_
.	_	_

#448
In	_	_
this	_	_
case	_	_
,	_	_
it	_	_
is	_	_
inappropriate	_	_
to	_	_
only	_	_
minimize	_	_
the	_	_
distance	_	_
between	_	_
synthetic	_	_
and	_	_
ground-truth	_	_
images	_	_
,	_	_
since	_	_
we	_	_
want	_	_
the	_	_
generator	_	_
to	_	_
learn	_	_
the	_	_
data	_	_
distribution	_	_
instead	_	_
of	_	_
remembering	_	_
training	_	_
samples	_	_
.	_	_

#449
Although	_	_
we	_	_
can	_	_
design	_	_
feature-based	_	_
losses	_	_
that	_	_
try	_	_
to	_	_
preserve	_	_
feature	_	_
consistency	_	_
instead	_	_
of	_	_
at	_	_
raw	_	_
pixel	_	_
level	_	_
,	_	_
as	_	_
done	_	_
in	_	_
the	_	_
perceptual	_	_
loss	_	_
[	_	_
68	_	_
]	_	_
for	_	_
image	_	_
style	_	_
,	_	_
such	_	_
losses	_	_
are	_	_
constrained	_	_
by	_	_
pre-trained	_	_
image	_	_
classification	_	_
models	_	_
they	_	_
use	_	_
,	_	_
and	_	_
it	_	_
remains	_	_
a	_	_
question	_	_
of	_	_
which	_	_
layers	_	_
to	_	_
pick	_	_
for	_	_
calculating	_	_
feature	_	_
loss	_	_
when	_	_
we	_	_
switch	_	_
to	_	_
another	_	_
pre-trained	_	_
model	_	_
.	_	_

#450
A	_	_
discriminator	_	_
,	_	_
on	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
does	_	_
not	_	_
require	_	_
explicit	_	_
definition	_	_
of	_	_
the	_	_
loss	_	_
,	_	_
since	_	_
it	_	_
learns	_	_
how	_	_
to	_	_
evaluate	_	_
a	_	_
data	_	_
sample	_	_
as	_	_
it	_	_
trains	_	_
against	_	_
the	_	_
generator	_	_
.	_	_

#451
Thus	_	_
the	_	_
discriminator	_	_
is	_	_
able	_	_
to	_	_
learn	_	_
a	_	_
better	_	_
loss	_	_
function	_	_
given	_	_
enough	_	_
training	_	_
data	_	_
.	_	_

#452
The	_	_
fact	_	_
that	_	_
the	_	_
discriminator	_	_
acts	_	_
as	_	_
a	_	_
learned	_	_
loss	_	_
function	_	_
has	_	_
significant	_	_
meaning	_	_
for	_	_
general	_	_
artificial	_	_
intelligence	_	_
.	_	_

#453
Traditional	_	_
pattern	_	_
recognition	_	_
and	_	_
machine	_	_
learning	_	_
require	_	_
us	_	_
to	_	_
define	_	_
what	_	_
features	_	_
to	_	_
be	_	_
used	_	_
(	_	_
e.g.	_	_
SIFT	_	_
[	_	_
99	_	_
]	_	_
and	_	_
HOG	_	_
[	_	_
100	_	_
]	_	_
descriptors	_	_
)	_	_
,	_	_
and	_	_
we	_	_
design	_	_
specific	_	_
loss	_	_
functions	_	_
and	_	_
decide	_	_
what	_	_
optimization	_	_
methods	_	_
to	_	_
be	_	_
applied	_	_
.	_	_

#454
Deep	_	_
learning	_	_
free	_	_
us	_	_
from	_	_
carefully	_	_
designing	_	_
features	_	_
,	_	_
by	_	_
learning	_	_
low-level	_	_
and	_	_
high-level	_	_
feature	_	_
representations	_	_
by	_	_
itself	_	_
during	_	_
training	_	_
(	_	_
e.g.	_	_
CNN	_	_
kernels	_	_
)	_	_
,	_	_
but	_	_
we	_	_
still	_	_
need	_	_
to	_	_
work	_	_
hard	_	_
at	_	_
designing	_	_
loss	_	_
functions	_	_
that	_	_
work	_	_
well	_	_
.	_	_

#455
GAN	_	_
takes	_	_
us	_	_
one	_	_
step	_	_
forward	_	_
on	_	_
our	_	_
path	_	_
towards	_	_
artificial	_	_
intelligence	_	_
,	_	_
in	_	_
that	_	_
it	_	_
learns	_	_
how	_	_
to	_	_
evaluate	_	_
data	_	_
samples	_	_
instead	_	_
of	_	_
being	_	_
told	_	_
how	_	_
to	_	_
do	_	_
so	_	_
,	_	_
although	_	_
we	_	_
still	_	_
need	_	_
to	_	_
design	_	_
the	_	_
adversarial	_	_
loss	_	_
and	_	_
combine	_	_
it	_	_
with	_	_
other	_	_
auxiliary	_	_
losses	_	_
.	_	_

#456
In	_	_
other	_	_
words	_	_
,	_	_
previously	_	_
we	_	_
design	_	_
how	_	_
to	_	_
calculate	_	_
how	_	_
close	_	_
an	_	_
output	_	_
is	_	_
to	_	_
the	_	_
corresponding	_	_
ground-truth	_	_
(	_	_
L	_	_
(	_	_
x	_	_
,	_	_
x̂	_	_
)	_	_
)	_	_
,	_	_
but	_	_
the	_	_
discriminator	_	_
learns	_	_
how	_	_
to	_	_
calculate	_	_
how	_	_
well	_	_
an	_	_
output	_	_
matches	_	_
the	_	_
true	_	_
data	_	_
distribution	_	_
(	_	_
L	_	_
(	_	_
x	_	_
)	_	_
)	_	_
.	_	_

#457
Such	_	_
property	_	_
allows	_	_
models	_	_
to	_	_
be	_	_
more	_	_
flexible	_	_
and	_	_
more	_	_
likely	_	_
to	_	_
generalize	_	_
well	_	_
.	_	_

#458
Furthermore	_	_
,	_	_
with	_	_
learn2learn	_	_
[	_	_
101	_	_
]	_	_
which	_	_
allows	_	_
neural	_	_
networks	_	_
to	_	_
learn	_	_
to	_	_
optimize	_	_
themselves	_	_
,	_	_
there	_	_
is	_	_
a	_	_
possibility	_	_
that	_	_
we	_	_
may	_	_
no	_	_
longer	_	_
need	_	_
to	_	_
choose	_	_
what	_	_
optimizers	_	_
(	_	_
such	_	_
as	_	_
RMSprop	_	_
[	_	_
102	_	_
]	_	_
,	_	_
Adam	_	_
[	_	_
103	_	_
]	_	_
etc	_	_
.	_	_
)	_	_

#459
to	_	_
use	_	_
and	_	_
let	_	_
models	_	_
handle	_	_
everything	_	_
themselves	_	_
.	_	_

#460
8	_	_
DISCUSSION	_	_
AND	_	_
CONCLUSION	_	_

#461
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
review	_	_
some	_	_
basics	_	_
of	_	_
Generative	_	_
Adversarial	_	_
Nets	_	_
(	_	_
GAN	_	_
)	_	_
[	_	_
4	_	_
]	_	_
,	_	_
and	_	_
classify	_	_
image	_	_
synthesis	_	_
methods	_	_
into	_	_
three	_	_
main	_	_
approaches	_	_
,	_	_
i.e.	_	_
direct	_	_
method	_	_
,	_	_
hierarchical	_	_
method	_	_
and	_	_
iterative	_	_
method	_	_
,	_	_
and	_	_
mention	_	_
some	_	_
other	_	_
generation	_	_
methods	_	_
such	_	_
as	_	_
iterative	_	_
sampling	_	_
[	_	_
50	_	_
]	_	_
.	_	_

#462
We	_	_
also	_	_
discuss	_	_
in	_	_
details	_	_
two	_	_
main	_	_
forms	_	_
of	_	_
image	_	_
synthesis	_	_
,	_	_
i.e.	_	_
text-to-image	_	_
synthesis	_	_
and	_	_
image-to-image	_	_
translation	_	_
.	_	_

#463
For	_	_
text-to-image	_	_
synthesis	_	_
,	_	_
current	_	_
methods	_	_
work	_	_
well	_	_
on	_	_
datasets	_	_
where	_	_
each	_	_
image	_	_
contains	_	_
single	_	_
object	_	_
such	_	_
as	_	_
CUB	_	_
[	_	_
57	_	_
]	_	_
and	_	_
Oxford-102	_	_
[	_	_
55	_	_
]	_	_
,	_	_
but	_	_
the	_	_
performance	_	_
on	_	_
complex	_	_
datasets	_	_
such	_	_
as	_	_
MSCOCO	_	_
[	_	_
61	_	_
]	_	_
is	_	_
much	_	_
worse	_	_
.	_	_

#464
Although	_	_
some	_	_
models	_	_
can	_	_
produce	_	_
realistic	_	_
images	_	_
of	_	_
rooms	_	_
in	_	_
LSUN	_	_
[	_	_
64	_	_
]	_	_
,	_	_
it	_	_
should	deontic-rhetorical	_
be	_	_
noted	_	_
that	_	_
rooms	_	_
do	_	_
not	_	_
contain	_	_
living	_	_
things	_	_
,	_	_
and	_	_
a	_	_
living	_	_
thing	_	_
is	_	_
certainly	_	_
much	_	_
more	_	_
complicated	_	_
than	_	_
static	_	_
objects	_	_
.	_	_

#465
This	_	_
limitation	_	_
probably	_	_
stems	_	_
from	_	_
the	_	_
models’	_	_
inability	_	_
to	_	_
learn	_	_
different	_	_
concepts	_	_
of	_	_
objects	_	_
.	_	_

#466
We	_	_
also	_	_
propose	_	_
that	_	_
one	_	_
possible	_	_
way	_	_
to	_	_
improve	_	_
GAN’s	_	_
performance	_	_
in	_	_
this	_	_
task	_	_
is	_	_
to	_	_
train	_	_
different	_	_
models	_	_
that	_	_
generate	_	_
single	_	_
object	_	_
well	_	_
and	_	_
train	_	_
another	_	_
model	_	_
that	_	_
learns	_	_
to	_	_
combine	_	_
different	_	_
objects	_	_
according	_	_
to	_	_
text	_	_
descriptions	_	_
,	_	_
and	_	_
that	_	_
CapsNet	_	_
[	_	_
66	_	_
]	_	_
may	_	_
be	_	_
useful	_	_
in	_	_
such	_	_
tasks	_	_
.	_	_

#467
For	_	_
image-to-image	_	_
translation	_	_
,	_	_
we	_	_
review	_	_
some	_	_
general	_	_
methods	_	_
from	_	_
supervised	_	_
to	_	_
unsupervised	_	_
settings	_	_
,	_	_
such	_	_
as	_	_
pixel-wise	_	_
loss	_	_
[	_	_
67	_	_
]	_	_
,	_	_
cyclic	_	_
loss	_	_
[	_	_
73	_	_
]	_	_
and	_	_
self-distance	_	_
loss	_	_
[	_	_
79	_	_
]	_	_
.	_	_

#468
Besides	_	_
,	_	_
we	_	_
also	_	_
introduce	_	_
some	_	_
task-specific	_	_
image-to-image	_	_
translation	_	_
models	_	_
for	_	_
face	_	_
editing	_	_
,	_	_
video	_	_
prediction	_	_
and	_	_
image	_	_
super-resolution	_	_
.	_	_

#469
Image-to-image	_	_
translation	_	_
is	_	_
certainly	_	_
an	_	_
interesting	_	_
application	_	_
of	_	_
GAN	_	_
,	_	_
which	_	_
has	_	_
great	_	_
potential	_	_
to	_	_
be	_	_
incorporated	_	_
into	_	_
other	_	_
software	_	_
products	_	_
,	_	_
especially	_	_
mobile	_	_
apps	_	_
.	_	_

#470
Although	_	_
research	_	_
in	_	_
unsupervised	_	_
methods	_	_
seems	_	_
more	_	_
popular	_	_
,	_	_
supervised	_	_
methods	_	_
may	_	_
be	_	_
more	_	_
practical	_	_
since	_	_
they	_	_
still	_	_
produce	_	_
better	_	_
synthetic	_	_
images	_	_
than	_	_
unsupervised	_	_
methods	_	_
.	_	_

#471
Finally	_	_
,	_	_
we	_	_
review	_	_
some	_	_
evaluation	_	_
metrics	_	_
for	_	_
synthetic	_	_
images	_	_
,	_	_
and	_	_
discuss	_	_
GAN’s	_	_
role	_	_
on	_	_
our	_	_
path	_	_
towards	_	_
artificial	_	_
intelligence	_	_
.	_	_

#472
The	_	_
power	_	_
of	_	_
GAN	_	_
largely	_	_
lies	_	_
in	_	_
its	_	_
discriminator’s	_	_
acting	_	_
as	_	_
a	_	_
learned	_	_
loss	_	_
function	_	_
,	_	_
which	_	_
makes	_	_
the	_	_
model	_	_
perform	_	_
better	_	_
on	_	_
tasks	_	_
whose	_	_
output	_	_
is	_	_
hard	_	_
to	_	_
evaluate	_	_
by	_	_
designing	_	_
an	_	_
explicit	_	_
math	_	_
equation	_	_
.	_	_

#473
ACKNOWLEDGMENTS	_	_
He	_	_
Huang	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
Chenwei	_	_
Zhang	_	_
and	_	_
Bokai	_	_
Cao	_	_
for	_	_
the	_	_
valuable	_	_
discussions	_	_
and	_	_
feedback	_	_
.	_	_