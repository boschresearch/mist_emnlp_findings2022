#0
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
Shihao	_	_
Wu1	_	_
Hui	_	_
Huang2	_	_
?	_	_

#1
Tiziano	_	_
Portenier1	_	_
Matan	_	_
Sela3	_	_
Daniel	_	_
Cohen-Or4	_	_
Ron	_	_
Kimmel3	_	_
Matthias	_	_
Zwicker5	_	_
1University	_	_
of	_	_
Bern	_	_
2Shenzhen	_	_
University	_	_
3Technion	_	_
-	_	_
Israel	_	_
Institute	_	_
of	_	_
Technology	_	_
4Tel-Aviv	_	_
University	_	_
5University	_	_
of	_	_
Maryland	_	_
Abstract	_	_
.	_	_

#2
Most	_	_
multi-view	_	_
3D	_	_
reconstruction	_	_
algorithms	_	_
,	_	_
especially	_	_
when	_	_
shape-from-shading	_	_
cues	_	_
are	_	_
used	_	_
,	_	_
assume	_	_
that	_	_
object	_	_
appearance	_	_
is	_	_
predominantly	_	_
diffuse	_	_
.	_	_

#3
To	_	_
alleviate	_	_
this	_	_
restriction	_	_
,	_	_
we	_	_
introduce	_	_
S2Dnet	_	_
,	_	_
a	_	_
generative	_	_
adversarial	_	_
network	_	_
for	_	_
transferring	_	_
multiple	_	_
views	_	_
of	_	_
objects	_	_
with	_	_
specular	_	_
reflection	_	_
into	_	_
diffuse	_	_
ones	_	_
,	_	_
so	_	_
that	_	_
multi-view	_	_
reconstruction	_	_
methods	_	_
can	_	_
be	_	_
applied	_	_
more	_	_
effectively	_	_
.	_	_

#4
Our	_	_
network	_	_
extends	_	_
unsupervised	_	_
image-to-image	_	_
translation	_	_
to	_	_
multi-view	_	_
“specular	_	_
to	_	_
diffuse”	_	_
translation	_	_
.	_	_

#5
To	_	_
preserve	_	_
object	_	_
appearance	_	_
across	_	_
multiple	_	_
views	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
Multi-View	_	_
Coherence	_	_
loss	_	_
(	_	_
MVC	_	_
)	_	_
that	_	_
evaluates	_	_
the	_	_
similarity	_	_
and	_	_
faithfulness	_	_
of	_	_
local	_	_
patches	_	_
after	_	_
the	_	_
view-transformation	_	_
.	_	_

#6
Our	_	_
MVC	_	_
loss	_	_
ensures	_	_
that	_	_
the	_	_
similarity	_	_
of	_	_
local	_	_
correspondences	_	_
among	_	_
multi-view	_	_
images	_	_
is	_	_
preserved	_	_
under	_	_
the	_	_
image-to-image	_	_
translation	_	_
.	_	_

#7
As	_	_
a	_	_
result	_	_
,	_	_
our	_	_
network	_	_
yields	_	_
significantly	_	_
better	_	_
results	_	_
than	_	_
several	_	_
single-view	_	_
baseline	_	_
techniques	_	_
.	_	_

#8
In	_	_
addition	_	_
,	_	_
we	_	_
carefully	_	_
design	_	_
and	_	_
generate	_	_
a	_	_
large	_	_
synthetic	_	_
training	_	_
data	_	_
set	_	_
using	_	_
physically-based	_	_
rendering	_	_
.	_	_

#9
During	_	_
testing	_	_
,	_	_
our	_	_
network	_	_
takes	_	_
only	_	_
the	_	_
raw	_	_
glossy	_	_
images	_	_
as	_	_
input	_	_
,	_	_
without	_	_
extra	_	_
information	_	_
such	_	_
as	_	_
segmentation	_	_
masks	_	_
or	_	_
lighting	_	_
estimation	_	_
.	_	_

#10
Results	_	_
demonstrate	_	_
that	_	_
multi-view	_	_
reconstruction	_	_
can	_	_
be	_	_
significantly	_	_
improved	_	_
using	_	_
the	_	_
images	_	_
filtered	_	_
by	_	_
our	_	_
network	_	_
.	_	_

#11
We	_	_
also	_	_
show	_	_
promising	_	_
performance	_	_
on	_	_
real	_	_
world	_	_
training	_	_
and	_	_
testing	_	_
data	_	_
.	_	_

#12
Keywords	_	_
:	_	_
Generative	_	_
adversarial	_	_
network	_	_
,	_	_
multi-view	_	_
reconstruction	_	_
,	_	_
multi-view	_	_
coherence	_	_
,	_	_
specular-to-diffuse	_	_
,	_	_
image	_	_
translation	_	_

#13
1	_	_
Introduction	_	_

#14
Three-dimensional	_	_
reconstruction	_	_
from	_	_
multi-view	_	_
images	_	_
is	_	_
a	_	_
long	_	_
standing	_	_
problem	_	_
in	_	_
computer	_	_
vision	_	_
.	_	_

#15
State-of-the-art	_	_
shape-from-shading	_	_
techniques	_	_
achieve	_	_
impressive	_	_
results	_	_
[	_	_
1	_	_
,	_	_
2	_	_
]	_	_
.	_	_

#16
These	_	_
techniques	_	_
,	_	_
however	_	_
,	_	_
make	_	_
rather	_	_
strong	_	_
assumptions	_	_
about	_	_
the	_	_
data	_	_
,	_	_
mainly	_	_
that	_	_
target	_	_
objects	_	_
are	_	_
predominantly	_	_
diffuse	_	_
with	_	_
almost	_	_
no	_	_
specular	_	_
reflectance	_	_
.	_	_

#17
Multi-view	_	_
reconstruction	_	_
of	_	_
glossy	_	_
surfaces	_	_
is	_	_
a	_	_
challenging	_	_
problem	_	_
,	_	_
which	_	_
has	_	_
been	_	_
addressed	_	_
by	_	_
adding	_	_
specialized	_	_
hardware	_	_
(	_	_
e.g.	_	_
,	_	_
coded	_	_
pattern	_	_
projection	_	_
[	_	_
3	_	_
]	_	_
and	_	_
two-layer	_	_
LCD	_	_
[	_	_
4	_	_
]	_	_
)	_	_
,	_	_
imposing	_	_
surface	_	_
constraints	_	_
[	_	_
5,6	_	_
]	_	_
,	_	_
or	_	_
making	_	_
use	_	_
of	_	_
additional	_	_
information	_	_
like	_	_
silhouettes	_	_
and	_	_
environment	_	_
maps	_	_
[	_	_
7	_	_
]	_	_
,	_	_
or	_	_
the	_	_
Blinn-Phong	_	_
model	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#18
?	_	_

#19
Corresponding	_	_
author	_	_
:	_	_
Hui	_	_
Huang	_	_
(	_	_
hhzhiyan	_	_
@	_	_
gmail.com	_	_
)	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
7	_	_
.	_	_

#20
9v	_	_
3	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
3	_	_
0	_	_
Ju	_	_
l	_	_
2	_	_
2	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Fig.	_	_
1	_	_
:	_	_
Specular-to-diffuse	_	_
translation	_	_
of	_	_
multi-view	_	_
images	_	_
.	_	_

#21
We	_	_
show	_	_
eleven	_	_
views	_	_
of	_	_
a	_	_
glossy	_	_
object	_	_
(	_	_
top	_	_
)	_	_
,	_	_
and	_	_
the	_	_
specular-free	_	_
images	_	_
generated	_	_
by	_	_
our	_	_
network	_	_
(	_	_
bottom	_	_
)	_	_
.	_	_

#22
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
present	_	_
a	_	_
generative	_	_
adversarial	_	_
neural	_	_
network	_	_
(	_	_
GAN	_	_
)	_	_
that	_	_
translates	_	_
multi-view	_	_
images	_	_
of	_	_
objects	_	_
with	_	_
specular	_	_
reflection	_	_
to	_	_
diffuse	_	_
ones	_	_
.	_	_

#23
The	_	_
network	_	_
aims	_	_
to	_	_
generate	_	_
a	_	_
specular-free	_	_
surface	_	_
,	_	_
which	_	_
then	_	_
can	_	_
be	_	_
reconstructed	_	_
by	_	_
a	_	_
standard	_	_
multi-view	_	_
reconstruction	_	_
technique	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#24
We	_	_
name	_	_
our	_	_
translation	_	_
network	_	_
,	_	_
S2Dnet	_	_
,	_	_
for	_	_
Specular-to-Diffuse	_	_
.	_	_

#25
Our	_	_
approach	_	_
is	_	_
inspired	_	_
by	_	_
recent	_	_
GAN-based	_	_
image	_	_
translation	_	_
methods	_	_
,	_	_
like	_	_
pix2pix	_	_
[	_	_
9	_	_
]	_	_
or	_	_
cycleGAN	_	_
[	_	_
10	_	_
]	_	_
,	_	_
that	_	_
can	_	_
transform	_	_
an	_	_
image	_	_
from	_	_
one	_	_
domain	_	_
to	_	_
another	_	_
.	_	_

#26
Such	_	_
techniques	_	_
,	_	_
however	_	_
,	_	_
are	_	_
not	_	_
designed	_	_
for	_	_
multi-view	_	_
image	_	_
translation	_	_
.	_	_

#27
Directly	_	_
applying	_	_
these	_	_
translation	_	_
techniques	_	_
to	_	_
individual	_	_
views	_	_
is	_	_
prone	_	_
to	_	_
reconstruction	_	_
artifacts	_	_
due	_	_
to	_	_
the	_	_
lack	_	_
of	_	_
coherence	_	_
among	_	_
the	_	_
transformed	_	_
images	_	_
.	_	_

#28
Hence	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
single	_	_
views	_	_
,	_	_
our	_	_
network	_	_
considers	_	_
a	_	_
triplet	_	_
of	_	_
nearby	_	_
views	_	_
as	_	_
input	_	_
.	_	_

#29
These	_	_
triplets	_	_
allow	_	_
learning	_	_
the	_	_
mutual	_	_
information	_	_
of	_	_
neighboring	_	_
views	_	_
.	_	_

#30
More	_	_
specifically	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
global-local	_	_
discriminator	_	_
and	_	_
a	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
that	_	_
evaluate	_	_
the	_	_
multi-view	_	_
coherency	_	_
of	_	_
local	_	_
corresponding	_	_
image	_	_
patches	_	_
.	_	_

#31
Experiments	_	_
show	_	_
that	_	_
our	_	_
method	_	_
outperforms	_	_
baseline	_	_
image	_	_
translation	_	_
methods	_	_
.	_	_

#32
Another	_	_
obstacle	_	_
of	_	_
applying	_	_
image	_	_
translation	_	_
techniques	_	_
to	_	_
specularity	_	_
removal	_	_
is	_	_
the	_	_
lack	_	_
of	_	_
good	_	_
training	_	_
data	_	_
.	_	_

#33
It	_	_
is	_	_
rather	_	_
impractical	_	_
to	_	_
take	_	_
enough	_	_
paired	_	_
or	_	_
even	_	_
unpaired	_	_
photos	_	_
to	_	_
successfully	_	_
train	_	_
a	_	_
deep	_	_
network	_	_
.	_	_

#34
Inspired	_	_
by	_	_
the	_	_
recent	_	_
works	_	_
of	_	_
simulating	_	_
training	_	_
data	_	_
by	_	_
physically-based	_	_
rendering	_	_
[	_	_
11–14	_	_
]	_	_
and	_	_
domain	_	_
adaptation	_	_
[	_	_
15–18	_	_
]	_	_
,	_	_
we	_	_
present	_	_
a	_	_
fine-tuned	_	_
process	_	_
for	_	_
generating	_	_
training	_	_
data	_	_
,	_	_
then	_	_
adapting	_	_
it	_	_
to	_	_
real	_	_
world	_	_
data	_	_
.	_	_

#35
Instead	_	_
of	_	_
using	_	_
Shapenet	_	_
[	_	_
19	_	_
]	_	_
,	_	_
we	_	_
develop	_	_
a	_	_
new	_	_
training	_	_
dataset	_	_
that	_	_
includes	_	_
models	_	_
with	_	_
richer	_	_
geometric	_	_
details	_	_
,	_	_
which	_	_
allows	_	_
us	_	_
to	_	_
apply	_	_
our	_	_
method	_	_
to	_	_
complex	_	_
real-world	_	_
data	_	_
.	_	_

#36
Both	_	_
quantitative	_	_
and	_	_
qualitative	_	_
evaluations	_	_
demonstrate	_	_
that	_	_
the	_	_
performance	_	_
of	_	_
multi-view	_	_
reconstruction	_	_
can	_	_
be	_	_
significantly	_	_
improved	_	_
using	_	_
the	_	_
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
3	_	_
images	_	_
filtered	_	_
by	_	_
our	_	_
network	_	_
.	_	_

#37
We	_	_
show	_	_
also	_	_
the	_	_
performance	_	_
of	_	_
adapting	_	_
our	_	_
network	_	_
on	_	_
real	_	_
world	_	_
training	_	_
and	_	_
testing	_	_
data	_	_
with	_	_
some	_	_
promising	_	_
results	_	_
.	_	_

#38
2	_	_
Related	_	_
work	_	_

#39
Specular	_	_
Object	_	_
Reconstruction	_	_
.	_	_

#40
Image	_	_
based	_	_
3D	_	_
reconstruction	_	_
has	_	_
been	_	_
widely	_	_
used	_	_
for	_	_
AR/VR	_	_
applications	_	_
,	_	_
and	_	_
the	_	_
reconstruction	_	_
speed	_	_
and	_	_
quality	_	_
have	_	_
been	_	_
improved	_	_
dramatically	_	_
in	_	_
recent	_	_
years	_	_
.	_	_

#41
However	_	_
,	_	_
most	_	_
photometric	_	_
stereo	_	_
methods	_	_
are	_	_
based	_	_
on	_	_
the	_	_
assumption	_	_
that	_	_
the	_	_
object	_	_
surface	_	_
is	_	_
diffuse	_	_
,	_	_
that	_	_
is	_	_
,	_	_
the	_	_
appearance	_	_
of	_	_
the	_	_
object	_	_
is	_	_
view	_	_
independent	_	_
.	_	_

#42
Such	_	_
assumptions	_	_
,	_	_
however	_	_
,	_	_
are	_	_
not	_	_
valid	_	_
for	_	_
glossy	_	_
or	_	_
specular	_	_
objects	_	_
in	_	_
uncontrolled	_	_
environments	_	_
.	_	_

#43
It	_	_
is	_	_
well	_	_
known	_	_
that	_	_
modeling	_	_
the	_	_
specularity	_	_
is	_	_
difficult	_	_
as	_	_
the	_	_
specular	_	_
effects	_	_
are	_	_
largely	_	_
caused	_	_
by	_	_
the	_	_
complicated	_	_
global	_	_
illumination	_	_
that	_	_
is	_	_
usually	_	_
unknown	_	_
.	_	_

#44
For	_	_
example	_	_
,	_	_
Godard	_	_
et	_	_
al.	_	_
[	_	_
7	_	_
]	_	_
first	_	_
reconstruct	_	_
a	_	_
rough	_	_
model	_	_
by	_	_
silhouette	_	_
and	_	_
then	_	_
refine	_	_
it	_	_
using	_	_
the	_	_
specified	_	_
environment	_	_
map	_	_
.	_	_

#45
Their	_	_
method	_	_
can	_	_
reconstruct	_	_
high	_	_
quality	_	_
specular	_	_
surfaces	_	_
from	_	_
HDR	_	_
images	_	_
with	_	_
extra	_	_
information	_	_
,	_	_
such	_	_
as	_	_
silhouette	_	_
and	_	_
environment	_	_
map	_	_
.	_	_

#46
In	_	_
contrast	_	_
,	_	_
our	_	_
method	_	_
requires	_	_
only	_	_
the	_	_
multi-view	_	_
images	_	_
as	_	_
input	_	_
.	_	_

#47
Researchers	_	_
have	_	_
proposed	_	_
sophisticated	_	_
equipment	_	_
,	_	_
such	_	_
as	_	_
a	_	_
setup	_	_
with	_	_
two-layer	_	_
LCDs	_	_
to	_	_
encode	_	_
the	_	_
directions	_	_
of	_	_
the	_	_
emitted	_	_
light	_	_
field	_	_
[	_	_
4	_	_
]	_	_
,	_	_
taking	_	_
advantages	_	_
of	_	_
the	_	_
IR	_	_
images	_	_
recorded	_	_
by	_	_
RGB-D	_	_
scanners	_	_
[	_	_
20	_	_
,	_	_
21	_	_
]	_	_
or	_	_
casting	_	_
coded	_	_
patterns	_	_
onto	_	_
mirror-like	_	_
objects	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#48
While	_	_
such	_	_
techniques	_	_
can	_	_
effectively	_	_
handle	_	_
challenging	_	_
non-diffuse	_	_
effects	_	_
,	_	_
they	_	_
require	_	_
additional	_	_
hardware	_	_
and	_	_
user	_	_
expertise	_	_
.	_	_

#49
Another	_	_
way	_	_
to	_	_
tackle	_	_
this	_	_
problem	_	_
is	_	_
by	_	_
introducing	_	_
additional	_	_
assumptions	_	_
,	_	_
such	_	_
as	_	_
surface	_	_
constraints	_	_
[	_	_
5	_	_
,	_	_
6	_	_
]	_	_
,	_	_
the	_	_
Blinn-Phong	_	_
model	_	_
[	_	_
8	_	_
]	_	_
,	_	_
and	_	_
shape-from-specularity	_	_
[	_	_
22	_	_
]	_	_
.	_	_

#50
These	_	_
methods	_	_
can	_	_
also	_	_
benefit	_	_
from	_	_
our	_	_
network	_	_
that	_	_
outputs	_	_
diffuse	_	_
images	_	_
,	_	_
where	_	_
strong	_	_
specularities	_	_
are	_	_
removed	_	_
from	_	_
uncontrolled	_	_
illumination	_	_
.	_	_

#51
Please	_	_
refer	_	_
to	_	_
[	_	_
23	_	_
]	_	_
for	_	_
a	_	_
survey	_	_
on	_	_
specular	_	_
object	_	_
reconstruction	_	_
.	_	_

#52
GAN-based	_	_
Image-to-Image	_	_
Translation	_	_
.	_	_

#53
We	_	_
are	_	_
inspired	_	_
by	_	_
the	_	_
latest	_	_
success	_	_
of	_	_
learning	_	_
based	_	_
image-to-image	_	_
translation	_	_
methods	_	_
,	_	_
such	_	_
as	_	_
ConditionalGAN	_	_
[	_	_
9	_	_
]	_	_
,	_	_
cycleGAN	_	_
[	_	_
10	_	_
]	_	_
,	_	_
[	_	_
24	_	_
]	_	_
dualGAN	_	_
,	_	_
and	_	_
discoGAN	_	_
[	_	_
17	_	_
]	_	_
.	_	_

#54
The	_	_
remarkable	_	_
capacity	_	_
of	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
(	_	_
GANs	_	_
)	_	_
[	_	_
25	_	_
]	_	_
in	_	_
modeling	_	_
data	_	_
distributions	_	_
allows	_	_
these	_	_
methods	_	_
to	_	_
transform	_	_
images	_	_
from	_	_
one	_	_
domain	_	_
to	_	_
another	_	_
with	_	_
relatively	_	_
small	_	_
amounts	_	_
of	_	_
training	_	_
data	_	_
,	_	_
while	_	_
preserving	_	_
the	_	_
intrinsic	_	_
structure	_	_
of	_	_
original	_	_
images	_	_
faithfully	_	_
.	_	_

#55
With	_	_
improved	_	_
multi-scale	_	_
training	_	_
techniques	_	_
,	_	_
such	_	_
as	_	_
Progressive	_	_
GAN	_	_
[	_	_
26	_	_
]	_	_
and	_	_
pix2pixHD	_	_
[	_	_
27	_	_
]	_	_
,	_	_
image-to-image	_	_
translation	_	_
can	_	_
be	_	_
performed	_	_
at	_	_
mega	_	_
pixel	_	_
resolutions	_	_
and	_	_
achieve	_	_
results	_	_
of	_	_
stunning	_	_
visual	_	_
quality	_	_
.	_	_

#56
Recently	_	_
,	_	_
modified	_	_
image-to-image	_	_
translation	_	_
architectures	_	_
have	_	_
been	_	_
successfully	_	_
applied	_	_
to	_	_
ill-posed	_	_
or	_	_
underconstrained	_	_
vision	_	_
tasks	_	_
,	_	_
including	_	_
face	_	_
frontal	_	_
view	_	_
synthesis	_	_
[	_	_
28	_	_
]	_	_
,	_	_
facial	_	_
geometry	_	_
reconstruction	_	_
[	_	_
29–32	_	_
]	_	_
,	_	_
raindrop	_	_
removal	_	_
[	_	_
33	_	_
]	_	_
,	_	_
or	_	_
shadow	_	_
removal	_	_
[	_	_
34	_	_
]	_	_
.	_	_

#57
These	_	_
applications	_	_
motivate	_	_
us	_	_
to	_	_
develop	_	_
a	_	_
glossiness	_	_
removal	_	_
method	_	_
based	_	_
on	_	_
GANs	_	_
to	_	_
facilitate	_	_
multi-view	_	_
3D	_	_
reconstruction	_	_
of	_	_
non-diffuse	_	_
objects	_	_
.	_	_

#58
4	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Learning-based	_	_
Multi-View	_	_
3D	_	_
Reconstruction	_	_
.	_	_

#59
Learning	_	_
surface	_	_
reconstruction	_	_
from	_	_
multi-view	_	_
images	_	_
end-to-end	_	_
has	_	_
been	_	_
an	_	_
active	_	_
research	_	_
direction	_	_
recently	_	_
[	_	_
35–38	_	_
]	_	_
.	_	_

#60
Wu	_	_
et	_	_
al.	_	_
[	_	_
39	_	_
]	_	_
and	_	_
Gwak	_	_
et	_	_
al.	_	_
[	_	_
40	_	_
]	_	_
use	_	_
GANs	_	_
to	_	_
learn	_	_
the	_	_
latent	_	_
space	_	_
of	_	_
shapes	_	_
and	_	_
apply	_	_
it	_	_
to	_	_
single	_	_
image	_	_
3D	_	_
reconstruction	_	_
.	_	_

#61
3D-R2N2	_	_
[	_	_
36	_	_
]	_	_
designs	_	_
a	_	_
recurrent	_	_
network	_	_
for	_	_
unified	_	_
single	_	_
and	_	_
multi-view	_	_
reconstruction	_	_
.	_	_

#62
Image2Mesh	_	_
[	_	_
41	_	_
]	_	_
learns	_	_
parameters	_	_
of	_	_
free-form-deformation	_	_
of	_	_
a	_	_
base	_	_
model	_	_
.	_	_

#63
Nonetheless	_	_
,	_	_
in	_	_
general	_	_
,	_	_
the	_	_
reconstruction	_	_
quality	_	_
of	_	_
these	_	_
methods	_	_
can	_	_
not	_	_
really	_	_
surpass	_	_
that	_	_
of	_	_
traditional	_	_
approaches	_	_
that	_	_
exploit	_	_
multiple-view	_	_
geometry	_	_
and	_	_
heavily	_	_
engineered	_	_
photometric	_	_
stereo	_	_
pipelines	_	_
.	_	_

#64
To	_	_
take	_	_
the	_	_
local	_	_
image	_	_
feature	_	_
coherence	_	_
into	_	_
account	_	_
,	_	_
we	_	_
focus	_	_
on	_	_
removing	_	_
the	_	_
specular	_	_
effect	_	_
on	_	_
the	_	_
image	_	_
level	_	_
and	_	_
resort	_	_
to	_	_
the	_	_
power	_	_
of	_	_
multi-view	_	_
reconstruction	_	_
as	_	_
a	_	_
post-processing	_	_
and	_	_
also	_	_
a	_	_
production	_	_
step	_	_
.	_	_

#65
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
there	_	_
are	_	_
works	_	_
,	_	_
closer	_	_
to	_	_
ours	_	_
,	_	_
that	_	_
focus	_	_
on	_	_
applying	_	_
deep	_	_
learning	_	_
on	_	_
subparts	_	_
of	_	_
the	_	_
stereo	_	_
reconstruction	_	_
pipeline	_	_
,	_	_
such	_	_
as	_	_
depth	_	_
and	_	_
pose	_	_
estimation	_	_
[	_	_
42	_	_
]	_	_
,	_	_
feature	_	_
point	_	_
detection	_	_
and	_	_
description	_	_
[	_	_
43,44	_	_
]	_	_
,	_	_
semantic	_	_
segmentation	_	_
[	_	_
45	_	_
]	_	_
,	_	_
and	_	_
bundle	_	_
adjustment	_	_
[	_	_
46	_	_
,	_	_
47	_	_
]	_	_
.	_	_

#66
These	_	_
methods	_	_
still	_	_
impose	_	_
the	_	_
Lambertian	_	_
assumption	_	_
for	_	_
objects	_	_
or	_	_
scenes	_	_
,	_	_
where	_	_
our	_	_
method	_	_
can	_	_
serve	_	_
as	_	_
a	_	_
preprocessing	_	_
step	_	_
to	_	_
deal	_	_
with	_	_
glossiness	_	_
.	_	_

#67
Learning-based	_	_
Intrinsic	_	_
Image	_	_
Decomposition	_	_
.	_	_

#68
Our	_	_
method	_	_
is	_	_
also	_	_
loosely	_	_
related	_	_
to	_	_
some	_	_
recent	_	_
works	_	_
on	_	_
learning	_	_
intrinsic	_	_
image	_	_
decomposition	_	_
.	_	_

#69
These	_	_
methods	_	_
include	_	_
training	_	_
a	_	_
CNN	_	_
to	_	_
reconstruct	_	_
rendering	_	_
parameters	_	_
,	_	_
e.g.	_	_
,	_	_
material	_	_
[	_	_
48	_	_
,	_	_
49	_	_
]	_	_
,	_	_
reflectance	_	_
maps	_	_
[	_	_
50	_	_
]	_	_
,	_	_
illumination	_	_
[	_	_
51	_	_
]	_	_
,	_	_
or	_	_
some	_	_
combination	_	_
of	_	_
those	_	_
components	_	_
[	_	_
13	_	_
,	_	_
48,52	_	_
]	_	_
.	_	_

#70
These	_	_
methods	_	_
are	_	_
often	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
and	_	_
are	_	_
usually	_	_
applied	_	_
to	_	_
the	_	_
re-rendering	_	_
of	_	_
single	_	_
images	_	_
.	_	_

#71
Our	_	_
method	_	_
shares	_	_
certain	_	_
similarity	_	_
with	_	_
these	_	_
methods	_	_
.	_	_

#72
However	_	_
,	_	_
our	_	_
goal	_	_
is	_	_
not	_	_
to	_	_
recover	_	_
intrinsic	_	_
images	_	_
with	_	_
albedos	_	_
.	_	_

#73
Disregarding	_	_
albedo	_	_
,	_	_
we	_	_
aim	_	_
for	_	_
output	_	_
images	_	_
with	_	_
a	_	_
consistent	_	_
appearance	_	_
across	_	_
the	_	_
entire	_	_
training	_	_
set	_	_
that	_	_
reflects	_	_
the	_	_
structure	_	_
of	_	_
the	_	_
object	_	_
.	_	_

#74
3	_	_
Multi-view	_	_
Specular-to-Diffuse	_	_
GAN	_	_

#75
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
introduce	_	_
S2Dnet	_	_
,	_	_
a	_	_
conditional	_	_
GAN	_	_
that	_	_
translates	_	_
multi-view	_	_
images	_	_
of	_	_
highly	_	_
specular	_	_
scenes	_	_
into	_	_
corresponding	_	_
diffuse	_	_
images	_	_
.	_	_

#76
The	_	_
input	_	_
to	_	_
our	_	_
model	_	_
is	_	_
a	_	_
multi-view	_	_
sequence	_	_
of	_	_
a	_	_
glossy	_	_
scene	_	_
without	_	_
any	_	_
additional	_	_
input	_	_
such	_	_
as	_	_
segmentation	_	_
masks	_	_
,	_	_
camera	_	_
parameters	_	_
,	_	_
or	_	_
light	_	_
probes	_	_
.	_	_

#77
This	_	_
enables	_	_
our	_	_
model	_	_
to	_	_
process	_	_
real-world	_	_
data	_	_
,	_	_
where	_	_
such	_	_
additional	_	_
information	_	_
is	_	_
not	_	_
readily	_	_
available	_	_
.	_	_

#78
The	_	_
output	_	_
of	_	_
our	_	_
model	_	_
directly	_	_
serves	_	_
as	_	_
input	_	_
to	_	_
state-of-the-art	_	_
photometric	_	_
stereo	_	_
pipelines	_	_
,	_	_
resulting	_	_
in	_	_
improved	_	_
3D	_	_
reconstruction	_	_
without	_	_
additional	_	_
effort	_	_
.	_	_

#79
Figure	_	_
2	_	_
shows	_	_
a	_	_
visualization	_	_
of	_	_
the	_	_
proposed	_	_
model	_	_
.	_	_

#80
We	_	_
discuss	_	_
the	_	_
training	_	_
data	_	_
,	_	_
one	_	_
of	_	_
our	_	_
major	_	_
contributions	_	_
,	_	_
in	_	_
Section	_	_
3.1	_	_
.	_	_

#81
In	_	_
Section	_	_
3.2	_	_
we	_	_
introduce	_	_
the	_	_
concept	_	_
of	_	_
inter-view	_	_
coherence	_	_
that	_	_
enables	_	_
our	_	_
model	_	_
to	_	_
process	_	_
multiple	_	_
views	_	_
of	_	_
a	_	_
scene	_	_
in	_	_
a	_	_
consistent	_	_
manner	_	_
,	_	_
which	_	_
is	_	_
important	_	_
in	_	_
the	_	_
context	_	_
of	_	_
multi-view	_	_
reconstruction	_	_
.	_	_

#82
Then	_	_
,	_	_
we	_	_
outline	_	_
in	_	_
Section	_	_
3.3	_	_
the	_	_
overall	_	_
end-to-end	_	_
training	_	_
procedure	_	_
.	_	_

#83
Implementation	_	_
details	_	_
are	_	_
discussed	_	_
in	_	_
Section	_	_
3.4	_	_
.	_	_

#84
Upon	_	_
publication	_	_
we	_	_
will	_	_
release	_	_
both	_	_
our	_	_
data	_	_
(	_	_
synthetic	_	_
and	_	_
real	_	_
)	_	_
and	_	_
the	_	_
proposed	_	_
model	_	_
to	_	_
foster	_	_
further	_	_
work	_	_
.	_	_

#85
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
5	_	_
Glossy	_	_
Real	_	_
Diffuse	_	_
Fake	_	_
Glossy	_	_
Fake	_	_
Diffuse	_	_
Real	_	_
Multi-view	_	_
coherence	_	_
loss	_	_
Multi-view	_	_
coherence	_	_
loss	_	_
matchmatch	_	_
matchmatchCycle	_	_
Consistency	_	_
loss	_	_
Cycle	_	_
Consistency	_	_
loss	_	_
Adversarial	_	_
losses	_	_
Fig.	_	_
2	_	_
:	_	_
Overview	_	_
of	_	_
S2Dnet	_	_
.	_	_

#86
Two	_	_
generators	_	_
and	_	_
two	_	_
discriminators	_	_
are	_	_
trained	_	_
simultaneously	_	_
to	_	_
learn	_	_
cross-domain	_	_
translations	_	_
between	_	_
the	_	_
glossy	_	_
and	_	_
the	_	_
diffuse	_	_
domain	_	_
.	_	_

#87
In	_	_
each	_	_
training	_	_
iteration	_	_
,	_	_
the	_	_
model	_	_
randomly	_	_
picks	_	_
and	_	_
forwards	_	_
a	_	_
real	_	_
glossy	_	_
and	_	_
diffuse	_	_
image	_	_
sequence	_	_
,	_	_
computes	_	_
the	_	_
loss	_	_
functions	_	_
and	_	_
updates	_	_
the	_	_
model	_	_
parameters	_	_
.	_	_

#88
Fig.	_	_
3	_	_
:	_	_
Gallery	_	_
of	_	_
our	_	_
synthetically	_	_
rendered	_	_
specular-to-diffuse	_	_
training	_	_
data	_	_
.	_	_

#89
3.1	_	_
Training	_	_
Data	_	_

#90
To	_	_
train	_	_
our	_	_
model	_	_
to	_	_
translate	_	_
multi-view	_	_
glossy	_	_
images	_	_
to	_	_
diffuse	_	_
correspondents	_	_
,	_	_
we	_	_
need	_	_
appropriate	_	_
data	_	_
for	_	_
both	_	_
domains	_	_
,	_	_
i.e.	_	_
,	_	_
glossy	_	_
source	_	_
domain	_	_
images	_	_
as	_	_
inputs	_	_
,	_	_
and	_	_
diffuse	_	_
images	_	_
as	_	_
the	_	_
target	_	_
domain	_	_
.	_	_

#91
Yi	_	_
et	_	_
al.	_	_
[	_	_
24	_	_
]	_	_
propose	_	_
a	_	_
MATERIAL	_	_
dataset	_	_
consisting	_	_
of	_	_
unlabeled	_	_
data	_	_
grouped	_	_
in	_	_
different	_	_
material	_	_
classes	_	_
,	_	_
such	_	_
as	_	_
plastic	_	_
,	_	_
fabric	_	_
,	_	_
metal	_	_
,	_	_
and	_	_
leather	_	_
,	_	_
and	_	_
they	_	_
train	_	_
GANs	_	_
to	_	_
perform	_	_
material	_	_
transfer	_	_
.	_	_

#92
However	_	_
,	_	_
the	_	_
MATERIAL	_	_
dataset	_	_
does	_	_
not	_	_
contain	_	_
multi-view	_	_
images	_	_
and	_	_
thus	_	_
is	_	_
not	_	_
suited	_	_
for	_	_
our	_	_
application	_	_
.	_	_

#93
Moreover	_	_
,	_	_
the	_	_
dataset	_	_
is	_	_
rather	_	_
small	_	_
and	_	_
we	_	_
expect	_	_
our	_	_
deep	_	_
model	_	_
to	_	_
require	_	_
a	_	_
larger	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
.	_	_

#94
Hence	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
synthetic	_	_
dataset	_	_
consisting	_	_
of	_	_
multi-view	_	_
images	_	_
,	_	_
which	_	_
is	_	_
both	_	_
sufficiently	_	_
large	_	_
to	_	_
train	_	_
deep	_	_
networks	_	_
and	_	_
complex	_	_
to	_	_
generalize	_	_
to	_	_
real-world	_	_
objects	_	_
.	_	_

#95
For	_	_
this	_	_
purpose	_	_
,	_	_
we	_	_
collect	_	_
and	_	_
align	_	_
91	_	_
watertight	_	_
and	_	_
noise-free	_	_
geometric	_	_
models	_	_
featuring	_	_
rich	_	_
geometric	_	_
details	_	_
from	_	_
SketchFab	_	_
(	_	_
Figure	_	_
3	_	_
)	_	_
.	_	_

#96
We	_	_
exclude	_	_
three	_	_
models	_	_
for	_	_
testing	_	_
and	_	_
use	_	_
the	_	_
remaining	_	_
88	_	_
models	_	_
for	_	_
training	_	_
.	_	_

#97
To	_	_
obtain	_	_
a	_	_
dataset	_	_
that	_	_
generalizes	_	_
well	_	_
to	_	_
real-world	_	_
images	_	_
,	_	_
we	_	_
use	_	_
PBRT	_	_
,	_	_
a	_	_
physically	_	_
based	_	_
renderer	_	_
[	_	_
53	_	_
]	_	_
to	_	_
render	_	_
these	_	_
geometric	_	_
models	_	_
in	_	_
various	_	_
environments	_	_
with	_	_
a	_	_
wide	_	_
variety	_	_
of	_	_
glossy	_	_
materials	_	_
applied	_	_
to	_	_
form	_	_
our	_	_
source	_	_
domain	_	_
.	_	_

#98
Next	_	_
,	_	_
we	_	_
render	_	_
the	_	_
target	_	_
domain	_	_
images	_	_
by	_	_
applying	_	_
a	_	_
Lambertian	_	_
material	_	_
to	_	_
our	_	_
geometric	_	_
models	_	_
.	_	_

#99
6	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Our	_	_
experiments	_	_
show	_	_
that	_	_
the	_	_
choice	_	_
of	_	_
the	_	_
rendering	_	_
parameters	_	_
has	_	_
a	_	_
strong	_	_
impact	_	_
on	_	_
the	_	_
translation	_	_
performance	_	_
.	_	_

#100
On	_	_
one	_	_
hand	_	_
,	_	_
making	_	_
the	_	_
two	_	_
domains	_	_
more	_	_
similar	_	_
by	_	_
choosing	_	_
similar	_	_
materials	_	_
for	_	_
both	_	_
domains	_	_
improves	_	_
the	_	_
translation	_	_
quality	_	_
on	_	_
synthetic	_	_
data	_	_
.	_	_

#101
Moreover	_	_
,	_	_
simple	_	_
environments	_	_
,	_	_
such	_	_
as	_	_
a	_	_
constant	_	_
ground	_	_
plane	_	_
,	_	_
also	_	_
increase	_	_
the	_	_
quality	_	_
on	_	_
synthetic	_	_
data	_	_
.	_	_

#102
On	_	_
the	_	_
other	_	_
hand	_	_
,	_	_
such	_	_
simplifications	_	_
cause	_	_
the	_	_
model	_	_
to	_	_
overfit	_	_
and	_	_
prevent	_	_
generalization	_	_
to	_	_
real-world	_	_
data	_	_
.	_	_

#103
Hence	_	_
,	_	_
a	_	_
main	_	_
goal	_	_
of	_	_
our	_	_
dataset	_	_
is	_	_
to	_	_
provide	_	_
enough	_	_
complexity	_	_
to	_	_
allow	_	_
generalization	_	_
to	_	_
real	_	_
data	_	_
.	_	_

#104
To	_	_
achieve	_	_
realistic	_	_
illumination	_	_
,	_	_
we	_	_
randomly	_	_
sample	_	_
one	_	_
of	_	_
20	_	_
different	_	_
HDR	_	_
indoor	_	_
environment	_	_
maps	_	_
and	_	_
randomly	_	_
rotate	_	_
it	_	_
for	_	_
each	_	_
scene	_	_
.	_	_

#105
In	_	_
addition	_	_
,	_	_
we	_	_
orient	_	_
a	_	_
directional	_	_
light	_	_
source	_	_
pointing	_	_
from	_	_
the	_	_
camera	_	_
approximately	_	_
towards	_	_
the	_	_
center	_	_
of	_	_
the	_	_
scene	_	_
and	_	_
position	_	_
two	_	_
additional	_	_
light	_	_
sources	_	_
above	_	_
the	_	_
scene	_	_
.	_	_

#106
The	_	_
intensities	_	_
,	_	_
positions	_	_
,	_	_
and	_	_
directions	_	_
of	_	_
these	_	_
additional	_	_
light	_	_
sources	_	_
are	_	_
randomly	_	_
jittered	_	_
.	_	_

#107
This	_	_
setup	_	_
guarantees	_	_
a	_	_
rather	_	_
even	_	_
,	_	_
but	_	_
still	_	_
random	_	_
illumination	_	_
.	_	_

#108
To	_	_
render	_	_
the	_	_
source	_	_
domain	_	_
images	_	_
,	_	_
we	_	_
applied	_	_
the	_	_
various	_	_
metal	_	_
materials	_	_
defined	_	_
in	_	_
PBRT	_	_
,	_	_
including	_	_
copper	_	_
,	_	_
silver	_	_
,	_	_
and	_	_
gold	_	_
.	_	_

#109
Material	_	_
roughness	_	_
and	_	_
index	_	_
of	_	_
refraction	_	_
are	_	_
randomly	_	_
sampled	_	_
to	_	_
cover	_	_
a	_	_
large	_	_
variety	_	_
of	_	_
glossy	_	_
materials	_	_
.	_	_

#110
We	_	_
randomly	_	_
sample	_	_
camera	_	_
positions	_	_
on	_	_
the	_	_
upper	_	_
hemisphere	_	_
around	_	_
the	_	_
scene	_	_
pointing	_	_
towards	_	_
the	_	_
center	_	_
of	_	_
the	_	_
scene	_	_
.	_	_

#111
To	_	_
obtain	_	_
multi-view	_	_
data	_	_
,	_	_
we	_	_
always	_	_
sample	_	_
5	_	_
close-by	_	_
,	_	_
consecutive	_	_
camera	_	_
positions	_	_
in	_	_
clock-wise	_	_
order	_	_
while	_	_
keeping	_	_
the	_	_
scene	_	_
parameters	_	_
fixed	_	_
to	_	_
mimic	_	_
the	_	_
common	_	_
procedure	_	_
of	_	_
taking	_	_
photos	_	_
for	_	_
stereo	_	_
reconstruction	_	_
.	_	_

#112
Since	_	_
we	_	_
collect	_	_
5	_	_
images	_	_
of	_	_
the	_	_
same	_	_
scene	_	_
and	_	_
the	_	_
input	_	_
to	_	_
our	_	_
network	_	_
consists	_	_
of	_	_
3	_	_
views	_	_
,	_	_
we	_	_
obtain	_	_
3	_	_
training	_	_
samples	_	_
per	_	_
scene	_	_
.	_	_

#113
All	_	_
rendered	_	_
images	_	_
are	_	_
of	_	_
512	_	_
×	_	_
512	_	_
resolution	_	_
,	_	_
which	_	_
is	_	_
the	_	_
limit	_	_
for	_	_
our	_	_
GPU	_	_
memory	_	_
.	_	_

#114
However	_	_
,	_	_
it	_	_
is	_	_
likely	_	_
that	_	_
higher	_	_
resolutions	_	_
would	_	_
further	_	_
improve	_	_
the	_	_
reconstruction	_	_
quality	_	_
.	_	_

#115
Finally	_	_
,	_	_
we	_	_
render	_	_
the	_	_
exact	_	_
same	_	_
images	_	_
again	_	_
with	_	_
a	_	_
white	_	_
,	_	_
Lambertian	_	_
material	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
mapping	_	_
from	_	_
the	_	_
source	_	_
to	_	_
the	_	_
target	_	_
domain	_	_
is	_	_
bijective	_	_
.	_	_

#116
The	_	_
proposed	_	_
procedure	_	_
results	_	_
in	_	_
a	_	_
training	_	_
dataset	_	_
of	_	_
more	_	_
than	_	_
647k	_	_
images	_	_
,	_	_
i.e.	_	_
,	_	_
more	_	_
than	_	_
320k	_	_
images	_	_
per	_	_
domain	_	_
.	_	_

#117
For	_	_
testing	_	_
,	_	_
we	_	_
rendered	_	_
2k	_	_
sequences	_	_
of	_	_
images	_	_
,	_	_
each	_	_
consisting	_	_
of	_	_
50	_	_
images	_	_
.	_	_

#118
All	_	_
qualitative	_	_
results	_	_
on	_	_
synthetic	_	_
data	_	_
shown	_	_
in	_	_
this	_	_
paper	_	_
belong	_	_
to	_	_
this	_	_
test	_	_
set	_	_
.	_	_

#119
3.2	_	_
Inter-view	_	_
Coherence	_	_

#120
Multi-view	_	_
reconstruction	_	_
algorithms	_	_
leverage	_	_
corresponding	_	_
features	_	_
in	_	_
different	_	_
views	_	_
to	_	_
accurately	_	_
estimate	_	_
the	_	_
3D	_	_
geometry	_	_
.	_	_

#121
Therefore	_	_
,	_	_
we	_	_
can	_	_
not	_	_
expect	_	_
good	_	_
reconstruction	_	_
quality	_	_
if	_	_
the	_	_
glossy	_	_
images	_	_
in	_	_
a	_	_
multi-view	_	_
sequence	_	_
are	_	_
translated	_	_
independently	_	_
using	_	_
standard	_	_
image	_	_
translation	_	_
methods	_	_
,	_	_
e.g.	_	_
,	_	_
[	_	_
9,10	_	_
]	_	_
.	_	_

#122
This	_	_
will	_	_
introduce	_	_
inconsistencies	_	_
along	_	_
the	_	_
different	_	_
views	_	_
,	_	_
and	_	_
thus	_	_
cause	_	_
artifacts	_	_
in	_	_
the	_	_
subsequent	_	_
reconstruction	_	_
.	_	_

#123
We	_	_
therefore	_	_
propose	_	_
a	_	_
novel	_	_
model	_	_
that	_	_
enforces	_	_
inter-view	_	_
coherence	_	_
by	_	_
processing	_	_
multiple	_	_
views	_	_
simultaneously	_	_
.	_	_

#124
Our	_	_
approach	_	_
consists	_	_
of	_	_
a	_	_
global	_	_
and	_	_
local	_	_
consistency	_	_
constraint	_	_
:	_	_
the	_	_
global	_	_
constraint	_	_
is	_	_
implemented	_	_
using	_	_
an	_	_
appropriate	_	_
network	_	_
architecture	_	_
,	_	_
and	_	_
the	_	_
local	_	_
consistency	_	_
is	_	_
enforced	_	_
using	_	_
a	_	_
novel	_	_
loss	_	_
function	_	_
.	_	_

#125
Global	_	_
Inter-view	_	_
Coherence	_	_
.	_	_

#126
A	_	_
straightforward	_	_
idea	_	_
to	_	_
incorporate	_	_
multiple	_	_
views	_	_
is	_	_
to	_	_
stack	_	_
them	_	_
pixel-by-pixel	_	_
before	_	_
feeding	_	_
them	_	_
to	_	_
the	_	_
network	_	_
.	_	_

#127
We	_	_
found	_	_
that	_	_
this	_	_
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
7	_	_
Fig.	_	_
4	_	_
:	_	_
Two	_	_
examples	_	_
of	_	_
the	_	_
SIFT	_	_
correspondences	_	_
pre-computed	_	_
for	_	_
our	_	_
training	_	_
.	_	_

#128
does	_	_
not	_	_
lead	_	_
to	_	_
strong	_	_
enough	_	_
constraints	_	_
,	_	_
since	_	_
the	_	_
network	_	_
can	_	_
still	_	_
learn	_	_
independent	_	_
filter	_	_
weights	_	_
for	_	_
the	_	_
different	_	_
views	_	_
.	_	_

#129
This	_	_
results	_	_
in	_	_
blurry	_	_
translations	_	_
,	_	_
especially	_	_
if	_	_
corresponding	_	_
pixels	_	_
in	_	_
different	_	_
views	_	_
are	_	_
not	_	_
aligned	_	_
,	_	_
which	_	_
is	_	_
typically	_	_
the	_	_
case	_	_
.	_	_

#130
Instead	_	_
,	_	_
we	_	_
concatenate	_	_
the	_	_
different	_	_
views	_	_
along	_	_
the	_	_
spatial	_	_
axis	_	_
before	_	_
feeding	_	_
them	_	_
to	_	_
the	_	_
network	_	_
.	_	_

#131
This	_	_
solution	_	_
,	_	_
although	_	_
simple	_	_
,	_	_
enforces	_	_
the	_	_
network	_	_
to	_	_
use	_	_
the	_	_
same	_	_
filter	_	_
weights	_	_
for	_	_
all	_	_
views	_	_
,	_	_
and	_	_
thus	_	_
effectively	_	_
avoids	_	_
inconsistencies	_	_
on	_	_
a	_	_
global	_	_
scale	_	_
.	_	_

#132
Local	_	_
Inter-view	_	_
Coherence	_	_
.	_	_

#133
Incorporating	_	_
loss	_	_
functions	_	_
based	_	_
on	_	_
local	_	_
image	_	_
patches	_	_
has	_	_
been	_	_
successfully	_	_
applied	_	_
to	_	_
generative	_	_
adversarial	_	_
models	_	_
,	_	_
such	_	_
as	_	_
image	_	_
completion	_	_
[	_	_
54	_	_
]	_	_
or	_	_
texture	_	_
synthesis	_	_
[	_	_
55	_	_
]	_	_
.	_	_

#134
However	_	_
,	_	_
comparing	_	_
image	_	_
patches	_	_
at	_	_
random	_	_
locations	_	_
is	_	_
not	_	_
meaningful	_	_
in	_	_
a	_	_
multi-view	_	_
setup	_	_
for	_	_
stereo	_	_
reconstruction	_	_
.	_	_

#135
Instead	_	_
,	_	_
we	_	_
encourage	_	_
the	_	_
network	_	_
to	_	_
maintain	_	_
feature	_	_
point	_	_
correspondences	_	_
in	_	_
the	_	_
input	_	_
sequence	_	_
,	_	_
i.e.	_	_
,	_	_
inter-view	_	_
correspondences	_	_
should	_	_
be	_	_
invariant	_	_
to	_	_
the	_	_
translation	_	_
.	_	_

#136
Since	_	_
the	_	_
subsequent	_	_
reconstruction	_	_
pipeline	_	_
relies	_	_
on	_	_
such	_	_
correspondences	_	_
,	_	_
maintaining	_	_
them	_	_
during	_	_
translation	_	_
should	_	_
improve	_	_
reconstruction	_	_
quality	_	_
.	_	_

#137
To	_	_
achieve	_	_
this	_	_
,	_	_
we	_	_
first	_	_
extract	_	_
SIFT	_	_
feature	_	_
correspondences	_	_
for	_	_
all	_	_
training	_	_
images	_	_
.	_	_

#138
For	_	_
each	_	_
training	_	_
sequence	_	_
consisting	_	_
of	_	_
three	_	_
views	_	_
,	_	_
we	_	_
compute	_	_
corresponding	_	_
feature	_	_
points	_	_
between	_	_
the	_	_
different	_	_
views	_	_
in	_	_
the	_	_
source	_	_
domain	_	_
;	_	_
see	_	_
Figure	_	_
4	_	_
for	_	_
two	_	_
examples	_	_
.	_	_

#139
During	_	_
training	_	_
,	_	_
we	_	_
encourage	_	_
the	_	_
network	_	_
output	_	_
at	_	_
the	_	_
SIFT	_	_
feature	_	_
locations	_	_
to	_	_
be	_	_
similar	_	_
along	_	_
the	_	_
views	_	_
using	_	_
a	_	_
perceptual	_	_
loss	_	_
in	_	_
VGG	_	_
feature	_	_
space	_	_
[	_	_
27,56–58	_	_
]	_	_
.	_	_

#140
The	_	_
key	_	_
idea	_	_
is	_	_
to	_	_
measure	_	_
both	_	_
highand	_	_
low-level	_	_
similarity	_	_
of	_	_
two	_	_
images	_	_
by	_	_
considering	_	_
their	_	_
feature	_	_
activations	_	_
in	_	_
a	_	_
deep	_	_
CNN	_	_
like	_	_
VGG	_	_
.	_	_

#141
We	_	_
adopt	_	_
this	_	_
idea	_	_
to	_	_
keep	_	_
local	_	_
image	_	_
patches	_	_
around	_	_
corresponding	_	_
SIFT	_	_
features	_	_
perceptually	_	_
similar	_	_
in	_	_
the	_	_
translated	_	_
output	_	_
.	_	_

#142
The	_	_
perceptual	_	_
loss	_	_
in	_	_
VGG	_	_
feature	_	_
space	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
LV	_	_
GG	_	_
(	_	_
x	_	_
,	_	_
x̂	_	_
)	_	_
=	_	_
N∑	_	_
i=1	_	_
Mi	_	_
‖F	_	_
(	_	_
i	_	_
)	_	_
(	_	_
x	_	_
)	_	_
−	_	_
F	_	_
(	_	_
i	_	_
)	_	_
(	_	_
x̂	_	_
)	_	_
‖1	_	_
,	_	_
(	_	_
1	_	_
)	_	_
where	_	_
F	_	_
(	_	_
i	_	_
)	_	_
denotes	_	_
the	_	_
i-th	_	_
layer	_	_
in	_	_
the	_	_
VGG	_	_
network	_	_
consisting	_	_
of	_	_
Mi	_	_
elements	_	_
.	_	_

#143
Now	_	_
consider	_	_
a	_	_
glossy	_	_
input	_	_
sequence	_	_
consisting	_	_
of	_	_
three	_	_
images	_	_
X1	_	_
,	_	_
X2	_	_
,	_	_
X3	_	_
,	_	_
and	_	_
the	_	_
corresponding	_	_
diffuse	_	_
sequence	_	_
X̃1	_	_
,	_	_
X̃2	_	_
,	_	_
X̃3	_	_
produced	_	_
by	_	_
our	_	_
model	_	_
.	_	_

#144
A	_	_
SIFT	_	_
correspondence	_	_
for	_	_
this	_	_
sequence	_	_
consists	_	_
of	_	_
three	_	_
image	_	_
coordinates	_	_
p1	_	_
,	_	_
p2	_	_
,	_	_
p3	_	_
,	_	_
one	_	_
in	_	_
each	_	_
glossy	_	_
image	_	_
,	_	_
and	_	_
all	_	_
three	_	_
pixels	_	_
at	_	_
the	_	_
corresponding	_	_
coordinates	_	_
represent	_	_
the	_	_
same	_	_
feature	_	_
.	_	_

#145
We	_	_
then	_	_
extract	_	_
local	_	_
image	_	_
patches	_	_
x̃i	_	_
centered	_	_
at	_	_
pi	_	_
from	_	_
X̃i	_	_
,	_	_
and	_	_
define	_	_
the	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
as	_	_
:	_	_
Lcorr	_	_
(	_	_
X̃1	_	_
,	_	_
X̃2	_	_
,	_	_
X̃3	_	_
)	_	_
=	_	_
LV	_	_
GG	_	_
(	_	_
x̃1	_	_
,	_	_
x̃2	_	_
)	_	_
+	_	_
LV	_	_
GG	_	_
(	_	_
x̃2	_	_
,	_	_
x̃3	_	_
)	_	_
+	_	_
LV	_	_
GG	_	_
(	_	_
x̃1	_	_
,	_	_
x̃3	_	_
)	_	_
.	_	_

#146
(	_	_
2	_	_
)	_	_
8	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_

#147
3.3	_	_
Training	_	_
Procedure	_	_

#148
Given	_	_
two	_	_
sets	_	_
of	_	_
data	_	_
samples	_	_
from	_	_
two	_	_
domains	_	_
,	_	_
a	_	_
source	_	_
domain	_	_
A	_	_
and	_	_
a	_	_
target	_	_
domain	_	_
B	_	_
,	_	_
the	_	_
goal	_	_
of	_	_
image	_	_
translation	_	_
is	_	_
to	_	_
find	_	_
a	_	_
mapping	_	_
T	_	_
that	_	_
transforms	_	_
data	_	_
points	_	_
Xi	_	_
∈	_	_
A	_	_
to	_	_
B	_	_
such	_	_
that	_	_
T	_	_
(	_	_
Xi	_	_
)	_	_
=	_	_
X̃i	_	_
∈	_	_
B	_	_
,	_	_
while	_	_
the	_	_
intrinsic	_	_
structure	_	_
of	_	_
Xi	_	_
should	_	_
be	_	_
preserved	_	_
under	_	_
T	_	_
.	_	_

#149
Training	_	_
GANs	_	_
has	_	_
been	_	_
proven	_	_
to	_	_
produce	_	_
astonishing	_	_
results	_	_
on	_	_
this	_	_
task	_	_
,	_	_
both	_	_
in	_	_
supervised	_	_
settings	_	_
where	_	_
the	_	_
data	_	_
of	_	_
the	_	_
two	_	_
domains	_	_
are	_	_
paired	_	_
[	_	_
9	_	_
]	_	_
,	_	_
and	_	_
in	_	_
unsupervised	_	_
cases	_	_
using	_	_
unpaired	_	_
data	_	_
[	_	_
10	_	_
]	_	_
.	_	_

#150
In	_	_
our	_	_
experiments	_	_
,	_	_
we	_	_
observed	_	_
that	_	_
both	_	_
approaches	_	_
(	_	_
ConditionalGAN	_	_
[	_	_
9	_	_
]	_	_
and	_	_
cycleGAN	_	_
[	_	_
10	_	_
]	_	_
)	_	_
perform	_	_
similarly	_	_
well	_	_
on	_	_
our	_	_
dataset	_	_
.	_	_

#151
However	_	_
,	_	_
while	_	_
paired	_	_
training	_	_
data	_	_
might	options	concessive
be	_	_
readily	_	_
available	_	_
for	_	_
synthetic	_	_
data	_	_
,	_	_
paired	_	_
real-world	_	_
data	_	_
is	_	_
difficult	_	_
to	_	_
obtain	_	_
.	_	_

#152
Therefore	_	_
we	_	_
come	_	_
up	_	_
with	_	_
a	_	_
design	_	_
for	_	_
unsupervised	_	_
learning	_	_
that	_	_
can	_	_
easily	_	_
be	_	_
fine-tuned	_	_
on	_	_
unpaired	_	_
real-world	_	_
data	_	_
.	_	_

#153
Cycle-consistency	_	_
Loss	_	_
.	_	_

#154
Similar	_	_
to	_	_
CycleGAN	_	_
[	_	_
10	_	_
]	_	_
,	_	_
we	_	_
learn	_	_
the	_	_
mapping	_	_
between	_	_
domain	_	_
A	_	_
and	_	_
B	_	_
with	_	_
two	_	_
translators	_	_
GB	_	_
:	_	_
A	_	_
→	_	_
B	_	_
and	_	_
GA	_	_
:	_	_
B	_	_
→	_	_
A	_	_
that	_	_
are	_	_
trained	_	_
simultaneously	_	_
.	_	_

#155
The	_	_
key	_	_
idea	_	_
is	_	_
to	_	_
train	_	_
with	_	_
cycle-consistency	_	_
loss	_	_
,	_	_
i.e.	_	_
,	_	_
to	_	_
enforce	_	_
that	_	_
GA	_	_
(	_	_
GB	_	_
(	_	_
X	_	_
)	_	_
)	_	_
≈	_	_
X	_	_
and	_	_
GB	_	_
(	_	_
GA	_	_
(	_	_
Y	_	_
)	_	_
)	_	_
≈	_	_
Y	_	_
,	_	_
where	_	_
X	_	_
∈	_	_
A	_	_
and	_	_
Y	_	_
∈	_	_
B	_	_
.	_	_

#156
This	_	_
cycle-consistency	_	_
loss	_	_
guarantees	_	_
that	_	_
data	_	_
points	_	_
preserve	_	_
their	_	_
intrinsic	_	_
structure	_	_
under	_	_
the	_	_
learned	_	_
mapping	_	_
.	_	_

#157
Formally	_	_
,	_	_
the	_	_
cycle-consistency	_	_
loss	_	_
is	_	_
defined	_	_
as	_	_
:	_	_
Lcyc	_	_
(	_	_
X	_	_
,	_	_
Y	_	_
)	_	_
=	_	_
‖GA	_	_
(	_	_
GB	_	_
(	_	_
X	_	_
)	_	_
)	_	_
−X‖1	_	_
+	_	_
‖GB	_	_
(	_	_
GA	_	_
(	_	_
Y	_	_
)	_	_
)	_	_
−	_	_
Y	_	_
‖1	_	_
.	_	_

#158
(	_	_
3	_	_
)	_	_
Adversarial	_	_
Loss	_	_
.	_	_

#159
To	_	_
enforce	_	_
the	_	_
translation	_	_
networks	_	_
to	_	_
produce	_	_
data	_	_
that	_	_
is	_	_
indistinguishable	_	_
from	_	_
genuine	_	_
images	_	_
,	_	_
we	_	_
also	_	_
include	_	_
an	_	_
adversarial	_	_
loss	_	_
to	_	_
train	_	_
our	_	_
model	_	_
.	_	_

#160
For	_	_
both	_	_
translators	_	_
,	_	_
in	_	_
GAN	_	_
context	_	_
often	_	_
called	_	_
generators	_	_
,	_	_
we	_	_
train	_	_
two	_	_
additional	_	_
discriminator	_	_
networks	_	_
DA	_	_
and	_	_
DB	_	_
that	_	_
are	_	_
trained	_	_
to	_	_
distinguish	_	_
translated	_	_
from	_	_
genuine	_	_
images	_	_
.	_	_

#161
To	_	_
train	_	_
our	_	_
model	_	_
,	_	_
we	_	_
use	_	_
the	_	_
following	_	_
adversarial	_	_
term	_	_
:	_	_
Ladv	_	_
=	_	_
LGAN	_	_
(	_	_
GA	_	_
,	_	_
DA	_	_
)	_	_
+	_	_
LGAN	_	_
(	_	_
GB	_	_
,	_	_
DB	_	_
)	_	_
,	_	_
(	_	_
4	_	_
)	_	_
where	_	_
LGAN	_	_
(	_	_
G	_	_
,	_	_
D	_	_
)	_	_
is	_	_
the	_	_
LSGAN	_	_
formulation	_	_
[	_	_
59	_	_
]	_	_
.	_	_

#162
Overall	_	_
,	_	_
we	_	_
train	_	_
our	_	_
model	_	_
using	_	_
the	_	_
following	_	_
loss	_	_
function	_	_
:	_	_
L	_	_
=	_	_
λadvLadv	_	_
+	_	_
λcycLcyc	_	_
+	_	_
λcorrLcorr	_	_
,	_	_
(	_	_
5	_	_
)	_	_
where	_	_
λadv	_	_
,	_	_
λcyc	_	_
,	_	_
and	_	_
λcorr	_	_
are	_	_
user-defined	_	_
hyperparameters	_	_
.	_	_

#163
3.4	_	_
Implementation	_	_
Details	_	_

#164
Our	_	_
model	_	_
is	_	_
based	_	_
on	_	_
cycleGAN	_	_
and	_	_
implemented	_	_
in	_	_
Pytorch	_	_
.	_	_

#165
We	_	_
experimented	_	_
with	_	_
different	_	_
architectures	_	_
for	_	_
the	_	_
translation	_	_
networks	_	_
,	_	_
including	_	_
U-Net	_	_
[	_	_
60	_	_
]	_	_
,	_	_
ResNet	_	_
[	_	_
61	_	_
]	_	_
,	_	_
and	_	_
RNN-blocks	_	_
[	_	_
62	_	_
]	_	_
.	_	_

#166
Given	_	_
enough	_	_
training	_	_
time	_	_
,	_	_
we	_	_
found	_	_
that	_	_
all	_	_
networks	_	_
produce	_	_
similar	_	_
results	_	_
.	_	_

#167
Due	_	_
to	_	_
its	_	_
memory	_	_
efficiency	_	_
and	_	_
fast	_	_
convergence	_	_
,	_	_
we	_	_
chose	_	_
U-Net	_	_
for	_	_
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
9	_	_
Fig.	_	_
5	_	_
:	_	_
Illustration	_	_
of	_	_
the	_	_
generator	_	_
and	_	_
discriminator	_	_
network	_	_
.	_	_

#168
The	_	_
generator	_	_
uses	_	_
the	_	_
U-net	_	_
architecture	_	_
and	_	_
both	_	_
input	_	_
and	_	_
output	_	_
are	_	_
a	_	_
multi-view	_	_
sequence	_	_
consisting	_	_
of	_	_
three	_	_
views	_	_
.	_	_

#169
A	_	_
random	_	_
SIFT	_	_
correspondence	_	_
is	_	_
sampled	_	_
during	_	_
training	_	_
to	_	_
compute	_	_
the	_	_
correspondence	_	_
loss	_	_
.	_	_

#170
The	_	_
multi-scale	_	_
joint	_	_
discriminator	_	_
examines	_	_
three	_	_
scales	_	_
of	_	_
the	_	_
image	_	_
sequence	_	_
and	_	_
two	_	_
scales	_	_
of	_	_
corresponding	_	_
local	_	_
patches	_	_
.	_	_

#171
The	_	_
width	_	_
and	_	_
height	_	_
of	_	_
each	_	_
rectangular	_	_
block	_	_
indicate	_	_
the	_	_
channel	_	_
size	_	_
and	_	_
the	_	_
spatial	_	_
dimension	_	_
of	_	_
the	_	_
output	_	_
feature	_	_
map	_	_
,	_	_
respectively	_	_
.	_	_

#172
our	_	_
final	_	_
model	_	_
.	_	_

#173
As	_	_
shown	_	_
in	_	_
Figure	_	_
5	_	_
,	_	_
we	_	_
use	_	_
the	_	_
multi-scale	_	_
discriminator	_	_
introduced	_	_
in	_	_
[	_	_
27	_	_
]	_	_
that	_	_
downsamples	_	_
by	_	_
a	_	_
rate	_	_
of	_	_
2	_	_
,	_	_
which	_	_
generally	_	_
works	_	_
better	_	_
for	_	_
high	_	_
resolution	_	_
images	_	_
.	_	_

#174
Our	_	_
discriminator	_	_
also	_	_
considers	_	_
the	_	_
local	_	_
correspondence	_	_
patches	_	_
as	_	_
additional	_	_
input	_	_
,	_	_
which	_	_
helps	_	_
to	_	_
produce	_	_
coherent	_	_
translations	_	_
.	_	_

#175
Followed	_	_
by	_	_
the	_	_
training	_	_
guidances	_	_
proposed	_	_
in	_	_
[	_	_
26	_	_
]	_	_
,	_	_
we	_	_
use	_	_
pixel-wise	_	_
normalization	_	_
in	_	_
the	_	_
generators	_	_
and	_	_
add	_	_
a	_	_
1-strided	_	_
convolutional	_	_
layer	_	_
after	_	_
each	_	_
deconvolutional	_	_
layer	_	_
.	_	_

#176
For	_	_
computing	_	_
the	_	_
correspondence	_	_
loss	_	_
,	_	_
we	_	_
use	_	_
a	_	_
patch	_	_
size	_	_
of	_	_
256	_	_
×	_	_
256	_	_
and	_	_
sample	_	_
a	_	_
single	_	_
SIFT	_	_
correspondence	_	_
per	_	_
training	_	_
iteration	_	_
randomly	_	_
.	_	_

#177
The	_	_
discriminator	_	_
follows	_	_
the	_	_
architecture	_	_
as	_	_
:	_	_
C64C128C256C512C1	_	_
.	_	_

#178
The	_	_
generator’s	_	_
encoder	_	_
architecture	_	_
is	_	_
:	_	_
C64C128C256C512C512C512C512C512	_	_
.	_	_

#179
We	_	_
use	_	_
λadv	_	_
=	_	_
1	_	_
,	_	_
λcyc	_	_
=	_	_
10	_	_
,	_	_
λcorr	_	_
=	_	_
5	_	_
in	_	_
all	_	_
our	_	_
experiments	_	_
and	_	_
train	_	_
using	_	_
the	_	_
ADAM	_	_
optimizer	_	_
with	_	_
a	_	_
learning	_	_
rate	_	_
of	_	_
0.0002	_	_
.	_	_

#180
4	_	_
Evaluation	_	_

#181
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
present	_	_
qualitative	_	_
and	_	_
quantitative	_	_
evaluations	_	_
of	_	_
our	_	_
proposed	_	_
S2Dnet	_	_
.	_	_

#182
For	_	_
this	_	_
purpose	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
model	_	_
on	_	_
both	_	_
the	_	_
translation	_	_
task	_	_
and	_	_
the	_	_
subsequent	_	_
3D	_	_
reconstruction	_	_
,	_	_
and	_	_
we	_	_
compare	_	_
to	_	_
several	_	_
baseline	_	_
systems	_	_
.	_	_

#183
In	_	_
Section	_	_
4.1	_	_
we	_	_
report	_	_
results	_	_
on	_	_
our	_	_
synthetic	_	_
test	_	_
set	_	_
and	_	_
we	_	_
also	_	_
perform	_	_
an	_	_
evaluation	_	_
on	_	_
real-world	_	_
data	_	_
in	_	_
Section	_	_
4.2	_	_
.	_	_

#184
To	_	_
evaluate	_	_
the	_	_
benefit	_	_
of	_	_
our	_	_
proposed	_	_
inter-view	_	_
coherence	_	_
,	_	_
we	_	_
perform	_	_
a	_	_
comparison	_	_
to	_	_
a	_	_
single-view	_	_
translation	_	_
baseline	_	_
by	_	_
training	_	_
a	_	_
cycleGAN	_	_
network	_	_
[	_	_
10	_	_
]	_	_
on	_	_
glossy	_	_
to	_	_
diffuse	_	_
translation	_	_
.	_	_

#185
Since	_	_
our	_	_
synthetic	_	_
dataset	_	_
features	_	_
a	_	_
bijective	_	_
mapping	_	_
between	_	_
10	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Glossy	_	_
pix2pix	_	_
cycleGAN	_	_
S2Dnet	_	_
Image	_	_
MSE	_	_
118.39	_	_
56.20	_	_
69.15	_	_
57.78	_	_
Table	_	_
1	_	_
:	_	_
Quantitative	_	_
evaluation	_	_
of	_	_
the	_	_
image	_	_
error	_	_
on	_	_
our	_	_
synthetic	_	_
testing	_	_
data	_	_
.	_	_

#186
Fig.	_	_
6	_	_
:	_	_
Qualitative	_	_
translation	_	_
results	_	_
on	_	_
a	_	_
synthetic	_	_
input	_	_
sequence	_	_
consisting	_	_
of	_	_
8	_	_
views	_	_
.	_	_

#187
From	_	_
top	_	_
down	_	_
:	_	_
the	_	_
glossy	_	_
input	_	_
sequence	_	_
,	_	_
the	_	_
ground	_	_
truth	_	_
diffuse	_	_
rendering	_	_
,	_	_
and	_	_
the	_	_
translation	_	_
results	_	_
for	_	_
the	_	_
baselines	_	_
pix2pix	_	_
and	_	_
cycleGAN	_	_
,	_	_
and	_	_
our	_	_
S2Dnet	_	_
.	_	_

#188
The	_	_
output	_	_
of	_	_
pix2pix	_	_
is	_	_
generally	_	_
blurry	_	_
.	_	_

#189
The	_	_
cycleGAN	_	_
output	_	_
,	_	_
although	_	_
sharp	_	_
,	_	_
lacks	_	_
inter-view	_	_
consistency	_	_
.	_	_

#190
Our	_	_
S2Dnet	_	_
produces	_	_
both	_	_
crisp	_	_
and	_	_
coherent	_	_
translations	_	_
.	_	_

#191
glossy	_	_
and	_	_
diffuse	_	_
images	_	_
,	_	_
we	_	_
also	_	_
train	_	_
a	_	_
pix2pix	_	_
network	_	_
[	_	_
9	_	_
]	_	_
for	_	_
a	_	_
supervised	_	_
baseline	_	_
on	_	_
synthetic	_	_
data	_	_
.	_	_

#192
In	_	_
addition	_	_
,	_	_
we	_	_
compare	_	_
reconstruction	_	_
quality	_	_
to	_	_
performing	_	_
stereo	_	_
reconstruction	_	_
directly	_	_
on	_	_
the	_	_
glossy	_	_
multi-view	_	_
sequence	_	_
to	_	_
demonstrate	_	_
the	_	_
benefit	_	_
of	_	_
translating	_	_
the	_	_
input	_	_
as	_	_
a	_	_
preprocessing	_	_
step	_	_
.	_	_

#193
For	_	_
3D	_	_
reconstruction	_	_
,	_	_
we	_	_
apply	_	_
a	_	_
state-of-the-art	_	_
multi-view	_	_
surface	_	_
reconstruction	_	_
method	_	_
[	_	_
1	_	_
]	_	_
on	_	_
input	_	_
sequences	_	_
consisting	_	_
of	_	_
10	_	_
to	_	_
15	_	_
views	_	_
.	_	_

#194
For	_	_
our	_	_
method	_	_
,	_	_
we	_	_
translate	_	_
each	_	_
input	_	_
view	_	_
sequentially	_	_
but	_	_
we	_	_
feed	_	_
the	_	_
two	_	_
neighboring	_	_
views	_	_
as	_	_
additional	_	_
inputs	_	_
to	_	_
our	_	_
multi-view	_	_
network	_	_
.	_	_

#195
For	_	_
the	_	_
two	_	_
baseline	_	_
translation	_	_
methods	_	_
,	_	_
we	_	_
translate	_	_
each	_	_
view	_	_
independently	_	_
.	_	_

#196
The	_	_
3D	_	_
reconstruction	_	_
pipeline	_	_
then	_	_
uses	_	_
the	_	_
entire	_	_
translated	_	_
multi-view	_	_
sequence	_	_
as	_	_
input	_	_
.	_	_

#197
4.1	_	_
Synthetic	_	_
Data	_	_

#198
For	_	_
a	_	_
quantitative	_	_
evaluation	_	_
of	_	_
the	_	_
image	_	_
translation	_	_
performance	_	_
,	_	_
we	_	_
compute	_	_
MSE	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
ground	_	_
truth	_	_
diffuse	_	_
renderings	_	_
on	_	_
our	_	_
synthetic	_	_
test	_	_
set	_	_
.	_	_

#199
Table	_	_
1	_	_
shows	_	_
a	_	_
comparison	_	_
of	_	_
our	_	_
S2Dnet	_	_
to	_	_
pix2pix	_	_
and	_	_
cycleGAN	_	_
.	_	_

#200
Unsurprisingly	_	_
,	_	_
the	_	_
supervised	_	_
pix2pix	_	_
network	_	_
performs	_	_
best	_	_
,	_	_
closely	_	_
followed	_	_
by	_	_
our	_	_
S2Dnet	_	_
,	_	_
which	_	_
outperforms	_	_
the	_	_
unsupervised	_	_
baseline	_	_
by	_	_
a	_	_
significant	_	_
margin	_	_
.	_	_

#201
In	_	_
Figure	_	_
6	_	_
we	_	_
show	_	_
qualitative	_	_
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
11	_	_
Model	_	_
1	_	_
2	_	_
3	_	_
4	_	_
5	_	_
6	_	_
7	_	_
8	_	_
9	_	_
10	_	_
AVG	_	_
Glossy	_	_
0.67	_	_
0.88	_	_
1.35	_	_
0.76	_	_
1.15	_	_
1.13	_	_
1.15	_	_
0.78	_	_
0.54	_	_
0.66	_	_
0.90	_	_
cycleGAN	_	_
1.18	_	_
0.72	_	_
0.89	_	_
0.59	_	_
1.35	_	_
0.72	_	_
0.99	_	_
0.62	_	_
0.51	_	_
0.42	_	_
0.80	_	_
S2Dnet	_	_
0.52	_	_
0.67	_	_
0.72	_	_
0.43	_	_
0.87	_	_
0.54	_	_
0.92	_	_
0.65	_	_
0.55	_	_
0.56	_	_
0.64	_	_
Table	_	_
2	_	_
:	_	_
Quantitative	_	_
evaluation	_	_
of	_	_
surface	_	_
reconstruction	_	_
performance	_	_
on	_	_
10	_	_
different	_	_
scenes	_	_
.	_	_

#202
The	_	_
error	_	_
metric	_	_
is	_	_
the	_	_
percentage	_	_
of	_	_
bounding	_	_
box	_	_
diagonal	_	_
.	_	_

#203
Our	_	_
S2Dnet	_	_
performs	_	_
best	_	_
,	_	_
and	_	_
the	_	_
translation	_	_
baseline	_	_
still	_	_
performs	_	_
significantly	_	_
better	_	_
than	_	_
directly	_	_
reconstructing	_	_
from	_	_
the	_	_
glossy	_	_
images	_	_
.	_	_

#204
The	_	_
numbering	_	_
of	_	_
the	_	_
models	_	_
follows	_	_
the	_	_
visualization	_	_
in	_	_
Figure	_	_
7	_	_
,	_	_
using	_	_
the	_	_
same	_	_
left	_	_
to	_	_
right	_	_
order	_	_
.	_	_

#205
Fig.	_	_
7	_	_
:	_	_
Qualitative	_	_
surface	_	_
reconstruction	_	_
results	_	_
on	_	_
10	_	_
different	_	_
scenes	_	_
.	_	_

#206
From	_	_
top	_	_
to	_	_
bottom	_	_
:	_	_
glossy	_	_
input	_	_
,	_	_
ground	_	_
truth	_	_
diffuse	_	_
renderings	_	_
,	_	_
cycleGAN	_	_
translation	_	_
outputs	_	_
,	_	_
our	_	_
S2Dnet	_	_
translation	_	_
outputs	_	_
,	_	_
reconstructions	_	_
from	_	_
glossy	_	_
images	_	_
,	_	_
reconstructions	_	_
from	_	_
ground	_	_
truth	_	_
diffuse	_	_
images	_	_
,	_	_
reconstructions	_	_
from	_	_
cycleGAN	_	_
output	_	_
,	_	_
and	_	_
reconstructions	_	_
from	_	_
our	_	_
S2Dnet	_	_
output	_	_
.	_	_

#207
All	_	_
sequences	_	_
are	_	_
excluded	_	_
from	_	_
our	_	_
training	_	_
set	_	_
,	_	_
and	_	_
the	_	_
objects	_	_
in	_	_
column	_	_
3	_	_
and	_	_
4	_	_
have	_	_
not	_	_
even	_	_
been	_	_
seen	_	_
during	_	_
training	_	_
.	_	_

#208
translation	_	_
results	_	_
.	_	_

#209
Note	_	_
that	_	_
the	_	_
output	_	_
of	_	_
pix2pix	_	_
is	_	_
generally	_	_
blurry	_	_
.	_	_

#210
Since	_	_
MSE	_	_
penalizes	_	_
outliers	_	_
and	_	_
prefers	_	_
a	_	_
smooth	_	_
solution	_	_
,	_	_
pix2pix	_	_
still	_	_
achieves	_	_
a	_	_
low	_	_
MSE	_	_
error	_	_
.	_	_

#211
While	_	_
the	_	_
output	_	_
of	_	_
cycleGAN	_	_
is	_	_
sharper	_	_
,	_	_
the	_	_
translated	_	_
sequence	_	_
lacks	_	_
inter-view	_	_
consistency	_	_
,	_	_
whereas	_	_
our	_	_
S2Dnet	_	_
produces	_	_
both	_	_
highly	_	_
detailed	_	_
and	_	_
coherent	_	_
translations	_	_
.	_	_

#212
12	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Next	_	_
,	_	_
we	_	_
evaluate	_	_
the	_	_
quality	_	_
of	_	_
the	_	_
surface	_	_
reconstruction	_	_
by	_	_
feeding	_	_
the	_	_
translated	_	_
sequences	_	_
to	_	_
the	_	_
reconstruction	_	_
pipeline	_	_
.	_	_

#213
We	_	_
found	_	_
that	_	_
the	_	_
blurry	_	_
output	_	_
of	_	_
pix2pix	_	_
is	_	_
not	_	_
suitable	_	_
for	_	_
stereo	_	_
reconstruction	_	_
,	_	_
since	_	_
already	_	_
the	_	_
first	_	_
step	_	_
,	_	_
estimating	_	_
camera	_	_
parameters	_	_
based	_	_
on	_	_
feature	_	_
correspondences	_	_
,	_	_
fails	_	_
on	_	_
this	_	_
data	_	_
.	_	_

#214
We	_	_
therefore	_	_
exclude	_	_
pix2pix	_	_
from	_	_
the	_	_
surface	_	_
reconstruction	_	_
evaluation	_	_
but	_	_
include	_	_
the	_	_
trivial	_	_
baseline	_	_
of	_	_
directly	_	_
reconstructing	_	_
from	_	_
the	_	_
glossy	_	_
input	_	_
sequence	_	_
to	_	_
demonstrate	_	_
the	_	_
benefit	_	_
of	_	_
the	_	_
translation	_	_
step	_	_
.	_	_

#215
In	_	_
order	_	_
to	_	_
compute	_	_
the	_	_
geometric	_	_
error	_	_
of	_	_
the	_	_
surface	_	_
reconstruction	_	_
output	_	_
,	_	_
we	_	_
register	_	_
the	_	_
reconstructed	_	_
geometry	_	_
to	_	_
the	_	_
ground	_	_
truth	_	_
mesh	_	_
using	_	_
a	_	_
variant	_	_
of	_	_
ICP	_	_
[	_	_
63	_	_
]	_	_
.	_	_

#216
Next	_	_
,	_	_
we	_	_
compute	_	_
the	_	_
Euclidean	_	_
distance	_	_
of	_	_
each	_	_
reconstructed	_	_
surface	_	_
point	_	_
to	_	_
its	_	_
nearest	_	_
neighbor	_	_
in	_	_
the	_	_
ground	_	_
truth	_	_
mesh	_	_
and	_	_
report	_	_
the	_	_
per-model	_	_
mean	_	_
value	_	_
.	_	_

#217
Table	_	_
2	_	_
shows	_	_
the	_	_
surface	_	_
reconstruction	_	_
error	_	_
for	_	_
our	_	_
S2Dnet	_	_
in	_	_
comparison	_	_
to	_	_
the	_	_
three	_	_
baselines	_	_
.	_	_

#218
The	_	_
numbers	_	_
show	_	_
that	_	_
our	_	_
S2Dnet	_	_
performs	_	_
best	_	_
,	_	_
and	_	_
that	_	_
preprocessing	_	_
the	_	_
glossy	_	_
input	_	_
sequences	_	_
clearly	_	_
helps	_	_
to	_	_
obtain	_	_
a	_	_
more	_	_
accurate	_	_
reconstruction	_	_
,	_	_
even	_	_
when	_	_
using	_	_
the	_	_
cycleGAN	_	_
baseline	_	_
.	_	_

#219
In	_	_
Figure	_	_
7	_	_
we	_	_
show	_	_
qualitative	_	_
surface	_	_
reconstruction	_	_
results	_	_
for	_	_
10	_	_
different	_	_
scenes	_	_
in	_	_
various	_	_
environments	_	_
.	_	_

#220
4.2	_	_
Real-world	_	_
Data	_	_

#221
Since	_	_
we	_	_
do	_	_
not	_	_
have	_	_
real-world	_	_
ground	_	_
truth	_	_
data	_	_
,	_	_
we	_	_
compile	_	_
a	_	_
real-world	_	_
test	_	_
set	_	_
and	_	_
perform	_	_
a	_	_
qualitative	_	_
comparison	_	_
on	_	_
it	_	_
.	_	_

#222
For	_	_
all	_	_
methods	_	_
,	_	_
we	_	_
compare	_	_
generalization	_	_
performance	_	_
when	_	_
training	_	_
on	_	_
our	_	_
synthetic	_	_
dataset	_	_
.	_	_

#223
Moreover	_	_
,	_	_
we	_	_
evaluate	_	_
how	_	_
the	_	_
different	_	_
models	_	_
perform	_	_
when	_	_
fine-tuning	_	_
on	_	_
real-world	_	_
data	_	_
,	_	_
or	_	_
training	_	_
on	_	_
real-world	_	_
data	_	_
from	_	_
scratch	_	_
.	_	_

#224
For	_	_
this	_	_
purpose	_	_
,	_	_
we	_	_
compile	_	_
a	_	_
dataset	_	_
by	_	_
shooting	_	_
photos	_	_
of	_	_
real-world	_	_
objects	_	_
.	_	_

#225
We	_	_
choose	_	_
5	_	_
diffuse	_	_
real-world	_	_
objects	_	_
and	_	_
take	_	_
5k	_	_
pictures	_	_
in	_	_
total	_	_
from	_	_
different	_	_
camera	_	_
positions	_	_
and	_	_
under	_	_
varying	_	_
lighting	_	_
conditions	_	_
.	_	_

#226
Next	_	_
,	_	_
we	_	_
use	_	_
a	_	_
glossy	_	_
spray	_	_
paint	_	_
to	_	_
cover	_	_
our	_	_
objects	_	_
with	_	_
a	_	_
glossy	_	_
coat	_	_
and	_	_
shoot	_	_
another	_	_
5k	_	_
pictures	_	_
to	_	_
represent	_	_
the	_	_
glossy	_	_
domain	_	_
.	_	_

#227
The	_	_
resulting	_	_
dataset	_	_
consists	_	_
of	_	_
unpaired	_	_
samples	_	_
of	_	_
glossy	_	_
and	_	_
diffuse	_	_
objects	_	_
under	_	_
real-world	_	_
conditions	_	_
,	_	_
see	_	_
Figure	_	_
10	_	_
a	_	_
)	_	_
and	_	_
b	_	_
)	_	_
.	_	_

#228
In	_	_
Figure	_	_
8	_	_
we	_	_
show	_	_
qualitative	_	_
translation	_	_
results	_	_
on	_	_
real-world	_	_
data	_	_
.	_	_

#229
All	_	_
networks	_	_
are	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
only	_	_
here	_	_
,	_	_
and	_	_
they	_	_
all	_	_
manage	_	_
to	_	_
generalize	_	_
to	_	_
some	_	_
extent	_	_
to	_	_
real-world	_	_
data	_	_
,	_	_
thanks	_	_
to	_	_
our	_	_
high-quality	_	_
synthetic	_	_
dataset	_	_
.	_	_

#230
Similar	_	_
to	_	_
the	_	_
synthetic	_	_
results	_	_
in	_	_
Figure	_	_
6	_	_
,	_	_
pix2pix	_	_
produces	_	_
blurry	_	_
results	_	_
,	_	_
while	_	_
cycleGAN	_	_
introduces	_	_
inconsistent	_	_
high-frequency	_	_
artifacts	_	_
.	_	_

#231
S2Dnet	_	_
is	_	_
able	_	_
the	_	_
remove	_	_
most	_	_
of	_	_
the	_	_
specular	_	_
effects	_	_
and	_	_
preserves	_	_
geometric	_	_
details	_	_
in	_	_
a	_	_
consistent	_	_
manner	_	_
.	_	_

#232
In	_	_
Figure	_	_
9	_	_
we	_	_
show	_	_
qualitative	_	_
surface	_	_
reconstruction	_	_
results	_	_
for	_	_
7	_	_
different	_	_
scenes	_	_
.	_	_

#233
Artifacts	_	_
occur	_	_
mainly	_	_
close	_	_
to	_	_
the	_	_
object	_	_
silhouettes	_	_
in	_	_
complex	_	_
background	_	_
environments	_	_
.	_	_

#234
This	_	_
could	_	_
be	_	_
mitigated	_	_
by	_	_
training	_	_
with	_	_
segmentation	_	_
masks	_	_
.	_	_

#235
Finally	_	_
,	_	_
we	_	_
evaluate	_	_
performance	_	_
when	_	_
either	_	_
fine-tuning	_	_
or	_	_
training	_	_
from	_	_
scratch	_	_
on	_	_
real-world	_	_
data	_	_
.	_	_

#236
We	_	_
retrain	_	_
or	_	_
fine-tune	_	_
S2Dnet	_	_
and	_	_
cycleGAN	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
,	_	_
but	_	_
can	_	_
not	_	_
retrain	_	_
pix2pix	_	_
for	_	_
this	_	_
purpose	_	_
,	_	_
since	_	_
it	_	_
relies	_	_
on	_	_
a	_	_
supervision	_	_
signal	_	_
that	_	_
is	_	_
not	_	_
present	_	_
in	_	_
our	_	_
unpaired	_	_
real-world	_	_
dataset	_	_
.	_	_

#237
Our	_	_
experiments	_	_
show	_	_
that	_	_
training	_	_
or	_	_
fine-tuning	_	_
using	_	_
such	_	_
a	_	_
small	_	_
dataset	_	_
leads	_	_
to	_	_
heavy	_	_
overfitting	_	_
.	_	_

#238
The	_	_
translation	_	_
performance	_	_
for	_	_
real-world	_	_
objects	_	_
that	_	_
were	_	_
not	_	_
seen	_	_
during	_	_
training	_	_
decreases	_	_
significantly	_	_
compared	_	_
to	_	_
the	_	_
models	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
only	_	_
.	_	_

#239
In	_	_
Figure	_	_
10	_	_
c	_	_
)	_	_
and	_	_
d	_	_
)	_	_
we	_	_
show	_	_
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
13	_	_
Fig.	_	_
8	_	_
:	_	_
Qualitative	_	_
translation	_	_
results	_	_
on	_	_
a	_	_
real-world	_	_
input	_	_
sequence	_	_
consisting	_	_
of	_	_
11	_	_
views	_	_
.	_	_

#240
The	_	_
first	_	_
row	_	_
shows	_	_
the	_	_
glossy	_	_
input	_	_
sequence	_	_
and	_	_
the	_	_
remaining	_	_
rows	_	_
show	_	_
the	_	_
translation	_	_
results	_	_
of	_	_
pix2pix	_	_
,	_	_
cycleGAN	_	_
,	_	_
and	_	_
our	_	_
S2Dnet	_	_
.	_	_

#241
All	_	_
networks	_	_
are	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
only	_	_
.	_	_

#242
Similar	_	_
to	_	_
the	_	_
synthetic	_	_
case	_	_
,	_	_
cycleGAN	_	_
outperforms	_	_
pix2pix	_	_
,	_	_
but	_	_
it	_	_
produces	_	_
high-frequency	_	_
artifacts	_	_
that	_	_
are	_	_
not	_	_
consistent	_	_
along	_	_
the	_	_
views	_	_
.	_	_

#243
Our	_	_
S2Dnet	_	_
is	_	_
able	_	_
to	_	_
remove	_	_
most	_	_
of	_	_
the	_	_
specular	_	_
effects	_	_
and	_	_
preserves	_	_
all	_	_
the	_	_
geometric	_	_
details	_	_
in	_	_
a	_	_
consistent	_	_
manner	_	_
.	_	_

#244
Fig.	_	_
9	_	_
:	_	_
Qualitative	_	_
surface	_	_
reconstruction	_	_
results	_	_
on	_	_
7	_	_
different	_	_
real-world	_	_
scenes	_	_
.	_	_

#245
Top	_	_
to	_	_
bottom	_	_
:	_	_
glossy	_	_
input	_	_
,	_	_
cycleGAN	_	_
translation	_	_
outputs	_	_
,	_	_
our	_	_
S2Dnet	_	_
translation	_	_
outputs	_	_
,	_	_
reconstructions	_	_
from	_	_
glossy	_	_
images	_	_
,	_	_
reconstructions	_	_
from	_	_
cycleGAN	_	_
output	_	_
,	_	_
and	_	_
reconstructions	_	_
from	_	_
our	_	_
S2Dnet	_	_
output	_	_
.	_	_

#246
All	_	_
networks	_	_
are	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
only	_	_
.	_	_

#247
14	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
(	_	_
a	_	_
)	_	_
(	_	_
b	_	_
)	_	_
(	_	_
c	_	_
)	_	_
(	_	_
d	_	_
)	_	_
(	_	_
e	_	_
)	_	_
(	_	_
f	_	_
)	_	_
Fig.	_	_
10	_	_
:	_	_
a	_	_
)	_	_
,	_	_
b	_	_
)	_	_
A	_	_
sample	_	_
of	_	_
our	_	_
real-world	_	_
dataset	_	_
.	_	_

#248
c	_	_
)	_	_
translation	_	_
result	_	_
of	_	_
cycleGAN	_	_
when	_	_
training	_	_
from	_	_
scratch	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
.	_	_

#249
d	_	_
)	_	_
S2Dnet	_	_
output	_	_
,	_	_
trained	_	_
from	_	_
scratch	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
.	_	_

#250
e	_	_
)	_	_
S2Dnet	_	_
output	_	_
,	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
only	_	_
.	_	_

#251
f	_	_
)	_	_
S2Dnet	_	_
output	_	_
,	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
,	_	_
fine-tuned	_	_
on	_	_
real-world	_	_
data	_	_
.	_	_

#252
image	_	_
translation	_	_
results	_	_
of	_	_
cycleGAN	_	_
and	_	_
S2Dnet	_	_
when	_	_
training	_	_
from	_	_
scratch	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
.	_	_

#253
Since	_	_
the	_	_
scene	_	_
in	_	_
Figure	_	_
10	_	_
is	_	_
part	_	_
of	_	_
the	_	_
training	_	_
set	_	_
(	_	_
although	_	_
the	_	_
input	_	_
image	_	_
itself	_	_
is	_	_
excluded	_	_
from	_	_
the	_	_
training	_	_
set	_	_
)	_	_
,	_	_
our	_	_
S2Dnet	_	_
produces	_	_
decent	_	_
translation	_	_
results	_	_
,	_	_
which	_	_
is	_	_
not	_	_
the	_	_
case	_	_
for	_	_
scenes	_	_
not	_	_
seen	_	_
during	_	_
training	_	_
.	_	_

#254
Fine-tuning	_	_
our	_	_
S2Dnet	_	_
produces	_	_
similar	_	_
results	_	_
(	_	_
Figure	_	_
10	_	_
f	_	_
)	_	_
)	_	_
.	_	_

#255
5	_	_
Limitations	_	_
and	_	_
Future	_	_
Work	_	_

#256
Although	_	_
the	_	_
proposed	_	_
framework	_	_
enables	_	_
reconstructing	_	_
glossy	_	_
and	_	_
specular	_	_
objects	_	_
more	_	_
accurately	_	_
compared	_	_
to	_	_
state-of-the-art	_	_
3D	_	_
reconstruction	_	_
algorithms	_	_
,	_	_
a	_	_
few	_	_
limitations	_	_
do	_	_
exist	_	_
.	_	_

#257
First	_	_
,	_	_
since	_	_
the	_	_
network	_	_
architecture	_	_
contains	_	_
an	_	_
encoder	_	_
and	_	_
a	_	_
decoder	_	_
with	_	_
skip	_	_
connections	_	_
,	_	_
the	_	_
glossy-to-Lambertian	_	_
image	_	_
translation	_	_
is	_	_
limited	_	_
to	_	_
images	_	_
of	_	_
a	_	_
fixed	_	_
resolution	_	_
.	_	_

#258
This	_	_
resolutions	_	_
might	speculation	_
be	_	_
too	_	_
low	_	_
for	_	_
certain	_	_
types	_	_
of	_	_
applications	_	_
.	_	_

#259
Next	_	_
,	_	_
due	_	_
to	_	_
the	_	_
variability	_	_
of	_	_
the	_	_
background	_	_
in	_	_
real	_	_
images	_	_
,	_	_
the	_	_
translation	_	_
network	_	_
might	options-speculation	_
treat	_	_
a	_	_
portion	_	_
of	_	_
the	_	_
background	_	_
as	_	_
part	_	_
of	_	_
the	_	_
reconstructed	_	_
object	_	_
.	_	_

#260
Similarly	_	_
,	_	_
the	_	_
network	_	_
occasionally	_	_
misclassifies	_	_
the	_	_
foreground	_	_
as	_	_
part	_	_
of	_	_
the	_	_
background	_	_
,	_	_
especially	_	_
in	_	_
very	_	_
light	_	_
domains	_	_
on	_	_
specular	_	_
objects	_	_
.	_	_

#261
Finally	_	_
,	_	_
as	_	_
the	_	_
simulated	_	_
training	_	_
data	_	_
was	_	_
rendered	_	_
by	_	_
assuming	_	_
a	_	_
fixed	_	_
albedo	_	_
,	_	_
the	_	_
network	_	_
can	_	_
not	_	_
consistently	_	_
translate	_	_
glossy	_	_
materials	_	_
with	_	_
spatially	_	_
varying	_	_
albedo	_	_
into	_	_
a	_	_
Lambertian	_	_
surface	_	_
.	_	_

#262
We	_	_
predict	_	_
that	_	_
given	_	_
a	_	_
larger	_	_
and	_	_
more	_	_
diverse	_	_
training	_	_
set	_	_
in	_	_
terms	_	_
of	_	_
shapes	_	_
,	_	_
backgrounds	_	_
,	_	_
albedos	_	_
and	_	_
materials	_	_
,	_	_
the	_	_
accuracy	_	_
of	_	_
the	_	_
proposed	_	_
method	_	_
in	_	_
recovering	_	_
real	_	_
object	_	_
would	_	_
be	_	_
largely	_	_
enhanced	_	_
.	_	_

#263
Our	_	_
current	_	_
training	_	_
dataset	_	_
includes	_	_
the	_	_
most	_	_
common	_	_
types	_	_
of	_	_
specular	_	_
material	_	_
.	_	_

#264
The	_	_
proposed	_	_
translation	_	_
network	_	_
has	_	_
potential	_	_
to	_	_
be	_	_
extended	_	_
to	_	_
other	_	_
more	_	_
challenging	_	_
materials	_	_
,	_	_
such	_	_
as	_	_
transparent	_	_
objects	_	_
,	_	_
given	_	_
proper	_	_
training	_	_
data	_	_
.	_	_

#265
6	_	_
Supplementary	_	_
Material	_	_

#266
This	_	_
supplementary	_	_
material	_	_
provides	_	_
more	_	_
technical	_	_
details	_	_
and	_	_
experimental	_	_
results	_	_
for	_	_
our	_	_
specular-to-diffuse	_	_
translation	_	_
network	_	_
,	_	_
S2Dnet	_	_
.	_	_

#267
Upon	_	_
publication	_	_
we	_	_
will	_	_
also	_	_
release	_	_
the	_	_
full	_	_
training	_	_
and	_	_
test	_	_
data	_	_
set	_	_
as	_	_
well	_	_
as	_	_
the	_	_
trained	_	_
network	_	_
and	_	_
code	_	_
.	_	_

#268
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
15	_	_
Training	_	_
Details	_	_
.	_	_

#269
Tables	_	_
3	_	_
and	_	_
4	_	_
give	_	_
more	_	_
detail	_	_
of	_	_
our	_	_
network	_	_
architecture	_	_
.	_	_

#270
Xavier	_	_
[	_	_
64	_	_
]	_	_
is	_	_
used	_	_
for	_	_
weights	_	_
initialization	_	_
.	_	_

#271
We	_	_
train	_	_
our	_	_
models	_	_
on	_	_
an	_	_
NVIDIA	_	_
1080Ti	_	_
GPU	_	_
with	_	_
11GB	_	_
GPU	_	_
memory	_	_
,	_	_
which	_	_
only	_	_
allows	_	_
us	_	_
to	_	_
use	_	_
a	_	_
training	_	_
batch	_	_
size	_	_
of	_	_
1	_	_
.	_	_

#272
Additional	_	_
Results	_	_
.	_	_

#273
Figure	_	_
11	_	_
presents	_	_
a	_	_
gallery	_	_
of	_	_
our	_	_
real-world	_	_
training	_	_
data	_	_
as	_	_
described	_	_
in	_	_
the	_	_
paper	_	_
.	_	_

#274
Figure	_	_
12	_	_
shows	_	_
more	_	_
results	_	_
of	_	_
our	_	_
S2Dnet	_	_
,	_	_
given	_	_
input	_	_
scenes	_	_
with	_	_
various	_	_
illumination	_	_
and	_	_
different	_	_
objects	_	_
.	_	_

#275
Figure	_	_
13	_	_
is	_	_
an	_	_
extension	_	_
of	_	_
Figure	_	_
10	_	_
in	_	_
paper	_	_
,	_	_
illustrating	_	_
the	_	_
results	_	_
of	_	_
training	_	_
setups	_	_
using	_	_
different	_	_
kinds	_	_
of	_	_
training	_	_
data	_	_
.	_	_

#276
Figures	_	_
14	_	_
and	_	_
15	_	_
demonstrate	_	_
two	_	_
additional	_	_
examples	_	_
of	_	_
image	_	_
translation	_	_
and	_	_
reconstruction	_	_
,	_	_
where	_	_
S2Dnet	_	_
outperforms	_	_
both	_	_
pix2pix	_	_
and	_	_
cycleGAN	_	_
.	_	_

#277
We	_	_
also	_	_
observe	_	_
that	_	_
when	_	_
the	_	_
weight	_	_
for	_	_
the	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
is	_	_
λcorr	_	_
=	_	_
0	_	_
,	_	_
i.e.	_	_
,	_	_
removing	_	_
the	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
,	_	_
the	_	_
output	_	_
of	_	_
S2Dnet	_	_
lacks	_	_
of	_	_
inter-view	_	_
consistency	_	_
.	_	_

#278
Layer	_	_
Input	_	_
→	_	_
Output	_	_
Shape	_	_
Layer	_	_
Information	_	_
Input	_	_
Layer	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
3	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
CONV-	_	_
(	_	_
N64	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
LeReLU	_	_
Hidden	_	_
Layers	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
128	_	_
)	_	_
CONV-	_	_
(	_	_
N128	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
128	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
CONV-	_	_
(	_	_
N256	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
Output	_	_
Layer	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1	_	_
)	_	_
CONV-	_	_
(	_	_
N1	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
Table	_	_
3	_	_
:	_	_
Discriminator	_	_
network	_	_
architecture	_	_
.	_	_

#279
We	_	_
use	_	_
5	_	_
such	_	_
discriminators	_	_
that	_	_
have	_	_
an	_	_
identical	_	_
network	_	_
structure	_	_
but	_	_
operate	_	_
at	_	_
three	_	_
scales	_	_
of	_	_
the	_	_
image	_	_
sequence	_	_
and	_	_
two	_	_
scales	_	_
of	_	_
corresponding	_	_
local	_	_
patches	_	_
using	_	_
LSGAN	_	_
(	_	_
see	_	_
Figure	_	_
5	_	_
in	_	_
the	_	_
paper	_	_
)	_	_
.	_	_

#280
N	_	_
:	_	_
the	_	_
number	_	_
of	_	_
output	_	_
channels	_	_
,	_	_
K	_	_
:	_	_
kernel	_	_
size	_	_
,	_	_
S	_	_
:	_	_
stride	_	_
size	_	_
,	_	_
P	_	_
:	_	_
padding	_	_
size	_	_
,	_	_
PN	_	_
:	_	_
pixel-wise	_	_
Normalization	_	_
,	_	_
LeReLU	_	_
:	_	_
LeakyReLU	_	_
with	_	_
α	_	_
=	_	_
0.2	_	_
,	_	_
w	_	_
,	_	_
h	_	_
:	_	_
width	_	_
and	_	_
height	_	_
of	_	_
input	_	_
images	_	_
.	_	_

#281
Note	_	_
that	_	_
the	_	_
input	_	_
width	_	_
is	_	_
3w	_	_
because	_	_
we	_	_
spatially	_	_
concatenate	_	_
the	_	_
three	_	_
views	_	_
of	_	_
the	_	_
input	_	_
sequences	_	_
.	_	_

#282
Fig.	_	_
11	_	_
:	_	_
Gallery	_	_
of	_	_
our	_	_
glossy-to-diffuse	_	_
real-world	_	_
training	_	_
data	_	_
and	_	_
the	_	_
spray	_	_
(	_	_
leftmost	_	_
column	_	_
)	_	_
we	_	_
used	_	_
to	_	_
paint	_	_
the	_	_
objects	_	_
.	_	_

#283
We	_	_
first	_	_
choose	_	_
5	_	_
diffuse	_	_
real-world	_	_
objects	_	_
and	_	_
take	_	_
5k	_	_
pictures	_	_
in	_	_
total	_	_
from	_	_
different	_	_
camera	_	_
positions	_	_
and	_	_
under	_	_
varying	_	_
lighting	_	_
conditions	_	_
.	_	_

#284
We	_	_
then	_	_
use	_	_
a	_	_
glossy	_	_
spray	_	_
paint	_	_
to	_	_
cover	_	_
our	_	_
objects	_	_
with	_	_
a	_	_
glossy	_	_
coat	_	_
and	_	_
shoot	_	_
another	_	_
5k	_	_
pictures	_	_
to	_	_
represent	_	_
the	_	_
glossy	_	_
domain	_	_
.	_	_

#285
16	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Part	_	_
Input	_	_
→	_	_
Output	_	_
Shape	_	_
Layer	_	_
Information	_	_
Down-sampling	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
3	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
CONV-	_	_
(	_	_
N64	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
128	_	_
)	_	_
CONV-	_	_
(	_	_
N128	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
128	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
CONV-	_	_
(	_	_
N256	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
LeReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
ReLU	_	_
Up-sampling	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
DECONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1024	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
DECONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1024	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
DECONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1024	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
DECONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1024	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
DECONV-	_	_
(	_	_
N512	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
1024	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
DECONV-	_	_
(	_	_
N256	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N256	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
512	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
128	_	_
)	_	_
DECONV-	_	_
(	_	_
N128	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N128	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
256	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
DECONV-	_	_
(	_	_
N64	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N64	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
PN	_	_
,	_	_
ReLU	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
64	_	_
)	_	_
→	_	_
(	_	_
h	_	_
,	_	_
3w	_	_
,	_	_
3	_	_
)	_	_
DECONV-	_	_
(	_	_
N3	_	_
,	_	_
K4x4	_	_
,	_	_
S2	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
CONV-	_	_
(	_	_
N512	_	_
,	_	_
K3x3	_	_
,	_	_
S1	_	_
,	_	_
P1	_	_
)	_	_
,	_	_
Tanh	_	_
Table	_	_
4	_	_
:	_	_
Generator	_	_
network	_	_
architecture	_	_
.	_	_

#286
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
17	_	_
Fig.	_	_
12	_	_
:	_	_
Gallery	_	_
of	_	_
our	_	_
glossy-to-diffuse	_	_
results	_	_
of	_	_
40	_	_
synthetic	_	_
and	_	_
10	_	_
real-world	_	_
(	_	_
last	_	_
two	_	_
rows	_	_
)	_	_
scenes	_	_
.	_	_

#287
All	_	_
sequences	_	_
are	_	_
excluded	_	_
from	_	_
our	_	_
training	_	_
set	_	_
.	_	_

#288
Three	_	_
synthetic	_	_
(	_	_
Armadillo	_	_
,	_	_
Standing	_	_
Buddha	_	_
,	_	_
Roman	_	_
Head	_	_
Sculpture	_	_
)	_	_
and	_	_
all	_	_
real-world	_	_
objects	_	_
have	_	_
not	_	_
even	_	_
been	_	_
seen	_	_
during	_	_
training	_	_
.	_	_

#289
18	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Fig.	_	_
13	_	_
:	_	_
A	_	_
sample	_	_
of	_	_
our	_	_
real-world	_	_
dataset	_	_
is	_	_
shown	_	_
in	_	_
(	_	_
a-b	_	_
)	_	_
.	_	_

#290
Translation	_	_
results	_	_
of	_	_
cycleGAN	_	_
when	_	_
training	_	_
from	_	_
scratch	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
or	_	_
synthetic	_	_
data	_	_
only	_	_
are	_	_
shown	_	_
in	_	_
(	_	_
c	_	_
)	_	_
and	_	_
(	_	_
d	_	_
)	_	_
,	_	_
respectively	_	_
.	_	_

#291
S2Dnet	_	_
outputs	_	_
,	_	_
trained	_	_
from	_	_
scratch	_	_
on	_	_
our	_	_
real-world	_	_
dataset	_	_
or	_	_
synthetic	_	_
data	_	_
only	_	_
,	_	_
are	_	_
shown	_	_
in	_	_
(	_	_
e	_	_
)	_	_
and	_	_
(	_	_
f	_	_
)	_	_
,	_	_
respectively	_	_
.	_	_

#292
Another	_	_
output	_	_
of	_	_
S2Dnet	_	_
,	_	_
trained	_	_
on	_	_
synthetic	_	_
data	_	_
and	_	_
then	_	_
fine-tuned	_	_
on	_	_
real-world	_	_
data	_	_
is	_	_
presented	_	_
in	_	_
(	_	_
g	_	_
)	_	_
.	_	_

#293
The	_	_
last	_	_
row	_	_
demonstrates	_	_
the	_	_
corresponding	_	_
reconstruction	_	_
results	_	_
.	_	_

#294
Note	_	_
that	_	_
the	_	_
output	_	_
images	_	_
are	_	_
blurry	_	_
when	_	_
training	_	_
from	_	_
scratch	_	_
on	_	_
real-world	_	_
data	_	_
,	_	_
i.e.	_	_
(	_	_
c	_	_
)	_	_
and	_	_
(	_	_
e	_	_
)	_	_
,	_	_
and	_	_
thus	_	_
not	_	_
suitable	_	_
for	_	_
stereo	_	_
reconstruction	_	_
.	_	_

#295
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
19	_	_
Fig.	_	_
14	_	_
:	_	_
Qualitative	_	_
comparison	_	_
of	_	_
image	_	_
translation	_	_
and	_	_
surface	_	_
reconstruction	_	_
on	_	_
a	_	_
synthetic	_	_
sequence	_	_
consisting	_	_
of	_	_
10	_	_
views	_	_
.	_	_

#296
From	_	_
top	_	_
to	_	_
bottom	_	_
:	_	_
glossy	_	_
input	_	_
,	_	_
ground	_	_
truth	_	_
diffuse	_	_
renderings	_	_
,	_	_
pix2pix	_	_
translation	_	_
outputs	_	_
,	_	_
cycleGAN	_	_
translation	_	_
outputs	_	_
,	_	_
our	_	_
S2Dnet	_	_
translation	_	_
outputs	_	_
using	_	_
λcorr	_	_
=	_	_
0	_	_
(	_	_
no	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
)	_	_
,	_	_
our	_	_
S2Dnet	_	_
translation	_	_
outputs	_	_
using	_	_
λcorr	_	_
=	_	_
5	_	_
.	_	_

#297
The	_	_
last	_	_
row	_	_
shows	_	_
the	_	_
corresponding	_	_
reconstruction	_	_
results	_	_
.	_	_

#298
All	_	_
sequences	_	_
are	_	_
excluded	_	_
from	_	_
our	_	_
training	_	_
set	_	_
.	_	_

#299
The	_	_
output	_	_
of	_	_
pix2pix	_	_
is	_	_
blurry	_	_
and	_	_
is	_	_
not	_	_
suitable	_	_
for	_	_
multi-view	_	_
reconstruction	_	_
.	_	_

#300
The	_	_
outputs	_	_
of	_	_
cycleGAN	_	_
and	_	_
our	_	_
S2Dnet	_	_
without	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
,	_	_
although	_	_
sharp	_	_
,	_	_
lack	_	_
of	_	_
inter-view	_	_
consistency	_	_
.	_	_

#301
Our	_	_
S2Dnet	_	_
with	_	_
perceptual	_	_
correspondence	_	_
loss	_	_
(	_	_
λcorr	_	_
=	_	_
5	_	_
)	_	_
produces	_	_
crisp	_	_
and	_	_
coherent	_	_
translations	_	_
,	_	_
resulting	_	_
in	_	_
a	_	_
better	_	_
reconstruction	_	_
.	_	_

#302
20	_	_
S.	_	_
Wu	_	_
,	_	_
H.	_	_
Huang	_	_
,	_	_
T.	_	_
Portenier	_	_
,	_	_
M.	_	_
Sela	_	_
,	_	_
D.	_	_
Cohen-Or	_	_
,	_	_
R.	_	_
Kimmel	_	_
and	_	_
M.	_	_
Zwicker	_	_
Fig.	_	_
15	_	_
:	_	_
Another	_	_
set	_	_
of	_	_
image	_	_
translation	_	_
and	_	_
surface	_	_
reconstruction	_	_
comparison	_	_
on	_	_
a	_	_
synthetic	_	_
input	_	_
sequence	_	_
consisting	_	_
of	_	_
10	_	_
views	_	_
.	_	_

#303
Acknowledgement	_	_
We	_	_
thank	_	_
the	_	_
anonymous	_	_
reviewers	_	_
for	_	_
their	_	_
constructive	_	_
comments	_	_
.	_	_

#304
This	_	_
work	_	_
was	_	_
supported	_	_
in	_	_
parts	_	_
by	_	_
Swiss	_	_
National	_	_
Science	_	_
Foundation	_	_
(	_	_
169151	_	_
)	_	_
,	_	_
NSFC	_	_
(	_	_
61522213	_	_
,	_	_
61761146002	_	_
,	_	_
61861130365	_	_
)	_	_
,	_	_
973	_	_
Program	_	_
(	_	_
2015CB352501	_	_
)	_	_
,	_	_
Guangdong	_	_
Science	_	_
and	_	_
Technology	_	_
Program	_	_
(	_	_
2015A030312015	_	_
)	_	_
,	_	_
ISF-NSFC	_	_
Joint	_	_
Research	_	_
Program	_	_
(	_	_
2472/17	_	_
)	_	_
and	_	_
Shenzhen	_	_
Innovation	_	_
Program	_	_
(	_	_
KQJSCX20170727101233642	_	_
)	_	_
.	_	_

#305
Specular-to-Diffuse	_	_
Translation	_	_
for	_	_
Multi-View	_	_
Reconstruction	_	_
21	_	_