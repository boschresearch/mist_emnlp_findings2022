#0
Revisiting	_	_
Video	_	_
Saliency	_	_
:	_	_
A	_	_
Large-scale	_	_
Benchmark	_	_
and	_	_
a	_	_
New	_	_
Model	_	_
Wenguan	_	_
Wang	_	_
1	_	_
,	_	_
Jianbing	_	_
Shen∗	_	_
1	_	_
,	_	_
Fang	_	_
Guo	_	_
1	_	_
,	_	_
Ming-Ming	_	_
Cheng	_	_
2	_	_
,	_	_
Ali	_	_
Borji	_	_
3	_	_
1Beijing	_	_
Lab	_	_
of	_	_
Intelligent	_	_
Information	_	_
Technology	_	_
,	_	_
School	_	_
of	_	_
Computer	_	_
Science	_	_
,	_	_
Beijing	_	_
Institute	_	_
of	_	_
Technology	_	_
,	_	_
China	_	_
2CCCE	_	_
,	_	_
Nankai	_	_
University	_	_
,	_	_
China	_	_
3Department	_	_
of	_	_
Computer	_	_
Science	_	_
,	_	_
University	_	_
of	_	_
Central	_	_
Florida	_	_
,	_	_
USA	_	_
wenguanwang.ai	_	_
@	_	_
gmail.com	_	_
,	_	_
{	_	_
shenjianbing	_	_
,	_	_
guofang	_	_
}	_	_
@	_	_
bit.edu.cn	_	_
cmm	_	_
@	_	_
nankai.edu.cn	_	_
,	_	_
aborji	_	_
@	_	_
crcv.ucf.edu	_	_
https	_	_
:	_	_
//github.com/wenguanwang/DHF1K	_	_

#1
Abstract	_	_

#2
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
contribute	_	_
to	_	_
video	_	_
saliency	_	_
research	_	_
in	_	_
two	_	_
ways	_	_
.	_	_

#3
First	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
new	_	_
benchmark	_	_
for	_	_
predicting	_	_
human	_	_
eye	_	_
movements	_	_
during	_	_
dynamic	_	_
scene	_	_
freeviewing	_	_
,	_	_
which	_	_
is	_	_
long-time	_	_
urged	_	_
in	_	_
this	_	_
field	_	_
.	_	_

#4
Our	_	_
dataset	_	_
,	_	_
named	_	_
DHF1K	_	_
(	_	_
Dynamic	_	_
Human	_	_
Fixation	_	_
)	_	_
,	_	_
consists	_	_
of	_	_
1K	_	_
high-quality	_	_
,	_	_
elaborately	_	_
selected	_	_
video	_	_
sequences	_	_
spanning	_	_
a	_	_
large	_	_
range	_	_
of	_	_
scenes	_	_
,	_	_
motions	_	_
,	_	_
object	_	_
types	_	_
and	_	_
background	_	_
complexity	_	_
.	_	_

#5
Existing	_	_
video	_	_
saliency	_	_
datasets	_	_
lack	_	_
variety	_	_
and	_	_
generality	_	_
of	_	_
common	_	_
dynamic	_	_
scenes	_	_
and	_	_
fall	_	_
short	_	_
in	_	_
covering	_	_
challenging	_	_
situations	_	_
in	_	_
unconstrained	_	_
environments	_	_
.	_	_

#6
In	_	_
contrast	_	_
,	_	_
DHF1K	_	_
makes	_	_
a	_	_
significant	_	_
leap	_	_
in	_	_
terms	_	_
of	_	_
scalability	_	_
,	_	_
diversity	_	_
and	_	_
difficulty	_	_
,	_	_
and	_	_
is	_	_
expected	_	_
to	_	_
boost	_	_
video	_	_
saliency	_	_
modeling	_	_
.	_	_

#7
Second	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
video	_	_
saliency	_	_
model	_	_
that	_	_
augments	_	_
the	_	_
CNN-LSTM	_	_
network	_	_
architecture	_	_
with	_	_
an	_	_
attention	_	_
mechanism	_	_
to	_	_
enable	_	_
fast	_	_
,	_	_
end-to-end	_	_
saliency	_	_
learning	_	_
.	_	_

#8
The	_	_
attention	_	_
mechanism	_	_
explicitly	_	_
encodes	_	_
static	_	_
saliency	_	_
information	_	_
,	_	_
thus	_	_
allowing	_	_
LSTM	_	_
to	_	_
focus	_	_
on	_	_
learning	_	_
more	_	_
flexible	_	_
temporal	_	_
saliency	_	_
representation	_	_
across	_	_
successive	_	_
frames	_	_
.	_	_

#9
Such	_	_
a	_	_
design	_	_
fully	_	_
leverages	_	_
existing	_	_
large-scale	_	_
static	_	_
fixation	_	_
datasets	_	_
,	_	_
avoids	_	_
overfitting	_	_
,	_	_
and	_	_
significantly	_	_
improves	_	_
training	_	_
efficiency	_	_
and	_	_
testing	_	_
performance	_	_
.	_	_

#10
We	_	_
thoroughly	_	_
examine	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
model	_	_
,	_	_
with	_	_
respect	_	_
to	_	_
state-of-the-art	_	_
saliency	_	_
models	_	_
,	_	_
on	_	_
three	_	_
large-scale	_	_
datasets	_	_
(	_	_
i.e.	_	_
,	_	_
DHF1K	_	_
,	_	_
Hollywood2	_	_
,	_	_
UCF	_	_
sports	_	_
)	_	_
.	_	_

#11
Experimental	_	_
results	_	_
over	_	_
more	_	_
than	_	_
1.2K	_	_
testing	_	_
videos	_	_
containing	_	_
400K	_	_
frames	_	_
demonstrate	_	_
that	_	_
our	_	_
model	_	_
outperforms	_	_
other	_	_
competitors	_	_
.	_	_

#12
∗Corresponding	_	_
author	_	_
:	_	_
Jianbing	_	_
Shen	_	_
.	_	_

#13
This	_	_
work	_	_
was	_	_
supported	_	_
in	_	_
part	_	_
by	_	_
the	_	_
Beijing	_	_
Natural	_	_
Science	_	_
Foundation	_	_
under	_	_
Grant	_	_
4182056	_	_
,	_	_
the	_	_
National	_	_
Basic	_	_
Research	_	_
Program	_	_
of	_	_
China	_	_
under	_	_
Grant	_	_
2013CB328805	_	_
,	_	_
the	_	_
Fok	_	_
Ying	_	_
Tung	_	_
Education	_	_
Foundation	_	_
under	_	_
Grant	_	_
141067	_	_
,	_	_
and	_	_
the	_	_
Specialized	_	_
Fund	_	_
for	_	_
Joint	_	_
Building	_	_
Program	_	_
of	_	_
Beijing	_	_
Municipal	_	_
Education	_	_
Commission	_	_
.	_	_

#14
1	_	_
.	_	_

#15
Introduction	_	_
Human	_	_
visual	_	_
system	_	_
(	_	_
HVS	_	_
)	_	_
has	_	_
an	_	_
astonishing	_	_
ability	_	_
to	_	_
quickly	_	_
select	_	_
visually	_	_
important	_	_
regions	_	_
in	_	_
its	_	_
visual	_	_
field	_	_
.	_	_

#16
This	_	_
cognitive	_	_
process	_	_
enables	_	_
humans	_	_
to	_	_
easily	_	_
interpret	_	_
complex	_	_
scenes	_	_
in	_	_
real	_	_
time	_	_
.	_	_

#17
Over	_	_
the	_	_
last	_	_
few	_	_
decades	_	_
,	_	_
several	_	_
computational	_	_
models	_	_
have	_	_
been	_	_
proposed	_	_
for	_	_
imitating	_	_
attentional	_	_
mechanisms	_	_
of	_	_
HVS	_	_
during	_	_
static	_	_
scene	_	_
viewing	_	_
.	_	_

#18
Significant	_	_
advances	_	_
have	_	_
been	_	_
achieved	_	_
recently	_	_
with	_	_
the	_	_
rapid	_	_
spread	_	_
of	_	_
deep	_	_
learning	_	_
techniques	_	_
and	_	_
the	_	_
availability	_	_
of	_	_
large-scale	_	_
static	_	_
gaze	_	_
datasets	_	_
(	_	_
e.g.	_	_
,	_	_
SALICON	_	_
[	_	_
31	_	_
]	_	_
)	_	_
.	_	_

#19
In	_	_
stark	_	_
contrast	_	_
,	_	_
predicting	_	_
observers’	_	_
fixations	_	_
during	_	_
dynamic	_	_
scene	_	_
free-viewing	_	_
has	_	_
less	_	_
been	_	_
explored	_	_
.	_	_

#20
This	_	_
task	_	_
,	_	_
referred	_	_
to	_	_
as	_	_
dynamic	_	_
fixation	_	_
prediction	_	_
or	_	_
video	_	_
saliency	_	_
detection	_	_
is	_	_
very	_	_
useful	_	_
for	_	_
understanding	_	_
human	_	_
attentional	_	_
behaviors	_	_
and	_	_
has	_	_
several	_	_
practical	_	_
real-word	_	_
applications	_	_
(	_	_
e.g.	_	_
,	_	_
video	_	_
captioning	_	_
,	_	_
compression	_	_
,	_	_
question	_	_
answering	_	_
,	_	_
object	_	_
segmentation	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#21
It	_	_
is	_	_
thus	_	_
highly	_	_
desired	_	_
to	_	_
have	_	_
a	_	_
standard	_	_
,	_	_
high-quality	_	_
dataset	_	_
composed	_	_
of	_	_
diverse	_	_
and	_	_
representative	_	_
video	_	_
stimuli	_	_
.	_	_

#22
Exiting	_	_
datasets	_	_
are	_	_
severely	_	_
limited	_	_
in	_	_
their	_	_
coverage	_	_
and	_	_
scalability	_	_
,	_	_
and	_	_
they	_	_
only	_	_
include	_	_
special	_	_
scenarios	_	_
such	_	_
as	_	_
limited	_	_
human	_	_
activities	_	_
in	_	_
constrained	_	_
situations	_	_
.	_	_

#23
None	_	_
of	_	_
them	_	_
includes	_	_
general	_	_
,	_	_
representative	_	_
,	_	_
and	_	_
diverse	_	_
instances	_	_
in	_	_
unconstrained	_	_
,	_	_
task-independent	_	_
scenarios	_	_
.	_	_

#24
As	_	_
a	_	_
consequence	_	_
,	_	_
existing	_	_
datasets	_	_
often	_	_
fail	_	_
to	_	_
offer	_	_
a	_	_
rich	_	_
set	_	_
of	_	_
fixations	_	_
for	_	_
learning	_	_
video	_	_
saliency	_	_
and	_	_
to	_	_
assess	_	_
models	_	_
.	_	_

#25
Moreover	_	_
,	_	_
the	_	_
existing	_	_
datasets	_	_
did	_	_
not	_	_
provide	_	_
an	_	_
evaluation	_	_
server	_	_
with	_	_
standalone	_	_
held	_	_
out	_	_
test	_	_
set	_	_
to	_	_
avoid	_	_
potential	_	_
dataset	_	_
overfitting	_	_
,	_	_
which	_	_
hinders	_	_
further	_	_
development	_	_
on	_	_
this	_	_
topic	_	_
.	_	_

#26
While	_	_
saliency	_	_
benchmarks	_	_
(	_	_
e.g.	_	_
,	_	_
MIT300	_	_
[	_	_
32	_	_
]	_	_
and	_	_
LSUN	_	_
[	_	_
68	_	_
]	_	_
)	_	_
have	_	_
been	_	_
very	_	_
instrumental	_	_
in	_	_
progressing	_	_
the	_	_
static	_	_
saliency	_	_
field	_	_
,	_	_
such	_	_
standard	_	_
widespread	_	_
benchmarks	_	_
are	_	_
missing	_	_
for	_	_
video	_	_
saliency	_	_
modeling	_	_
.	_	_

#27
We	_	_
believe	_	_
such	_	_
benchmarks	_	_
are	_	_
highly	_	_
needed	_	_
to	_	_
move	_	_
the	_	_
field	_	_
forward	_	_
.	_	_

#28
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
new	_	_
benchmark	_	_
“DHF1K	_	_
(	_	_
Dynamic	_	_
Human	_	_
Fixation	_	_
1K	_	_
)	_	_
”	_	_
with	_	_
a	_	_
public	_	_
server	_	_
for	_	_
reportar	_	_
X	_	_
iv	_	_
:1	_	_
1	_	_
.	_	_

#29
4v	_	_
3	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
6	_	_
M	_	_
ay	_	_
2	_	_
ing	_	_
evaluation	_	_
results	_	_
on	_	_
a	_	_
preserved	_	_
test	_	_
set	_	_
.	_	_

#30
Our	_	_
benchmark	_	_
contains	_	_
a	_	_
dataset	_	_
that	_	_
is	_	_
unique	_	_
in	_	_
terms	_	_
of	_	_
generality	_	_
,	_	_
diversity	_	_
and	_	_
difficulty	_	_
.	_	_

#31
It	_	_
includes	_	_
1K	_	_
videos	_	_
with	_	_
more	_	_
than	_	_
600K	_	_
frames	_	_
and	_	_
per-frame	_	_
fixation	_	_
annotations	_	_
from	_	_
17	_	_
observers	_	_
.	_	_

#32
The	_	_
sequences	_	_
have	_	_
been	_	_
carefully	_	_
collected	_	_
to	_	_
include	_	_
diverse	_	_
scenes	_	_
,	_	_
motion	_	_
patterns	_	_
,	_	_
object	_	_
categories	_	_
,	_	_
and	_	_
activities	_	_
.	_	_

#33
DHF1K	_	_
is	_	_
accompanied	_	_
with	_	_
a	_	_
comprehensive	_	_
evaluation	_	_
of	_	_
several	_	_
state-of-the-art	_	_
approaches	_	_
[	_	_
16	_	_
,	_	_
52	_	_
,	_	_
50	_	_
,	_	_
23	_	_
,	_	_
13	_	_
,	_	_
20	_	_
,	_	_
37	_	_
,	_	_
30	_	_
,	_	_
2	_	_
,	_	_
28	_	_
,	_	_
18	_	_
,	_	_
24	_	_
,	_	_
57	_	_
,	_	_
47	_	_
]	_	_
.	_	_

#34
Moreover	_	_
,	_	_
each	_	_
video	_	_
is	_	_
annotated	_	_
with	_	_
a	_	_
main	_	_
category	_	_
label	_	_
(	_	_
e.g.	_	_
,	_	_
daily	_	_
activities	_	_
,	_	_
animals	_	_
)	_	_
and	_	_
rich	_	_
attributes	_	_
(	_	_
e.g.	_	_
,	_	_
camera/content	_	_
movement	_	_
,	_	_
scene	_	_
lighting	_	_
,	_	_
presence	_	_
of	_	_
humans	_	_
)	_	_
,	_	_
which	_	_
would	_	_
enable	_	_
a	_	_
deeper	_	_
understanding	_	_
of	_	_
gaze	_	_
guidance	_	_
in	_	_
free	_	_
viewing	_	_
of	_	_
dynamic	_	_
scenes	_	_
.	_	_

#35
Further	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
CNN-LSTM	_	_
architecture	_	_
[	_	_
12	_	_
,	_	_
46	_	_
]	_	_
based	_	_
video	_	_
saliency	_	_
model	_	_
with	_	_
a	_	_
supervised	_	_
attention	_	_
mechanism	_	_
.	_	_

#36
CNN	_	_
layers	_	_
are	_	_
utilized	_	_
for	_	_
extracting	_	_
static	_	_
features	_	_
within	_	_
input	_	_
frames	_	_
,	_	_
while	_	_
convolutional	_	_
LSTM	_	_
(	_	_
convLSTM	_	_
)	_	_
[	_	_
66	_	_
]	_	_
is	_	_
utilized	_	_
for	_	_
sequential	_	_
fixation	_	_
prediction	_	_
over	_	_
successive	_	_
frames	_	_
.	_	_

#37
An	_	_
attention	_	_
module	_	_
,	_	_
learned	_	_
from	_	_
existing	_	_
large-scale	_	_
image	_	_
saliency	_	_
datasets	_	_
,	_	_
is	_	_
used	_	_
to	_	_
enhance	_	_
spatially	_	_
informative	_	_
features	_	_
of	_	_
the	_	_
CNN	_	_
.	_	_

#38
Such	_	_
a	_	_
design	_	_
helps	_	_
disentangle	_	_
underlying	_	_
spatial	_	_
and	_	_
temporal	_	_
factors	_	_
of	_	_
dynamic	_	_
attention	_	_
and	_	_
allows	_	_
convLSTM	_	_
to	_	_
learn	_	_
temporal	_	_
saliency	_	_
representations	_	_
efficiently	_	_
.	_	_

#39
Our	_	_
contributions	_	_
are	_	_
three-fold	_	_
.	_	_

#40
First	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
standard	_	_
benchmark	_	_
of	_	_
1K	_	_
videos	_	_
covering	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
scenes	_	_
,	_	_
motions	_	_
,	_	_
activities	_	_
,	_	_
etc	_	_
.	_	_

#41
To	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
the	_	_
proposed	_	_
dataset	_	_
is	_	_
the	_	_
largest	_	_
eye-tracking	_	_
dataset	_	_
for	_	_
dynamic	_	_
,	_	_
free-viewing	_	_
fixation	_	_
prediction	_	_
.	_	_

#42
Second	_	_
,	_	_
we	_	_
present	_	_
a	_	_
novel	_	_
attentive	_	_
CNN-LSTM	_	_
architecture	_	_
for	_	_
predicting	_	_
human	_	_
gaze	_	_
in	_	_
dynamic	_	_
scenes	_	_
,	_	_
which	_	_
explicitly	_	_
encodes	_	_
static	_	_
attention	_	_
into	_	_
dynamic	_	_
saliency	_	_
representation	_	_
learning	_	_
by	_	_
leveraging	_	_
both	_	_
static	_	_
and	_	_
dynamic	_	_
fixation	_	_
data	_	_
.	_	_

#43
Third	_	_
,	_	_
we	_	_
present	_	_
a	_	_
comprehensive	_	_
analysis	_	_
of	_	_
video	_	_
saliency	_	_
models	_	_
(	_	_
the	_	_
first	_	_
one	_	_
,	_	_
to	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
)	_	_
on	_	_
existing	_	_
datasets	_	_
(	_	_
Hollywood-2	_	_
,	_	_
UCF	_	_
sports	_	_
)	_	_
,	_	_
and	_	_
our	_	_
new	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#44
Results	_	_
show	_	_
that	_	_
our	_	_
model	_	_
significantly	_	_
outperforms	_	_
previous	_	_
methods	_	_
.	_	_

#45
2	_	_
.	_	_

#46
Related	_	_
Work	_	_
2.1	_	_
.	_	_

#47
Video	_	_
Eye-Tracking	_	_
Datasets	_	_
There	_	_
exist	_	_
several	_	_
datasets	_	_
[	_	_
43	_	_
,	_	_
44	_	_
,	_	_
25	_	_
,	_	_
17	_	_
]	_	_
for	_	_
dynamic	_	_
visual	_	_
saliency	_	_
prediction	_	_
,	_	_
but	_	_
they	_	_
are	_	_
limited	_	_
and	_	_
often	_	_
lack	_	_
variety	_	_
,	_	_
generality	_	_
and	_	_
scalability	_	_
of	_	_
instances	_	_
.	_	_

#48
Some	_	_
statistics	_	_
of	_	_
these	_	_
datasets	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#49
The	_	_
Hollywood-2	_	_
dataset	_	_
[	_	_
43	_	_
]	_	_
comprises	_	_
all	_	_
the	_	_
1	_	_
,	_	_
707	_	_
videos	_	_
from	_	_
Hollywood-2	_	_
action	_	_
recognition	_	_
dataset	_	_
[	_	_
42	_	_
]	_	_
.	_	_

#50
The	_	_
videos	_	_
are	_	_
collected	_	_
from	_	_
69	_	_
Hollywood	_	_
movies	_	_
with	_	_
12	_	_
action	_	_
categories	_	_
,	_	_
such	_	_
as	_	_
eating	_	_
,	_	_
kissing	_	_
and	_	_
running	_	_
.	_	_

#51
The	_	_
human	_	_
fixation	_	_
data	_	_
were	_	_
tracked	_	_
from	_	_
19	_	_
observers	_	_
belonging	_	_
to	_	_
3	_	_
groups	_	_
for	_	_
free	_	_
viewing	_	_
(	_	_
3	_	_
observers	_	_
)	_	_
,	_	_
action	_	_
recognition	_	_
(	_	_
12	_	_
observers	_	_
)	_	_
,	_	_
and	_	_
context	_	_
recognition	_	_
(	_	_
4	_	_
obDataset	_	_
Year	_	_
Videos	_	_
Resolution	_	_
Duration	_	_
(	_	_
s	_	_
)	_	_
Viewers	_	_
Task	_	_
CRCNS	_	_
[	_	_
25	_	_
]	_	_
2004	_	_
50	_	_
640×	_	_
480	_	_
6-94	_	_
15	_	_
task-goal	_	_
Hollywood-2	_	_
[	_	_
43	_	_
]	_	_
2012	_	_
1,707	_	_
720×	_	_
480	_	_
2-120	_	_
19	_	_
task-goal	_	_
UCF	_	_
sports	_	_
[	_	_
43	_	_
]	_	_
2012	_	_
150	_	_
720×	_	_
480	_	_
2-14	_	_
19	_	_
task-goal	_	_
DIEM	_	_
[	_	_
44	_	_
]	_	_
2011	_	_
84	_	_
1280×	_	_
720	_	_
27-217	_	_
∼50	_	_
free-view	_	_
SFU	_	_
[	_	_
17	_	_
]	_	_
2012	_	_
12	_	_
352×	_	_
288	_	_
3-10	_	_
15	_	_
free-view	_	_
DHF1K	_	_
(	_	_
Ours	_	_
)	_	_
2017	_	_
1,000	_	_
640×	_	_
360	_	_
17-42	_	_
17	_	_
free-view	_	_
Table	_	_
1	_	_
.	_	_

#52
Statistics	_	_
of	_	_
typical	_	_
dynamic	_	_
eye-tracking	_	_
datasets	_	_
.	_	_

#53
servers	_	_
)	_	_
.	_	_

#54
Although	_	_
this	_	_
dataset	_	_
is	_	_
large	_	_
,	_	_
its	_	_
content	_	_
is	_	_
limited	_	_
to	_	_
human	_	_
actions	_	_
and	_	_
movie	_	_
scenes	_	_
.	_	_

#55
It	_	_
mainly	_	_
focuses	_	_
on	_	_
task-driven	_	_
viewing	_	_
mode	_	_
,	_	_
rather	_	_
than	_	_
free	_	_
viewing	_	_
.	_	_

#56
With	_	_
1	_	_
,	_	_
000	_	_
frames	_	_
randomly	_	_
sampled	_	_
from	_	_
Hollywood-2	_	_
,	_	_
we	_	_
found	_	_
84.5	_	_
%	_	_
fixations	_	_
are	_	_
located	_	_
around	_	_
the	_	_
faces	_	_
.	_	_

#57
The	_	_
UCF	_	_
sports	_	_
fixation	_	_
dataset	_	_
[	_	_
43	_	_
]	_	_
contains	_	_
150	_	_
videos	_	_
taken	_	_
from	_	_
the	_	_
UCF	_	_
sports	_	_
action	_	_
dataset	_	_
[	_	_
49	_	_
]	_	_
.	_	_

#58
The	_	_
videos	_	_
cover	_	_
9	_	_
common	_	_
sports	_	_
action	_	_
classes	_	_
,	_	_
such	_	_
as	_	_
diving	_	_
,	_	_
swinging	_	_
and	_	_
walking	_	_
.	_	_

#59
Similar	_	_
to	_	_
Hollywood-2	_	_
,	_	_
the	_	_
viewers	_	_
have	_	_
been	_	_
biased	_	_
towards	_	_
task-aware	_	_
observation	_	_
by	_	_
being	_	_
instructed	_	_
to	_	_
“identify	_	_
the	_	_
actions	_	_
occurring	_	_
in	_	_
the	_	_
video	_	_
sequence”	_	_
.	_	_

#60
From	_	_
the	_	_
statistics	_	_
of	_	_
1	_	_
,	_	_
000	_	_
frames	_	_
randomly	_	_
selected	_	_
from	_	_
UCF	_	_
sports	_	_
,	_	_
we	_	_
found	_	_
82.3	_	_
%	_	_
fixations	_	_
fall	_	_
inside	_	_
the	_	_
human	_	_
body	_	_
area	_	_
.	_	_

#61
The	_	_
DIEM	_	_
dataset	_	_
[	_	_
44	_	_
]	_	_
is	_	_
a	_	_
public	_	_
video	_	_
eye-tracking	_	_
dataset	_	_
that	_	_
has	_	_
84	_	_
videos	_	_
collected	_	_
from	_	_
publicly	_	_
accessible	_	_
video	_	_
resources	_	_
(	_	_
e.g.	_	_
,	_	_
advertisements	_	_
,	_	_
documentaries	_	_
,	_	_
sport	_	_
events	_	_
,	_	_
and	_	_
movie	_	_
trailers	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#62
For	_	_
each	_	_
video	_	_
,	_	_
free-viewing	_	_
fixations	_	_
of	_	_
around	_	_
50	_	_
observers	_	_
were	_	_
collected	_	_
.	_	_

#63
This	_	_
dataset	_	_
is	_	_
mainly	_	_
limited	_	_
in	_	_
its	_	_
coverage	_	_
and	_	_
scale	_	_
.	_	_

#64
Other	_	_
datasets	_	_
are	_	_
either	_	_
limited	_	_
in	_	_
terms	_	_
of	_	_
variety	_	_
and	_	_
scale	_	_
of	_	_
video	_	_
stimuli	_	_
[	_	_
25	_	_
,	_	_
17	_	_
]	_	_
,	_	_
or	_	_
collected	_	_
for	_	_
a	_	_
special	_	_
purpose	_	_
(	_	_
e.g.	_	_
,	_	_
salient	_	_
objects	_	_
in	_	_
videos	_	_
[	_	_
59	_	_
]	_	_
)	_	_
.	_	_

#65
More	_	_
importantly	_	_
,	_	_
none	_	_
of	_	_
the	_	_
aforementioned	_	_
datasets	_	_
includes	_	_
a	_	_
preserved	_	_
test	_	_
set	_	_
for	_	_
avoiding	_	_
potential	_	_
data	_	_
overfitting	_	_
,	_	_
which	_	_
has	_	_
seriously	_	_
hampered	_	_
the	_	_
research	_	_
process	_	_
.	_	_

#66
2.2	_	_
.	_	_

#67
Computational	_	_
Models	_	_
for	_	_
Fixation	_	_
Prediction	_	_
The	_	_
study	_	_
of	_	_
human	_	_
gaze	_	_
patterns	_	_
in	_	_
static	_	_
scenes	_	_
has	_	_
received	_	_
significant	_	_
interests	_	_
,	_	_
which	_	_
can	_	_
be	_	_
dated	_	_
back	_	_
to	_	_
[	_	_
28	_	_
,	_	_
27	_	_
]	_	_
.	_	_

#68
Early	_	_
static	_	_
saliency	_	_
models	_	_
[	_	_
36	_	_
,	_	_
69	_	_
,	_	_
15	_	_
,	_	_
7	_	_
,	_	_
18	_	_
,	_	_
22	_	_
,	_	_
33	_	_
,	_	_
63	_	_
]	_	_
are	_	_
mostly	_	_
based	_	_
on	_	_
the	_	_
contrast	_	_
assumption	_	_
that	_	_
conspicuous	_	_
visual	_	_
features	_	_
“pop-out”	_	_
and	_	_
involuntarily	_	_
capture	_	_
attention	_	_
(	_	_
see	_	_
[	_	_
5	_	_
,	_	_
6	_	_
]	_	_
for	_	_
review	_	_
)	_	_
.	_	_

#69
Computational	_	_
models	_	_
compute	_	_
multiple	_	_
visual	_	_
features	_	_
such	_	_
as	_	_
color	_	_
,	_	_
edge	_	_
,	_	_
and	_	_
orientation	_	_
at	_	_
multiple	_	_
spatial	_	_
scales	_	_
to	_	_
produce	_	_
a	_	_
“saliency	_	_
map”	_	_
:	_	_
an	_	_
image	_	_
distribution	_	_
predicting	_	_
the	_	_
conspicuity	_	_
of	_	_
specific	_	_
locations	_	_
and	_	_
their	_	_
likelihood	_	_
in	_	_
attracting	_	_
attention	_	_
[	_	_
27	_	_
,	_	_
44	_	_
]	_	_
.	_	_

#70
The	_	_
locations	_	_
with	_	_
more	_	_
distinct	_	_
feature	_	_
responses	_	_
over	_	_
surroundings	_	_
usually	_	_
gain	_	_
higher	_	_
saliency	_	_
values	_	_
.	_	_

#71
Deep	_	_
learning	_	_
based	_	_
static	_	_
saliency	_	_
models	_	_
[	_	_
54	_	_
,	_	_
35	_	_
,	_	_
24	_	_
,	_	_
39	_	_
,	_	_
47	_	_
,	_	_
29	_	_
,	_	_
56	_	_
,	_	_
57	_	_
]	_	_
have	_	_
achieved	_	_
astonishing	_	_
improvements	_	_
,	_	_
relying	_	_
on	_	_
the	_	_
powerful	_	_
end-to-end	_	_
learning	_	_
ability	_	_
of	_	_
neural	_	_
network	_	_
and	_	_
the	_	_
availability	_	_
of	_	_
large-scale	_	_
static	_	_
saliency	_	_
datasets	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#72
Previous	_	_
investigations	_	_
of	_	_
dynamic	_	_
human	_	_
fixation	_	_
(	_	_
a	_	_
)	_	_
(	_	_
b	_	_
)	_	_
(	_	_
c	_	_
)	_	_
Figure	_	_
1	_	_
.	_	_

#73
Average	_	_
annotation	_	_
maps	_	_
of	_	_
three	_	_
datasets	_	_
used	_	_
in	_	_
benchmarking	_	_
:	_	_
(	_	_
a	_	_
)	_	_
Hollywood-2	_	_
,	_	_
(	_	_
b	_	_
)	_	_
UCF	_	_
sports	_	_
,	_	_
(	_	_
c	_	_
)	_	_
DHF1K	_	_
.	_	_

#74
[	_	_
14	_	_
,	_	_
16	_	_
,	_	_
41	_	_
,	_	_
50	_	_
,	_	_
52	_	_
,	_	_
23	_	_
,	_	_
13	_	_
,	_	_
20	_	_
,	_	_
37	_	_
]	_	_
leveraged	_	_
both	_	_
static	_	_
stimulus	_	_
features	_	_
and	_	_
temporal	_	_
information	_	_
(	_	_
e.g.	_	_
,	_	_
optical	_	_
flow	_	_
,	_	_
difference-over-time	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#75
Some	_	_
of	_	_
those	_	_
studies	_	_
[	_	_
14	_	_
,	_	_
41	_	_
,	_	_
52	_	_
]	_	_
can	_	_
be	_	_
viewed	_	_
as	_	_
extensions	_	_
of	_	_
exiting	_	_
static	_	_
saliency	_	_
models	_	_
with	_	_
additional	_	_
motion	_	_
features	_	_
.	_	_

#76
Those	_	_
models	_	_
are	_	_
mainly	_	_
bound	_	_
to	_	_
significant	_	_
feature	_	_
engineering	_	_
and	_	_
limited	_	_
representation	_	_
ability	_	_
of	_	_
hand-crafted	_	_
features	_	_
.	_	_

#77
To	_	_
date	_	_
,	_	_
only	_	_
a	_	_
few	_	_
deep	_	_
learning	_	_
based	_	_
video	_	_
saliency	_	_
models	_	_
[	_	_
2	_	_
,	_	_
30	_	_
]	_	_
exist	_	_
in	_	_
this	_	_
field	_	_
.	_	_

#78
They	_	_
are	_	_
mainly	_	_
based	_	_
on	_	_
two-stream	_	_
network	_	_
architecture	_	_
[	_	_
2	_	_
]	_	_
that	_	_
accounts	_	_
for	_	_
color	_	_
images	_	_
and	_	_
motion	_	_
fields	_	_
separately	_	_
,	_	_
or	_	_
two-layer	_	_
LSTM	_	_
with	_	_
object	_	_
information	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#79
These	_	_
works	_	_
show	_	_
a	_	_
better	_	_
performance	_	_
and	_	_
demonstrate	_	_
the	_	_
potential	_	_
advantages	_	_
in	_	_
applying	_	_
neural	_	_
networks	_	_
to	_	_
this	_	_
problem	_	_
.	_	_

#80
However	_	_
,	_	_
they	_	_
do	_	_
not	_	_
1	_	_
)	_	_
consider	_	_
attentive	_	_
mechanisms	_	_
;	_	_
2	_	_
)	_	_
utilize	_	_
existing	_	_
large-scale	_	_
static	_	_
fixation	_	_
datasets	_	_
;	_	_
and	_	_
3	_	_
)	_	_
exhaustively	_	_
assess	_	_
their	_	_
performance	_	_
over	_	_
large	_	_
amount	_	_
of	_	_
data	_	_
.	_	_

#81
There	_	_
are	_	_
some	_	_
salient	_	_
object	_	_
detection	_	_
models	_	_
[	_	_
40	_	_
,	_	_
1	_	_
,	_	_
11	_	_
,	_	_
61	_	_
,	_	_
58	_	_
,	_	_
60	_	_
,	_	_
4	_	_
,	_	_
62	_	_
,	_	_
21	_	_
]	_	_
that	_	_
attempt	_	_
to	_	_
uniformly	_	_
highlight	_	_
salient	_	_
object	_	_
regions	_	_
in	_	_
images	_	_
or	_	_
videos	_	_
.	_	_

#82
Those	_	_
models	_	_
are	_	_
often	_	_
task-driven	_	_
and	_	_
focus	_	_
on	_	_
inferring	_	_
the	_	_
main	_	_
object	_	_
,	_	_
instead	_	_
of	_	_
investigating	_	_
the	_	_
behavior	_	_
of	_	_
the	_	_
HVS	_	_
during	_	_
scene	_	_
free	_	_
viewing	_	_
.	_	_

#83
2.3	_	_
.	_	_

#84
Attention	_	_
Mechanisms	_	_
in	_	_
Neural	_	_
Networks	_	_
Recently	_	_
,	_	_
incorporating	_	_
attention	_	_
mechanisms	_	_
into	_	_
network	_	_
architectures	_	_
has	_	_
shown	_	_
great	_	_
success	_	_
in	_	_
several	_	_
computer	_	_
vision	_	_
[	_	_
67	_	_
,	_	_
9	_	_
,	_	_
55	_	_
]	_	_
and	_	_
natural	_	_
language	_	_
processing	_	_
tasks	_	_
[	_	_
51	_	_
,	_	_
48	_	_
]	_	_
.	_	_

#85
In	_	_
such	_	_
studies	_	_
,	_	_
attention	_	_
is	_	_
learned	_	_
in	_	_
an	_	_
automatic	_	_
,	_	_
top-down	_	_
,	_	_
and	_	_
task-specific	_	_
manner	_	_
,	_	_
allowing	_	_
the	_	_
network	_	_
to	_	_
focus	_	_
on	_	_
the	_	_
most	_	_
relevant	_	_
parts	_	_
in	_	_
images	_	_
or	_	_
sentences	_	_
.	_	_

#86
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
use	_	_
attention	_	_
for	_	_
enhancing	_	_
intra-frame	_	_
salient	_	_
features	_	_
,	_	_
thus	_	_
allowing	_	_
the	_	_
LSTM	_	_
to	_	_
model	_	_
dynamic	_	_
representations	_	_
more	_	_
easily	_	_
.	_	_

#87
In	_	_
contrast	_	_
to	_	_
previous	_	_
models	_	_
learning	_	_
attentions	_	_
implicitly	_	_
,	_	_
our	_	_
attention	_	_
module	_	_
encodes	_	_
strong	_	_
static	_	_
saliency	_	_
information	_	_
and	_	_
can	_	_
be	_	_
learned	_	_
from	_	_
existing	_	_
static	_	_
saliency	_	_
dataset	_	_
in	_	_
a	_	_
supervised	_	_
manner	_	_
.	_	_

#88
This	_	_
design	_	_
leads	_	_
to	_	_
improved	_	_
generality	_	_
and	_	_
prediction	_	_
performance	_	_
.	_	_

#89
It	_	_
is	_	_
the	_	_
first	_	_
attempt	_	_
to	_	_
incorporate	_	_
a	_	_
supervised	_	_
attention	_	_
mechanism	_	_
into	_	_
the	_	_
network	_	_
structure	_	_
to	_	_
achieve	_	_
state-of-art	_	_
results	_	_
in	_	_
dynamic	_	_
fixation	_	_
prediction	_	_
.	_	_

#90
3	_	_
.	_	_

#91
DHF1K	_	_
Dataset	_	_
We	_	_
introduce	_	_
DHF1K	_	_
,	_	_
a	_	_
large-scale	_	_
dataset	_	_
of	_	_
gaze	_	_
in	_	_
free-viewing	_	_
of	_	_
videos	_	_
.	_	_

#92
Our	_	_
dataset	_	_
includes	_	_
1K	_	_
videos	_	_
with	_	_
DHF1K	_	_
Human	_	_
Animal	_	_
Artifact	_	_
SceneryDaily	_	_
ac	_	_
.	_	_

#93
Sports	_	_
Social	_	_
ac	_	_
.	_	_

#94
Art	_	_
#	_	_
sub-classes*	_	_
20	_	_
29	_	_
13	_	_
10	_	_
36	_	_
21	_	_
21	_	_
#	_	_
videos	_	_
134	_	_
185	_	_
116	_	_
101	_	_
192	_	_
162	_	_
110	_	_
∗Number	_	_
of	_	_
sub-classes	_	_
in	_	_
each	_	_
category	_	_
is	_	_
reported	_	_
.	_	_

#95
For	_	_
example	_	_
,	_	_
Sports	_	_
has	_	_
sub-classes	_	_
like	_	_
swimming	_	_
,	_	_
jumping	_	_
,	_	_
etc	_	_
.	_	_

#96
Table	_	_
2	_	_
.	_	_

#97
Statistics	_	_
for	_	_
video	_	_
categories	_	_
in	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#98
DHF1K	_	_
Content	_	_
motion	_	_
Camera	_	_
motion	_	_
#	_	_
Objects	_	_
stable	_	_
slow	_	_
fast	_	_
stable	_	_
slow	_	_
fast	_	_
0	_	_
1	_	_
2	_	_
≥3	_	_
#	_	_
videos	_	_
126	_	_
505	_	_
369	_	_
343	_	_
386	_	_
271	_	_
56	_	_
335	_	_
254	_	_
355	_	_
Table	_	_
3	_	_
.	_	_

#99
Statistics	_	_
regarding	_	_
motion	_	_
patterns	_	_
and	_	_
number	_	_
of	_	_
main	_	_
objects	_	_
in	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#100
DHF1K	_	_
Scene	_	_
illumination	_	_
#	_	_
People	_	_
day	_	_
night	_	_
indoor	_	_
0	_	_
1	_	_
2	_	_
≥3	_	_
#	_	_
videos	_	_
577	_	_
37	_	_
386	_	_
345	_	_
307	_	_
236	_	_
112	_	_
Table	_	_
4	_	_
.	_	_

#101
Statistics	_	_
regarding	_	_
scene	_	_
illumination	_	_
and	_	_
number	_	_
of	_	_
people	_	_
in	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#102
diverse	_	_
content	_	_
and	_	_
length	_	_
,	_	_
with	_	_
eye-tracking	_	_
annotations	_	_
from	_	_
17	_	_
observers	_	_
.	_	_

#103
Fig.	_	_
1	_	_
shows	_	_
the	_	_
center	_	_
bias	_	_
of	_	_
DHF1K	_	_
,	_	_
compared	_	_
to	_	_
Hollywood-2	_	_
,	_	_
and	_	_
UCF	_	_
sports	_	_
datasets	_	_
.	_	_

#104
Stimuli	_	_
.	_	_

#105
The	_	_
collection	_	_
of	_	_
dynamic	_	_
stimuli	_	_
mainly	_	_
follows	_	_
the	_	_
following	_	_
4	_	_
principles	_	_
.	_	_

#106
•	_	_
Large	_	_
scale	_	_
and	_	_
high	_	_
quality	_	_
.	_	_

#107
Both	_	_
scale	_	_
and	_	_
quality	_	_
are	_	_
necessary	_	_
to	_	_
ensure	_	_
the	_	_
content	_	_
diversity	_	_
of	_	_
a	_	_
dataset	_	_
and	_	_
is	_	_
crucial	_	_
to	_	_
guarantee	_	_
a	_	_
longer	_	_
lifespan	_	_
for	_	_
a	_	_
benchmark	_	_
.	_	_

#108
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
searched	_	_
the	_	_
Youtube	_	_
engine	_	_
with	_	_
about	_	_
200	_	_
key	_	_
terms	_	_
(	_	_
e.g.	_	_
,	_	_
dog	_	_
,	_	_
walking	_	_
,	_	_
car	_	_
,	_	_
etc	_	_
)	_	_
and	_	_
carefully	_	_
selected	_	_
1	_	_
,	_	_
000	_	_
video	_	_
sequences	_	_
from	_	_
the	_	_
retrieval	_	_
results	_	_
.	_	_

#109
All	_	_
videos	_	_
were	_	_
converted	_	_
from	_	_
their	_	_
original	_	_
sources	_	_
to	_	_
a	_	_
30	_	_
fps	_	_
Xvid	_	_
MPEG-4	_	_
video	_	_
file	_	_
in	_	_
an	_	_
AVI	_	_
container	_	_
and	_	_
were	_	_
resized	_	_
uniformly	_	_
into	_	_
640	_	_
×	_	_
360	_	_
spatial	_	_
resolution	_	_
.	_	_

#110
Thus	_	_
,	_	_
DHF1K	_	_
comprises	_	_
a	_	_
total	_	_
1	_	_
,	_	_
000	_	_
video	_	_
sequences	_	_
with	_	_
582	_	_
,	_	_
605	_	_
frames	_	_
with	_	_
total	_	_
duration	_	_
of	_	_
19	_	_
,	_	_
420	_	_
seconds	_	_
.	_	_

#111
•	_	_
Diverse	_	_
content	_	_
.	_	_

#112
Stimulus	_	_
diversity	_	_
is	_	_
essential	_	_
for	_	_
avoiding	_	_
overfitting	_	_
and	_	_
to	_	_
delay	_	_
performance	_	_
saturation	_	_
.	_	_

#113
It	_	_
offers	_	_
evenly	_	_
distributed	_	_
exogenous	_	_
control	_	_
for	_	_
studying	_	_
personexternal	_	_
stimulus	_	_
factors	_	_
during	_	_
scene	_	_
free-viewing	_	_
.	_	_

#114
In	_	_
DHF1K	_	_
,	_	_
each	_	_
video	_	_
is	_	_
manually	_	_
annotated	_	_
with	_	_
a	_	_
category	_	_
label	_	_
(	_	_
totally	_	_
150	_	_
classes	_	_
)	_	_
.	_	_

#115
Those	_	_
labels	_	_
are	_	_
further	_	_
classified	_	_
into	_	_
7	_	_
main	_	_
categories	_	_
(	_	_
see	_	_
Table	_	_
2	_	_
)	_	_
.	_	_

#116
Those	_	_
semantic	_	_
annotations	_	_
would	_	_
enable	_	_
a	_	_
deeper	_	_
understanding	_	_
of	_	_
the	_	_
high-level	_	_
stimuli	_	_
factors	_	_
guiding	_	_
human	_	_
gaze	_	_
in	_	_
dynamic	_	_
scenes	_	_
and	_	_
be	_	_
indicative	_	_
for	_	_
potential	_	_
research	_	_
.	_	_

#117
In	_	_
Fig.	_	_
2	_	_
,	_	_
we	_	_
show	_	_
example	_	_
frames	_	_
from	_	_
each	_	_
category	_	_
.	_	_

#118
•	_	_
Varied	_	_
motion	_	_
patterns	_	_
.	_	_

#119
Previous	_	_
investigations	_	_
[	_	_
26	_	_
,	_	_
14	_	_
,	_	_
44	_	_
]	_	_
suggested	_	_
that	_	_
motion	_	_
is	_	_
one	_	_
of	_	_
the	_	_
key	_	_
factors	_	_
that	_	_
directs	_	_
attention	_	_
allocation	_	_
in	_	_
dynamic	_	_
viewing	_	_
.	_	_

#120
For	_	_
this	_	_
,	_	_
DHF1K	_	_
is	_	_
designed	_	_
to	_	_
span	_	_
varied	_	_
motion	_	_
patterns	_	_
(	_	_
stable/slow/fastmotion	_	_
of	_	_
content	_	_
and	_	_
camera	_	_
)	_	_
.	_	_

#121
Please	_	_
see	_	_
Table	_	_
3	_	_
for	_	_
the	_	_
information	_	_
regarding	_	_
motion	_	_
patterns	_	_
.	_	_

#122
•	_	_
Various	_	_
objects	_	_
.	_	_

#123
Previous	_	_
studies	_	_
[	_	_
65	_	_
,	_	_
38	_	_
,	_	_
3	_	_
]	_	_
in	_	_
cognitive	_	_
and	_	_
computer	_	_
vision	_	_
confirmed	_	_
that	_	_
object	_	_
information	_	_
is	_	_
indicative	_	_
to	_	_
human	_	_
fixations	_	_
.	_	_

#124
The	_	_
objects	_	_
in	_	_
the	_	_
dataset	_	_
vary	_	_
Daily	_	_
activity	_	_
Sport	_	_
Artistic	_	_
performanceSocial	_	_
activity	_	_
Animal	_	_
Artifact	_	_
Landscape	_	_
Figure	_	_
2	_	_
.	_	_

#125
Example	_	_
frames	_	_
from	_	_
DHF1K	_	_
with	_	_
fixations	_	_
(	_	_
red	_	_
dots	_	_
)	_	_
and	_	_
corresponding	_	_
categories	_	_
.	_	_

#126
in	_	_
their	_	_
type	_	_
(	_	_
e.g.	_	_
,	_	_
human	_	_
,	_	_
animal	_	_
,	_	_
in	_	_
Table	_	_
2	_	_
)	_	_
and	_	_
frequency	_	_
(	_	_
see	_	_
Table	_	_
3	_	_
)	_	_
.	_	_

#127
For	_	_
each	_	_
video	_	_
,	_	_
five	_	_
subjects	_	_
were	_	_
instructed	_	_
to	_	_
count	_	_
the	_	_
number	_	_
of	_	_
the	_	_
main	_	_
objects	_	_
.	_	_

#128
The	_	_
majority	_	_
vote	_	_
of	_	_
their	_	_
counts	_	_
was	_	_
considered	_	_
as	_	_
the	_	_
final	_	_
count	_	_
.	_	_

#129
For	_	_
completeness	_	_
,	_	_
in	_	_
Table	_	_
4	_	_
,	_	_
we	_	_
offer	_	_
the	_	_
information	_	_
of	_	_
the	_	_
scene	_	_
illumination	_	_
and	_	_
the	_	_
amount	_	_
of	_	_
humans	_	_
in	_	_
the	_	_
dataset	_	_
.	_	_

#130
As	_	_
demonstrated	_	_
in	_	_
[	_	_
45	_	_
]	_	_
,	_	_
luminance	_	_
is	_	_
an	_	_
important	_	_
exogenous	_	_
factor	_	_
for	_	_
attentive	_	_
selection	_	_
.	_	_

#131
Further	_	_
,	_	_
human	_	_
beings	_	_
are	_	_
important	_	_
high-level	_	_
stimuli	_	_
[	_	_
10	_	_
,	_	_
8	_	_
]	_	_
in	_	_
free-viewing	_	_
.	_	_

#132
Apparatus	_	_
and	_	_
technical	_	_
specifications	_	_
.	_	_

#133
Participants’	_	_
eye	_	_
movements	_	_
were	_	_
monitored	_	_
binocularly	_	_
using	_	_
a	_	_
Senso	_	_
Motoric	_	_
Instruments	_	_
(	_	_
SMI	_	_
)	_	_
RED	_	_
250	_	_
system	_	_
at	_	_
a	_	_
sampling	_	_
rate	_	_
of	_	_
250	_	_
Hz	_	_
.	_	_

#134
The	_	_
dynamic	_	_
stimuli	_	_
were	_	_
displayed	_	_
on	_	_
a	_	_
19”	_	_
display	_	_
(	_	_
resolution	_	_
1440	_	_
×	_	_
900	_	_
)	_	_
.	_	_

#135
A	_	_
headrest	_	_
was	_	_
used	_	_
to	_	_
help	_	_
participants’	_	_
heads	_	_
still	_	_
at	_	_
a	_	_
distance	_	_
of	_	_
around	_	_
68	_	_
cm	_	_
,	_	_
as	_	_
advised	_	_
by	_	_
the	_	_
product	_	_
manual	_	_
.	_	_

#136
Participants	_	_
.	_	_

#137
17	_	_
participants	_	_
(	_	_
10	_	_
males	_	_
and	_	_
7	_	_
females	_	_
,	_	_
aging	_	_
between	_	_
20	_	_
and	_	_
28	_	_
)	_	_
who	_	_
passed	_	_
the	_	_
calibration	_	_
of	_	_
the	_	_
eye	_	_
tracker	_	_
and	_	_
had	_	_
less	_	_
than	_	_
10	_	_
%	_	_
fixation	_	_
dropping	_	_
rate	_	_
,	_	_
were	_	_
quantified	_	_
for	_	_
our	_	_
eye	_	_
tracking	_	_
experiment	_	_
.	_	_

#138
All	_	_
participants	_	_
had	_	_
normal	_	_
or	_	_
corrected-to-normal	_	_
vision	_	_
.	_	_

#139
All	_	_
subjects	_	_
had	_	_
not	_	_
seen	_	_
the	_	_
stimuli	_	_
in	_	_
DHF1K	_	_
before	_	_
.	_	_

#140
All	_	_
provided	_	_
informed	_	_
consent	_	_
and	_	_
were	_	_
naı̈ve	_	_
to	_	_
the	_	_
underlying	_	_
purposes	_	_
of	_	_
the	_	_
experiment	_	_
.	_	_

#141
Data	_	_
capturing	_	_
.	_	_

#142
The	_	_
stimuli	_	_
were	_	_
equally	_	_
partitioned	_	_
into	_	_
10	_	_
non-overlapping	_	_
sessions	_	_
.	_	_

#143
Participants	_	_
were	_	_
required	_	_
to	_	_
freeview	_	_
10	_	_
sessions	_	_
of	_	_
videos	_	_
in	_	_
random	_	_
order	_	_
.	_	_

#144
In	_	_
each	_	_
session	_	_
,	_	_
the	_	_
videos	_	_
were	_	_
also	_	_
displayed	_	_
at	_	_
random	_	_
.	_	_

#145
Before	_	_
the	_	_
experiments	_	_
,	_	_
every	_	_
participant	_	_
was	_	_
calibrated	_	_
using	_	_
the	_	_
standard	_	_
routine	_	_
in	_	_
product	_	_
manual	_	_
with	_	_
recommended	_	_
settings	_	_
for	_	_
the	_	_
best	_	_
results	_	_
.	_	_

#146
To	_	_
avoid	_	_
eye	_	_
fatigue	_	_
,	_	_
each	_	_
video	_	_
presentation	_	_
was	_	_
followed	_	_
by	_	_
a	_	_
5-second	_	_
waiting	_	_
interval	_	_
with	_	_
black	_	_
screen	_	_
.	_	_

#147
After	_	_
undergoing	_	_
a	_	_
session	_	_
of	_	_
videos	_	_
,	_	_
the	_	_
participant	_	_
can	_	_
take	_	_
a	_	_
rest	_	_
until	_	_
she	_	_
was	_	_
ready	_	_
for	_	_
viewing	_	_
the	_	_
next	_	_
session	_	_
.	_	_

#148
Finally	_	_
,	_	_
51	_	_
,	_	_
038	_	_
,	_	_
600	_	_
fixations	_	_
were	_	_
recorded	_	_
from	_	_
17	_	_
subjects	_	_
on	_	_
1	_	_
,	_	_
000	_	_
videos	_	_
.	_	_

#149
Training/testing	_	_
split	_	_
.	_	_

#150
We	_	_
split	_	_
1	_	_
,	_	_
000	_	_
dynamic	_	_
stimuli	_	_
into	_	_
separate	_	_
training	_	_
,	_	_
validation	_	_
and	_	_
test	_	_
sets	_	_
.	_	_

#151
Following	_	_
random	_	_
selection	_	_
,	_	_
we	_	_
arrive	_	_
at	_	_
a	_	_
unique	_	_
split	_	_
consisting	_	_
of	_	_
600	_	_
training	_	_
and	_	_
100	_	_
validation	_	_
videos	_	_
with	_	_
publicly	_	_
available	_	_
fixation	_	_
records	_	_
,	_	_
as	_	_
well	_	_
as	_	_
300	_	_
test	_	_
videos	_	_
with	_	_
annotations	_	_
held-out	_	_
for	_	_
benchmarking	_	_
purpose	_	_
.	_	_

#152
4	_	_
.	_	_

#153
Our	_	_
Approach	_	_
Overview	_	_
.	_	_

#154
Fig.	_	_
3	_	_
presents	_	_
the	_	_
overall	_	_
architecture	_	_
of	_	_
our	_	_
video	_	_
saliency	_	_
model	_	_
.	_	_

#155
It	_	_
is	_	_
based	_	_
on	_	_
a	_	_
CNN-LSTM	_	_
architecture	_	_
that	_	_
combines	_	_
convolutional	_	_
network	_	_
and	_	_
recurrent	_	_
model	_	_
to	_	_
exploit	_	_
both	_	_
spatial	_	_
and	_	_
temporal	_	_
information	_	_
for	_	_
predicting	_	_
video	_	_
saliency	_	_
.	_	_

#156
The	_	_
CNN-LSTM	_	_
network	_	_
is	_	_
extended	_	_
with	_	_
a	_	_
supervised	_	_
attention	_	_
mechanism	_	_
,	_	_
which	_	_
explicitly	_	_
captures	_	_
static	_	_
saliency	_	_
information	_	_
and	_	_
allows	_	_
the	_	_
LSTM	_	_
to	_	_
focus	_	_
on	_	_
learning	_	_
dynamic	_	_
information	_	_
.	_	_

#157
The	_	_
attention	_	_
module	_	_
is	_	_
trained	_	_
from	_	_
rich	_	_
static	_	_
eye-tracking	_	_
data	_	_
.	_	_

#158
Thus	_	_
our	_	_
model	_	_
is	_	_
able	_	_
to	_	_
produce	_	_
accurate	_	_
,	_	_
spatiotemporal	_	_
saliency	_	_
with	_	_
improved	_	_
generalization	_	_
ability	_	_
.	_	_

#159
Next	_	_
,	_	_
we	_	_
explain	_	_
each	_	_
component	_	_
of	_	_
our	_	_
model	_	_
in	_	_
detail	_	_
.	_	_

#160
CNN-LSTM	_	_
architecture	_	_
.	_	_

#161
Formally	_	_
,	_	_
given	_	_
an	_	_
input	_	_
video	_	_
{	_	_
It	_	_
}	_	_
t	_	_
,	_	_
we	_	_
first	_	_
obtain	_	_
a	_	_
sequence	_	_
of	_	_
convolutional	_	_
features	_	_
{	_	_
Xt	_	_
}	_	_
t	_	_
from	_	_
CNN	_	_
.	_	_

#162
Then	_	_
the	_	_
features	_	_
{	_	_
Xt	_	_
}	_	_
t	_	_
are	_	_
fed	_	_
into	_	_
a	_	_
convLSTM	_	_
[	_	_
66	_	_
]	_	_
as	_	_
input	_	_
.	_	_

#163
Here	_	_
,	_	_
the	_	_
convLSTM	_	_
is	_	_
used	_	_
for	_	_
modeling	_	_
the	_	_
temporal	_	_
dynamic	_	_
nature	_	_
of	_	_
this	_	_
sequential	_	_
problem	_	_
,	_	_
which	_	_
is	_	_
achieved	_	_
by	_	_
incorporating	_	_
memory	_	_
units	_	_
with	_	_
gated	_	_
operations	_	_
.	_	_

#164
Additionally	_	_
,	_	_
through	_	_
replacing	_	_
dot	_	_
products	_	_
with	_	_
convolutional	_	_
operations	_	_
,	_	_
the	_	_
convLSTM	_	_
is	_	_
able	_	_
to	_	_
preserve	_	_
spatial	_	_
information	_	_
,	_	_
which	_	_
is	_	_
essential	_	_
for	_	_
making	_	_
spatially-variant	_	_
pixel-level	_	_
prediction	_	_
.	_	_

#165
More	_	_
specifically	_	_
,	_	_
the	_	_
convLSTM	_	_
utilizes	_	_
three	_	_
convolution	_	_
gates	_	_
(	_	_
input	_	_
,	_	_
output	_	_
and	_	_
forget	_	_
)	_	_
to	_	_
control	_	_
the	_	_
flow	_	_
of	_	_
signal	_	_
within	_	_
the	_	_
cell	_	_
.	_	_

#166
With	_	_
the	_	_
input	_	_
feature	_	_
Xt	_	_
at	_	_
time	_	_
step	_	_
t	_	_
,	_	_
the	_	_
convLSTM	_	_
outputs	_	_
a	_	_
hidden	_	_
stateHt	_	_
and	_	_
maintains	_	_
a	_	_
memory	_	_
cell	_	_
Ct	_	_
for	_	_
controlling	_	_
state	_	_
update	_	_
and	_	_
output	_	_
:	_	_
it=	_	_
σ	_	_
(	_	_
WXi	_	_
∗Xt+W	_	_
H	_	_
i	_	_
∗Ht−1+W	_	_
C	_	_
i	_	_
◦	_	_
Ct−1+bi	_	_
)	_	_
,	_	_
(	_	_
1	_	_
)	_	_
ft=	_	_
σ	_	_
(	_	_
WXf	_	_
∗Xt+W	_	_
H	_	_
f	_	_
∗Ht−1+W	_	_
C	_	_
f	_	_
◦	_	_
Ct−1+bf	_	_
)	_	_
,	_	_
(	_	_
2	_	_
)	_	_
ot=	_	_
σ	_	_
(	_	_
WXo	_	_
∗Xt+W	_	_
H	_	_
o	_	_
∗Ht−1+W	_	_
C	_	_
o	_	_
◦	_	_
Ct+bo	_	_
)	_	_
,	_	_
(	_	_
3	_	_
)	_	_
Ct=	_	_
ft	_	_
◦	_	_
Ct−1+it◦tanh	_	_
(	_	_
WXc	_	_
∗Xt+W	_	_
H	_	_
c	_	_
∗Ht−1+bc	_	_
)	_	_
,	_	_
(	_	_
4	_	_
)	_	_
Ht=	_	_
ot	_	_
◦	_	_
tanh	_	_
(	_	_
Ct	_	_
)	_	_
,	_	_
(	_	_
5	_	_
)	_	_
it	_	_
,	_	_
ft	_	_
,	_	_
ot	_	_
are	_	_
the	_	_
gates	_	_
.	_	_

#167
σ	_	_
and	_	_
tanh	_	_
are	_	_
the	_	_
activation	_	_
functions	_	_
of	_	_
logistic	_	_
sigmoid	_	_
and	_	_
hyperbolic	_	_
tangent	_	_
,	_	_
respectively	_	_
.	_	_

#168
‘∗’	_	_
denotes	_	_
the	_	_
convolution	_	_
operator	_	_
and	_	_
‘◦’	_	_
represents	_	_
Hadamard	_	_
product	_	_
.	_	_

#169
The	_	_
dynamic	_	_
fixation	_	_
map	_	_
can	_	_
be	_	_
obtained	_	_
via	_	_
convolving	_	_
the	_	_
hidden	_	_
states	_	_
H	_	_
with	_	_
a	_	_
1	_	_
×	_	_
1	_	_
kernel	_	_
(	_	_
see	_	_
Fig.	_	_
3	_	_
(	_	_
c	_	_
)	_	_
)	_	_
.	_	_

#170
In	_	_
our	_	_
implementation	_	_
,	_	_
the	_	_
first	_	_
five	_	_
conv	_	_
blocks	_	_
of	_	_
VGG-16	_	_
[	_	_
53	_	_
]	_	_
are	_	_
used	_	_
.	_	_

#171
For	_	_
preserving	_	_
more	_	_
spatial	_	_
details	_	_
,	_	_
we	_	_
remove	_	_
pool4	_	_
and	_	_
pool5	_	_
layers	_	_
,	_	_
which	_	_
results	_	_
in	_	_
×8	_	_
instead	_	_
of	_	_
×32	_	_
downsampling	_	_
.	_	_

#172
At	_	_
time	_	_
step	_	_
t	_	_
,	_	_
with	_	_
an	_	_
input	_	_
frame	_	_
It	_	_
with	_	_
224×224	_	_
resolution	_	_
,	_	_
we	_	_
have	_	_
Xt	_	_
∈	_	_
R28×28×512	_	_
and	_	_
a	_	_
28×28	_	_
dynamic	_	_
saliency	_	_
map	_	_
from	_	_
the	_	_
convLSTM	_	_
.	_	_

#173
The	_	_
kernel	_	_
size	_	_
of	_	_
the	_	_
conv	_	_
layer	_	_
in	_	_
convLSTM	_	_
is	_	_
set	_	_
as	_	_
3	_	_
.	_	_

#174
Attention	_	_
module	_	_
.	_	_

#175
We	_	_
extend	_	_
above	_	_
CNN-LSTM	_	_
architecture	_	_
with	_	_
an	_	_
attention	_	_
mechanism	_	_
,	_	_
which	_	_
is	_	_
learned	_	_
from	_	_
existing	_	_
static	_	_
fixation	_	_
data	_	_
in	_	_
a	_	_
supervised	_	_
manner	_	_
.	_	_

#176
Such	_	_
design	_	_
is	_	_
mainly	_	_
driven	_	_
by	_	_
the	_	_
following	_	_
three	_	_
motivations	_	_
:	_	_
Dynamic	_	_
stimuli	_	_
Video	_	_
saliency	_	_
CNN+Attention	_	_
module	_	_
LSTM	_	_
𝒇𝒕	_	_
𝒊𝒕	_	_
𝒐𝒕	_	_
𝒕𝒂𝒏𝒉	_	_
𝑯𝒕−𝟏	_	_
𝑪𝒕−𝟏	_	_
σ	_	_
σ	_	_
𝒇𝒕	_	_
𝒊𝒕	_	_
𝒐𝒕	_	_
𝒕𝒂𝒏𝒉	_	_
𝒕𝒂𝒏𝒉	_	_
𝑯𝒕−𝟏	_	_
𝑪𝒕−𝟏	_	_
σ	_	_
𝑿	_	_
𝒕	_	_
𝒕𝒂𝒏𝒉	_	_
σ	_	_
σ	_	_
σ	_	_
𝑯𝒕	_	_
𝑪𝒕	_	_
Attention	_	_
M	_	_
VGG-16	_	_
Net	_	_
1×1	_	_
conv	_	_
Dynamic	_	_
saliency	_	_
Y	_	_
Conv	_	_
block	_	_
ConvLSTM	_	_
uint	_	_
Attention	_	_
module	_	_
Input	_	_
frame	_	_
Element-wise	_	_
sum	_	_
Hadamard	_	_
product	_	_
Pooling/Up-samlping	_	_
(	_	_
a	_	_
)	_	_
(	_	_
b	_	_
)	_	_
(	_	_
c	_	_
)	_	_
𝑿𝒕	_	_
Figure	_	_
3	_	_
.	_	_

#177
Network	_	_
architecture	_	_
of	_	_
the	_	_
proposed	_	_
video	_	_
saliency	_	_
model	_	_
.	_	_

#178
(	_	_
a	_	_
)	_	_
Attentive	_	_
CNN-LSTM	_	_
architecture	_	_
.	_	_

#179
(	_	_
b	_	_
)	_	_
CNN	_	_
layers	_	_
with	_	_
attention	_	_
module	_	_
are	_	_
used	_	_
for	_	_
learning	_	_
intra-frame	_	_
static	_	_
features	_	_
,	_	_
where	_	_
the	_	_
attention	_	_
module	_	_
is	_	_
learned	_	_
with	_	_
the	_	_
supervision	_	_
from	_	_
static	_	_
saliency	_	_
data	_	_
.	_	_

#180
(	_	_
c	_	_
)	_	_
ConvLSTM	_	_
used	_	_
for	_	_
learning	_	_
sequential	_	_
saliency	_	_
representations	_	_
.	_	_

#181
•	_	_
Previous	_	_
studies	_	_
[	_	_
26	_	_
,	_	_
64	_	_
]	_	_
shown	_	_
that	_	_
human	_	_
attention	_	_
is	_	_
guided	_	_
by	_	_
both	_	_
static	_	_
and	_	_
dynamic	_	_
factors	_	_
.	_	_

#182
Through	_	_
the	_	_
additional	_	_
attention	_	_
module	_	_
,	_	_
the	_	_
CNN	_	_
is	_	_
enforced	_	_
to	_	_
generate	_	_
a	_	_
more	_	_
explicit	_	_
spatial	_	_
saliency	_	_
representation	_	_
.	_	_

#183
This	_	_
helps	_	_
disentangle	_	_
underlying	_	_
spatial	_	_
and	_	_
temporal	_	_
factors	_	_
of	_	_
dynamic	_	_
attention	_	_
,	_	_
and	_	_
allows	_	_
convLSTM	_	_
better	_	_
capture	_	_
temporal	_	_
dynamics	_	_
.	_	_

#184
•	_	_
CNN-LSTM	_	_
architecture	_	_
introduces	_	_
a	_	_
large	_	_
number	_	_
of	_	_
parameters	_	_
for	_	_
modeling	_	_
spatial	_	_
and	_	_
temporal	_	_
patterns	_	_
.	_	_

#185
However	_	_
,	_	_
for	_	_
sequential	_	_
data	_	_
such	_	_
as	_	_
videos	_	_
,	_	_
obtaining	_	_
labelled	_	_
data	_	_
is	_	_
costly	_	_
.	_	_

#186
Even	_	_
though	_	_
there	_	_
are	_	_
large-scale	_	_
datasets	_	_
like	_	_
DHF1K	_	_
that	_	_
have	_	_
1K	_	_
videos	_	_
,	_	_
the	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
is	_	_
still	_	_
insufficient	_	_
,	_	_
considering	_	_
the	_	_
high	_	_
correlation	_	_
among	_	_
frames	_	_
within	_	_
same	_	_
video	_	_
.	_	_

#187
The	_	_
supervised	_	_
attentive	_	_
module	_	_
is	_	_
able	_	_
to	_	_
leverage	_	_
existing	_	_
rich	_	_
static	_	_
fixation	_	_
data	_	_
to	_	_
improve	_	_
the	_	_
generalization	_	_
power	_	_
of	_	_
our	_	_
model	_	_
.	_	_

#188
•	_	_
In	_	_
VGG-16	_	_
,	_	_
we	_	_
remove	_	_
the	_	_
last	_	_
two	_	_
pooling	_	_
layers	_	_
to	_	_
obtain	_	_
a	_	_
large	_	_
feature	_	_
map	_	_
.	_	_

#189
This	_	_
dramatically	_	_
decreases	_	_
the	_	_
receptive	_	_
field	_	_
(	_	_
212×212→140×140	_	_
)	_	_
,	_	_
which	_	_
can	_	_
not	_	_
cover	_	_
the	_	_
whole	_	_
frame	_	_
(	_	_
224×224	_	_
)	_	_
.	_	_

#190
To	_	_
remedy	_	_
this	_	_
,	_	_
we	_	_
insert	_	_
a	_	_
set	_	_
of	_	_
down-	_	_
and	_	_
up-sampling	_	_
operations	_	_
into	_	_
the	_	_
attention	_	_
module	_	_
,	_	_
which	_	_
would	_	_
enhance	_	_
the	_	_
intra-frame	_	_
saliency	_	_
information	_	_
with	_	_
an	_	_
enlarged	_	_
receptive	_	_
field	_	_
.	_	_

#191
By	_	_
this	_	_
,	_	_
our	_	_
model	_	_
is	_	_
able	_	_
to	_	_
make	_	_
more	_	_
accurate	_	_
predictions	_	_
from	_	_
a	_	_
global	_	_
view	_	_
.	_	_

#192
As	_	_
demonstrated	_	_
in	_	_
Fig.	_	_
3	_	_
(	_	_
b	_	_
)	_	_
,	_	_
our	_	_
attentive	_	_
module	_	_
is	_	_
built	_	_
upon	_	_
the	_	_
conv5-3	_	_
layer	_	_
,	_	_
as	_	_
an	_	_
additional	_	_
branch	_	_
of	_	_
several	_	_
conv	_	_
layers	_	_
interleaved	_	_
with	_	_
pooling	_	_
,	_	_
and	_	_
upsampling	_	_
operations	_	_
.	_	_

#193
Given	_	_
the	_	_
input	_	_
feature	_	_
X	_	_
,	_	_
with	_	_
pooling	_	_
layers	_	_
,	_	_
the	_	_
attention	_	_
module	_	_
generates	_	_
a	_	_
downsampled	_	_
attention	_	_
map	_	_
(	_	_
7×7	_	_
)	_	_
with	_	_
an	_	_
enlarged	_	_
receptive	_	_
field	_	_
(	_	_
260×260	_	_
)	_	_
.	_	_

#194
Then	_	_
the	_	_
small	_	_
attention	_	_
map	_	_
is	_	_
×4	_	_
upsampled	_	_
as	_	_
the	_	_
same	_	_
spatial	_	_
dimensions	_	_
of	_	_
X	_	_
.	_	_

#195
Let	_	_
M	_	_
∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
28×28	_	_
be	_	_
the	_	_
up-sampled	_	_
attention	_	_
map	_	_
,	_	_
the	_	_
feature	_	_
X	_	_
∈	_	_
R28×28×512	_	_
from	_	_
conv5-3	_	_
layer	_	_
can	_	_
be	_	_
further	_	_
enhanced	_	_
by	_	_
:	_	_
X̂	_	_
c	_	_
=M	_	_
◦	_	_
X	_	_
c	_	_
,	_	_
(	_	_
6	_	_
)	_	_
where	_	_
c	_	_
∈	_	_
{	_	_
1	_	_
,	_	_
...	_	_
,	_	_
512	_	_
}	_	_
is	_	_
the	_	_
index	_	_
of	_	_
the	_	_
channel	_	_
.	_	_

#196
Here	_	_
,	_	_
the	_	_
attention	_	_
module	_	_
work	_	_
as	_	_
a	_	_
feature	_	_
selector	_	_
to	_	_
enhance	_	_
the	_	_
feature	_	_
representation	_	_
.	_	_

#197
The	_	_
above	_	_
attention	_	_
module	_	_
may	options	_
lose	_	_
useful	_	_
information	_	_
for	_	_
learning	_	_
a	_	_
dynamic	_	_
saliency	_	_
representation	_	_
,	_	_
as	_	_
the	_	_
attention	_	_
module	_	_
only	_	_
considers	_	_
static	_	_
saliency	_	_
information	_	_
in	_	_
still	_	_
video	_	_
frames	_	_
.	_	_

#198
For	_	_
this	_	_
,	_	_
inspired	_	_
by	_	_
the	_	_
recent	_	_
advances	_	_
of	_	_
attention	_	_
mechanism	_	_
and	_	_
residual	_	_
connection	_	_
[	_	_
19	_	_
,	_	_
55	_	_
]	_	_
,	_	_
we	_	_
improve	_	_
Eq.	_	_
6	_	_
in	_	_
residual	_	_
form	_	_
:	_	_
X̂	_	_
c	_	_
=	_	_
(	_	_
1	_	_
+M	_	_
)	_	_
◦	_	_
X	_	_
c.	_	_
(	_	_
7	_	_
)	_	_

#199
With	_	_
the	_	_
residual	_	_
connection	_	_
,	_	_
both	_	_
the	_	_
original	_	_
CNN	_	_
features	_	_
and	_	_
the	_	_
enhanced	_	_
features	_	_
are	_	_
combined	_	_
and	_	_
fed	_	_
to	_	_
the	_	_
LSTM	_	_
model	_	_
.	_	_

#200
In	_	_
§5.2	_	_
and	_	_
§5.4	_	_
,	_	_
more	_	_
detailed	_	_
explorations	_	_
for	_	_
the	_	_
attention	_	_
module	_	_
are	_	_
offered	_	_
.	_	_

#201
Different	_	_
from	_	_
previous	_	_
attention	_	_
mechanisms	_	_
that	_	_
learn	_	_
task-related	_	_
attention	_	_
in	_	_
an	_	_
implicit	_	_
way	_	_
,	_	_
our	_	_
attention	_	_
module	_	_
can	_	_
learn	_	_
from	_	_
existing	_	_
large-scale	_	_
static	_	_
fixation	_	_
data	_	_
in	_	_
an	_	_
explicit	_	_
and	_	_
supervised	_	_
manner	_	_
(	_	_
detailed	_	_
in	_	_
next	_	_
part	_	_
)	_	_
.	_	_

#202
Loss	_	_
function	_	_
.	_	_

#203
We	_	_
use	_	_
the	_	_
following	_	_
loss	_	_
function	_	_
[	_	_
24	_	_
]	_	_
that	_	_
considers	_	_
three	_	_
different	_	_
saliency	_	_
evaluation	_	_
metrics	_	_
instead	_	_
of	_	_
one	_	_
.	_	_

#204
The	_	_
rationale	_	_
here	_	_
is	_	_
that	_	_
no	_	_
single	_	_
metric	_	_
can	_	_
fully	_	_
capture	_	_
how	_	_
satisfactory	_	_
a	_	_
saliency	_	_
map	_	_
is	_	_
.	_	_

#205
We	_	_
denote	_	_
the	_	_
predicted	_	_
saliency	_	_
map	_	_
as	_	_
Y∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
28×28	_	_
,	_	_
the	_	_
map	_	_
of	_	_
fixation	_	_
locations	_	_
as	_	_
P	_	_
∈	_	_
{	_	_
0	_	_
,	_	_
1	_	_
}	_	_
28×28	_	_
and	_	_
the	_	_
continuous	_	_
saliency	_	_
map	_	_
(	_	_
distribution	_	_
)	_	_
as	_	_
Q∈	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
28×28	_	_
.	_	_

#206
Here	_	_
the	_	_
fixation	_	_
map	_	_
P	_	_
is	_	_
discrete	_	_
,	_	_
that	_	_
records	_	_
whether	_	_
a	_	_
pixel	_	_
receives	_	_
human	_	_
fixation	_	_
.	_	_

#207
The	_	_
continuous	_	_
saliency	_	_
map	_	_
is	_	_
obtained	_	_
via	_	_
blurring	_	_
each	_	_
fixation	_	_
location	_	_
with	_	_
a	_	_
small	_	_
Gaussian	_	_
kernel	_	_
.	_	_

#208
Our	_	_
loss	_	_
functions	_	_
is	_	_
defined	_	_
as	_	_
follows	_	_
:	_	_
L	_	_
(	_	_
Y	_	_
,	_	_
P	_	_
,	_	_
Q	_	_
)	_	_
=LKL	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
+α1LCC	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
+α2LNSS	_	_
(	_	_
Y	_	_
,	_	_
P	_	_
)	_	_
,	_	_
(	_	_
8	_	_
)	_	_
where	_	_
LKL	_	_
,	_	_
LCC	_	_
and	_	_
LNSS	_	_
are	_	_
the	_	_
Kullback-Leibler	_	_
(	_	_
KL	_	_
)	_	_
divergence	_	_
,	_	_
the	_	_
Linear	_	_
Correlation	_	_
Coefficient	_	_
(	_	_
CC	_	_
)	_	_
,	_	_
and	_	_
the	_	_
Normalized	_	_
Scanpath	_	_
Saliency	_	_
(	_	_
NSS	_	_
)	_	_
,	_	_
respectively	_	_
,	_	_
which	_	_
are	_	_
derived	_	_
from	_	_
commonly	_	_
used	_	_
metrics	_	_
to	_	_
evaluate	_	_
saliency	_	_
prediction	_	_
models	_	_
.	_	_

#209
αs	_	_
are	_	_
balance	_	_
parameters	_	_
and	_	_
are	_	_
empirically	_	_
set	_	_
to	_	_
α1	_	_
=	_	_
α2	_	_
=	_	_
0.1	_	_
.	_	_

#210
LKL	_	_
is	_	_
widely	_	_
adopted	_	_
for	_	_
training	_	_
saliency	_	_
models	_	_
and	_	_
is	_	_
chosen	_	_
as	_	_
the	_	_
primary	_	_
loss	_	_
in	_	_
our	_	_
work	_	_
:	_	_
LKL	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
=	_	_
∑	_	_
x	_	_
Q	_	_
(	_	_
x	_	_
)	_	_
log	_	_
(	_	_
Q	_	_
(	_	_
x	_	_
)	_	_
Y	_	_
(	_	_
x	_	_
)	_	_
)	_	_
.	_	_

#211
(	_	_
9	_	_
)	_	_
LCC	_	_
measures	_	_
the	_	_
linear	_	_
relationship	_	_
between	_	_
Y	_	_
and	_	_
Q	_	_
:	_	_
LCC	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
=	_	_
−	_	_
cov	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
ρ	_	_
(	_	_
Y	_	_
)	_	_
ρ	_	_
(	_	_
Q	_	_
)	_	_
,	_	_
(	_	_
10	_	_
)	_	_
where	_	_
cov	_	_
(	_	_
Y	_	_
,	_	_
Q	_	_
)	_	_
is	_	_
the	_	_
covariance	_	_
of	_	_
Y	_	_
and	_	_
Q	_	_
,	_	_
and	_	_
ρ	_	_
(	_	_
·	_	_
)	_	_
stands	_	_
for	_	_
standard	_	_
deviation	_	_
.	_	_

#212
LNSS	_	_
is	_	_
derived	_	_
from	_	_
NSS	_	_
metric	_	_
:	_	_
LNSS	_	_
(	_	_
Y	_	_
,	_	_
P	_	_
)	_	_
=	_	_
−	_	_
1	_	_
N	_	_
∑	_	_
x	_	_
Y	_	_
(	_	_
x	_	_
)	_	_
×	_	_
P	_	_
(	_	_
x	_	_
)	_	_
,	_	_
(	_	_
11	_	_
)	_	_
where	_	_
Y	_	_
=	_	_
Y−µ	_	_
(	_	_
Y	_	_
)	_	_
ρ	_	_
(	_	_
Y	_	_
)	_	_
and	_	_
N	_	_
=	_	_
∑	_	_
x	_	_
P	_	_
(	_	_
x	_	_
)	_	_
.	_	_

#213
It	_	_
is	_	_
calculated	_	_
by	_	_
taking	_	_
the	_	_
mean	_	_
of	_	_
scores	_	_
from	_	_
the	_	_
normalized	_	_
saliency	_	_
map	_	_
Y	_	_
(	_	_
with	_	_
zero	_	_
mean	_	_
and	_	_
unit	_	_
standard	_	_
deviation	_	_
)	_	_
at	_	_
human	_	_
eye	_	_
fixations	_	_
P	_	_
.	_	_

#214
Since	_	_
CC	_	_
and	_	_
NSS	_	_
are	_	_
similarity	_	_
metrics	_	_
,	_	_
their	_	_
negatives	_	_
are	_	_
adopted	_	_
for	_	_
minimization	_	_
.	_	_

#215
Training	_	_
protocol	_	_
.	_	_

#216
Our	_	_
model	_	_
is	_	_
iteratively	_	_
trained	_	_
with	_	_
sequential	_	_
fixation	_	_
and	_	_
image	_	_
data	_	_
.	_	_

#217
In	_	_
training	_	_
,	_	_
a	_	_
video	_	_
training	_	_
batch	_	_
is	_	_
cascaded	_	_
with	_	_
an	_	_
image	_	_
training	_	_
batch	_	_
.	_	_

#218
More	_	_
specifically	_	_
,	_	_
in	_	_
a	_	_
video	_	_
training	_	_
batch	_	_
,	_	_
we	_	_
apply	_	_
a	_	_
loss	_	_
defined	_	_
over	_	_
the	_	_
final	_	_
dynamic	_	_
saliency	_	_
prediction	_	_
from	_	_
LSTM	_	_
.	_	_

#219
Let	_	_
{	_	_
Y	_	_
dt	_	_
}	_	_
Tt=1	_	_
,	_	_
{	_	_
P	_	_
dt	_	_
}	_	_
Tt=1	_	_
,	_	_
and	_	_
{	_	_
Qdt	_	_
}	_	_
Tt=1	_	_
denote	_	_
the	_	_
dynamic	_	_
saliency	_	_
predictions	_	_
,	_	_
the	_	_
dynamic	_	_
fixation	_	_
sequence	_	_
and	_	_
the	_	_
continuous	_	_
ground-truth	_	_
saliency	_	_
maps	_	_
,	_	_
we	_	_
minimize	_	_
the	_	_
following	_	_
loss	_	_
:	_	_
Ld	_	_
=	_	_
∑T	_	_
t=1	_	_
L	_	_
(	_	_
Y	_	_
d	_	_
t	_	_
,	_	_
P	_	_
d	_	_
t	_	_
,	_	_
Q	_	_
d	_	_
t	_	_
)	_	_
.	_	_

#220
(	_	_
12	_	_
)	_	_
In	_	_
this	_	_
process	_	_
,	_	_
the	_	_
attention	_	_
module	_	_
is	_	_
trained	_	_
in	_	_
an	_	_
implicit	_	_
way	_	_
,	_	_
since	_	_
we	_	_
do	_	_
not	_	_
have	_	_
the	_	_
groundtruth	_	_
fixation	_	_
of	_	_
each	_	_
frame	_	_
in	_	_
static	_	_
scene	_	_
.	_	_

#221
In	_	_
an	_	_
image	_	_
training	_	_
batch	_	_
,	_	_
we	_	_
only	_	_
train	_	_
our	_	_
attention	_	_
module	_	_
via	_	_
minimizing	_	_
:	_	_
Ls	_	_
=	_	_
L	_	_
(	_	_
M	_	_
,	_	_
P	_	_
s	_	_
,	_	_
Qs	_	_
)	_	_
,	_	_
(	_	_
13	_	_
)	_	_
where	_	_
the	_	_
M	_	_
,	_	_
P	_	_
s	_	_
,	_	_
Qs	_	_
indicate	_	_
the	_	_
attention	_	_
map	_	_
for	_	_
our	_	_
static	_	_
attention	_	_
module	_	_
,	_	_
the	_	_
ground-truth	_	_
static	_	_
fixation	_	_
map	_	_
,	_	_
and	_	_
the	_	_
ground-truth	_	_
static	_	_
saliency	_	_
map	_	_
.	_	_

#222
In	_	_
this	_	_
process	_	_
,	_	_
the	_	_
training	_	_
of	_	_
attention	_	_
module	_	_
is	_	_
supervised	_	_
by	_	_
the	_	_
ground-truth	_	_
static	_	_
fixation	_	_
.	_	_

#223
Note	_	_
that	_	_
,	_	_
in	_	_
image	_	_
training	_	_
batch	_	_
,	_	_
we	_	_
do	_	_
not	_	_
train	_	_
our	_	_
LSTM	_	_
module	_	_
,	_	_
as	_	_
it	_	_
is	_	_
used	_	_
for	_	_
learning	_	_
the	_	_
dynamic	_	_
representation	_	_
.	_	_

#224
For	_	_
each	_	_
video	_	_
training	_	_
batch	_	_
,	_	_
20	_	_
consecutive	_	_
frames	_	_
from	_	_
the	_	_
same	_	_
video	_	_
are	_	_
used	_	_
.	_	_

#225
Both	_	_
the	_	_
video	_	_
and	_	_
the	_	_
start	_	_
frame	_	_
are	_	_
randomly	_	_
selected	_	_
.	_	_

#226
For	_	_
each	_	_
image	_	_
training	_	_
batch	_	_
,	_	_
we	_	_
set	_	_
the	_	_
batch	_	_
size	_	_
as	_	_
20	_	_
,	_	_
and	_	_
the	_	_
images	_	_
are	_	_
randomly	_	_
sampled	_	_
from	_	_
existing	_	_
static	_	_
fixation	_	_
dataset	_	_
.	_	_

#227
More	_	_
implementation	_	_
details	_	_
can	_	_
be	_	_
found	_	_
in	_	_
§	_	_
5.1	_	_
.	_	_

#228
5	_	_
.	_	_

#229
Experiments	_	_
5.1	_	_
.	_	_

#230
Experimental	_	_
Setup	_	_
Training/testing	_	_
protocols	_	_
.	_	_

#231
We	_	_
use	_	_
the	_	_
static	_	_
stimuli	_	_
(	_	_
10	_	_
,	_	_
000	_	_
images	_	_
)	_	_
from	_	_
the	_	_
training	_	_
set	_	_
of	_	_
SALICON	_	_
[	_	_
31	_	_
]	_	_
dataset	_	_
for	_	_
training	_	_
our	_	_
attention	_	_
module	_	_
.	_	_

#232
For	_	_
dynamic	_	_
stimuli	_	_
,	_	_
we	_	_
consider	_	_
4	_	_
settings	_	_
:	_	_
using	_	_
the	_	_
training	_	_
set	_	_
(	_	_
s	_	_
)	_	_
from	_	_
(	_	_
i	_	_
)	_	_
DHF1K	_	_
,	_	_
(	_	_
ii	_	_
)	_	_
Hollywood-2	_	_
,	_	_
(	_	_
iii	_	_
)	_	_
UCF	_	_
sports	_	_
,	_	_
and	_	_
(	_	_
iv	_	_
)	_	_
DHF1K+Hollywood-2+UCF	_	_
sports	_	_
.	_	_

#233
For	_	_
DHF1K	_	_
,	_	_
we	_	_
use	_	_
the	_	_
original	_	_
training/validation/testing	_	_
splitting	_	_
(	_	_
600/100/300	_	_
)	_	_
.	_	_

#234
For	_	_
Hollywood-2	_	_
,	_	_
following	_	_
[	_	_
42	_	_
]	_	_
,	_	_
823	_	_
videos	_	_
for	_	_
training	_	_
and	_	_
884	_	_
videos	_	_
for	_	_
testing	_	_
.	_	_

#235
For	_	_
UCF	_	_
sports	_	_
,	_	_
the	_	_
training	_	_
and	_	_
testing	_	_
sets	_	_
include	_	_
103	_	_
and	_	_
47	_	_
videos	_	_
,	_	_
respectively	_	_
,	_	_
as	_	_
suggested	_	_
by	_	_
[	_	_
49	_	_
]	_	_
.	_	_

#236
We	_	_
randomly	_	_
sample	_	_
10	_	_
%	_	_
videos	_	_
from	_	_
the	_	_
training	_	_
sets	_	_
of	_	_
Hollywood-2	_	_
,	_	_
and	_	_
UCF	_	_
sports	_	_
as	_	_
their	_	_
validation	_	_
sets	_	_
.	_	_

#237
We	_	_
evaluate	_	_
our	_	_
model	_	_
on	_	_
the	_	_
testing	_	_
sets	_	_
of	_	_
DHF1K	_	_
,	_	_
Hollywood-2	_	_
,	_	_
and	_	_
UCF	_	_
sports	_	_
dataset	_	_
,	_	_
in	_	_
total	_	_
1	_	_
,	_	_
231	_	_
video	_	_
sequences	_	_
with	_	_
more	_	_
than	_	_
400	_	_
,	_	_
000	_	_
frames	_	_
.	_	_

#238
Implementation	_	_
details	_	_
.	_	_

#239
Our	_	_
model	_	_
is	_	_
implemented	_	_
in	_	_
Python	_	_
on	_	_
Keras	_	_
,	_	_
and	_	_
trained	_	_
with	_	_
the	_	_
Adam	_	_
optimizer	_	_
[	_	_
34	_	_
]	_	_
.	_	_

#240
During	_	_
the	_	_
training	_	_
phase	_	_
,	_	_
the	_	_
learning	_	_
rate	_	_
was	_	_
set	_	_
to	_	_
0.0001	_	_
and	_	_
was	_	_
decreased	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
10	_	_
every	_	_
2	_	_
epochs	_	_
.	_	_

#241
The	_	_
network	_	_
was	_	_
trained	_	_
for	_	_
10	_	_
epochs	_	_
.	_	_

#242
We	_	_
perform	_	_
earlystopping	_	_
on	_	_
the	_	_
validation	_	_
set	_	_
.	_	_

#243
Competitors	_	_
.	_	_

#244
We	_	_
compare	_	_
our	_	_
model	_	_
with	_	_
nine	_	_
dynamic	_	_
saliency	_	_
models	_	_
:	_	_
PQFT	_	_
[	_	_
16	_	_
]	_	_
,	_	_
Seo	_	_
et	_	_
al.	_	_
[	_	_
52	_	_
]	_	_
,	_	_
Rudoy	_	_
et	_	_
al.	_	_
[	_	_
50	_	_
]	_	_
,	_	_
Hou	_	_
et	_	_
al.	_	_
[	_	_
23	_	_
]	_	_
,	_	_
Fang	_	_
et	_	_
al.	_	_
[	_	_
13	_	_
]	_	_
,	_	_
OBDL	_	_
[	_	_
20	_	_
]	_	_
,	_	_
AWS-D	_	_
[	_	_
37	_	_
]	_	_
,	_	_
OM-CNN	_	_
[	_	_
30	_	_
]	_	_
,	_	_
and	_	_
Two-stream	_	_
[	_	_
2	_	_
]	_	_
1	_	_
.	_	_

#245
For	_	_
the	_	_
sake	_	_
of	_	_
complementary	_	_
,	_	_
we	_	_
further	_	_
compare	_	_
with	_	_
six	_	_
state-of-the-art	_	_
static	_	_
attention	_	_
models	_	_
:	_	_
ITTI	_	_
[	_	_
28	_	_
]	_	_
,	_	_
GBVS	_	_
[	_	_
18	_	_
]	_	_
,	_	_
SALICON	_	_
[	_	_
24	_	_
]	_	_
,	_	_
DVA	_	_
[	_	_
57	_	_
]	_	_
,	_	_
Shallow-Net	_	_
[	_	_
47	_	_
]	_	_
,	_	_
and	_	_
Deep-Net	_	_
[	_	_
47	_	_
]	_	_
.	_	_

#246
OM-CNN	_	_
,	_	_
Two-stream	_	_
,	_	_
SALICON	_	_
,	_	_
DVA	_	_
,	_	_
ShallowNet	_	_
,	_	_
and	_	_
Deep-Net	_	_
are	_	_
deep	_	_
learning	_	_
models	_	_
,	_	_
and	_	_
others	_	_
are	_	_
classical	_	_
saliency	_	_
models	_	_
.	_	_

#247
Those	_	_
models	_	_
are	_	_
selected	_	_
due	_	_
to	_	_
:	_	_
1	_	_
)	_	_
their	_	_
representability	_	_
of	_	_
the	_	_
diversity	_	_
of	_	_
the	_	_
stateoftheart	_	_
;	_	_
or	_	_
2	_	_
)	_	_
publicly	_	_
available	_	_
implementations	_	_
.	_	_

#248
Baselines	_	_
.	_	_

#249
We	_	_
further	_	_
derive	_	_
8	_	_
baselines	_	_
.	_	_

#250
For	_	_
each	_	_
training	_	_
setting	_	_
,	_	_
we	_	_
derive	_	_
two	_	_
baselines	_	_
:	_	_
Our	_	_
and	_	_
Attention	_	_
module	_	_
,	_	_
refer	_	_
to	_	_
our	_	_
final	_	_
dynamic	_	_
saliency	_	_
prediction	_	_
and	_	_
the	_	_
intermediate	_	_
output	_	_
of	_	_
our	_	_
attention	_	_
module	_	_
,	_	_
respectively	_	_
.	_	_

#251
Evaluation	_	_
metrics	_	_
.	_	_

#252
Here	_	_
,	_	_
we	_	_
employ	_	_
five	_	_
classic	_	_
metrics	_	_
,	_	_
namely	_	_
Normalized	_	_
Scanpath	_	_
Saliency	_	_
(	_	_
NSS	_	_
)	_	_
,	_	_
Similarity	_	_
Metric	_	_
(	_	_
SIM	_	_
)	_	_
,	_	_
Linear	_	_
Correlation	_	_
Coefficient	_	_
(	_	_
CC	_	_
)	_	_
,	_	_
AUC-Judd	_	_
(	_	_
AUC-J	_	_
)	_	_
,	_	_
and	_	_
shuffled	_	_
AUC	_	_
(	_	_
s-AUC	_	_
)	_	_
.	_	_

#253
Please	_	_
refer	_	_
to	_	_
[	_	_
5	_	_
,	_	_
57	_	_
]	_	_
for	_	_
detailed	_	_
descriptions	_	_
of	_	_
these	_	_
metrics	_	_
.	_	_

#254
Computation	_	_
load	_	_
.	_	_

#255
The	_	_
whole	_	_
model	_	_
is	_	_
trained	_	_
in	_	_
an	_	_
end-to-end	_	_
manner	_	_
.	_	_

#256
The	_	_
entire	_	_
training	_	_
procedure	_	_
takes	_	_
about	_	_
30	_	_
hours	_	_
with	_	_
a	_	_
single	_	_
NVIDIA	_	_
TITAN	_	_
X	_	_
GPU	_	_
and	_	_
a	_	_
4.0GHz	_	_
Intel	_	_
processor	_	_
(	_	_
in	_	_
training	_	_
setting	_	_
(	_	_
iv	_	_
)	_	_
)	_	_
.	_	_

#257
Since	_	_
our	_	_
model	_	_
does	_	_
not	_	_
need	_	_
any	_	_
pre-	_	_
or	_	_
post-processing	_	_
,	_	_
it	_	_
takes	_	_
only	_	_
about	_	_
0.08s	_	_
to	_	_
process	_	_
an	_	_
frame	_	_
image	_	_
of	_	_
size	_	_
224×	_	_
224	_	_
.	_	_

#258
5.2	_	_
.	_	_

#259
Performance	_	_
comparison	_	_
Performance	_	_
on	_	_
DHF1K	_	_
.	_	_

#260
Table	_	_
5	_	_
reports	_	_
the	_	_
comparative	_	_
results	_	_
with	_	_
the	_	_
aforementioned	_	_
saliency	_	_
models	_	_
,	_	_
on	_	_
the	_	_
testing	_	_
set	_	_
(	_	_
300	_	_
video	_	_
sequences	_	_
)	_	_
of	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#261
In	_	_
can	_	_
be	_	_
observed	_	_
that	_	_
the	_	_
proposed	_	_
model	_	_
consistently	_	_
and	_	_
significantly	_	_
outperforms	_	_
other	_	_
competitors	_	_
,	_	_
across	_	_
all	_	_
the	_	_
metrics	_	_
.	_	_

#262
This	_	_
can	_	_
be	_	_
contributed	_	_
to	_	_
our	_	_
specially	_	_
designed	_	_
atten1We	_	_
re-implemented	_	_
[	_	_
2	_	_
]	_	_
since	_	_
the	_	_
official	_	_
codes	_	_
can	_	_
not	_	_
run	_	_
correctly	_	_
.	_	_

#263
Method	_	_
Dataset	_	_
DHF1K	_	_
Hollywood-2	_	_
UCF	_	_
sports	_	_
AUC-J↑	_	_
SIM↑	_	_
s-AUC↑	_	_
CC↑	_	_
NSS↑	_	_
AUC-J↑	_	_
SIM↑	_	_
s-AUC↑	_	_
CC↑	_	_
NSS↑	_	_
AUC-J↑	_	_
SIM↑	_	_
s-AUC↑	_	_
CC↑	_	_
NSS↑	_	_
∗PQFT	_	_
[	_	_
16	_	_
]	_	_
0.699	_	_
0.139	_	_
0.562	_	_
0.137	_	_
0.749	_	_
0.723	_	_
0.201	_	_
0.621	_	_
0.153	_	_
0.755	_	_
0.825	_	_
0.250	_	_
0.722	_	_
0.338	_	_
1.780	_	_
∗Seo	_	_
et	_	_
al.	_	_
[	_	_
52	_	_
]	_	_
0.635	_	_
0.142	_	_
0.499	_	_
0.070	_	_
0.334	_	_
0.652	_	_
0.155	_	_
0.530	_	_
0.076	_	_
0.346	_	_
0.831	_	_
0.308	_	_
0.666	_	_
0.336	_	_
1.690	_	_
Dynamic	_	_
∗Rudoy	_	_
et	_	_
al.	_	_
[	_	_
50	_	_
]	_	_
0.769	_	_
0.214	_	_
0.501	_	_
0.285	_	_
1.498	_	_
0.783	_	_
0.315	_	_
0.536	_	_
0.302	_	_
1.570	_	_
0.763	_	_
0.271	_	_
0.637	_	_
0.344	_	_
1.619	_	_
models	_	_
∗Hou	_	_
et	_	_
al.	_	_
[	_	_
23	_	_
]	_	_
0.726	_	_
0.167	_	_
0.545	_	_
0.150	_	_
0.847	_	_
0.731	_	_
0.202	_	_
0.580	_	_
0.146	_	_
0.684	_	_
0.819	_	_
0.276	_	_
0.674	_	_
0.292	_	_
1.399	_	_
∗Fang	_	_
et	_	_
al.	_	_
[	_	_
13	_	_
]	_	_
0.819	_	_
0.198	_	_
0.537	_	_
0.273	_	_
1.539	_	_
0.859	_	_
0.272	_	_
0.659	_	_
0.358	_	_
1.667	_	_
0.845	_	_
0.307	_	_
0.674	_	_
0.395	_	_
1.787	_	_
∗OBDL	_	_
[	_	_
20	_	_
]	_	_
0.638	_	_
0.171	_	_
0.500	_	_
0.117	_	_
0.495	_	_
0.640	_	_
0.170	_	_
0.541	_	_
0.106	_	_
0.462	_	_
0.759	_	_
0.193	_	_
0.634	_	_
0.234	_	_
1.382	_	_
∗AWS-D	_	_
[	_	_
37	_	_
]	_	_
0.703	_	_
0.157	_	_
0.513	_	_
0.174	_	_
0.940	_	_
0.694	_	_
0.175	_	_
0.637	_	_
0.146	_	_
0.742	_	_
0.823	_	_
0.228	_	_
0.750	_	_
0.306	_	_
1.631	_	_
OM-CNN	_	_
[	_	_
30	_	_
]	_	_
0.856	_	_
0.256	_	_
0.583	_	_
0.344	_	_
1.911	_	_
0.887	_	_
0.356	_	_
0.693	_	_
0.446	_	_
2.313	_	_
0.870	_	_
0.321	_	_
0.691	_	_
0.405	_	_
2.089	_	_
Two-stream	_	_
[	_	_
2	_	_
]	_	_
0.834	_	_
0.197	_	_
0.581	_	_
0.325	_	_
1.632	_	_
0.863	_	_
0.276	_	_
0.710	_	_
0.382	_	_
1.748	_	_
0.832	_	_
0.264	_	_
0.685	_	_
0.343	_	_
1.753	_	_
∗ITTI	_	_
[	_	_
28	_	_
]	_	_
0.774	_	_
0.162	_	_
0.553	_	_
0.233	_	_
1.207	_	_
0.788	_	_
0.221	_	_
0.607	_	_
0.257	_	_
1.076	_	_
0.847	_	_
0.251	_	_
0.725	_	_
0.356	_	_
1.640	_	_
∗GBVS	_	_
[	_	_
18	_	_
]	_	_
0.828	_	_
0.186	_	_
0.554	_	_
0.283	_	_
1.474	_	_
0.837	_	_
0.257	_	_
0.633	_	_
0.308	_	_
1.336	_	_
0.859	_	_
0.274	_	_
0.697	_	_
0.396	_	_
1.818	_	_
Static	_	_
SALICON	_	_
[	_	_
24	_	_
]	_	_
0.857	_	_
0.232	_	_
0.590	_	_
0.327	_	_
1.901	_	_
0.856	_	_
0.321	_	_
0.711	_	_
0.425	_	_
2.013	_	_
0.848	_	_
0.304	_	_
0.738	_	_
0.375	_	_
1.838	_	_
models	_	_
Shallow-Net	_	_
[	_	_
47	_	_
]	_	_
0.833	_	_
0.182	_	_
0.529	_	_
0.295	_	_
1.509	_	_
0.851	_	_
0.276	_	_
0.694	_	_
0.423	_	_
1.680	_	_
0.846	_	_
0.276	_	_
0.691	_	_
0.382	_	_
1.789	_	_
Deep-Net	_	_
[	_	_
47	_	_
]	_	_
0.855	_	_
0.201	_	_
0.592	_	_
0.331	_	_
1.775	_	_
0.884	_	_
0.300	_	_
0.736	_	_
0.451	_	_
2.066	_	_
0.861	_	_
0.282	_	_
0.719	_	_
0.414	_	_
1.903	_	_
DVA	_	_
[	_	_
57	_	_
]	_	_
0.860	_	_
0.262	_	_
0.595	_	_
0.358	_	_
2.013	_	_
0.886	_	_
0.372	_	_
0.727	_	_
0.482	_	_
2.459	_	_
0.872	_	_
0.339	_	_
0.725	_	_
0.439	_	_
2.311	_	_
Training	_	_
Ours	_	_
0.885	_	_
0.311	_	_
0.553	_	_
0.415	_	_
2.259	_	_
0.905	_	_
0.471	_	_
0.757	_	_
0.577	_	_
2.517	_	_
0.894	_	_
0.403	_	_
0.742	_	_
0.517	_	_
2.559	_	_
setting	_	_
(	_	_
i	_	_
)	_	_
Attention	_	_
module	_	_
0.854	_	_
0.251	_	_
0.545	_	_
0.332	_	_
1.755	_	_
0.880	_	_
0.415	_	_
0.748	_	_
0.529	_	_
2.283	_	_
0.853	_	_
0.333	_	_
0.719	_	_
0.435	_	_
1.946	_	_
Training	_	_
Ours	_	_
0.878	_	_
0.297	_	_
0.543	_	_
0.388	_	_
2.125	_	_
0.912	_	_
0.519	_	_
0.754	_	_
0.609	_	_
3.049	_	_
0.874	_	_
0.364	_	_
0.727	_	_
0.452	_	_
2.186	_	_
setting	_	_
(	_	_
ii	_	_
)	_	_
Attention	_	_
module	_	_
0.855	_	_
0.250	_	_
0.541	_	_
0.318	_	_
1.703	_	_
0.885	_	_
0.416	_	_
0.690	_	_
0.490	_	_
2.113	_	_
0.860	_	_
0.322	_	_
0.656	_	_
0.367	_	_
1.667	_	_
Training	_	_
Ours	_	_
0.866	_	_
0.277	_	_
0.596	_	_
0.362	_	_
1.951	_	_
0.884	_	_
0.449	_	_
0.749	_	_
0.534	_	_
2.647	_	_
0.905	_	_
0.496	_	_
0.767	_	_
0.603	_	_
3.200	_	_
setting	_	_
(	_	_
iii	_	_
)	_	_
Attention	_	_
module	_	_
0.852	_	_
0.260	_	_
0.582	_	_
0.350	_	_
1.945	_	_
0.898	_	_
0.429	_	_
0.763	_	_
0.543	_	_
2.409	_	_
0.884	_	_
0.354	_	_
0.743	_	_
0.500	_	_
2.339	_	_
Training	_	_
Ours	_	_
0.890	_	_
0.315	_	_
0.601	_	_
0.434	_	_
2.354	_	_
0.913	_	_
0.542	_	_
0.757	_	_
0.623	_	_
3.086	_	_
0.897	_	_
0.406	_	_
0.744	_	_
0.510	_	_
2.567	_	_
setting	_	_
(	_	_
iv	_	_
)	_	_
Attention	_	_
module	_	_
0.870	_	_
0.273	_	_
0.577	_	_
0.380	_	_
2.077	_	_
0.878	_	_
0.479	_	_
0.686	_	_
0.478	_	_
2.060	_	_
0.877	_	_
0.379	_	_
0.685	_	_
0.411	_	_
1.899	_	_
∗	_	_
Non-deep	_	_
learning	_	_
model	_	_
.	_	_

#264
Table	_	_
5	_	_
.	_	_

#265
Quantitative	_	_
results	_	_
on	_	_
DHF1K	_	_
,	_	_
Hollywood2	_	_
,	_	_
and	_	_
UCF	_	_
sports	_	_
datasets	_	_
.	_	_

#266
The	_	_
best	_	_
scores	_	_
are	_	_
marked	_	_
in	_	_
bold	_	_
.	_	_

#267
Training	_	_
settings	_	_
(	_	_
§5.1	_	_
)	_	_
for	_	_
video	_	_
saliency	_	_
datasets	_	_
:	_	_
(	_	_
i	_	_
)	_	_
DHF1K	_	_
,	_	_
(	_	_
ii	_	_
)	_	_
Hollywood-2	_	_
,	_	_
(	_	_
iii	_	_
)	_	_
UCF	_	_
sports	_	_
,	_	_
and	_	_
(	_	_
iv	_	_
)	_	_
DHF1K+Hollywood-2+UCF	_	_
sports	_	_
.	_	_

#268
tion	_	_
module	_	_
,	_	_
which	_	_
makes	_	_
our	_	_
model	_	_
explicitly	_	_
learn	_	_
static	_	_
and	_	_
dynamic	_	_
saliency	_	_
representations	_	_
in	_	_
CNN	_	_
and	_	_
LSTM	_	_
separately	_	_
.	_	_

#269
Our	_	_
model	_	_
even	_	_
does	_	_
not	_	_
use	_	_
any	_	_
optical	_	_
flow	_	_
algorithm	_	_
(	_	_
different	_	_
with	_	_
Fang	_	_
et	_	_
al.	_	_
[	_	_
13	_	_
]	_	_
,	_	_
Two-stream	_	_
[	_	_
2	_	_
]	_	_
)	_	_
.	_	_

#270
This	_	_
significantly	_	_
improves	_	_
the	_	_
applicability	_	_
of	_	_
our	_	_
model	_	_
and	_	_
demonstrates	_	_
the	_	_
effectiveness	_	_
of	_	_
our	_	_
training	_	_
protocol	_	_
that	_	_
leveraging	_	_
both	_	_
static	_	_
and	_	_
dynamic	_	_
stimuli	_	_
.	_	_

#271
Performance	_	_
on	_	_
Hollywood-2	_	_
.	_	_

#272
We	_	_
further	_	_
test	_	_
our	_	_
model	_	_
on	_	_
Hollywood-2	_	_
dataset	_	_
,	_	_
where	_	_
the	_	_
testing	_	_
sets	_	_
comprises	_	_
884	_	_
video	_	_
sequences	_	_
.	_	_

#273
The	_	_
results	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
5	_	_
.	_	_

#274
Again	_	_
,	_	_
our	_	_
model	_	_
consistently	_	_
significantly	_	_
higher	_	_
than	_	_
other	_	_
methods	_	_
across	_	_
various	_	_
metrics	_	_
.	_	_

#275
Besides	_	_
,	_	_
when	_	_
we	_	_
go	_	_
insight	_	_
into	_	_
the	_	_
performance	_	_
with	_	_
training	_	_
settings	_	_
,	_	_
the	_	_
performance	_	_
would	_	_
increase	_	_
with	_	_
increasing	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
.	_	_

#276
This	_	_
suggests	_	_
that	_	_
the	_	_
large-scale	_	_
training	_	_
data	_	_
volume	_	_
is	_	_
important	_	_
for	_	_
the	_	_
performance	_	_
of	_	_
neural	_	_
network	_	_
.	_	_

#277
Performance	_	_
on	_	_
UCF	_	_
sports	_	_
.	_	_

#278
With	_	_
the	_	_
test	_	_
set	_	_
(	_	_
47	_	_
video	_	_
sequences	_	_
)	_	_
of	_	_
UCF	_	_
sports	_	_
dataset	_	_
,	_	_
we	_	_
again	_	_
observe	_	_
the	_	_
proposed	_	_
model	_	_
provides	_	_
consistently	_	_
good	_	_
results	_	_
,	_	_
compared	_	_
to	_	_
related	_	_
state-of-the-art	_	_
(	_	_
see	_	_
Table	_	_
5	_	_
)	_	_
.	_	_

#279
Interestingly	_	_
,	_	_
we	_	_
find	_	_
that	_	_
,	_	_
with	_	_
small	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
(	_	_
training	_	_
setting	_	_
(	_	_
iii	_	_
)	_	_
,	_	_
103	_	_
video	_	_
stimuli	_	_
from	_	_
UCF	_	_
sports	_	_
dataset	_	_
)	_	_
,	_	_
the	_	_
proposed	_	_
model	_	_
achieves	_	_
a	_	_
very	_	_
high	_	_
performance	_	_
,	_	_
even	_	_
better	_	_
than	_	_
the	_	_
model	_	_
(	_	_
Our	_	_
,	_	_
training	_	_
setting	_	_
(	_	_
iv	_	_
)	_	_
)	_	_
trained	_	_
with	_	_
large-scale	_	_
data	_	_
(	_	_
1.5K	_	_
video	_	_
stimuli	_	_
)	_	_
.	_	_

#280
This	_	_
could	feasibility-speculation	_
be	_	_
explained	_	_
by	_	_
lack	_	_
of	_	_
diversity	_	_
in	_	_
the	_	_
video	_	_
training	_	_
data	_	_
,	_	_
as	_	_
the	_	_
videos	_	_
in	_	_
UCF	_	_
sports	_	_
dataset	_	_
are	_	_
highly	_	_
related	_	_
(	_	_
with	_	_
similar	_	_
scenes	_	_
and	_	_
actors	_	_
)	_	_
and	_	_
small	_	_
scale	_	_
.	_	_

#281
This	_	_
is	_	_
also	_	_
consistent	_	_
with	_	_
our	_	_
research	_	_
for	_	_
UCF	_	_
sports	_	_
which	_	_
shows	_	_
that	_	_
82.3	_	_
%	_	_
fixations	_	_
are	_	_
located	_	_
on	_	_
the	_	_
human	_	_
body	_	_
area	_	_
(	_	_
see	_	_
§	_	_
2.1	_	_
)	_	_
.	_	_

#282
5.3.	_	_
Analysis	_	_

#283
Based	_	_
on	_	_
our	_	_
extensive	_	_
experiments	_	_
,	_	_
we	_	_
provide	_	_
more	_	_
detailed	_	_
analyses	_	_
,	_	_
which	_	_
would	_	_
give	_	_
deeper	_	_
insights	_	_
of	_	_
previous	_	_
studies	_	_
and	_	_
suggest	_	_
some	_	_
hints	_	_
for	_	_
future	_	_
research	_	_
.	_	_

#284
Dynamic	_	_
saliency	_	_
models	_	_
:	_	_
deep	_	_
vs	_	_
non-deep	_	_
learning	_	_
.	_	_

#285
In	_	_
dynamic	_	_
scenes	_	_
,	_	_
previous	_	_
deep	_	_
learning	_	_
based	_	_
dynamic	_	_
saliency	_	_
models	_	_
(	_	_
i.e.	_	_
,	_	_
OM-CNN	_	_
,	_	_
Two-stream	_	_
)	_	_
show	_	_
significant	_	_
improvements	_	_
over	_	_
classic	_	_
dynamic	_	_
models	_	_
(	_	_
e.g.	_	_
,	_	_
PQFT	_	_
,	_	_
Seo	_	_
et	_	_
al.	_	_
et	_	_
al.	_	_
,	_	_
Rudoy	_	_
et	_	_
al.	_	_
,	_	_
Hou	_	_
et	_	_
al.	_	_
,	_	_
Fang	_	_
et	_	_
al.	_	_
)	_	_
.	_	_

#286
This	_	_
demonstrates	_	_
the	_	_
strong	_	_
learning	_	_
ability	_	_
of	_	_
neural	_	_
network	_	_
and	_	_
the	_	_
promise	_	_
of	_	_
developing	_	_
neural	_	_
network	_	_
in	_	_
this	_	_
challenging	_	_
area	_	_
.	_	_

#287
Non-deep	_	_
learning	_	_
models	_	_
:	_	_
static	_	_
vs	_	_
dynamic	_	_
.	_	_

#288
An	_	_
interesting	_	_
finding	_	_
is	_	_
classic	_	_
dynamic	_	_
methods	_	_
(	_	_
i.e.	_	_
,	_	_
PQFT	_	_
,	_	_
Seo	_	_
et	_	_
al.	_	_
,	_	_
Rudoy	_	_
et	_	_
al.	_	_
,	_	_
Hou	_	_
et	_	_
al.	_	_
,	_	_
Fang	_	_
et	_	_
al	_	_
.	_	_
)	_	_

#289
did	_	_
not	_	_
perform	_	_
better	_	_
than	_	_
their	_	_
static	_	_
counterparts	_	_
:	_	_
ITTI	_	_
,	_	_
GBVS	_	_
.	_	_

#290
This	_	_
is	_	_
probably	_	_
due	_	_
to	_	_
two	_	_
reasons	_	_
.	_	_

#291
First	_	_
,	_	_
the	_	_
perceptual	_	_
cues	_	_
and	_	_
D	_	_
y	_	_
n	_	_
am	_	_
ic	_	_
st	_	_
im	_	_
u	_	_
li	_	_
G	_	_
ro	_	_
u	_	_
n	_	_
d	_	_
tr	_	_
u	_	_
th	_	_
S	_	_
al	_	_
ie	_	_
n	_	_
cy	_	_
p	_	_
re	_	_
d	_	_
ic	_	_
ti	_	_
o	_	_
n	_	_
D	_	_
y	_	_
n	_	_
am	_	_
ic	_	_
stim	_	_
u	_	_
li	_	_
G	_	_
ro	_	_
u	_	_
n	_	_
d	_	_
tru	_	_
th	_	_
S	_	_
alien	_	_
cy	_	_
p	_	_
red	_	_
ictio	_	_
n	_	_
A	_	_
tt	_	_
en	_	_
ti	_	_
o	_	_
n	_	_
m	_	_
o	_	_
d	_	_
u	_	_
le	_	_
A	_	_
tten	_	_
tio	_	_
n	_	_
m	_	_
o	_	_
d	_	_
u	_	_
le	_	_
DHF1K	_	_
Hollywood-2	_	_
UCF	_	_
sports	_	_
Figure	_	_
4	_	_
.	_	_

#292
Qualitative	_	_
results	_	_
of	_	_
our	_	_
video	_	_
saliency	_	_
model	_	_
on	_	_
three	_	_
datasets	_	_
.	_	_

#293
Best	_	_
viewed	_	_
in	_	_
color	_	_
.	_	_

#294
Aspects	_	_
Variants	_	_
AUC-J↑	_	_
SIM↑	_	_
s-AUC↑	_	_
CC↑	_	_
NSS↑	_	_
Baseline	_	_
training	_	_
setting	_	_
(	_	_
iv	_	_
)	_	_
(	_	_
1.5K	_	_
videos+10K	_	_
images	_	_
)	_	_
0.890	_	_
0.315	_	_
0.601	_	_
0.434	_	_
2.354	_	_
Attention	_	_
w/o	_	_
attention	_	_
(	_	_
1.5K	_	_
videos	_	_
)	_	_
0.847	_	_
0.236	_	_
0.579	_	_
0.306	_	_
1.685	_	_
module	_	_
w/o	_	_
residual	_	_
connection	_	_
(	_	_
1.5K	_	_
videos+10K	_	_
images	_	_
)	_	_
0.874	_	_
0.303	_	_
0.594	_	_
0.401	_	_
2.174	_	_
w/o	_	_
downsampling	_	_
(	_	_
1.5K	_	_
videos+10K	_	_
images	_	_
)	_	_
0.870	_	_
0.298	_	_
0.583	_	_
0.389	_	_
2.085	_	_
Training	_	_
reduced	_	_
training	_	_
samples	_	_
(	_	_
1.5K	_	_
videos+5K	_	_
images	_	_
)	_	_
0.877	_	_
0.297	_	_
0.588	_	_
0.372	_	_
2.098	_	_
convLSTM	_	_
w/o	_	_
convLSTM	_	_
(	_	_
1.5K	_	_
videos+10K	_	_
images	_	_
)	_	_
0.867	_	_
0.269	_	_
0.573	_	_
0.382	_	_
2.034	_	_
Table	_	_
6	_	_
.	_	_

#295
Ablation	_	_
study	_	_
on	_	_
DHF1K	_	_
.	_	_

#296
See	_	_
§5.4	_	_
for	_	_
details	_	_
.	_	_

#297
underlying	_	_
mechanisms	_	_
of	_	_
visual	_	_
attention	_	_
allocation	_	_
during	_	_
dynamic	_	_
viewing	_	_
are	_	_
more	_	_
complex	_	_
and	_	_
still	_	_
not	_	_
clear	_	_
.	_	_

#298
Second	_	_
,	_	_
previous	_	_
studies	_	_
are	_	_
more	_	_
focused	_	_
on	_	_
computational	_	_
models	_	_
of	_	_
static	_	_
saliency	_	_
,	_	_
while	_	_
less	_	_
efforts	_	_
were	_	_
paid	_	_
for	_	_
modeling	_	_
dynamic	_	_
saliency	_	_
.	_	_

#299
Deep	_	_
learning	_	_
models	_	_
:	_	_
static	_	_
vs	_	_
dynamic	_	_
.	_	_

#300
Compared	_	_
with	_	_
state-of-the-art	_	_
deep	_	_
learning	_	_
based	_	_
static	_	_
models	_	_
(	_	_
i.e.	_	_
,	_	_
DVA	_	_
,	_	_
Deep-Net	_	_
)	_	_
,	_	_
previous	_	_
deep	_	_
learning	_	_
based	_	_
dynamic	_	_
models	_	_
(	_	_
i.e.	_	_
,	_	_
OM-CNN	_	_
,	_	_
Two-stream	_	_
)	_	_
only	_	_
obtain	_	_
slightly	_	_
better	_	_
performance	_	_
(	_	_
or	_	_
only	_	_
competitive	_	_
)	_	_
.	_	_

#301
Although	_	_
strong	_	_
motion	_	_
information	_	_
(	_	_
i.e.	_	_
,	_	_
optical	_	_
flow	_	_
,	_	_
motion	_	_
network	_	_
)	_	_
have	_	_
been	_	_
encoded	_	_
into	_	_
OM-CNN	_	_
and	_	_
Two-stream	_	_
,	_	_
their	_	_
performance	_	_
are	_	_
still	_	_
limited	_	_
.	_	_

#302
We	_	_
attribute	_	_
this	_	_
into	_	_
the	_	_
inherent	_	_
difficulties	_	_
of	_	_
video	_	_
saliency	_	_
prediction	_	_
and	_	_
previous	_	_
models’	_	_
neglect	_	_
of	_	_
utilizing	_	_
existing	_	_
rich	_	_
static	_	_
saliency	_	_
data	_	_
.	_	_

#303
5.4	_	_
.	_	_

#304
Ablation	_	_
study	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
offer	_	_
a	_	_
more	_	_
detailed	_	_
exploration	_	_
of	_	_
our	_	_
proposed	_	_
approach	_	_
in	_	_
several	_	_
aspects	_	_
with	_	_
DHF1K	_	_
dataset	_	_
.	_	_

#305
We	_	_
verify	_	_
the	_	_
effectiveness	_	_
of	_	_
the	_	_
proposed	_	_
mechanism	_	_
,	_	_
and	_	_
examine	_	_
the	_	_
influence	_	_
of	_	_
different	_	_
training	_	_
protocols	_	_
.	_	_

#306
The	_	_
results	_	_
are	_	_
summarized	_	_
in	_	_
Table	_	_
6	_	_
.	_	_

#307
Effect	_	_
of	_	_
attention	_	_
mechanism	_	_
.	_	_

#308
By	_	_
disabling	_	_
the	_	_
attention	_	_
module	_	_
,	_	_
and	_	_
only	_	_
training	_	_
with	_	_
video	_	_
stimuli	_	_
we	_	_
observe	_	_
a	_	_
performance	_	_
drop	_	_
(	_	_
e.g.	_	_
,	_	_
AUC-J	_	_
:	_	_
0.890→0.847	_	_
)	_	_
,	_	_
verifying	_	_
the	_	_
effectiveness	_	_
of	_	_
attention	_	_
module	_	_
and	_	_
showing	_	_
that	_	_
the	_	_
leverage	_	_
of	_	_
static	_	_
stimuli	_	_
indeed	_	_
improves	_	_
the	_	_
predication	_	_
accuracy	_	_
in	_	_
dynamic	_	_
scenes	_	_
.	_	_

#309
For	_	_
exploring	_	_
the	_	_
effect	_	_
of	_	_
the	_	_
residual	_	_
connection	_	_
in	_	_
attention	_	_
module	_	_
(	_	_
Eq.	_	_
8	_	_
)	_	_
,	_	_
we	_	_
train	_	_
the	_	_
model	_	_
based	_	_
on	_	_
Eq.	_	_
5	_	_
(	_	_
without	_	_
residual	_	_
connection	_	_
)	_	_
.	_	_

#310
We	_	_
observe	_	_
a	_	_
minor	_	_
decrease	_	_
;	_	_
showing	_	_
that	_	_
employing	_	_
residual	_	_
connection	_	_
could	capability	_
avoid	_	_
distorting	_	_
spatial	_	_
features	_	_
in	_	_
frames	_	_
.	_	_

#311
In	_	_
our	_	_
attention	_	_
module	_	_
,	_	_
we	_	_
apply	_	_
down-sampling	_	_
for	_	_
enlarging	_	_
the	_	_
receptive	_	_
field	_	_
.	_	_

#312
We	_	_
also	_	_
study	_	_
the	_	_
influence	_	_
of	_	_
such	_	_
design	_	_
.	_	_

#313
We	_	_
find	_	_
that	_	_
the	_	_
attention	_	_
module	_	_
with	_	_
enlarged	_	_
receptive	_	_
field	_	_
would	_	_
gain	_	_
better	_	_
performance	_	_
,	_	_
since	_	_
the	_	_
model	_	_
could	capability	_
make	_	_
prediction	_	_
in	_	_
global	_	_
view	_	_
.	_	_

#314
Training	_	_
.	_	_

#315
We	_	_
assess	_	_
different	_	_
training	_	_
protocols	_	_
.	_	_

#316
By	_	_
reducing	_	_
the	_	_
amount	_	_
of	_	_
static	_	_
training	_	_
stimuli	_	_
from	_	_
10K	_	_
to	_	_
5K	_	_
,	_	_
we	_	_
observe	_	_
a	_	_
performance	_	_
drop	_	_
(	_	_
e.g.	_	_
,	_	_
AUC-J	_	_
:	_	_
0.890→0.877	_	_
)	_	_
.	_	_

#317
The	_	_
baseline	_	_
(	_	_
w/o	_	_
attention	_	_
)	_	_
can	_	_
also	_	_
be	_	_
viewed	_	_
as	_	_
the	_	_
model	_	_
without	_	_
any	_	_
static	_	_
training	_	_
stimuli	_	_
,	_	_
which	_	_
gains	_	_
worse	_	_
performance	_	_
(	_	_
e.g.	_	_
,	_	_
AUC-J	_	_
:	_	_
0.890→0.847	_	_
)	_	_
.	_	_

#318
Effect	_	_
of	_	_
convLSTM	_	_
.	_	_

#319
To	_	_
study	_	_
the	_	_
influence	_	_
of	_	_
convLSTM	_	_
,	_	_
we	_	_
re-train	_	_
our	_	_
model	_	_
without	_	_
convLSTM	_	_
(	_	_
using	_	_
training	_	_
setting	_	_
(	_	_
iv	_	_
)	_	_
)	_	_
and	_	_
get	_	_
a	_	_
baseline	_	_
:	_	_
w/o	_	_
convLSTM	_	_
.	_	_

#320
We	_	_
observe	_	_
a	_	_
drop	_	_
of	_	_
performance	_	_
;	_	_
showing	_	_
that	_	_
the	_	_
dynamic	_	_
information	_	_
learnt	_	_
in	_	_
convLSTM	_	_
could	capability	_
boost	_	_
the	_	_
performance	_	_
.	_	_

#321
6.	_	_
Discussion	_	_
and	_	_
Conclusion	_	_

#322
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
presented	_	_
“Dynamic	_	_
Human	_	_
Fixation	_	_
(	_	_
DHF1K	_	_
)	_	_
”	_	_
,	_	_
a	_	_
large-scale	_	_
carefully	_	_
designed	_	_
and	_	_
systematically	_	_
collected	_	_
benchmark	_	_
dataset	_	_
to	_	_
facilitate	_	_
research	_	_
in	_	_
video	_	_
saliency	_	_
modeling	_	_
.	_	_

#323
To	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
our	_	_
work	_	_
is	_	_
the	_	_
most	_	_
comprehensive	_	_
performance	_	_
evaluation	_	_
of	_	_
video	_	_
saliency	_	_
models	_	_
.	_	_

#324
DHF1K	_	_
contains	_	_
1K	_	_
videos	_	_
,	_	_
which	_	_
capture	_	_
representative	_	_
instances	_	_
,	_	_
diverse	_	_
contents	_	_
and	_	_
various	_	_
motions	_	_
,	_	_
with	_	_
human	_	_
eye-tracking	_	_
annotations	_	_
.	_	_

#325
Further	_	_
,	_	_
we	_	_
proposed	_	_
a	_	_
novel	_	_
deep	_	_
learning	_	_
based	_	_
video	_	_
saliency	_	_
model	_	_
,	_	_
which	_	_
encodes	_	_
a	_	_
supervised	_	_
attention	_	_
mechanism	_	_
to	_	_
explicitly	_	_
capture	_	_
static	_	_
saliency	_	_
information	_	_
and	_	_
help	_	_
LSTM	_	_
better	_	_
capture	_	_
dynamic	_	_
saliency	_	_
representations	_	_
over	_	_
successive	_	_
frames	_	_
.	_	_

#326
We	_	_
performed	_	_
extensive	_	_
experiments	_	_
on	_	_
DHF1K	_	_
,	_	_
Hollywood-2	_	_
,	_	_
and	_	_
UCFsports	_	_
datasets	_	_
,	_	_
and	_	_
analyzed	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
model	_	_
compared	_	_
to	_	_
previous	_	_
attention	_	_
models	_	_
in	_	_
dynamic	_	_
scenes	_	_
.	_	_

#327
Our	_	_
experimental	_	_
results	_	_
demonstrate	_	_
that	_	_
our	_	_
proposed	_	_
model	_	_
outperforms	_	_
other	_	_
competitors	_	_
and	_	_
is	_	_
quite	_	_
efficient	_	_
.	_	_