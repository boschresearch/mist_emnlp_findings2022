#0
An	_	_
Algorithm	_	_
for	_	_
Learning	_	_
Shape	_	_
and	_	_
Appearance	_	_
Models	_	_
without	_	_
Annotations	_	_
John	_	_
Ashburnera	_	_
,	_	_
Mikael	_	_
Brudforsa	_	_
,	_	_
Kevin	_	_
Bronika	_	_
,	_	_
Yaël	_	_
Balbastrea	_	_
a	_	_
Wellcome	_	_
Centre	_	_
for	_	_
Human	_	_
Neuroimaging	_	_
UCL	_	_
Queen	_	_
Square	_	_
Institute	_	_
of	_	_
Neurology	_	_
12	_	_
Queen	_	_
Square	_	_
,	_	_
London	_	_
,	_	_
WC1N	_	_
3BG	_	_
,	_	_
UK	_	_
.	_	_

#1
Abstract	_	_

#2
This	_	_
paper	_	_
presents	_	_
a	_	_
framework	_	_
for	_	_
automatically	_	_
learning	_	_
shape	_	_
and	_	_
appearance	_	_
models	_	_
for	_	_
medical	_	_
(	_	_
and	_	_
certain	_	_
other	_	_
)	_	_
images	_	_
.	_	_

#3
It	_	_
is	_	_
based	_	_
on	_	_
the	_	_
idea	_	_
that	_	_
having	_	_
a	_	_
more	_	_
accurate	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
leads	_	_
to	_	_
more	_	_
accurate	_	_
image	_	_
registration	_	_
,	_	_
which	_	_
in	_	_
turn	_	_
leads	_	_
to	_	_
a	_	_
more	_	_
accurate	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
.	_	_

#4
This	_	_
leads	_	_
naturally	_	_
to	_	_
an	_	_
iterative	_	_
scheme	_	_
,	_	_
which	_	_
is	_	_
based	_	_
on	_	_
a	_	_
probabilistic	_	_
generative	_	_
model	_	_
that	_	_
is	_	_
fit	_	_
using	_	_
Gauss-Newton	_	_
updates	_	_
within	_	_
an	_	_
EM-like	_	_
framework	_	_
.	_	_

#5
It	_	_
was	_	_
developed	_	_
with	_	_
the	_	_
aim	_	_
of	_	_
enabling	_	_
distributed	_	_
privacy-preserving	_	_
analysis	_	_
of	_	_
brain	_	_
image	_	_
data	_	_
,	_	_
such	_	_
that	_	_
shared	_	_
information	_	_
(	_	_
shape	_	_
and	_	_
appearance	_	_
basis	_	_
functions	_	_
)	_	_
may	_	_
be	_	_
passed	_	_
across	_	_
sites	_	_
,	_	_
whereas	_	_
latent	_	_
variables	_	_
that	_	_
encode	_	_
individual	_	_
images	_	_
remain	_	_
secure	_	_
within	_	_
each	_	_
site	_	_
.	_	_

#6
These	_	_
latent	_	_
variables	_	_
are	_	_
proposed	_	_
as	_	_
features	_	_
for	_	_
privacy-preserving	_	_
data	_	_
mining	_	_
applications	_	_
.	_	_

#7
The	_	_
approach	_	_
is	_	_
demonstrated	_	_
qualitatively	_	_
on	_	_
the	_	_
KDEF	_	_
dataset	_	_
of	_	_
2D	_	_
face	_	_
images	_	_
,	_	_
showing	_	_
that	_	_
it	_	_
can	_	_
align	_	_
images	_	_
that	_	_
traditionally	_	_
require	_	_
shape	_	_
and	_	_
appearance	_	_
models	_	_
trained	_	_
using	_	_
manually	_	_
annotated	_	_
data	_	_
(	_	_
manually	_	_
defined	_	_
landmarks	_	_
etc.	_	_
)	_	_
.	_	_

#8
It	_	_
is	_	_
applied	_	_
to	_	_
MNIST	_	_
dataset	_	_
of	_	_
handwritten	_	_
digits	_	_
to	_	_
show	_	_
its	_	_
potential	_	_
for	_	_
machine	_	_
learning	_	_
applications	_	_
,	_	_
particularly	_	_
when	_	_
training	_	_
data	_	_
is	_	_
limited	_	_
.	_	_

#9
The	_	_
model	_	_
is	_	_
able	_	_
to	_	_
handle	_	_
“missing	_	_
data”	_	_
,	_	_
which	_	_
allows	_	_
it	_	_
to	_	_
be	_	_
cross-validated	_	_
according	_	_
to	_	_
how	_	_
well	_	_
it	_	_
can	_	_
predict	_	_
left-out	_	_
voxels	_	_
.	_	_

#10
The	_	_
Email	_	_
address	_	_
:	_	_
j.ashburner	_	_
@	_	_
ucl.ac.uk	_	_
(	_	_
John	_	_
Ashburner	_	_
)	_	_
Preprint	_	_
submitted	_	_
to	_	_
Medical	_	_
Image	_	_
Analysis	_	_
June	_	_
6	_	_
,	_	_
2021	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
7	_	_
.	_	_

#11
1v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
7	_	_
Ju	_	_
l	_	_
2	_	_

#12
1	_	_
INTRODUCTION	_	_
2	_	_

#13
suitability	_	_
of	_	_
the	_	_
derived	_	_
features	_	_
for	_	_
classifying	_	_
individuals	_	_
into	_	_
patient	_	_
groups	_	_
was	_	_
assessed	_	_
by	_	_
applying	_	_
it	_	_
to	_	_
a	_	_
dataset	_	_
of	_	_
over	_	_
1,900	_	_
segmented	_	_
T1-weighted	_	_
MR	_	_
images	_	_
,	_	_
which	_	_
included	_	_
images	_	_
from	_	_
the	_	_
COBRE	_	_
and	_	_
ABIDE	_	_
datasets	_	_
.	_	_

#14
Keywords	_	_
:	_	_
Machine	_	_
Learning	_	_
,	_	_
Latent	_	_
Variables	_	_
,	_	_
Diffeomorphisms	_	_
,	_	_
Geodesic	_	_
Shooting	_	_
,	_	_
Shape	_	_
Model	_	_
,	_	_
Appearance	_	_
Model	_	_
1	_	_
.	_	_

#15
Introduction	_	_
This	_	_
paper	_	_
introduces	_	_
an	_	_
algorithm	_	_
for	_	_
learning	_	_
a	_	_
model	_	_
of	_	_
shape	_	_
and	_	_
appearance	_	_
variability	_	_
from	_	_
a	_	_
collection	_	_
of	_	_
images	_	_
,	_	_
without	_	_
relying	_	_
on	_	_
manual	_	_
annotations	_	_
.	_	_

#16
The	_	_
shape	_	_
part	_	_
of	_	_
the	_	_
model	_	_
concerns	_	_
modelling	_	_
variability	_	_
with	_	_
diffeomorphic	_	_
deformations	_	_
,	_	_
which	_	_
is	_	_
essentially	_	_
image	_	_
registration	_	_
.	_	_

#17
In	_	_
contrast	_	_
,	_	_
the	_	_
appearance	_	_
part	_	_
is	_	_
about	_	_
accounting	_	_
for	_	_
signal	_	_
variability	_	_
that	_	_
is	_	_
not	_	_
well	_	_
described	_	_
by	_	_
deformations	_	_
,	_	_
and	_	_
is	_	_
essentially	_	_
about	_	_
adapting	_	_
a	_	_
“template”	_	_
to	_	_
enable	_	_
more	_	_
precise	_	_
registration	_	_
.	_	_

#18
The	_	_
problem	_	_
of	_	_
image	_	_
registration	_	_
is	_	_
often	_	_
viewed	_	_
from	_	_
a	_	_
Bayesian	_	_
perspective	_	_
,	_	_
whereby	_	_
the	_	_
aim	_	_
is	_	_
to	_	_
determine	_	_
the	_	_
most	_	_
probable	_	_
deformation	_	_
(	_	_
ψ	_	_
)	_	_
given	_	_
the	_	_
fixed	_	_
(	_	_
f	_	_
)	_	_
and	_	_
moving	_	_
(	_	_
µ	_	_
)	_	_
images	_	_
ψ̂	_	_
=	_	_
arg	_	_
max	_	_
ψ	_	_
log	_	_
p	_	_
(	_	_
ψ|f	_	_
,	_	_
µ	_	_
)	_	_
=	_	_
arg	_	_
max	_	_
ψ	_	_
(	_	_
log	_	_
p	_	_
(	_	_
f	_	_
|ψ	_	_
,	_	_
µ	_	_
)	_	_
+	_	_
log	_	_
p	_	_
(	_	_
ψ	_	_
)	_	_
)	_	_
.	_	_

#19
(	_	_
1	_	_
)	_	_
In	_	_
practice	_	_
,	_	_
the	_	_
regularisation	_	_
term	_	_
(	_	_
log	_	_
p	_	_
(	_	_
ψ	_	_
)	_	_
)	_	_
is	_	_
not	_	_
usually	_	_
defined	_	_
empirically	_	_
,	_	_
and	_	_
simply	_	_
involves	_	_
a	_	_
penalty	_	_
based	_	_
on	_	_
some	_	_
simple	_	_
measure	_	_
of	_	_
deformation	_	_
smoothness	_	_
.	_	_

#20
One	_	_
of	_	_
the	_	_
aims	_	_
of	_	_
this	_	_
work	_	_
is	_	_
to	_	_
try	_	_
to	_	_
improve	_	_
on	_	_
this	_	_
simple	_	_
model	_	_
.	_	_

#21
By	_	_
providing	_	_
empirically	_	_
derived	_	_
priors	_	_
for	_	_
the	_	_
allowable	_	_
deformations	_	_
,	_	_
trained	_	_
shape	_	_
models	_	_
have	_	_
been	_	_
shown	_	_
to	_	_
exhibit	_	_
more	_	_
robust	_	_
image	_	_
registration	_	_
.	_	_

#22
An	_	_
early	_	_
example	_	_
is	_	_
[	_	_
1	_	_
]	_	_
,	_	_
in	_	_
which	_	_
control	_	_
point	_	_
positions	_	_
are	_	_
constrained	_	_
by	_	_
their	_	_
first	_	_
few	_	_
modes	_	_
of	_	_
variability	_	_
.	_	_

#23
Training	_	_
this	_	_
model	_	_
involved	_	_
annotating	_	_
images	_	_
by	_	_
manually	_	_
placing	_	_
a	_	_
number	_	_
of	_	_
corresponding	_	_
landmarks	_	_
,	_	_
computing	_	_
the	_	_
mean	_	_
and	_	_
covariance	_	_
of	_	_
the	_	_
collection	_	_
of	_	_
landmarks	_	_
,	_	_
and	_	_
then	_	_
computing	_	_
the	_	_
eigenvectors	_	_
of	_	_
the	_	_
covariance	_	_
[	_	_
2	_	_
]	_	_
.	_	_

#24
In	_	_
neuroimaging	_	_
,	_	_
shape	_	_
models	_	_
have	_	_
previously	_	_
been	_	_
used	_	_
to	_	_
increase	_	_
the	_	_
robustness	_	_
of	_	_
brain	_	_
image	_	_
segmentation	_	_

#25
1	_	_
INTRODUCTION	_	_
3	_	_

#26
[	_	_
3	_	_
,	_	_
4	_	_
]	_	_
.	_	_

#27
The	_	_
current	_	_
work	_	_
involves	_	_
densely	_	_
parameterised	_	_
shape	_	_
models	_	_
within	_	_
the	_	_
diffeomorphic	_	_
setting	_	_
,	_	_
and	_	_
relates	_	_
to	_	_
previous	_	_
work	_	_
on	_	_
diffeomorphic	_	_
shape	_	_
models	_	_
[	_	_
5	_	_
]	_	_
,	_	_
as	_	_
well	_	_
as	_	_
those	_	_
using	_	_
more	_	_
densely	_	_
parameterised	_	_
deformations	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#28
Recently	_	_
,	_	_
[	_	_
7	_	_
]	_	_
developed	_	_
their	_	_
Principal	_	_
Geodesic	_	_
Analysis	_	_
(	_	_
PGA	_	_
)	_	_
framework	_	_
for	_	_
directly	_	_
computing	_	_
the	_	_
main	_	_
modes	_	_
of	_	_
shape	_	_
variation	_	_
within	_	_
a	_	_
diffeomorphic	_	_
setting	_	_
.	_	_

#29
The	_	_
work	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
borrows	_	_
heavily	_	_
from	_	_
that	_	_
of	_	_
[	_	_
7	_	_
]	_	_
,	_	_
but	_	_
extends	_	_
it	_	_
to	_	_
involve	_	_
a	_	_
joint	_	_
model	_	_
of	_	_
shape	_	_
and	_	_
appearance	_	_
variability	_	_
.	_	_

#30
In	_	_
addition	_	_
to	_	_
increasing	_	_
the	_	_
robustness	_	_
of	_	_
image	_	_
registration	_	_
tasks	_	_
,	_	_
shape	_	_
models	_	_
can	_	_
also	_	_
provide	_	_
features	_	_
that	_	_
may	_	_
be	_	_
used	_	_
for	_	_
statistical	_	_
shape	_	_
analysis	_	_
.	_	_

#31
This	_	_
is	_	_
related	_	_
to	_	_
approaches	_	_
used	_	_
in	_	_
geometric	_	_
morphometrics	_	_
[	_	_
8	_	_
]	_	_
,	_	_
where	_	_
the	_	_
aim	_	_
is	_	_
to	_	_
understand	_	_
shape	_	_
differences	_	_
among	_	_
anatomies	_	_
.	_	_

#32
Shape	_	_
descriptors	_	_
from	_	_
the	_	_
PGA	_	_
framework	_	_
have	_	_
previously	_	_
been	_	_
found	_	_
to	_	_
be	_	_
useful	_	_
features	_	_
for	_	_
data	_	_
mining	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#33
A	_	_
number	_	_
of	_	_
previous	_	_
works	_	_
have	_	_
investigated	_	_
combining	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
variability	_	_
into	_	_
the	_	_
same	_	_
model	_	_
[	_	_
2	_	_
,	_	_
10	_	_
,	_	_
11	_	_
,	_	_
5	_	_
,	_	_
12	_	_
,	_	_
4	_	_
]	_	_
.	_	_

#34
These	_	_
combined	_	_
shape	_	_
and	_	_
appearance	_	_
models	_	_
have	_	_
generally	_	_
shown	_	_
good	_	_
performance	_	_
in	_	_
a	_	_
number	_	_
of	_	_
medical	_	_
imaging	_	_
challenges	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#35
While	_	_
there	_	_
is	_	_
quite	_	_
a	_	_
lot	_	_
written	_	_
about	_	_
learning	_	_
appearance	_	_
variability	_	_
alone	_	_
,	_	_
the	_	_
literature	_	_
on	_	_
automatically	_	_
learning	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
together	_	_
is	_	_
fairly	_	_
limited	_	_
.	_	_

#36
Earlier	_	_
approaches	_	_
required	_	_
annotated	_	_
data	_	_
for	_	_
training	_	_
,	_	_
but	_	_
there	_	_
are	_	_
now	_	_
some	_	_
works	_	_
appearing	_	_
that	_	_
have	_	_
looked	_	_
into	_	_
the	_	_
possibility	_	_
of	_	_
using	_	_
unsupervised	_	_
or	_	_
semi-supervised	_	_
approaches	_	_
for	_	_
learning	_	_
shape	_	_
and	_	_
appearance	_	_
variability	_	_
.	_	_

#37
Examples	_	_
include	_	_
[	_	_
14	_	_
]	_	_
,	_	_
[	_	_
15	_	_
]	_	_
,	_	_
[	_	_
16	_	_
]	_	_
and	_	_
[	_	_
17	_	_
]	_	_
.	_	_

#38
The	_	_
current	_	_
work	_	_
is	_	_
about	_	_
an	_	_
unsupervised	_	_
approach	_	_
,	_	_
but	_	_
there	_	_
is	_	_
no	_	_
reason	_	_
why	_	_
it	_	_
could	_	_
not	_	_
be	_	_
made	_	_
semi-supervised	_	_
by	_	_
also	_	_
incorporating	_	_
some	_	_
manually	_	_
defined	_	_
landmarks	_	_
or	_	_
other	_	_
features	_	_
.	_	_

#39
This	_	_
work	_	_
was	_	_
undertaken	_	_
as	_	_
a	_	_
task	_	_
in	_	_
the	_	_
Medical	_	_
Informatics	_	_
Platform	_	_
of	_	_
the	_	_
EU	_	_
Human	_	_
Brain	_	_
Project	_	_
(	_	_
HBP	_	_
)	_	_
.	_	_

#40
The	_	_
original	_	_
aim	_	_
of	_	_
the	_	_
Medical	_	_
Informatics	_	_
Platform	_	_
was	_	_
to	_	_
develop	_	_
a	_	_
distributed	_	_
knowledge	_	_
discovery	_	_
framework	_	_
that	_	_
enables	_	_
data	_	_
mining	_	_
without	_	_
violating	_	_
patient	_	_
confidentiality	_	_
.	_	_

#41
The	_	_
strategy	_	_
was	_	_
to	_	_
involve	_	_
a	_	_
horizontally	_	_
partitioned	_	_
dataset	_	_
,	_	_
where	_	_
data	_	_
about	_	_
different	_	_
patients	_	_
is	_	_
stored	_	_
in	_	_
different	_	_
hospital	_	_
sites	_	_
,	_	_
and	_	_
only	_	_
data	_	_
aggregated	_	_
over	_	_
many	_	_

#42
1	_	_
INTRODUCTION	_	_
4	_	_

#43
subjects	_	_
may	_	_
leave	_	_
any	_	_
site	_	_
.	_	_

#44
While	_	_
aggregates	_	_
could	_	_
be	_	_
exploited	_	_
by	_	_
those	_	_
with	_	_
malicious	_	_
intent	_	_
in	_	_
order	_	_
to	_	_
extract	_	_
some	_	_
information	_	_
about	_	_
individual	_	_
patients	_	_
,	_	_
it	_	_
is	_	_
generally	_	_
more	_	_
difficult	_	_
to	_	_
do	_	_
so	_	_
,	_	_
particularly	_	_
when	_	_
there	_	_
are	_	_
constraints	_	_
on	_	_
how	_	_
much	_	_
aggregated	_	_
data	_	_
may	_	_
leave	_	_
each	_	_
site	_	_
.	_	_

#45
The	_	_
algorithm	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
can	_	_
be	_	_
implemented	_	_
in	_	_
a	_	_
way	_	_
that	_	_
does	_	_
not	_	_
require	_	_
patient-specific	_	_
information	_	_
to	_	_
leave	_	_
a	_	_
site	_	_
,	_	_
and	_	_
instead	_	_
only	_	_
shares	_	_
aggregates	_	_
,	_	_
which	_	_
reveal	_	_
less	_	_
about	_	_
the	_	_
individual	_	_
subjects	_	_
.	_	_

#46
Some	_	_
leakage	_	_
of	_	_
information	_	_
is	_	_
inevitable	_	_
,	_	_
particularly	_	_
for	_	_
sites	_	_
holding	_	_
data	_	_
on	_	_
only	_	_
small	_	_
numbers	_	_
of	_	_
individuals	_	_
,	_	_
but	_	_
we	_	_
leave	_	_
this	_	_
as	_	_
a	_	_
topic	_	_
to	_	_
be	_	_
addressed	_	_
elsewhere	_	_
.	_	_

#47
For	_	_
data	_	_
mining	_	_
,	_	_
in	_	_
situations	_	_
where	_	_
the	_	_
dimensionality	_	_
of	_	_
the	_	_
data	_	_
(	_	_
e.g.	_	_
number	_	_
of	_	_
voxels	_	_
in	_	_
an	_	_
image	_	_
)	_	_
is	_	_
greater	_	_
than	_	_
the	_	_
number	_	_
of	_	_
data	_	_
points	_	_
(	_	_
e.g.	_	_
number	_	_
of	_	_
images	_	_
)	_	_
,	_	_
it	_	_
is	_	_
sometimes	_	_
possible	_	_
to	_	_
make	_	_
use	_	_
of	_	_
the	_	_
Woodbury	_	_
matrix	_	_
identity1	_	_
,	_	_
which	_	_
leads	_	_
naturally	_	_
to	_	_
kernel-based	_	_
approaches	_	_
for	_	_
machine	_	_
learning	_	_
.	_	_

#48
However	_	_
,	_	_
these	_	_
would	_	_
require	_	_
dot-products	_	_
or	_	_
differences	_	_
to	_	_
be	_	_
computed	_	_
between	_	_
data	_	_
in	_	_
different	_	_
sites	_	_
,	_	_
and	_	_
this	_	_
would	_	_
be	_	_
prohibited	_	_
by	_	_
the	_	_
privacy	_	_
preserving	_	_
framework	_	_
.	_	_

#49
Also	_	_
,	_	_
the	_	_
Woodbury	_	_
identity	_	_
becomes	_	_
less	_	_
useful	_	_
for	_	_
extremely	_	_
large	_	_
datasets	_	_
,	_	_
because	_	_
of	_	_
both	_	_
memory	_	_
requirements	_	_
and	_	_
computational	_	_
complexity	_	_
.	_	_

#50
Aggregated	_	_
data	_	_
may	_	_
be	_	_
weighted	_	_
moments	_	_
(	_	_
e.g.	_	_
∑	_	_
n	_	_
rn	_	_
,	_	_
∑	_	_
n	_	_
rnzn	_	_
or	_	_
∑	_	_
n	_	_
rnznzTn	_	_
,	_	_
where	_	_
zn	_	_
is	_	_
a	_	_
vector	_	_
of	_	_
values	_	_
for	_	_
patient	_	_
n	_	_
,	_	_
and	_	_
rn	_	_
is	_	_
a	_	_
patient-specific	_	_
weight	_	_
generated	_	_
by	_	_
some	_	_
rule	_	_
)	_	_
,	_	_
which	_	_
could	_	_
then	_	_
be	_	_
used	_	_
for	_	_
clustering	_	_
or	_	_
other	_	_
forms	_	_
of	_	_
statistical	_	_
analysis	_	_
.	_	_

#51
Dimensionality	_	_
reduction	_	_
is	_	_
often	_	_
needed	_	_
to	_	_
enable	_	_
effective	_	_
data	_	_
mining	_	_
of	_	_
images	_	_
,	_	_
particularly	_	_
if	_	_
covariances	_	_
need	_	_
to	_	_
be	_	_
represented	_	_
(	_	_
such	_	_
as	_	_
for	_	_
clustering	_	_
into	_	_
patient	_	_
subgroups	_	_
using	_	_
Gaussian	_	_
mixture	_	_
models	_	_
)	_	_
.	_	_

#52
Recently	_	_
,	_	_
methods	_	_
such	_	_
as	_	_
convolutional	_	_
neural	_	_
networks	_	_
have	_	_
begun	_	_
to	_	_
show	_	_
1The	_	_
formula	_	_
is	_	_
frequently	_	_
encountered	_	_
as	_	_
(	_	_
Σ−1	_	_
+	_	_
FS−1FT	_	_
)	_	_
−1	_	_
=	_	_
Σ	_	_
−	_	_
ΣF	_	_
(	_	_
S	_	_
+	_	_
FT	_	_
ΣF	_	_
)	_	_
−1FT	_	_
Σ	_	_
(	_	_
see	_	_
https	_	_
:	_	_
//en.wikipedia.org/wiki/Woodbury_matrix_identity	_	_
)	_	_
.	_	_

#53
Typically	_	_
,	_	_
F	_	_
is	_	_
an	_	_
M	_	_
×	_	_
N	_	_
feature	_	_
matrix	_	_
,	_	_
Σ	_	_
is	_	_
an	_	_
M	_	_
×	_	_
M	_	_
prior	_	_
covariance	_	_
and	_	_
S	_	_
is	_	_
an	_	_
N	_	_
×	_	_
N	_	_
matrix	_	_
(	_	_
usually	_	_
diagonal	_	_
)	_	_
that	_	_
encodes	_	_
the	_	_
residual	_	_
covariance	_	_
.	_	_

#54
This	_	_
is	_	_
useful	_	_
when	_	_
M	_	_
N	_	_
,	_	_
as	_	_
inverses	_	_
of	_	_
only	_	_
N	_	_
×N	_	_
matrices	_	_
are	_	_
required	_	_
.	_	_

#55
Chapter	_	_
2	_	_
of	_	_
[	_	_
18	_	_
]	_	_
shows	_	_
how	_	_
this	_	_
relates	_	_
to	_	_
kernel	_	_
methods	_	_
.	_	_

#56
1	_	_
INTRODUCTION	_	_
5	_	_

#57
a	_	_
great	_	_
deal	_	_
of	_	_
promise	_	_
for	_	_
machine	_	_
learning	_	_
tasks	_	_
[	_	_
19	_	_
]	_	_
,	_	_
and	_	_
it	_	_
seems	_	_
that	_	_
much	_	_
of	_	_
the	_	_
previous	_	_
work	_	_
from	_	_
the	_	_
field	_	_
of	_	_
medical	_	_
imaging	_	_
will	_	_
be	_	_
replaced	_	_
by	_	_
new	_	_
ideas	_	_
from	_	_
the	_	_
machine	_	_
learning	_	_
field	_	_
.	_	_

#58
This	_	_
is	_	_
partly	_	_
due	_	_
to	_	_
the	_	_
very	_	_
large	_	_
sets	_	_
of	_	_
labelled	_	_
training	_	_
data	_	_
now	_	_
available	_	_
for	_	_
certain	_	_
applications	_	_
.	_	_

#59
When	_	_
the	_	_
number	_	_
of	_	_
training	_	_
samples	_	_
is	_	_
large	_	_
compared	_	_
to	_	_
the	_	_
dimensionality	_	_
of	_	_
each	_	_
example	_	_
,	_	_
these	_	_
methods	_	_
can	_	_
accurately	_	_
capture	_	_
the	_	_
non-linearities	_	_
in	_	_
the	_	_
training	_	_
data	_	_
.	_	_

#60
Recent	_	_
MICCAI	_	_
and	_	_
other	_	_
medical	_	_
imaging	_	_
conferences	_	_
have	_	_
shown	_	_
that	_	_
this	_	_
is	_	_
especially	_	_
true	_	_
for	_	_
tasks	_	_
such	_	_
as	_	_
image	_	_
segmentation	_	_
,	_	_
where	_	_
each	_	_
voxel	_	_
is	_	_
effectively	_	_
a	_	_
label	_	_
.	_	_

#61
However	_	_
,	_	_
they	_	_
can	_	_
be	_	_
less	_	_
accurate	_	_
when	_	_
the	_	_
number	_	_
of	_	_
training	_	_
samples	_	_
is	_	_
relatively	_	_
small	_	_
compared	_	_
to	_	_
the	_	_
complexity	_	_
of	_	_
each	_	_
data	_	_
point	_	_
,	_	_
when	_	_
other	_	_
approaches	_	_
,	_	_
such	_	_
as	_	_
image	_	_
registration	_	_
,	_	_
may	_	_
be	_	_
better	_	_
at	_	_
encoding	_	_
the	_	_
non-linearities	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#62
Conventional	_	_
deep	_	_
learning	_	_
simply	_	_
models	_	_
the	_	_
output	_	_
as	_	_
a	_	_
nonlinear	_	_
function	_	_
of	_	_
the	_	_
input	_	_
.	_	_

#63
In	_	_
contrast	_	_
,	_	_
generative	_	_
approaches	_	_
to	_	_
machine	_	_
learning	_	_
would	_	_
involve	_	_
constructing	_	_
probabilistic	_	_
models	_	_
of	_	_
the	_	_
input	_	_
,	_	_
such	_	_
that	_	_
random	_	_
samples	_	_
could	_	_
be	_	_
drawn	_	_
that	_	_
are	_	_
as	_	_
similar	_	_
as	_	_
possible	_	_
to	_	_
possible	_	_
future	_	_
examples	_	_
of	_	_
real	_	_
input	_	_
data	_	_
.	_	_

#64
A	_	_
recent	_	_
incarnation	_	_
is	_	_
the	_	_
family	_	_
of	_	_
generative	_	_
adversarial	_	_
networks	_	_
[	_	_
21	_	_
]	_	_
,	_	_
where	_	_
the	_	_
general	_	_
aim	_	_
is	_	_
to	_	_
learn	_	_
a	_	_
model	_	_
that	_	_
can	_	_
generate	_	_
samples	_	_
that	_	_
are	_	_
indistinguishable	_	_
from	_	_
real	_	_
data	_	_
.	_	_

#65
Accurate	_	_
generative	_	_
models	_	_
of	_	_
data	_	_
have	_	_
many	_	_
potential	_	_
applications	_	_
,	_	_
which	_	_
include	_	_
outlier	_	_
detection	_	_
and	_	_
augmenting	_	_
training	_	_
data	_	_
for	_	_
deep	_	_
learning	_	_
.	_	_

#66
There	_	_
is	_	_
increasing	_	_
interest	_	_
in	_	_
the	_	_
use	_	_
of	_	_
generative	_	_
approaches	_	_
for	_	_
machine	_	_
learning	_	_
,	_	_
partly	_	_
because	_	_
they	_	_
can	_	_
be	_	_
extended	_	_
to	_	_
work	_	_
in	_	_
a	_	_
semi-supervised	_	_
way	_	_
.	_	_

#67
This	_	_
enables	_	_
unlabelled	_	_
training	_	_
data	_	_
to	_	_
contribute	_	_
towards	_	_
the	_	_
model	_	_
,	_	_
potentially	_	_
allowing	_	_
more	_	_
complex	_	_
models	_	_
to	_	_
be	_	_
learned	_	_
from	_	_
fewer	_	_
labelled	_	_
examples	_	_
.	_	_

#68
A	_	_
simple	_	_
example	_	_
of	_	_
semi-supervised	_	_
learning	_	_
would	_	_
be	_	_
the	_	_
approach	_	_
of	_	_
[	_	_
22	_	_
]	_	_
,	_	_
which	_	_
used	_	_
both	_	_
labelled	_	_
and	_	_
unlabelled	_	_
images	_	_
for	_	_
generating	_	_
tissue	_	_
probability	_	_
maps	_	_
.	_	_

#69
Another	_	_
motivation	_	_
for	_	_
generative	_	_
modelling	_	_
approaches	_	_
is	_	_
to	_	_
enable	_	_
missing	_	_
data	_	_
to	_	_
be	_	_
dealt	_	_
with	_	_
.	_	_

#70
Brain	_	_
images	_	_
–	_	_
particularly	_	_
hospital	_	_
brain	_	_
images	_	_
–	_	_
often	_	_
have	_	_
different	_	_
fields	_	_
of	_	_
view	_	_
from	_	_
each	_	_
other	_	_
,	_	_
with	_	_
parts	_	_
of	_	_
the	_	_
brain	_	_
missing	_	_
from	_	_
some	_	_
of	_	_
the	_	_
scans	_	_
.	_	_

#71
Many	_	_
machine	_	_
learning	_	_
approaches	_	_
do	_	_
not	_	_
work	_	_
well	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
missing	_	_
data	_	_
,	_	_
so	_	_
imputing	_	_
missing	_	_
information	_	_
is	_	_
an	_	_
implicit	_	_

#72
2	_	_
METHODS	_	_
6	_	_

#73
part	_	_
of	_	_
the	_	_
framework	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#74
This	_	_
work	_	_
proposes	_	_
a	_	_
solution	_	_
based	_	_
on	_	_
learning	_	_
a	_	_
form	_	_
of	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
.	_	_

#75
The	_	_
overall	_	_
aim	_	_
is	_	_
to	_	_
capture	_	_
as	_	_
much	_	_
anatomical	_	_
variability	_	_
as	_	_
possible	_	_
using	_	_
a	_	_
relatively	_	_
small	_	_
number	_	_
of	_	_
latent	_	_
variables	_	_
.	_	_

#76
In	_	_
addition	_	_
to	_	_
3D	_	_
brain	_	_
image	_	_
data	_	_
,	_	_
a	_	_
number	_	_
of	_	_
other	_	_
types	_	_
of	_	_
images	_	_
will	_	_
be	_	_
used	_	_
to	_	_
illustrate	_	_
other	_	_
aspects	_	_
of	_	_
the	_	_
very	_	_
general	_	_
framework	_	_
that	_	_
we	_	_
present	_	_
.	_	_

#77
Often	_	_
,	_	_
it	_	_
can	_	_
be	_	_
difficult	_	_
to	_	_
understand	_	_
the	_	_
behaviour	_	_
of	_	_
an	_	_
algorithm	_	_
when	_	_
it	_	_
is	_	_
only	_	_
applied	_	_
to	_	_
3D	_	_
volumes	_	_
,	_	_
particularly	_	_
when	_	_
space	_	_
for	_	_
figures	_	_
in	_	_
papers	_	_
is	_	_
limited	_	_
.	_	_

#78
For	_	_
this	_	_
reason	_	_
,	_	_
the	_	_
work	_	_
is	_	_
also	_	_
applied	_	_
to	_	_
a	_	_
number	_	_
of	_	_
2D	_	_
images	_	_
,	_	_
ranging	_	_
from	_	_
the	_	_
tiny	_	_
images	_	_
that	_	_
make	_	_
up	_	_
the	_	_
MNIST	_	_
dataset	_	_
,	_	_
to	_	_
some	_	_
images	_	_
of	_	_
faces	_	_
,	_	_
along	_	_
with	_	_
some	_	_
single	_	_
slices	_	_
through	_	_
brain	_	_
images	_	_
.	_	_

#79
2	_	_
.	_	_

#80
Methods	_	_
The	_	_
proposed	_	_
framework	_	_
builds	_	_
heavily	_	_
on	_	_
the	_	_
principal	_	_
geodesic	_	_
analysis	_	_
model	_	_
of	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#81
Modifications	_	_
involve	_	_
extending	_	_
the	_	_
framework	_	_
to	_	_
use	_	_
a	_	_
Gauss-Newton	_	_
optimisation	_	_
strategy	_	_
and	_	_
incorporating	_	_
an	_	_
appearance	_	_
model	_	_
.	_	_

#82
The	_	_
Gauss-Newton	_	_
strategy	_	_
is	_	_
intended	_	_
to	_	_
be	_	_
more	_	_
suited	_	_
to	_	_
the	_	_
distributed	_	_
computing	_	_
framework	_	_
of	_	_
the	_	_
HBP	_	_
,	_	_
although	_	_
a	_	_
streaming	_	_
method	_	_
using	_	_
stochastic	_	_
gradient	_	_
descent	_	_
could	_	_
also	_	_
have	_	_
been	_	_
used	_	_
.	_	_

#83
The	_	_
basic	_	_
idea	_	_
is	_	_
that	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
may	_	_
be	_	_
modelled	_	_
by	_	_
linear	_	_
combinations	_	_
of	_	_
spatial	_	_
basis	_	_
functions	_	_
,	_	_
and	_	_
the	_	_
objective	_	_
is	_	_
to	_	_
automatically	_	_
learn	_	_
the	_	_
best	_	_
set	_	_
of	_	_
basis	_	_
functions	_	_
and	_	_
latent	_	_
variables	_	_
from	_	_
some	_	_
collection	_	_
of	_	_
images	_	_
.	_	_

#84
This	_	_
is	_	_
essentially	_	_
a	_	_
form	_	_
of	_	_
factorisation	_	_
of	_	_
the	_	_
data	_	_
.	_	_

#85
Each	_	_
of	_	_
the	_	_
N	_	_
images	_	_
will	_	_
be	_	_
denoted	_	_
by	_	_
fn	_	_
∈	_	_
RM	_	_
,	_	_
where	_	_
M	_	_
is	_	_
the	_	_
number	_	_
of	_	_
pixels/voxels	_	_
in	_	_
an	_	_
image	_	_
,	_	_
1	_	_
≤	_	_
n	_	_
≤	_	_
N	_	_
,	_	_
and	_	_
the	_	_
entire	_	_
collection	_	_
of	_	_
images	_	_
by	_	_
F.	_	_
An	_	_
appearance	_	_
model	_	_
for	_	_
the	_	_
nth	_	_
image	_	_
is	_	_
constructed	_	_
from	_	_
a	_	_
linear	_	_
combination	_	_
of	_	_
basis	_	_
functions	_	_
,	_	_
such	_	_
that	_	_
an	_	_
=	_	_
µ+	_	_
Wazn	_	_
.	_	_

#86
(	_	_
2	_	_
)	_	_
Here	_	_
,	_	_
Wa	_	_
is	_	_
a	_	_
matrix	_	_
containing	_	_
K	_	_
appearance	_	_
basis	_	_
functions	_	_
,	_	_
and	_	_
zn	_	_
is	_	_
a	_	_
vector	_	_
of	_	_
K	_	_
latent	_	_
variables	_	_
for	_	_
the	_	_
nth	_	_
image	_	_
.	_	_

#87
The	_	_
vector	_	_
µ	_	_
is	_	_
a	_	_
mean	_	_
image	_	_
,	_	_

#88
2	_	_
METHODS	_	_
7	_	_

#89
with	_	_
the	_	_
same	_	_
dimensions	_	_
as	_	_
a	_	_
column	_	_
of	_	_
Wa	_	_
.	_	_

#90
The	_	_
shape	_	_
model	_	_
is	_	_
encoded	_	_
similarly	_	_
,	_	_
with	_	_
vn	_	_
=	_	_
Wvzn	_	_
.	_	_

#91
(	_	_
3	_	_
)	_	_
The	_	_
approach	_	_
involves	_	_
warping	_	_
an	_	_
to	_	_
match	_	_
fn	_	_
,	_	_
using	_	_
a	_	_
diffeomorphic	_	_
deformation	_	_
parameterised	_	_
by	_	_
vn	_	_
.	_	_

#92
The	_	_
diffeomorphism	_	_
(	_	_
ψn	_	_
)	_	_
is	_	_
constructed	_	_
from	_	_
vn	_	_
by	_	_
a	_	_
procedure	_	_
known	_	_
as	_	_
“geodesic	_	_
shooting”	_	_
,	_	_
which	_	_
is	_	_
outlined	_	_
in	_	_
Section	_	_
2.1	_	_
.	_	_

#93
From	_	_
a	_	_
probabilistic	_	_
perspective	_	_
,	_	_
the	_	_
likelihood	_	_
can	_	_
be	_	_
summarised	_	_
by	_	_
p	_	_
(	_	_
fn|zn	_	_
,	_	_
µ	_	_
,	_	_
Wa	_	_
,	_	_
Wv	_	_
)	_	_
=	_	_
p	_	_
(	_	_
fn|an	_	_
(	_	_
ψn	_	_
)	_	_
)	_	_
.	_	_

#94
(	_	_
4	_	_
)	_	_
Different	_	_
forms	_	_
of	_	_
appearance	_	_
model	_	_
are	_	_
presented	_	_
in	_	_
Section	_	_
2.2	_	_
,	_	_
but	_	_
for	_	_
convenience	_	_
,	_	_
we	_	_
use	_	_
the	_	_
generic	_	_
definition	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
zn	_	_
,	_	_
µ	_	_
,	_	_
W	_	_
a	_	_
,	_	_
Wv	_	_
)	_	_
=	_	_
−	_	_
ln	_	_
p	_	_
(	_	_
fn|zn	_	_
,	_	_
µ	_	_
,	_	_
Wa	_	_
,	_	_
Wv	_	_
)	_	_
.	_	_

#95
(	_	_
5	_	_
)	_	_
In	_	_
practice	_	_
,	_	_
a	_	_
small	_	_
amount	_	_
of	_	_
regularisation	_	_
is	_	_
imposed	_	_
on	_	_
the	_	_
mean	_	_
(	_	_
µ	_	_
)	_	_
by	_	_
assuming	_	_
it	_	_
is	_	_
drawn	_	_
from	_	_
a	_	_
multivariate	_	_
Gaussian	_	_
distribution	_	_
of	_	_
precision	_	_
Lµ	_	_
p	_	_
(	_	_
µ	_	_
)	_	_
=	_	_
N	_	_
(	_	_
µ|0	_	_
,	_	_
(	_	_
Lµ	_	_
)	_	_
−1	_	_
)	_	_
.	_	_

#96
(	_	_
6	_	_
)	_	_
The	_	_
precision	_	_
matrix	_	_
is	_	_
only	_	_
involved	_	_
conceptually	_	_
.	_	_

#97
Because	_	_
the	_	_
matrix	_	_
is	_	_
circulant	_	_
(	_	_
a	_	_
special	_	_
kind	_	_
of	_	_
Toeplitz	_	_
matrix	_	_
where	_	_
data	_	_
are	_	_
assumed	_	_
to	_	_
wrap	_	_
around	_	_
at	_	_
the	_	_
boundaries	_	_
)	_	_
,	_	_
matrix	_	_
multiplication	_	_
and	_	_
division	_	_
may	_	_
be	_	_
effected	_	_
by	_	_
3D	_	_
convolutions	_	_
.	_	_

#98
Possible	_	_
forms	_	_
for	_	_
Lµ	_	_
are	_	_
described	_	_
in	_	_
Section	_	_
2.2.4	_	_
.	_	_

#99
A	_	_
weighted	_	_
average	_	_
of	_	_
two	_	_
strategies	_	_
for	_	_
regularising	_	_
the	_	_
basis	_	_
functions	_	_
(	_	_
Wa	_	_
and	_	_
Wv	_	_
)	_	_
and	_	_
latent	_	_
variables	_	_
(	_	_
zn	_	_
)	_	_
is	_	_
used	_	_
,	_	_
which	_	_
are	_	_
:	_	_
1	_	_
.	_	_

#100
The	_	_
first	_	_
strategy	_	_
involves	_	_
separate	_	_
priors	_	_
on	_	_
the	_	_
basis	_	_
functions	_	_
,	_	_
and	_	_
on	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#101
Each	_	_
of	_	_
the	_	_
basis	_	_
functions	_	_
is	_	_
assumed	_	_
to	_	_
be	_	_
drawn	_	_
from	_	_
zero-mean	_	_
highly	_	_
multivariate	_	_
Gaussian	_	_
,	_	_
parameterised	_	_
by	_	_
very	_	_
large	_	_
and	_	_
sparse	_	_
precision	_	_
matrices	_	_
.	_	_

#102
Possible	_	_
forms	_	_
of	_	_
the	_	_
matrices	_	_
for	_	_
regularising	_	_
shape	_	_
(	_	_
Lv	_	_
)	_	_
are	_	_
described	_	_
in	_	_
Section	_	_
2.1.1	_	_
,	_	_
whereas	_	_
those	_	_
for	_	_
appearance	_	_
(	_	_
La	_	_
)	_	_
are	_	_
described	_	_
in	_	_
Section	_	_
2.2.4	_	_
.	_	_

#103
Priors	_	_
for	_	_
the	_	_
basis	_	_

#104
2	_	_
METHODS	_	_
8	_	_

#105
functions	_	_
are	_	_
p	_	_
(	_	_
Wv	_	_
)	_	_
=	_	_
Kv∏	_	_
k=1	_	_
N	_	_
(	_	_
wv	_	_
k|0	_	_
,	_	_
(	_	_
NLv	_	_
)	_	_
−1	_	_
)	_	_
,	_	_
(	_	_
7	_	_
)	_	_
p	_	_
(	_	_
Wa	_	_
)	_	_
=	_	_
Ka∏	_	_
k=1	_	_
N	_	_
(	_	_
wa	_	_
k|0	_	_
,	_	_
(	_	_
NLa	_	_
)	_	_
−1	_	_
)	_	_
.	_	_

#106
(	_	_
8	_	_
)	_	_
The	_	_
latent	_	_
variables	_	_
(	_	_
Z	_	_
)	_	_
are	_	_
assumed	_	_
to	_	_
be	_	_
drawn	_	_
from	_	_
zero-mean	_	_
multivariate	_	_
Gaussian	_	_
distributions	_	_
,	_	_
parameterised	_	_
by	_	_
a	_	_
precision	_	_
matrix	_	_
(	_	_
A	_	_
)	_	_
that	_	_
is	_	_
derived	_	_
from	_	_
the	_	_
data2	_	_
.	_	_

#107
p	_	_
(	_	_
zn|A	_	_
)	_	_
=	_	_
N	_	_
(	_	_
zn|0	_	_
,	_	_
A−1	_	_
)	_	_
.	_	_

#108
(	_	_
9	_	_
)	_	_
Note	_	_
that	_	_
precision	_	_
matrices	_	_
for	_	_
the	_	_
basis	_	_
functions	_	_
,	_	_
Lv	_	_
and	_	_
La	_	_
,	_	_
are	_	_
scaled	_	_
by	_	_
N	_	_
.	_	_

#109
The	_	_
reason	_	_
for	_	_
this	_	_
is	_	_
that	_	_
it	_	_
keeps	_	_
the	_	_
variance	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
within	_	_
a	_	_
similar	_	_
range	_	_
,	_	_
irrespective	_	_
of	_	_
how	_	_
many	_	_
images	_	_
are	_	_
involved	_	_
,	_	_
which	_	_
makes	_	_
it	_	_
easier	_	_
to	_	_
define	_	_
priors	_	_
for	_	_
them	_	_
.	_	_

#110
The	_	_
simplest	_	_
approach	_	_
for	_	_
working	_	_
with	_	_
the	_	_
A	_	_
matrix	_	_
is	_	_
to	_	_
make	_	_
a	_	_
point	_	_
estimate	_	_
based	_	_
on	_	_
the	_	_
distribution	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#111
This	_	_
approach	_	_
allows	_	_
the	_	_
relevance	_	_
of	_	_
each	_	_
of	_	_
the	_	_
basis	_	_
functions	_	_
(	_	_
columns	_	_
of	_	_
Wa	_	_
and	_	_
Wv	_	_
)	_	_
to	_	_
be	_	_
automatically	_	_
determined	_	_
.	_	_

#112
If	_	_
they	_	_
are	_	_
not	_	_
needed	_	_
,	_	_
some	_	_
of	_	_
the	_	_
elements	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
(	_	_
i.e.	_	_
rows	_	_
of	_	_
Z	_	_
)	_	_
are	_	_
forced	_	_
to	_	_
zero	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#113
In	_	_
practice	_	_
,	_	_
this	_	_
could	_	_
lead	_	_
to	_	_
prematurely	_	_
sparse	_	_
solutions	_	_
that	_	_
may	_	_
be	_	_
difficult	_	_
to	_	_
recover	_	_
from	_	_
.	_	_

#114
Instead	_	_
,	_	_
sparsity	_	_
is	_	_
avoided	_	_
by	_	_
assuming	_	_
that	_	_
matrix	_	_
A	_	_
is	_	_
drawn	_	_
from	_	_
a	_	_
Wishart	_	_
distribution	_	_
.	_	_

#115
p	_	_
(	_	_
A	_	_
)	_	_
=WK	_	_
(	_	_
A|Λ0	_	_
,	_	_
ν0	_	_
)	_	_
=	_	_
|A|	_	_
(	_	_
ν0−K−1	_	_
)	_	_
/2	_	_
exp	_	_
(	_	_
−	_	_
1	_	_

#116
2	_	_
Tr	_	_
(	_	_
Λ−1	_	_

#117
0	_	_
A	_	_
)	_	_
)	_	_

#118
2	_	_
(	_	_
ν0K	_	_
)	_	_
/2|Λ0|ν0/2ΓK	_	_
(	_	_
ν0	_	_
)	_	_
,	_	_
(	_	_
10	_	_
)	_	_
where	_	_
ΓK	_	_
is	_	_
the	_	_
multivariate	_	_
gamma	_	_
function	_	_
.	_	_

#119
This	_	_
prior	_	_
can	_	_
be	_	_
made	_	_
as	_	_
uninformative	_	_
as	_	_
possible	_	_
by	_	_
using	_	_
ν0	_	_
=	_	_
K	_	_
and	_	_
Λ0	_	_
=	_	_
I/ν0	_	_
,	_	_
where	_	_
I	_	_
is	_	_
an	_	_
2Note	_	_
that	_	_
the	_	_
precision	_	_
matrix	_	_
A	_	_
should	_	_
not	_	_
be	_	_
confused	_	_
with	_	_
the	_	_
variables	_	_
an	_	_
,	_	_
which	_	_
were	_	_
introduced	_	_
earlier	_	_
.	_	_

#120
Hopefully	_	_
,	_	_
the	_	_
context	_	_
in	_	_
which	_	_
they	_	_
are	_	_
used	_	_
should	_	_
be	_	_
enough	_	_
to	_	_
prevent	_	_
any	_	_
confusion	_	_
.	_	_

#121
2	_	_
METHODS	_	_
9	_	_

#122
identity	_	_
matrix	_	_
.	_	_

#123
In	_	_
general	_	_
,	_	_
Λ0	_	_
should	_	_
be	_	_
a	_	_
positive	_	_
definite	_	_
symmetric	_	_
matrix	_	_
,	_	_
with	_	_
ν0	_	_
≥	_	_
K	_	_
so	_	_
that	_	_
the	_	_
distribution	_	_
can	_	_
be	_	_
normalised	_	_
.	_	_

#124
2	_	_
.	_	_

#125
The	_	_
second	_	_
strategy	_	_
is	_	_
similar	_	_
to	_	_
that	_	_
of	_	_
[	_	_
7	_	_
]	_	_
,	_	_
and	_	_
is	_	_
a	_	_
pragmatic	_	_
solution	_	_
to	_	_
ensuring	_	_
that	_	_
enough	_	_
regularisation	_	_
is	_	_
used	_	_
.	_	_

#126
ln	_	_
p	_	_
(	_	_
Z	_	_
,	_	_
Wa	_	_
,	_	_
Wv	_	_
)	_	_
=	_	_
−	_	_
1	_	_

#127
2	_	_
Tr	_	_
(	_	_
ZZT	_	_
(	_	_
(	_	_
Wa	_	_
)	_	_
TLaWa	_	_
+	_	_
(	_	_
Wv	_	_
)	_	_
TLvWv	_	_
)	_	_
)	_	_
+	_	_
const	_	_

#128
(	_	_
11	_	_
)	_	_
This	_	_
strategy	_	_
imposes	_	_
smoothness	_	_
on	_	_
the	_	_
reconstructions	_	_
by	_	_
assuming	_	_
penalties	_	_
based	_	_
on	_	_
lnN	_	_
(	_	_
Wazn|0	_	_
,	_	_
La	_	_
)	_	_
and	_	_
lnN	_	_
(	_	_
Wvzn|0	_	_
,	_	_
Lv	_	_
)	_	_
,	_	_
in	_	_
a	_	_
similar	_	_
way	_	_
to	_	_
more	_	_
conventional	_	_
regularisation	_	_
approaches	_	_
.	_	_

#129
The	_	_
relative	_	_
weighting	_	_
of	_	_
the	_	_
two	_	_
strategies	_	_
is	_	_
by	_	_
λ1	_	_
and	_	_
λ2	_	_
.	_	_

#130
Better	_	_
results	_	_
are	_	_
typically	_	_
achieved	_	_
when	_	_
hyper-parameters	_	_
are	_	_
specified	_	_
so	_	_
that	_	_
they	_	_
sum	_	_
to	_	_
a	_	_
value	_	_
greater	_	_
than	_	_
1	_	_
.	_	_

#131
When	_	_
everything	_	_
is	_	_
combined	_	_
(	_	_
see	_	_
Fig.	_	_
1	_	_
)	_	_
,	_	_
the	_	_
following	_	_
joint	_	_
log-probability	_	_
is	_	_
obtained	_	_
ln	_	_
p	_	_
(	_	_
F	_	_
,	_	_
µ	_	_
,	_	_
Wa	_	_
,	_	_
Wv	_	_
,	_	_
A	_	_
,	_	_
Z	_	_
)	_	_
=	_	_
−	_	_
N∑	_	_
n=1	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
zn	_	_
,	_	_
µ	_	_
,	_	_
W	_	_
a	_	_
,	_	_
Wv	_	_
)	_	_
−	_	_
1	_	_
2µ	_	_
TLµµ	_	_
−	_	_
λ1N	_	_
(	_	_
Tr	_	_
(	_	_
(	_	_
Wa	_	_
)	_	_
TLaWa	_	_
)	_	_
+	_	_
Tr	_	_
(	_	_
(	_	_
Wv	_	_
)	_	_
TLvWv	_	_
)	_	_
)	_	_
+	_	_
λ1	_	_
(	_	_
(	_	_
N	_	_
+	_	_
ν0	_	_
−K	_	_
−	_	_
1	_	_
)	_	_
ln	_	_
|A|	_	_
−	_	_
Tr	_	_
(	_	_
(	_	_
ZZT	_	_
+	_	_
Λ−1	_	_
0	_	_
)	_	_
A	_	_
)	_	_
)	_	_
−	_	_
λ2	_	_
2	_	_
Tr	_	_
(	_	_
ZZT	_	_
(	_	_
(	_	_
Wa	_	_
)	_	_
TLaWa	_	_
+	_	_
(	_	_
Wv	_	_
)	_	_
TLvWv	_	_
)	_	_
)	_	_
+	_	_
const	_	_
.	_	_

#132
(	_	_
12	_	_
)	_	_
Fitting	_	_
the	_	_
model	_	_
is	_	_
described	_	_
in	_	_
Appendix	_	_
Appendix	_	_
A	_	_
.	_	_

#133
Ideally	_	_
,	_	_
the	_	_
procedure	_	_
would	_	_
compute	_	_
distributions	_	_
for	_	_
all	_	_
variables	_	_
,	_	_
such	_	_
that	_	_
uncertainty	_	_
was	_	_
dealt	_	_
with	_	_
optimally	_	_
.	_	_

#134
Unfortunately	_	_
,	_	_
this	_	_
is	_	_
computationally	_	_
impractical	_	_
for	_	_
the	_	_
size	_	_
of	_	_
the	_	_
datasets	_	_
involved	_	_
.	_	_

#135
Instead	_	_
,	_	_
only	_	_
point	_	_
estimates	_	_
are	_	_
made	_	_
for	_	_
the	_	_
latent	_	_
variables	_	_
(	_	_
ẑn	_	_
)	_	_
and	_	_
various	_	_
parameters	_	_
(	_	_
µ̂	_	_
,	_	_
Wa	_	_
,	_	_
Wv	_	_
,	_	_
and	_	_
sometimes	_	_
σ̂2	_	_
)	_	_
,	_	_
apart	_	_
from	_	_
A	_	_
,	_	_
which	_	_
is	_	_
inferred	_	_
within	_	_
a	_	_
variational	_	_
Bayesian	_	_
framework	_	_
.	_	_

#136
The	_	_
approach	_	_
also	_	_
allows	_	_
shapes	_	_
and	_	_
appearances	_	_
to	_	_
be	_	_
modelled	_	_
separately	_	_
,	_	_
by	_	_
having	_	_
some	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
control	_	_
appearance	_	_
,	_	_
and	_	_
others	_	_
control	_	_

#137
2	_	_
METHODS	_	_
10	_	_

#138
f	_	_
z	_	_
A	_	_
WaWv	_	_
µ	_	_
NLv	_	_
NLa	_	_
Lµ	_	_
Λ0	_	_
,	_	_
ν0	_	_
N	_	_
Figure	_	_
1	_	_
:	_	_
A	_	_
graphical	_	_
representation	_	_
of	_	_
the	_	_
model	_	_
(	_	_
showing	_	_
only	_	_
the	_	_
1st	_	_
strategy	_	_
)	_	_
.	_	_

#139
Gray	_	_
circles	_	_
indicate	_	_
observed	_	_
data	_	_
,	_	_
whereas	_	_
white	_	_
circles	_	_
indicate	_	_
variables	_	_
that	_	_
are	_	_
either	_	_
estimated	_	_
(	_	_
Wv	_	_
,	_	_
Wa	_	_
,	_	_
µ	_	_
and	_	_
z	_	_
)	_	_
or	_	_
marginalised	_	_
out	_	_
(	_	_
A	_	_
)	_	_
.	_	_

#140
The	_	_
plate	_	_
indicates	_	_
replication	_	_
over	_	_
all	_	_
images	_	_
.	_	_

#141
2	_	_
METHODS	_	_
11	_	_

#142
shape	_	_
.	_	_

#143
This	_	_
may	_	_
be	_	_
denoted	_	_
by	_	_
an	_	_
=	_	_
µ+	_	_
Ka∑	_	_
k=1	_	_
wa	_	_
kzk	_	_
,	_	_
n	_	_
,	_	_
(	_	_
13	_	_
)	_	_
vn	_	_
=	_	_
Kv∑	_	_
k=1	_	_
wv	_	_
kzKa+k	_	_
,	_	_
n	_	_
.	_	_

#144
(	_	_
14	_	_
)	_	_
For	_	_
simplicity	_	_
,	_	_
only	_	_
the	_	_
form	_	_
where	_	_
each	_	_
latent	_	_
variable	_	_
controls	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
is	_	_
described	_	_
in	_	_
detail	_	_
.	_	_

#145
This	_	_
is	_	_
the	_	_
form	_	_
used	_	_
in	_	_
active	_	_
appearance	_	_
models	_	_
[	_	_
10	_	_
]	_	_
.	_	_

#146
Note	_	_
however	_	_
,	_	_
that	_	_
in	_	_
the	_	_
form	_	_
where	_	_
shape	_	_
and	_	_
appearance	_	_
are	_	_
controlled	_	_
by	_	_
separate	_	_
latent	_	_
variables	_	_
,	_	_
the	_	_
precision	_	_
matrix	_	_
A	_	_
still	_	_
encodes	_	_
covariance	_	_
between	_	_
the	_	_
two	_	_
types	_	_
of	_	_
variables	_	_
.	_	_

#147
This	_	_
means	_	_
that	_	_
latent	_	_
variables	_	_
controlling	_	_
either	_	_
shape	_	_
or	_	_
appearance	_	_
are	_	_
not	_	_
estimated	_	_
completely	_	_
independently	_	_
.	_	_

#148
2.1	_	_
.	_	_

#149
Shape	_	_
Model	_	_
Relative	_	_
shapes	_	_
are	_	_
encoded	_	_
by	_	_
diffeomorphisms	_	_
,	_	_
which	_	_
are	_	_
each	_	_
parameterised	_	_
by	_	_
a	_	_
set	_	_
of	_	_
initial	_	_
conditions	_	_
(	_	_
vn	_	_
=	_	_
Wvzn	_	_
)	_	_
.	_	_

#150
This	_	_
subsection	_	_
mixes	_	_
both	_	_
discrete	_	_
and	_	_
continuous	_	_
representations	_	_
of	_	_
the	_	_
same	_	_
objects	_	_
,	_	_
so	_	_
some	_	_
notation	_	_
is	_	_
now	_	_
introduced	_	_
.	_	_

#151
For	_	_
the	_	_
discrete	_	_
case	_	_
,	_	_
where	_	_
a	_	_
velocity	_	_
field	_	_
is	_	_
treated	_	_
as	_	_
a	_	_
vector	_	_
,	_	_
it	_	_
are	_	_
denoted	_	_
by	_	_
vn	_	_
.	_	_

#152
Alternatively	_	_
,	_	_
the	_	_
same	_	_
object	_	_
may	_	_
be	_	_
treated	_	_
as	_	_
a	_	_
continuous	_	_
3D	_	_
vector	_	_
field	_	_
,	_	_
where	_	_
it	_	_
is	_	_
denoted	_	_
by	_	_
vn	_	_
.	_	_

#153
In	_	_
addition	_	_
,	_	_
deformations	_	_
may	_	_
be	_	_
treated	_	_
as	_	_
discrete	_	_
or	_	_
continuous	_	_
.	_	_

#154
Within	_	_
the	_	_
continuous	_	_
setting	_	_
,	_	_
warping	_	_
an	_	_
image	_	_
by	_	_
a	_	_
diffeomorphism	_	_
ψ	_	_
may	_	_
be	_	_
denoted	_	_
by	_	_
a′	_	_
=	_	_
a	_	_
(	_	_
ψ	_	_
)	_	_
.	_	_

#155
In	_	_
the	_	_
discrete	_	_
setting	_	_
,	_	_
this	_	_
resampling	_	_
may	_	_
be	_	_
conceptualised	_	_
as	_	_
a	_	_
matrix	_	_
multiplication	_	_
,	_	_
where	_	_
a	_	_
very	_	_
large	_	_
sparse	_	_
matrix	_	_
Ψ	_	_
encodes	_	_
the	_	_
same	_	_
deformation	_	_
(	_	_
and	_	_
associated	_	_
trilinear	_	_
interpolation	_	_
)	_	_
,	_	_
such	_	_
that	_	_
a′	_	_
=	_	_
Ψa	_	_
.	_	_

#156
The	_	_
transpose	_	_
of	_	_
this	_	_
matrix	_	_
can	_	_
be	_	_
used	_	_
to	_	_
perform	_	_
a	_	_
push-forward	_	_
operation	_	_
,	_	_
which	_	_
is	_	_
frequently	_	_
used	_	_
in	_	_
this	_	_
work	_	_
and	_	_
which	_	_
we	_	_
denote	_	_
by	_	_
f	_	_
′	_	_
=	_	_
ΨT	_	_
f	_	_
.	_	_

#157
Diffeomorphisms	_	_
are	_	_
created	_	_
within	_	_
the	_	_
Large-Deformation	_	_
Diffeomorphic	_	_
Metric	_	_
Mapping	_	_
(	_	_
LDDMM	_	_
)	_	_
framework	_	_
[	_	_
23	_	_
]	_	_
,	_	_
which	_	_
allows	_	_
image	_	_
registration	_	_
to	_	_
produce	_	_
smooth	_	_
,	_	_
invertible	_	_
one-to-one	_	_
mappings	_	_
.	_	_

#158
A	_	_
simplification	_	_
of	_	_
the	_	_
basic	_	_
idea	_	_
is	_	_
that	_	_
large	_	_
deformation	_	_
diffeomorphisms	_	_
may	_	_
be	_	_
computed	_	_
by	_	_
composing	_	_

#159
2	_	_
METHODS	_	_
12	_	_

#160
together	_	_
a	_	_
series	_	_
of	_	_
much	_	_
smaller	_	_
deformations	_	_
.	_	_

#161
Providing	_	_
the	_	_
constituent	_	_
deformations	_	_
are	_	_
one-to-one	_	_
,	_	_
then	_	_
the	_	_
result	_	_
of	_	_
their	_	_
composition	_	_
should	_	_
also	_	_
be	_	_
one-to-one	_	_
.	_	_

#162
The	_	_
original	_	_
LDDMM	_	_
implementation	_	_
involved	_	_
a	_	_
variational	_	_
optimisation	_	_
,	_	_
which	_	_
involves	_	_
estimating	_	_
a	_	_
series	_	_
of	_	_
velocity	_	_
fields	_	_
(	_	_
vt	_	_
)	_	_
at	_	_
different	_	_
“time-points”	_	_
,	_	_
t.	_	_
An	_	_
alternative	_	_
to	_	_
the	_	_
variational	_	_
approach	_	_
involves	_	_
estimating	_	_
only	_	_
an	_	_
initial	_	_
velocity	_	_
(	_	_
v0	_	_
)	_	_
,	_	_
and	_	_
deriving	_	_
velocities	_	_
at	_	_
subsequent	_	_
time	_	_
points	_	_
by	_	_
a	_	_
procedure	_	_
known	_	_
as	_	_
“geodesic	_	_
shooting”	_	_
[	_	_
24	_	_
]	_	_
.	_	_

#163
Algorithm	_	_
4	_	_
in	_	_
the	_	_
Appendix	_	_
presents	_	_
a	_	_
simple	_	_
algorithm	_	_
for	_	_
geodesic	_	_
shooting	_	_
,	_	_
which	_	_
generates	_	_
a	_	_
deformation	_	_
ψ	_	_
from	_	_
an	_	_
initial	_	_
velocity	_	_
field	_	_
v.	_	_
Another	_	_
important	_	_
feature	_	_
is	_	_
that	_	_
when	_	_
image	_	_
registration	_	_
is	_	_
formulated	_	_
as	_	_
a	_	_
generative	_	_
model	_	_
of	_	_
the	_	_
individual	_	_
images	_	_
,	_	_
the	_	_
diffeomorphic	_	_
framework	_	_
can	_	_
be	_	_
set	_	_
up	_	_
such	_	_
that	_	_
the	_	_
parametrisation	_	_
of	_	_
the	_	_
deformations	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
initial	_	_
velocity	_	_
fields	_	_
)	_	_
are	_	_
all	_	_
in	_	_
alignment	_	_
with	_	_
each	_	_
other	_	_
in	_	_
the	_	_
space	_	_
of	_	_
the	_	_
template	_	_
.	_	_

#164
This	_	_
makes	_	_
them	_	_
more	_	_
useful	_	_
for	_	_
various	_	_
forms	_	_
of	_	_
multivariate	_	_
models	_	_
of	_	_
shape	_	_
variability	_	_
as	_	_
they	_	_
are	_	_
better	_	_
encoded	_	_
by	_	_
factorisation	_	_
models	_	_
.	_	_

#165
2.1.1	_	_
.	_	_

#166
Differential	_	_
operator	_	_
for	_	_
shape	_	_
model	_	_
Our	_	_
implementation	_	_
of	_	_
geodesic	_	_
shooting	_	_
uses	_	_
Fast	_	_
Fourier	_	_
Transform	_	_
(	_	_
FFT	_	_
)	_	_
methods	_	_
to	_	_
obtain	_	_
the	_	_
Green’s	_	_
function	_	_
[	_	_
25	_	_
]	_	_
.	_	_

#167
Because	_	_
FFTs	_	_
are	_	_
used	_	_
throughout	_	_
the	_	_
work	_	_
,	_	_
the	_	_
boundary	_	_
conditions	_	_
for	_	_
the	_	_
velocity	_	_
fields	_	_
are	_	_
assumed	_	_
to	_	_
be	_	_
periodic	_	_
.	_	_

#168
The	_	_
precision	_	_
matrix	_	_
used	_	_
in	_	_
Eqn	_	_
.	_	_

#169
7	_	_
has	_	_
the	_	_
form	_	_
vTLvv	_	_
=	_	_
∫	_	_
x∈Ω	_	_
(	_	_
ωv0‖v	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
+	_	_
ωv1‖∇v	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
+	_	_
ωv2‖∇2v	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
)	_	_
dx	_	_
+	_	_
∫	_	_
x∈Ω	_	_
(	_	_
ωv3	_	_
‖Dv	_	_
(	_	_
x	_	_
)	_	_
+	_	_
(	_	_
Dv	_	_
(	_	_
x	_	_
)	_	_
)	_	_
T	_	_
‖2F	_	_
+	_	_
ωv4	_	_
Tr	_	_
(	_	_
Dv	_	_
(	_	_
x	_	_
)	_	_
)	_	_
2	_	_
)	_	_
dx	_	_
(	_	_
15	_	_
)	_	_
where	_	_
|	_	_
·	_	_
|F	_	_
denotes	_	_
the	_	_
Frobenius	_	_
norm	_	_
(	_	_
the	_	_
square	_	_
root	_	_
of	_	_
the	_	_
sum	_	_
of	_	_
squares	_	_
of	_	_
the	_	_
matrix	_	_
elements	_	_
)	_	_
.	_	_

#170
The	_	_
above	_	_
integral	_	_
is	_	_
defined	_	_
in	_	_
Sobolev	_	_
space	_	_
,	_	_
which	_	_
is	_	_
a	_	_
weighted	_	_
Hilbert	_	_
space	_	_
where	_	_
spatial	_	_
derivatives	_	_
,	_	_
up	_	_
to	_	_
a	_	_
certain	_	_
degree	_	_
,	_	_
are	_	_
accounted	_	_
for	_	_
.	_	_

#171
Five	_	_
hyper-parameters	_	_
are	_	_
involved	_	_
:	_	_
•	_	_
ωv0	_	_
controls	_	_
absolute	_	_
displacements	_	_
,	_	_
and	_	_
is	_	_
typically	_	_
set	_	_
to	_	_
be	_	_
a	_	_
very	_	_
small	_	_
value	_	_
.	_	_

#172
2	_	_
METHODS	_	_
13	_	_

#173
•	_	_
ωv1	_	_
controls	_	_
stretching	_	_
,	_	_
shearing	_	_
and	_	_
rotation	_	_
.	_	_

#174
•	_	_
ωv2	_	_
controls	_	_
bending	_	_
energy	_	_
.	_	_

#175
This	_	_
ensures	_	_
that	_	_
the	_	_
resulting	_	_
velocity	_	_
fields	_	_
have	_	_
smooth	_	_
spatial	_	_
derivatives	_	_
.	_	_

#176
•	_	_
ωv3	_	_
controls	_	_
stretching	_	_
and	_	_
shearing	_	_
(	_	_
but	_	_
not	_	_
rotation	_	_
)	_	_
.	_	_

#177
•	_	_
ωv4	_	_
controls	_	_
the	_	_
divergence	_	_
,	_	_
which	_	_
in	_	_
turn	_	_
determines	_	_
the	_	_
amount	_	_
of	_	_
volumetric	_	_
expansion	_	_
and	_	_
contraction	_	_
.	_	_

#178
Most	_	_
of	_	_
the	_	_
regularisation	_	_
in	_	_
this	_	_
work	_	_
was	_	_
typically	_	_
based	_	_
on	_	_
a	_	_
combination	_	_
of	_	_
the	_	_
linear-elasticity	_	_
(	_	_
ωv3	_	_
and	_	_
ωv4	_	_
)	_	_
and	_	_
bending	_	_
energy	_	_
(	_	_
ωv2	_	_
)	_	_
penalties	_	_
.	_	_

#179
The	_	_
differential	_	_
operator	_	_
involved	_	_
very	_	_
little	_	_
penalty	_	_
against	_	_
absolute	_	_
displacements	_	_
(	_	_
ωv0	_	_
)	_	_
,	_	_
although	_	_
some	_	_
was	_	_
necessary	_	_
to	_	_
ensure	_	_
that	_	_
the	_	_
Green’s	_	_
function	_	_
could	_	_
be	_	_
computed	_	_
,	_	_
while	_	_
still	_	_
allowing	_	_
the	_	_
deformations	_	_
to	_	_
incorporate	_	_
uniform	_	_
translations	_	_
.	_	_

#180
The	_	_
types	_	_
of	_	_
differential	_	_
operators	_	_
used	_	_
in	_	_
[	_	_
23	_	_
]	_	_
would	_	_
be	_	_
constructed	_	_
from	_	_
ωv0	_	_
,	_	_
ωv1	_	_
and	_	_
ωv2	_	_
,	_	_
whereas	_	_
those	_	_
used	_	_
in	_	_
[	_	_
26	_	_
]	_	_
were	_	_
largely	_	_
based	_	_
on	_	_
Lamé’s	_	_
constants	_	_
ωv3	_	_
and	_	_
ωv4	_	_
.	_	_

#181
The	_	_
latter	_	_
parametrisation	_	_
allows	_	_
the	_	_
stresses	_	_
and	_	_
strains	_	_
of	_	_
deformations	_	_
to	_	_
be	_	_
formulated	_	_
in	_	_
terms	_	_
of	_	_
,	_	_
for	_	_
example	_	_
,	_	_
Poisson’s	_	_
ratio	_	_
and	_	_
Young’s	_	_
modulus3	_	_
.	_	_

#182
2.2	_	_
.	_	_

#183
Appearance	_	_
Models	_	_
A	_	_
number	_	_
of	_	_
different	_	_
choices	_	_
for	_	_
the	_	_
appearance	_	_
model	_	_
are	_	_
available	_	_
for	_	_
Eqn	_	_
.	_	_

#184
4	_	_
,	_	_
each	_	_
suitable	_	_
for	_	_
modelling	_	_
different	_	_
types	_	_
of	_	_
image	_	_
data	_	_
.	_	_

#185
These	_	_
models	_	_
are	_	_
based	_	_
on	_	_
p	_	_
(	_	_
fn|a′n	_	_
)	_	_
,	_	_
which	_	_
leads	_	_
to	_	_
an	_	_
“energy”	_	_
term	_	_
(	_	_
J	_	_
)	_	_
that	_	_
drives	_	_
the	_	_
model	_	_
fitting	_	_
and	_	_
is	_	_
assumed	_	_
to	_	_
be	_	_
independent	_	_
across	_	_
voxels	_	_
a′n	_	_
=	_	_
Ψn	_	_
(	_	_
µ+	_	_
Wazn	_	_
)	_	_
(	_	_
16	_	_
)	_	_
J	_	_
(	_	_
a′n	_	_
)	_	_
=	_	_
−	_	_
ln	_	_
p	_	_
(	_	_
fn|a′n	_	_
)	_	_
=	_	_
−	_	_
M∑	_	_
m=1	_	_
ln	_	_
p	_	_
(	_	_
fmn|a′mn	_	_
)	_	_
.	_	_

#186
(	_	_
17	_	_
)	_	_
Because	_	_
the	_	_
approach	_	_
is	_	_
generative	_	_
,	_	_
missing	_	_
data	_	_
can	_	_
be	_	_
handled	_	_
by	_	_
simply	_	_
ignoring	_	_
those	_	_
voxels	_	_
where	_	_
there	_	_
is	_	_
no	_	_
information	_	_
.	_	_

#187
By	_	_
doing	_	_
this	_	_
,	_	_
they	_	_
do	_	_
not	_	_
3See	_	_
https	_	_
:	_	_
//en.wikipedia.org/wiki/Linear_elasticity	_	_
.	_	_

#188
2	_	_
METHODS	_	_
14	_	_

#189
contribute	_	_
towards	_	_
the	_	_
objective	_	_
function	_	_
and	_	_
play	_	_
no	_	_
role	_	_
in	_	_
driving	_	_
the	_	_
model	_	_
fitting	_	_
.	_	_

#190
A	_	_
number	_	_
of	_	_
different	_	_
“energy”	_	_
functions	_	_
have	_	_
been	_	_
implemented	_	_
for	_	_
modelling	_	_
different	_	_
types	_	_
of	_	_
data	_	_
.	_	_

#191
These	_	_
are	_	_
listed	_	_
next	_	_
.	_	_

#192
2.2.1	_	_
.	_	_

#193
Gaussian	_	_
noise	_	_
model	_	_
Mean-squares	_	_
difference	_	_
is	_	_
a	_	_
widely	_	_
used	_	_
objective	_	_
functions	_	_
for	_	_
image	_	_
matching	_	_
,	_	_
which	_	_
is	_	_
based	_	_
on	_	_
the	_	_
assumption	_	_
of	_	_
stationary	_	_
Gaussian	_	_
noise	_	_
.	_	_

#194
For	_	_
an	_	_
image	_	_
consisting	_	_
of	_	_
M	_	_
pixels	_	_
or	_	_
voxels	_	_
,	_	_
the	_	_
function	_	_
would	_	_
be	_	_
−JL2	_	_
(	_	_
a′	_	_
)	_	_
=	_	_
ln	_	_
p	_	_
(	_	_
f	_	_
|a′	_	_
,	_	_
σ2	_	_
)	_	_
=	_	_
−M2	_	_
ln	_	_
(	_	_
2π	_	_
)	_	_
−	_	_
M	_	_
2	_	_
lnσ2	_	_
−	_	_
1	_	_
2σ2	_	_
||f	_	_
−	_	_
a′||22	_	_
,	_	_
(	_	_
18	_	_
)	_	_
where	_	_
||	_	_
·	_	_
||2	_	_
denotes	_	_
the	_	_
Euclidean	_	_
norm	_	_
.	_	_

#195
The	_	_
simplest	_	_
approach	_	_
to	_	_
compute	_	_
σ2	_	_
is	_	_
to	_	_
make	_	_
a	_	_
maximum	_	_
likelihood	_	_
estimate	_	_
from	_	_
the	_	_
variance	_	_
by	_	_
σ̂2	_	_
=	_	_
1	_	_
MN	_	_
N∑	_	_
n=1	_	_
||fn	_	_
−	_	_
a′n||22.	_	_
(	_	_
19	_	_
)	_	_

#196
2.2.2.	_	_
Logistic	_	_
function	_	_
with	_	_
Bernoulli	_	_
noise	_	_
model	_	_

#197
When	_	_
working	_	_
with	_	_
binary	_	_
images	_	_
,	_	_
such	_	_
as	_	_
single	_	_
tissue	_	_
type	_	_
maps	_	_
having	_	_
voxels	_	_
of	_	_
zeros	_	_
and	_	_
ones	_	_
(	_	_
or	_	_
values	_	_
very	_	_
close	_	_
to	_	_
zero	_	_
or	_	_
one	_	_
)	_	_
,	_	_
it	_	_
may	_	_
be	_	_
better	_	_
to	_	_
work	_	_
under	_	_
the	_	_
assumption	_	_
that	_	_
voxels	_	_
are	_	_
drawn	_	_
from	_	_
a	_	_
Bernoulli	_	_
distribution	_	_
,	_	_
which	_	_
is	_	_
a	_	_
special	_	_
case	_	_
of	_	_
the	_	_
binomial	_	_
distribution	_	_
.	_	_

#198
For	_	_
a	_	_
single	_	_
voxel	_	_
,	_	_
P	_	_
(	_	_
f	_	_
|s	_	_
)	_	_
=	_	_
sf	_	_
(	_	_
1−	_	_
s	_	_
)	_	_
1−f	_	_
.	_	_
(	_	_
20	_	_
)	_	_

#199
The	_	_
range	_	_
0	_	_
<	_	_
s	_	_
<	_	_
1	_	_
must	deontic	_
be	_	_
satisfied	_	_
,	_	_
which	_	_
can	_	_
be	_	_
achieved	_	_
using	_	_
a	_	_
logistic	_	_
sigmoid	_	_
function	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
=	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
−a′	_	_
)	_	_
.	_	_
(	_	_
21	_	_
)	_	_

#200
Putting	_	_
these	_	_
together	_	_
gives	_	_
P	_	_
(	_	_
f	_	_
|a′	_	_
)	_	_
=	_	_
(	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
−a′	_	_
)	_	_
)	_	_
f	_	_
(	_	_
1−	_	_
1	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
−a′	_	_
)	_	_
)	_	_
1−f	_	_
=	_	_
exp	_	_
(	_	_
a′f	_	_
)	_	_
s	_	_
(	_	_
−a′	_	_
)	_	_
.	_	_
(	_	_
22	_	_
)	_	_

#201
This	_	_
gives	_	_
the	_	_
matching	_	_
function	_	_
−JBern	_	_
(	_	_
a′	_	_
)	_	_
=	_	_
lnP	_	_
(	_	_
f	_	_
|a′	_	_
)	_	_
=	_	_
M∑	_	_
m=1	_	_
(	_	_
fma	_	_
′	_	_
m	_	_
+	_	_
ln	_	_
s	_	_
(	_	_
−a′m	_	_
)	_	_
)	_	_
.	_	_
(	_	_
23	_	_
)	_	_

#202
2	_	_
METHODS	_	_
15	_	_

#203
2.2.3	_	_
.	_	_

#204
Softmax	_	_
function	_	_
with	_	_
categorical	_	_
noise	_	_
model	_	_
If	_	_
there	_	_
are	_	_
several	_	_
binary	_	_
maps	_	_
to	_	_
align	_	_
simultaneously	_	_
,	_	_
for	_	_
example	_	_
maps	_	_
of	_	_
grey	_	_
matter	_	_
,	_	_
white	_	_
matter	_	_
and	_	_
background	_	_
,	_	_
then	_	_
a	_	_
categorical	_	_
noise	_	_
model	_	_
is	_	_
appropriate	_	_
.	_	_

#205
A	_	_
categorical	_	_
distribution	_	_
is	_	_
a	_	_
generalisation	_	_
of	_	_
the	_	_
Bernoulli	_	_
distribution	_	_
,	_	_
and	_	_
also	_	_
a	_	_
special	_	_
case	_	_
of	_	_
the	_	_
multinomial	_	_
distribution	_	_
.	_	_

#206
The	_	_
probability	_	_
of	_	_
a	_	_
vector	_	_
f	_	_
of	_	_
length	_	_
C	_	_
,	_	_
such	_	_
that	_	_
fc	_	_
∈	_	_
{	_	_
0	_	_
,	_	_
1	_	_
}	_	_
and	_	_
∑C	_	_
c=1	_	_
fc	_	_
=	_	_
1	_	_
,	_	_
is	_	_
given	_	_
by	_	_
P	_	_
(	_	_
f	_	_
|s	_	_
)	_	_
=	_	_
C∏	_	_
c=1	_	_
sfcc	_	_
,	_	_
(	_	_
24	_	_
)	_	_
where	_	_
sc	_	_
>	_	_
0	_	_
and	_	_
∑C	_	_
c=1	_	_
sc	_	_
=	_	_
1	_	_
.	_	_

#207
The	_	_
constraints	_	_
on	_	_
s	_	_
can	_	_
be	_	_
enforced	_	_
by	_	_
using	_	_
a	_	_
softmax	_	_
function	_	_
.	_	_

#208
sc	_	_
(	_	_
a	_	_
′	_	_
)	_	_
=	_	_
exp	_	_
a′c∑C	_	_
c=1	_	_
exp	_	_
a′c	_	_
(	_	_
25	_	_
)	_	_
Using	_	_
the	_	_
“log-sum-exp	_	_
trick”	_	_
,	_	_
numerical	_	_
overflow	_	_
or	_	_
underflow	_	_
can	_	_
be	_	_
prevented	_	_
by	_	_
first	_	_
subtracting	_	_
the	_	_
maximum	_	_
of	_	_
a	_	_
,	_	_
so	_	_
sc	_	_
(	_	_
a	_	_
′	_	_
)	_	_
=	_	_
exp	_	_
(	_	_
a′c	_	_
−	_	_
a∗	_	_
)	_	_
∑C	_	_
c=1	_	_
exp	_	_
(	_	_
a′c	_	_
−	_	_
a∗	_	_
)	_	_
,	_	_
where	_	_
a∗	_	_
=	_	_
max	_	_
{	_	_
a′1	_	_
,	_	_
.	_	_

#209
.	_	_

#210
.	_	_

#211
,	_	_
a′C	_	_
}	_	_
(	_	_
26	_	_
)	_	_
Noting	_	_
that	_	_
each	_	_
image	_	_
is	_	_
now	_	_
a	_	_
matrix	_	_
of	_	_
M	_	_
voxels	_	_
and	_	_
C	_	_
classes	_	_
,	_	_
the	_	_
objective	_	_
function	_	_
can	_	_
then	_	_
be	_	_
computed	_	_
as	_	_
−Jcat	_	_
(	_	_
A′	_	_
)	_	_
=	_	_
lnP	_	_
(	_	_
F|A′	_	_
)	_	_
=	_	_
M∑	_	_
m=1	_	_
(	_	_
C∑	_	_
c=1	_	_
a′mcfmc	_	_
−	_	_
a∗	_	_
−	_	_
log	_	_
(	_	_
C∑	_	_
c=1	_	_
exp	_	_
(	_	_
a′mc	_	_
−	_	_
a∗m	_	_
)	_	_
)	_	_
)	_	_
(	_	_
27	_	_
)	_	_
2.2.4	_	_
.	_	_

#212
Differential	_	_
operator	_	_
for	_	_
appearance	_	_
model	_	_
Regularisation	_	_
is	_	_
required	_	_
for	_	_
the	_	_
appearance	_	_
variability	_	_
,	_	_
as	_	_
it	_	_
helps	_	_
to	_	_
prevent	_	_
the	_	_
appearance	_	_
model	_	_
from	_	_
absorbing	_	_
too	_	_
much	_	_
of	_	_
the	_	_
variance	_	_
,	_	_
at	_	_
the	_	_
expense	_	_
of	_	_
the	_	_
shape	_	_
model	_	_
.	_	_

#213
This	_	_
differential	_	_
operator	_	_
(	_	_
again	_	_
based	_	_
on	_	_
a	_	_
Sobolev	_	_
space	_	_
)	_	_
is	_	_
used	_	_
in	_	_
Eqns	_	_
.	_	_

#214
6	_	_
and	_	_
8	_	_
,	_	_
and	_	_
controlled	_	_
by	_	_
three	_	_
hyper-parameters	_	_
.	_	_

#215
aTLaa	_	_
=	_	_
∫	_	_
x∈Ω	_	_
(	_	_
ωa0‖a	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
+	_	_
ωa1‖∇a	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
+	_	_
ωa2‖∇2a	_	_
(	_	_
x	_	_
)	_	_
‖2	_	_
)	_	_
dx	_	_
(	_	_
28	_	_
)	_	_

#216
3	_	_
RESULTS	_	_
16	_	_

#217
The	_	_
extent	_	_
to	_	_
which	_	_
intensity	_	_
differences	_	_
may	_	_
influence	_	_
apparent	_	_
spatial	_	_
deformations	_	_
was	_	_
briefly	_	_
discussed	_	_
in	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#218
Automatically	_	_
disambiguating	_	_
between	_	_
what	_	_
should	_	_
be	_	_
modelled	_	_
as	_	_
a	_	_
shape	_	_
change	_	_
,	_	_
and	_	_
what	_	_
should	_	_
be	_	_
treated	_	_
as	_	_
as	_	_
appearance	_	_
change	_	_
,	_	_
is	_	_
still	_	_
a	_	_
largely	_	_
unresolved	_	_
area	_	_
.	_	_

#219
The	_	_
approach	_	_
taken	_	_
in	_	_
this	_	_
work	_	_
is	_	_
simply	_	_
to	_	_
treat	_	_
the	_	_
construct	_	_
as	_	_
a	_	_
model	_	_
of	_	_
the	_	_
data	_	_
,	_	_
and	_	_
to	_	_
assess	_	_
it	_	_
according	_	_
to	_	_
how	_	_
well	_	_
it	_	_
describes	_	_
and	_	_
predicts	_	_
the	_	_
observations	_	_
,	_	_
rather	_	_
than	_	_
how	_	_
well	_	_
it	_	_
can	_	_
separately	_	_
estimate	_	_
shape	_	_
information	_	_
versus	_	_
appearance	_	_
information	_	_
.	_	_

#220
3	_	_
.	_	_

#221
Results	_	_
To	_	_
show	_	_
the	_	_
general	_	_
applicability	_	_
of	_	_
the	_	_
approach	_	_
,	_	_
evaluations	_	_
were	_	_
performed	_	_
with	_	_
a	_	_
number	_	_
of	_	_
datasets	_	_
of	_	_
varying	_	_
characteristics	_	_
.	_	_

#222
Our	_	_
implementation4	_	_
is	_	_
written	_	_
in	_	_
a	_	_
mixture	_	_
of	_	_
MATLAB	_	_
and	_	_
C	_	_
code	_	_
(	_	_
MATLAB	_	_
“mex”	_	_
files	_	_
for	_	_
the	_	_
computationally	_	_
expensive	_	_
parts	_	_
)	_	_
.	_	_

#223
3.1.	_	_
Qualitative	_	_
2D	_	_
experiments	_	_
with	_	_
faces	_	_

#224
After	_	_
years	_	_
of	_	_
exposure	_	_
to	_	_
faces	_	_
,	_	_
most	_	_
people	_	_
can	_	_
identify	_	_
whether	_	_
an	_	_
image	_	_
of	_	_
a	_	_
face	_	_
is	_	_
plausible	_	_
or	_	_
not	_	_
,	_	_
so	_	_
images	_	_
of	_	_
human	_	_
faces	_	_
provide	_	_
a	_	_
good	_	_
qualitative	_	_
test	_	_
of	_	_
how	_	_
well	_	_
the	_	_
algorithm	_	_
can	_	_
model	_	_
biological	_	_
variability	_	_
.	_	_

#225
Recent	_	_
evidence	_	_
[	_	_
28	_	_
]	_	_
suggests	_	_
that	_	_
the	_	_
primate	_	_
brain	_	_
might	speculation	_
even	_	_
use	_	_
some	_	_
form	_	_
of	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
for	_	_
encoding	_	_
faces	_	_
,	_	_
which	_	_
might	_	_
<	_	_
capability-rhetorical-uncertainty	_	_
>	_	_
indicate	_	_
that	_	_
the	_	_
type	_	_
of	_	_
model	_	_
used	_	_
in	_	_
this	_	_
work	_	_
could	_	_
be	_	_
quite	_	_
effective	_	_
.	_	_

#226
The	_	_
straight	_	_
on	_	_
views	_	_
from	_	_
the	_	_
Karolinska	_	_
Directed	_	_
Emotional	_	_
Faces	_	_
(	_	_
KDEF	_	_
)	_	_
data-set	_	_
[	_	_
29	_	_
]	_	_
were	_	_
used	_	_
to	_	_
make	_	_
a	_	_
visual	_	_
assessment	_	_
of	_	_
how	_	_
well	_	_
the	_	_
algorithm	_	_
performs	_	_
.	_	_

#227
This	_	_
data-set	_	_
consisted	_	_
of	_	_
photographs	_	_
of	_	_
70	_	_
participants	_	_
,	_	_
holding	_	_
seven	_	_
different	_	_
facial	_	_
expressions	_	_
,	_	_
which	_	_
was	_	_
repeated	_	_
twice	_	_
.	_	_

#228
Some	_	_
of	_	_
the	_	_
images	_	_
were	_	_
not	_	_
usable	_	_
,	_	_
so	_	_
the	_	_
final	_	_
dataset	_	_
consisted	_	_
of	_	_
932	_	_
colour	_	_
images	_	_
,	_	_
which	_	_
4Available	_	_
from	_	_
https	_	_
:	_	_
//github.com/WTCN-computational-anatomy-group/	_	_
Shape-Appearance-Model	_	_
.	_	_

#229
3	_	_
RESULTS	_	_
17	_	_

#230
were	_	_
downsampled	_	_
to	_	_
a	_	_
size	_	_
of	_	_
282	_	_
×	_	_
382	_	_
.	_	_

#231
The	_	_
original	_	_
intensities	_	_
were	_	_
in	_	_
the	_	_
range	_	_
of	_	_
0	_	_
to	_	_
255	_	_
,	_	_
but	_	_
these	_	_
values	_	_
were	_	_
re-scaled	_	_
by	_	_
1/255	_	_
.	_	_

#232
A	_	_
64	_	_
eigenmode	_	_
model	_	_
was	_	_
used	_	_
(	_	_
K	_	_
=	_	_
64	_	_
)	_	_
,	_	_
which	_	_
assumed	_	_
Gaussian	_	_
noise	_	_
.	_	_

#233
Model	_	_
fitting	_	_
(	_	_
i.e.	_	_
,	_	_
learning	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
basis	_	_
functions	_	_
,	_	_
etc	_	_
.	_	_
)	_	_

#234
was	_	_
run	_	_
for	_	_
20	_	_
iterations	_	_
,	_	_
with	_	_
ν0	_	_
=	_	_
1000	_	_
,	_	_
λ	_	_
=	_	_
[	_	_
15.2	_	_
0.8	_	_
]	_	_
,	_	_
ωa	_	_
=	_	_
[	_	_
4	_	_
512	_	_
64	_	_
]	_	_
,	_	_
ωµ	_	_
=	_	_
N	_	_
[	_	_
10−4	_	_
0.1	_	_
0.1	_	_
]	_	_
and	_	_
ωv	_	_
=	_	_
[	_	_
10−3	_	_
0	_	_
16	_	_
1	_	_
1	_	_
]	_	_
.	_	_

#235
It	_	_
was	_	_
fit	_	_
to	_	_
the	_	_
entire	_	_
field	_	_
of	_	_
view	_	_
of	_	_
the	_	_
images	_	_
,	_	_
rather	_	_
than	_	_
focusing	_	_
only	_	_
on	_	_
the	_	_
faces	_	_
,	_	_
and	_	_
some	_	_
of	_	_
the	_	_
resulting	_	_
fits	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
2	_	_
.	_	_

#236
The	_	_
first	_	_
set	_	_
of	_	_
images	_	_
are	_	_
a	_	_
random	_	_
selection	_	_
of	_	_
the	_	_
original	_	_
data	_	_
,	_	_
with	_	_
the	_	_
full	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
fits	_	_
shown	_	_
immediately	_	_
below	_	_
.	_	_

#237
As	_	_
can	_	_
be	_	_
seen	_	_
,	_	_
the	_	_
fit	_	_
is	_	_
reasonably	_	_
good	_	_
-	_	_
especially	_	_
given	_	_
that	_	_
only	_	_
64	_	_
modes	_	_
of	_	_
variability	_	_
were	_	_
used	_	_
,	_	_
and	_	_
that	_	_
these	_	_
have	_	_
to	_	_
account	_	_
for	_	_
a	_	_
lot	_	_
of	_	_
variability	_	_
of	_	_
hair	_	_
etc	_	_
.	_	_

#238
Below	_	_
these	_	_
are	_	_
the	_	_
shape	_	_
model	_	_
fits	_	_
,	_	_
generated	_	_
by	_	_
warping	_	_
the	_	_
mean	_	_
according	_	_
to	_	_
the	_	_
estimated	_	_
deformations	_	_
(	_	_
µ	_	_
(	_	_
ψn	_	_
)	_	_
)	_	_
.	_	_

#239
The	_	_
appearance	_	_
fits	_	_
are	_	_
shown	_	_
at	_	_
the	_	_
bottom	_	_
(	_	_
an	_	_
from	_	_
Eqn	_	_
.	_	_

#240
4	_	_
)	_	_
.	_	_

#241
Ideally	_	_
,	_	_
these	_	_
reconstructions	_	_
of	_	_
appearance	_	_
should	_	_
be	_	_
in	_	_
perfect	_	_
alignment	_	_
with	_	_
each	_	_
other	_	_
,	_	_
which	_	_
is	_	_
not	_	_
quite	_	_
achieved	_	_
in	_	_
certain	_	_
parts	_	_
of	_	_
the	_	_
images	_	_
.	_	_

#242
In	_	_
particular	_	_
,	_	_
the	_	_
thickness	_	_
of	_	_
the	_	_
neck	_	_
varies	_	_
according	_	_
to	_	_
whether	_	_
or	_	_
not	_	_
the	_	_
people	_	_
in	_	_
the	_	_
images	_	_
have	_	_
short	_	_
or	_	_
long	_	_
hair	_	_
.	_	_

#243
When	_	_
looked	_	_
at	_	_
separately	_	_
,	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
parts	_	_
of	_	_
the	_	_
model	_	_
do	_	_
not	_	_
behave	_	_
quite	_	_
so	_	_
well	_	_
,	_	_
but	_	_
when	_	_
combined	_	_
,	_	_
they	_	_
give	_	_
quite	_	_
a	_	_
good	_	_
fit	_	_
.	_	_

#244
Fig.	_	_
3	_	_
shows	_	_
a	_	_
simple	_	_
64-mode	_	_
principal	_	_
component	_	_
analysis	_	_
(	_	_
PCA	_	_
)	_	_
fit	_	_
to	_	_
the	_	_
same	_	_
data	_	_
,	_	_
which	_	_
clearly	_	_
does	_	_
not	_	_
capture	_	_
variability	_	_
quite	_	_
as	_	_
well	_	_
as	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
.	_	_

#245
The	_	_
first	_	_
12	_	_
modes	_	_
of	_	_
variability	_	_
are	_	_
illustrated	_	_
in	_	_
Fig.	_	_
4	_	_
.	_	_

#246
Much	_	_
of	_	_
the	_	_
variance	_	_
to	_	_
be	_	_
modelled	_	_
is	_	_
not	_	_
actually	_	_
part	_	_
of	_	_
the	_	_
face	_	_
,	_	_
which	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
how	_	_
the	_	_
hairstyles	_	_
of	_	_
the	_	_
subjects	_	_
drive	_	_
the	_	_
first	_	_
few	_	_
modes	_	_
.	_	_

#247
Better	_	_
fits	_	_
would	_	_
have	_	_
been	_	_
achieved	_	_
if	_	_
only	_	_
the	_	_
faces	_	_
themselves	_	_
had	_	_
been	_	_
modelled	_	_
,	_	_
but	_	_
the	_	_
main	_	_
aim	_	_
here	_	_
was	_	_
to	_	_
illustrate	_	_
the	_	_
behaviour	_	_
of	_	_
the	_	_
approach	_	_
in	_	_
more	_	_
challenging	_	_
situations	_	_
.	_	_

#248
Masking	_	_
out	_	_
parts	_	_
of	_	_
the	_	_
data	_	_
from	_	_
model	_	_
fitting	_	_
is	_	_
possible	_	_
,	_	_
as	_	_
one	_	_
of	_	_
the	_	_
objectives	_	_
of	_	_
the	_	_
work	_	_
was	_	_
to	_	_
be	_	_
able	_	_
to	_	_
handle	_	_
missing	_	_
data	_	_
(	_	_
see	_	_
Section	_	_
3.3.2	_	_
)	_	_
.	_	_

#249
For	_	_
these	_	_
examples	_	_
,	_	_
there	_	_
should	_	_
really	_	_
have	_	_
been	_	_
a	_	_
distinction	_	_
between	_	_
inter3	_	_
RESULTS	_	_
18	_	_
Original	_	_
KDEF	_	_
images	_	_
.	_	_

#250
Full	_	_
shape	_	_
and	_	_
appearance	_	_
fit	_	_
.	_	_

#251
Shape	_	_
fit	_	_
only	_	_
.	_	_

#252
Appearance	_	_
fit	_	_
only	_	_
.	_	_

#253
Figure	_	_
2	_	_
:	_	_
Shape	_	_
and	_	_
appearance	_	_
fit	_	_
shown	_	_
for	_	_
a	_	_
randomly	_	_
selected	_	_
sample	_	_
of	_	_
the	_	_
KDEF	_	_
face	_	_
images	_	_
.	_	_

#254
3	_	_
RESULTS	_	_
19	_	_

#255
Figure	_	_
3	_	_
:	_	_
Fits	_	_
using	_	_
a	_	_
simple	_	_
64-mode	_	_
principal	_	_
component	_	_
analysis	_	_
model	_	_
are	_	_
shown	_	_
above	_	_
(	_	_
cf	_	_
.	_	_
Fig.	_	_
2	_	_
)	_	_
,	_	_
and	_	_
random	_	_
faces	_	_
generated	_	_
from	_	_
the	_	_
same	_	_
PCA	_	_
model	_	_
are	_	_
shown	_	_
below	_	_
(	_	_
cf	_	_
.	_	_
Fig.	_	_
5	_	_
)	_	_
.	_	_

#256
Figure	_	_
4	_	_
:	_	_
The	_	_
first	_	_
12	_	_
modes	_	_
of	_	_
variability	_	_
,	_	_
shown	_	_
(	_	_
left	_	_
to	_	_
right	_	_
)	_	_
at	_	_
-5	_	_
,	_	_
-3	_	_
,	_	_
-1	_	_
,	_	_
1	_	_
,	_	_
3	_	_
and	_	_
5	_	_
standard	_	_
deviations	_	_
(	_	_
s.d.	_	_
)	_	_
.	_	_

#257
3	_	_
RESULTS	_	_
20	_	_

#258
subject	_	_
variability	_	_
and	_	_
intra-subject	_	_
variability	_	_
,	_	_
using	_	_
some	_	_
form	_	_
of	_	_
hierarchical	_	_
model	_	_
for	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#259
This	_	_
type	_	_
of	_	_
hierarchical	_	_
mixed-effects	_	_
model	_	_
is	_	_
widely	_	_
used	_	_
for	_	_
analysing	_	_
multi-subject	_	_
data	_	_
within	_	_
the	_	_
neuroimaging	_	_
field	_	_
[	_	_
30	_	_
]	_	_
,	_	_
and	_	_
a	_	_
number	_	_
of	_	_
works	_	_
have	_	_
applied	_	_
mixed	_	_
effects	_	_
modeling	_	_
to	_	_
image	_	_
registration	_	_
[	_	_
31	_	_
,	_	_
32	_	_
]	_	_
.	_	_

#260
Spatial	_	_
correlations	_	_
in	_	_
the	_	_
images	_	_
mean	_	_
the	_	_
assumptions	_	_
about	_	_
the	_	_
independence	_	_
of	_	_
the	_	_
noise	_	_
across	_	_
neighbouring	_	_
voxels	_	_
to	_	_
be	_	_
less	_	_
valid	_	_
,	_	_
which	_	_
causes	_	_
problems	_	_
for	_	_
the	_	_
simple	_	_
i.i.d	_	_
.	_	_

#261
noise	_	_
model	_	_
used	_	_
here	_	_
.	_	_

#262
This	_	_
is	_	_
why	_	_
the	_	_
regularisation	_	_
needed	_	_
to	_	_
be	_	_
weighted	_	_
more	_	_
heavily	_	_
relative	_	_
to	_	_
the	_	_
image	_	_
matching	_	_
term	_	_
.	_	_

#263
This	_	_
is	_	_
related	_	_
to	_	_
the	_	_
“virtual	_	_
decimation”	_	_
[	_	_
33	_	_
]	_	_
approach	_	_
of	_	_
down-weighting	_	_
the	_	_
matching	_	_
term	_	_
by	_	_
a	_	_
correction	_	_
factor	_	_
that	_	_
accounts	_	_
for	_	_
the	_	_
number	_	_
of	_	_
independent	_	_
observations	_	_
.	_	_

#264
Numbers	_	_
of	_	_
independent	_	_
observations	_	_
could	_	_
be	_	_
derived	_	_
using	_	_
random	_	_
field	_	_
theory	_	_
[	_	_
34	_	_
]	_	_
,	_	_
using	_	_
gradients	_	_
of	_	_
residuals	_	_
to	_	_
estimate	_	_
image	_	_
smoothness	_	_
in	_	_
terms	_	_
of	_	_
the	_	_
full-width	_	_
half	_	_
maximum	_	_
of	_	_
a	_	_
Gaussian	_	_
.	_	_

#265
3.1.1	_	_
.	_	_

#266
Simulating	_	_
faces	_	_
Once	_	_
the	_	_
model	_	_
is	_	_
learned	_	_
,	_	_
it	_	_
becomes	_	_
possible	_	_
to	_	_
generate	_	_
random	_	_
faces	_	_
from	_	_
the	_	_
estimated	_	_
distribution	_	_
.	_	_

#267
This	_	_
involves	_	_
drawing	_	_
a	_	_
random	_	_
vector	_	_
of	_	_
latent	_	_
variables	_	_
z	_	_
∼	_	_
N	_	_
(	_	_
0	_	_
,	_	_
Â−1	_	_
)	_	_
,	_	_
and	_	_
using	_	_
these	_	_
to	_	_
reconstruct	_	_
a	_	_
face	_	_
.	_	_

#268
Fig.	_	_
5	_	_
shows	_	_
two	_	_
sets	_	_
of	_	_
randomly	_	_
generated	_	_
faces	_	_
,	_	_
where	_	_
the	_	_
lower	_	_
set	_	_
used	_	_
the	_	_
same	_	_
latent	_	_
variables	_	_
as	_	_
the	_	_
upper	_	_
set	_	_
,	_	_
except	_	_
that	_	_
they	_	_
were	_	_
multiplied	_	_
by	_	_
-1	_	_
.	_	_

#269
Although	_	_
some	_	_
of	_	_
the	_	_
random	_	_
faces	_	_
are	_	_
not	_	_
entirely	_	_
plausible	_	_
,	_	_
they	_	_
are	_	_
much	_	_
more	_	_
realistic	_	_
than	_	_
faces	_	_
generated	_	_
from	_	_
a	_	_
simple	_	_
64-mode	_	_
PCA	_	_
model	_	_
(	_	_
shown	_	_
in	_	_
Fig.	_	_
3	_	_
)	_	_
.	_	_

#270
3.1.2	_	_
.	_	_

#271
Vector	_	_
arithmetic	_	_
In	_	_
many	_	_
machine	_	_
learning	_	_
applications	_	_
,	_	_
it	_	_
is	_	_
useful	_	_
to	_	_
be	_	_
able	_	_
to	_	_
model	_	_
certain	_	_
non-linearities	_	_
in	_	_
the	_	_
data	_	_
in	_	_
a	_	_
linear	_	_
way	_	_
,	_	_
allowing	_	_
more	_	_
interpretable	_	_
linear	_	_
methods	_	_
to	_	_
be	_	_
used	_	_
while	_	_
still	_	_
achieving	_	_
a	_	_
good	_	_
fit	_	_
.	_	_

#272
Following	_	_
[	_	_
35	_	_
]	_	_
,	_	_
this	_	_
section	_	_
shows	_	_
that	_	_
simple	_	_
arithmetic	_	_
on	_	_
the	_	_
latent	_	_
variables	_	_
can	_	_
give	_	_
intuitive	_	_
results	_	_
.	_	_

#273
The	_	_
first	_	_
three	_	_
columns	_	_
of	_	_
Fig.	_	_
6	_	_
show	_	_
the	_	_
full	_	_
shape	_	_
and	_	_
appearance	_	_

#274
3	_	_
RESULTS	_	_
21	_	_

#275
Figure	_	_
5	_	_
:	_	_
Random	_	_
faces	_	_
generated	_	_
from	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
.	_	_

#276
The	_	_
lower	_	_
set	_	_
of	_	_
faces	_	_
were	_	_
generated	_	_
with	_	_
the	_	_
same	_	_
latent	_	_
variables	_	_
as	_	_
those	_	_
shown	_	_
in	_	_
the	_	_
upper	_	_
set	_	_
,	_	_
except	_	_
the	_	_
values	_	_
were	_	_
multiplied	_	_
by	_	_
-1	_	_
and	_	_
thus	_	_
show	_	_
a	_	_
sort	_	_
of	_	_
“opposite”	_	_
face	_	_
.	_	_

#277
For	_	_
example	_	_
,	_	_
if	_	_
a	_	_
face	_	_
in	_	_
the	_	_
top	_	_
set	_	_
has	_	_
a	_	_
wide	_	_
open	_	_
mouth	_	_
,	_	_
then	_	_
the	_	_
mouth	_	_
should	_	_
be	_	_
tightly	_	_
closed	_	_
in	_	_
the	_	_
corresponding	_	_
image	_	_
of	_	_
the	_	_
bottom	_	_
set	_	_
.	_	_

#278
model	_	_
fits	_	_
to	_	_
various	_	_
faces	_	_
.	_	_

#279
Images	_	_
in	_	_
the	_	_
right	_	_
hand	_	_
column	_	_
of	_	_
Fig.	_	_
6	_	_
were	_	_
generated	_	_
by	_	_
making	_	_
linear	_	_
combinations	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
that	_	_
encode	_	_
the	_	_
images	_	_
in	_	_
the	_	_
first	_	_
three	_	_
columns	_	_
,	_	_
and	_	_
then	_	_
reconstructing	_	_
from	_	_
these	_	_
.	_	_

#280
Unlike	_	_
arithmetic	_	_
computed	_	_
in	_	_
pixel	_	_
space	_	_
(	_	_
not	_	_
shown	_	_
)	_	_
,	_	_
performing	_	_
arithmetic	_	_
on	_	_
the	_	_
vectors	_	_
encoding	_	_
the	_	_
images	_	_
gives	_	_
reasonably	_	_
plausible	_	_
results	_	_
.	_	_

#281
For	_	_
medical	_	_
imaging	_	_
applications	_	_
,	_	_
it	_	_
may	_	_
be	_	_
useful	_	_
(	_	_
for	_	_
example	_	_
)	_	_
to	_	_
transport	_	_
disease-related	_	_
changes	_	_
computed	_	_
from	_	_
one	_	_
patient	_	_
,	_	_
or	_	_
set	_	_
of	_	_
patients	_	_
,	_	_
on	_	_
to	_	_
the	_	_
image	_	_
of	_	_
a	_	_
new	_	_
individual	_	_
[	_	_
36	_	_
]	_	_
.	_	_

#282
Even	_	_
when	_	_
not	_	_
explicitly	_	_
transporting	_	_
such	_	_
information	_	_
,	_	_
it	_	_
is	_	_
useful	_	_
to	_	_
have	_	_
models	_	_
where	_	_
the	_	_
encoding	_	_
can	_	_
be	_	_
treated	_	_
in	_	_
an	_	_
approximately	_	_
linear	_	_
way	_	_
.	_	_

#283
3.2	_	_
.	_	_

#284
2D	_	_
experiments	_	_
with	_	_
MNIST	_	_
In	_	_
this	_	_
section	_	_
,	_	_
the	_	_
behaviour	_	_
of	_	_
the	_	_
approach	_	_
using	_	_
“big	_	_
data”	_	_
is	_	_
assessed	_	_
,	_	_
which	_	_
gives	_	_
more	_	_
of	_	_
an	_	_
idea	_	_
of	_	_
how	_	_
this	_	_
type	_	_
of	_	_
method	_	_
may	_	_
behave	_	_
with	_	_
some	_	_
of	_	_
the	_	_
very	_	_
large	_	_
image	_	_
datasets	_	_
currently	_	_
being	_	_
collected	_	_
.	_	_

#285
Instead	_	_
of	_	_
testing	_	_
on	_	_
a	_	_
large	_	_
collection	_	_
of	_	_
medical	_	_
images	_	_
,	_	_
the	_	_
approach	_	_
was	_	_
applied	_	_
to	_	_
a	_	_
large	_	_

#286
3	_	_
RESULTS	_	_
22	_	_

#287
Figure	_	_
6	_	_
:	_	_
An	_	_
example	_	_
of	_	_
simple	_	_
linear	_	_
additions	_	_
and	_	_
subtractions	_	_
applied	_	_
to	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#288
The	_	_
first	_	_
three	_	_
columns	_	_
show	_	_
the	_	_
full	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
fits	_	_
to	_	_
various	_	_
faces	_	_
.	_	_

#289
Images	_	_
in	_	_
the	_	_
right	_	_
hand	_	_
column	_	_
were	_	_
generated	_	_
by	_	_
making	_	_
linear	_	_
combinations	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
that	_	_
encode	_	_
the	_	_
images	_	_
in	_	_
the	_	_
first	_	_
three	_	_
columns	_	_
,	_	_
and	_	_
then	_	_
reconstructing	_	_
from	_	_
these	_	_
linear	_	_
combinations	_	_
.	_	_

#290
3	_	_
RESULTS	_	_
23	_	_

#291
set	_	_
of	_	_
tiny	_	_
images	_	_
of	_	_
hand-written	_	_
digits	_	_
.	_	_

#292
MNIST5	_	_
[	_	_
37	_	_
]	_	_
is	_	_
a	_	_
modified	_	_
version	_	_
of	_	_
the	_	_
handwritten	_	_
digits	_	_
from	_	_
the	_	_
National	_	_
Institute	_	_
of	_	_
Standards	_	_
and	_	_
Technology	_	_
(	_	_
NIST	_	_
)	_	_
Special	_	_
Database	_	_
19	_	_
.	_	_

#293
The	_	_
dataset	_	_
consists	_	_
of	_	_
a	_	_
training	_	_
set	_	_
of	_	_
60,000	_	_
28×28	_	_
pixel	_	_
images	_	_
of	_	_
the	_	_
digits	_	_
0	_	_
to	_	_
9	_	_
,	_	_
along	_	_
with	_	_
a	_	_
testing	_	_
set	_	_
of	_	_
10,000	_	_
digits	_	_
.	_	_

#294
MNIST	_	_
has	_	_
been	_	_
widely	_	_
used	_	_
for	_	_
assessing	_	_
the	_	_
accuracy	_	_
of	_	_
machine	_	_
learning	_	_
approaches	_	_
,	_	_
and	_	_
is	_	_
used	_	_
here	_	_
as	_	_
it	_	_
allows	_	_
behaviour	_	_
of	_	_
the	_	_
current	_	_
approach	_	_
to	_	_
be	_	_
compared	_	_
against	_	_
the	_	_
state-of-the-art	_	_
pattern	_	_
recognition	_	_
methods	_	_
.	_	_

#295
In	_	_
recent	_	_
years	_	_
,	_	_
the	_	_
medical	_	_
imaging	_	_
community	_	_
has	_	_
seen	_	_
many	_	_
of	_	_
the	_	_
established	_	_
“old-school”	_	_
approaches	_	_
replaced	_	_
by	_	_
deep	_	_
learning	_	_
,	_	_
but	_	_
in	_	_
doing	_	_
so	_	_
,	_	_
“have	_	_
we	_	_
thrown	_	_
the	_	_
baby	_	_
out	_	_
with	_	_
the	_	_
bath	_	_
water	_	_
?	_	_
”6	_	_
.	_	_

#296
There	_	_
may	_	_
still	_	_
be	_	_
widely	_	_
used	_	_
concepts	_	_
from	_	_
orthodox	_	_
medical	_	_
imaging	_	_
(	_	_
i.e.	_	_
,	_	_
not	_	_
deep	_	_
learning	_	_
)	_	_
that	_	_
are	_	_
still	_	_
useful	_	_
.	_	_

#297
In	_	_
particular	_	_
,	_	_
geometric	_	_
transformations	_	_
of	_	_
images	_	_
are	_	_
now	_	_
finding	_	_
their	_	_
way	_	_
into	_	_
various	_	_
machine	_	_
learning	_	_
approaches	_	_
(	_	_
e.g.	_	_
[	_	_
39	_	_
,	_	_
40	_	_
,	_	_
41	_	_
]	_	_
)	_	_
.	_	_

#298
Much	_	_
of	_	_
the	_	_
early	_	_
work	_	_
on	_	_
deep	_	_
learning	_	_
was	_	_
performed	_	_
using	_	_
MNIST	_	_
.	_	_

#299
Although	_	_
good	_	_
accuracies	_	_
were	_	_
achieved	_	_
,	_	_
the	_	_
computer	_	_
vision	_	_
community	_	_
did	_	_
not	_	_
take	_	_
such	_	_
work	_	_
seriously	_	_
because	_	_
the	_	_
images	_	_
were	_	_
so	_	_
small	_	_
.	_	_

#300
This	_	_
,	_	_
however	_	_
,	_	_
was	_	_
the	_	_
early	_	_
days	_	_
of	_	_
deep	_	_
learning	_	_
(	_	_
i.e.	_	_
,	_	_
before	_	_
2012	_	_
)	_	_
,	_	_
and	_	_
was	_	_
a	_	_
sign	_	_
of	_	_
things	_	_
to	_	_
come	_	_
.	_	_

#301
This	_	_
section	_	_
describes	_	_
an	_	_
attempt	_	_
to	_	_
begin	_	_
to	_	_
reclaim	_	_
some	_	_
of	_	_
the	_	_
territory	_	_
lost	_	_
to	_	_
deep	_	_
learning	_	_
.	_	_

#302
Unlike	_	_
most	_	_
conventional	_	_
pattern	_	_
recognition	_	_
approaches	_	_
,	_	_
the	_	_
strategy	_	_
adopted	_	_
here	_	_
is	_	_
generative	_	_
.	_	_

#303
Training	_	_
involves	_	_
learning	_	_
independent	_	_
models	_	_
of	_	_
the	_	_
ten	_	_
different	_	_
digits	_	_
in	_	_
the	_	_
training	_	_
set	_	_
,	_	_
while	_	_
testing	_	_
involves	_	_
fitting	_	_
each	_	_
model	_	_
in	_	_
turn	_	_
to	_	_
each	_	_
image	_	_
in	_	_
the	_	_
test	_	_
set	_	_
,	_	_
and	_	_
performing	_	_
model	_	_
comparison	_	_
to	_	_
assess	_	_
which	_	_
of	_	_
the	_	_
ten	_	_
models	_	_
better	_	_
explains	_	_
the	_	_
data	_	_
.	_	_

#304
The	_	_
training	_	_
stage	_	_
involved	_	_
learning	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
and	_	_
Â	_	_
for	_	_
each	_	_
digit	_	_
class	_	_
.	_	_

#305
A	_	_
similar	_	_
strategy	_	_
was	_	_
previously	_	_
adopted	_	_
by	_	_
[	_	_
42	_	_
]	_	_
.	_	_

#306
From	_	_
a	_	_
probabilistic	_	_
perspective	_	_
,	_	_
the	_	_
probability	_	_
of	_	_
the	_	_
kth	_	_
5http	_	_
:	_	_
//yann.lecun.com/exdb/mnist/	_	_
.	_	_

#307
6This	_	_
was	_	_
said	_	_
by	_	_
the	_	_
late	_	_
David	_	_
MacKay	_	_
[	_	_
38	_	_
]	_	_
in	_	_
relation	_	_
to	_	_
the	_	_
success	_	_
of	_	_
kernel	_	_
methods	_	_
,	_	_
such	_	_
as	_	_
support-vector	_	_
machines	_	_
or	_	_
Gaussian	_	_
processes	_	_
,	_	_
which	_	_
,	_	_
at	_	_
the	_	_
time	_	_
,	_	_
were	_	_
replacing	_	_
neural	_	_
networks	_	_
in	_	_
practical	_	_
applications	_	_
.	_	_

#308
3	_	_
RESULTS	_	_
24	_	_

#309
label	_	_
given	_	_
an	_	_
image	_	_
(	_	_
f	_	_
)	_	_
is	_	_
P	_	_
(	_	_
Mk|f	_	_
)	_	_
=	_	_
P	_	_
(	_	_
f	_	_
,	_	_
Mk	_	_
)	_	_
P	_	_
(	_	_
f	_	_
)	_	_
=	_	_
∫	_	_
z	_	_
P	_	_
(	_	_
f	_	_
|z	_	_
,	_	_
Mk	_	_
)	_	_
p	_	_
(	_	_
z|Mk	_	_
)	_	_
dzP	_	_
(	_	_
Mk	_	_
)	_	_
∑9	_	_
l=0	_	_
∫	_	_
z	_	_
P	_	_
(	_	_
f	_	_
|z	_	_
,	_	_
Ml	_	_
)	_	_
p	_	_
(	_	_
z|Ml	_	_
)	_	_
dzP	_	_
(	_	_
Ml	_	_
)	_	_
(	_	_
29	_	_
)	_	_
The	_	_
above	_	_
integrals	_	_
are	_	_
intractable	_	_
,	_	_
so	_	_
are	_	_
approximated	_	_
.	_	_

#310
This	_	_
was	_	_
done	_	_
by	_	_
a	_	_
“Laplace	_	_
approximation”7	_	_
whereby	_	_
the	_	_
approximate	_	_
distribution	_	_
of	_	_
z	_	_
is	_	_
given	_	_
by	_	_
q	_	_
(	_	_
z	_	_
)	_	_
=	_	_
N	_	_
(	_	_
z|ẑ	_	_
,	_	_
S−1	_	_
)	_	_
(	_	_
30	_	_
)	_	_
From	_	_
this	_	_
approximation	_	_
,	_	_
we	_	_
can	_	_
compute∫	_	_
z	_	_
P	_	_
(	_	_
f	_	_
,	_	_
z|M	_	_
)	_	_
dz	_	_
'P	_	_
(	_	_
f	_	_
,	_	_
ẑ|M	_	_
)	_	_
∫	_	_
z	_	_
exp	_	_
(	_	_
−	_	_
1	_	_
2	_	_
(	_	_
z−	_	_
ẑ	_	_
)	_	_
TS	_	_
(	_	_
z−	_	_
ẑ	_	_
)	_	_
)	_	_
dz	_	_
=	_	_
P	_	_
(	_	_
f	_	_
,	_	_
ẑ|M	_	_
)	_	_
|S/	_	_
(	_	_
2π	_	_
)	_	_
|1/2	_	_
(	_	_
31	_	_
)	_	_
For	_	_
each	_	_
image	_	_
(	_	_
f	_	_
)	_	_
,	_	_
the	_	_
mode	_	_
(	_	_
ẑ	_	_
)	_	_
of	_	_
p	_	_
(	_	_
f	_	_
,	_	_
z|Mk	_	_
)	_	_
was	_	_
computed	_	_
(	_	_
see	_	_
Section	_	_
Appendix	_	_
A.6	_	_
)	_	_
by	_	_
ẑ	_	_
=	_	_
arg	_	_
min	_	_
z	_	_
(	_	_
J	_	_
(	_	_
f	_	_
,	_	_
z	_	_
,	_	_
µ	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
+	_	_
1	_	_
2zT	_	_
(	_	_
λ1Â	_	_
+	_	_
λ2	_	_
(	_	_
Ŵa	_	_
)	_	_
TLaŴa	_	_
+	_	_
λ2	_	_
(	_	_
Ŵv	_	_
)	_	_
TLvŴv	_	_
)	_	_
z	_	_
)	_	_
.	_	_

#311
(	_	_
32	_	_
)	_	_
The	_	_
Hessian	_	_
of	_	_
the	_	_
objective	_	_
function	_	_
around	_	_
this	_	_
mode	_	_
(	_	_
Section	_	_
Appendix	_	_
A.6	_	_
)	_	_
was	_	_
used	_	_
to	_	_
approximate	_	_
the	_	_
uncertainty	_	_
(	_	_
S−1	_	_
)	_	_
.	_	_

#312
Training	_	_
was	_	_
done	_	_
with	_	_
different	_	_
sized	_	_
subsets	_	_
(	_	_
300	_	_
,	_	_
500	_	_
,	_	_
1,000	_	_
,	_	_
3,000	_	_
,	_	_
5,000	_	_
,	_	_
10,000	_	_
,	_	_
and	_	_
all	_	_
60,000	_	_
)	_	_
of	_	_
the	_	_
MNIST	_	_
training	_	_
data	_	_
,	_	_
whereas	_	_
testing	_	_
was	_	_
always	_	_
done	_	_
using	_	_
the	_	_
10,000	_	_
test	_	_
images	_	_
.	_	_

#313
In	_	_
each	_	_
of	_	_
the	_	_
training	_	_
subsets	_	_
,	_	_
the	_	_
first	_	_
of	_	_
the	_	_
images	_	_
were	_	_
always	_	_
used	_	_
,	_	_
which	_	_
generally	_	_
leads	_	_
to	_	_
slightly	_	_
different	_	_
sized	_	_
training	_	_
sets	_	_
for	_	_
each	_	_
of	_	_
the	_	_
digits	_	_
.	_	_

#314
Example	_	_
images	_	_
,	_	_
along	_	_
with	_	_
the	_	_
fit	_	_
from	_	_
the	_	_
models	_	_
trained	_	_
using	_	_
the	_	_
first	_	_
10,000	_	_
images	_	_
,	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
7	_	_
.	_	_

#315
Model	_	_
fitting	_	_
was	_	_
run	_	_
for	_	_
20	_	_
iterations	_	_
,	_	_
using	_	_
a	_	_
Bernoulli	_	_
likelihood	_	_
with	_	_
K	_	_
=	_	_
16	_	_
,	_	_
7While	_	_
many	_	_
readers	_	_
will	_	_
be	_	_
familiar	_	_
with	_	_
optimising	_	_
the	_	_
residual	_	_
squared	_	_
error	_	_
(	_	_
RSE	_	_
)	_	_
,	_	_
they	_	_
may	_	_
not	_	_
understand	_	_
how	_	_
this	_	_
differs	_	_
from	_	_
the	_	_
evidence	_	_
lower	_	_
bound	_	_
(	_	_
ELBO	_	_
)	_	_
.	_	_

#316
For	_	_
a	_	_
textbook	_	_
explanation	_	_
of	_	_
Bayesian	_	_
approaches	_	_
,	_	_
including	_	_
the	_	_
Laplace	_	_
approximation	_	_
,	_	_
see	_	_
[	_	_
38	_	_
]	_	_
,	_	_
[	_	_
43	_	_
]	_	_
or	_	_
[	_	_
44	_	_
]	_	_
.	_	_

#317
3	_	_
RESULTS	_	_
25	_	_

#318
A	_	_
selection	_	_
of	_	_
original	_	_
MNIST	_	_
images	_	_
.	_	_

#319
Full	_	_
model	_	_
fit	_	_
to	_	_
MNIST	_	_
images	_	_
.	_	_

#320
Figure	_	_
7	_	_
:	_	_
A	_	_
random	_	_
selection	_	_
of	_	_
digits	_	_
from	_	_
the	_	_
first	_	_
10,000	_	_
MNIST	_	_
training	_	_
images	_	_
,	_	_
along	_	_
with	_	_
the	_	_
model	_	_
fit	_	_
.	_	_

#321
In	_	_
general	_	_
,	_	_
good	_	_
alignment	_	_
is	_	_
achieved	_	_
.	_	_

#322
ν0	_	_
=	_	_
16	_	_
,	_	_
λ	_	_
=	_	_
[	_	_
0.95	_	_
0.05	_	_
]	_	_
,	_	_
ωa	_	_
=	_	_
[	_	_
0.002	_	_
0.2	_	_
0	_	_
]	_	_
,	_	_
ωµ	_	_
=	_	_
N	_	_
[	_	_
10−7	_	_
10−5	_	_
0	_	_
]	_	_
and	_	_
ωv	_	_
=	_	_
[	_	_
0.002	_	_
0.02	_	_
2	_	_
0.2	_	_
0.2	_	_
]	_	_
.	_	_

#323
When	_	_
applied	_	_
to	_	_
medical	_	_
images	_	_
,	_	_
machine	_	_
learning	_	_
can	_	_
suffer	_	_
from	_	_
the	_	_
curse	_	_
of	_	_
dimensionality	_	_
.	_	_

#324
The	_	_
number	_	_
of	_	_
pixels	_	_
or	_	_
voxels	_	_
in	_	_
each	_	_
image	_	_
(	_	_
M	_	_
)	_	_
is	_	_
often	_	_
much	_	_
greater	_	_
than	_	_
the	_	_
number	_	_
of	_	_
labelled	_	_
images	_	_
(	_	_
N	_	_
)	_	_
available	_	_
for	_	_
training	_	_
.	_	_

#325
For	_	_
MNIST	_	_
,	_	_
there	_	_
are	_	_
60,000	_	_
training	_	_
images	_	_
,	_	_
each	_	_
containing	_	_
784	_	_
pixels	_	_
,	_	_
giving	_	_
N/M	_	_
'	_	_
75	_	_
.	_	_

#326
In	_	_
contrast	_	_
,	_	_
even	_	_
after	_	_
down-sampling	_	_
to	_	_
a	_	_
lower	_	_
resolution	_	_
,	_	_
a	_	_
3D	_	_
MRI	_	_
scan	_	_
contains	_	_
in	_	_
the	_	_
order	_	_
of	_	_
20,000,000	_	_
voxels	_	_
.	_	_

#327
Achieving	_	_
a	_	_
similar	_	_
N/M	_	_
as	_	_
for	_	_
MNIST	_	_
would	_	_
require	_	_
about	_	_
1.5	_	_
billion	_	_
labelled	_	_
images	_	_
,	_	_
which	_	_
clearly	_	_
is	_	_
not	_	_
feasible	_	_
.	_	_

#328
For	_	_
this	_	_
reason	_	_
,	_	_
this	_	_
section	_	_
focuses	_	_
on	_	_
classification	_	_
methods	_	_
trained	_	_
using	_	_
smaller	_	_
subsets	_	_
of	_	_
the	_	_
MNIST	_	_
training	_	_
data	_	_
.	_	_

#329
Accuracies	_	_
are	_	_
compared	_	_
against	_	_
those	_	_
reported	_	_
by	_	_
[	_	_
45	_	_
]	_	_
for	_	_
their	_	_
Deeply	_	_
Supervised	_	_
Nets	_	_
,	_	_
which	_	_
is	_	_
a	_	_
deep	_	_
learning	_	_
approach	_	_
that	_	_
performs	_	_
close	_	_
to	_	_
state-of-the-art	_	_
(	_	_
for	_	_
2015	_	_
)	_	_
,	_	_
particularly	_	_
for	_	_
smaller	_	_
training	_	_
sets	_	_
.	_	_

#330
Invariant	_	_
scattering	_	_
convolutional	_	_
networks	_	_
are	_	_
also	_	_
known	_	_
to	_	_
work	_	_
well	_	_
for	_	_
smaller	_	_
training	_	_
sets	_	_
,	_	_
so	_	_
some	_	_
accuracies	_	_
taken	_	_
from	_	_
[	_	_
46	_	_
]	_	_
are	_	_
also	_	_
included	_	_
in	_	_
the	_	_
comparison	_	_
.	_	_

#331
We	_	_
are	_	_
not	_	_
aware	_	_
of	_	_
more	_	_
recent	_	_
papers	_	_
that	_	_
assess	_	_
the	_	_
accuracy	_	_
of	_	_
deep	_	_
learning	_	_
using	_	_
smaller	_	_
training	_	_
sets	_	_
.	_	_

#332
Plots	_	_
of	_	_
error	_	_
rate	_	_
against	_	_
training	_	_
set	_	_
size	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
8	_	_
,	_	_
along	_	_
with	_	_

#333
3	_	_
RESULTS	_	_
26	_	_

#334
Number	_	_
of	_	_
training	_	_
samples	_	_
300	_	_
500	_	_
1000	_	_
3000	_	_
5000	_	_
10000	_	_
60000	_	_
E	_	_
rr	_	_
o	_	_
r	_	_
(	_	_
%	_	_
)	_	_
Current	_	_
Lee	_	_
et	_	_
al	_	_
(	_	_
2015	_	_
)	_	_
Bruna	_	_
et	_	_
al	_	_
(	_	_
2012	_	_
)	_	_
PCA	_	_
Bruna	_	_
et	_	_
al	_	_
(	_	_
2012	_	_
)	_	_
SVM	_	_
Figure	_	_
8	_	_
:	_	_
Left	_	_
:	_	_
Test	_	_
errors	_	_
from	_	_
training	_	_
the	_	_
method	_	_
using	_	_
different	_	_
sized	_	_
subsets	_	_
of	_	_
the	_	_
MNIST	_	_
data	_	_
(	_	_
the	_	_
error	_	_
rate	_	_
from	_	_
random	_	_
guessing	_	_
would	_	_
be	_	_
90	_	_
%	_	_
)	_	_
.	_	_

#335
Right	_	_
:	_	_
All	_	_
the	_	_
MNIST	_	_
digits	_	_
the	_	_
method	_	_
failed	_	_
to	_	_
correctly	_	_
identify	_	_
(	_	_
after	_	_
training	_	_
with	_	_
the	_	_
full	_	_
60,000	_	_
)	_	_
are	_	_
shown	_	_
above	_	_
.	_	_

#336
These	_	_
are	_	_
followed	_	_
by	_	_
the	_	_
model	_	_
fits	_	_
for	_	_
the	_	_
true	_	_
digit	_	_
,	_	_
and	_	_
then	_	_
the	_	_
model	_	_
fits	_	_
for	_	_
the	_	_
incorrect	_	_
guess	_	_
(	_	_
i.e.	_	_
,	_	_
the	_	_
one	_	_
with	_	_
the	_	_
most	_	_
model	_	_
evidence	_	_
)	_	_
.	_	_

#337
the	_	_
approximate	_	_
error	_	_
rates	_	_
from	_	_
[	_	_
45	_	_
]	_	_
and	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#338
The	_	_
plot	_	_
shows	_	_
the	_	_
proposed	_	_
method	_	_
to	_	_
be	_	_
more	_	_
accurate	_	_
than	_	_
deep	_	_
learning	_	_
for	_	_
smaller	_	_
training	_	_
sets	_	_
,	_	_
but	_	_
it	_	_
is	_	_
less	_	_
accurate	_	_
when	_	_
using	_	_
the	_	_
full	_	_
training	_	_
set	_	_
,	_	_
as	_	_
the	_	_
error	_	_
rate	_	_
plateaus	_	_
to	_	_
a	_	_
value	_	_
of	_	_
about	_	_
0.85	_	_
%	_	_
for	_	_
training	_	_
set	_	_
sizes	_	_
of	_	_
around	_	_
5,000	_	_
onward	_	_
.	_	_

#339
Visual	_	_
assessment	_	_
of	_	_
the	_	_
fits	_	_
to	_	_
the	_	_
misclassified	_	_
digits	_	_
(	_	_
Fig.	_	_
8	_	_
)	_	_
suggests	_	_
that	_	_
relatively	_	_
few	_	_
of	_	_
the	_	_
failures	_	_
can	_	_
be	_	_
attributed	_	_
to	_	_
registration	_	_
errors	_	_
.	_	_

#340
These	_	_
experiments	_	_
with	_	_
MNIST	_	_
suggest	_	_
that	_	_
one	_	_
avenue	_	_
of	_	_
further	_	_
work	_	_
could	_	_
be	_	_
to	_	_
elaborate	_	_
on	_	_
the	_	_
simple	_	_
multivariate	_	_
Gaussian	_	_
model	_	_
for	_	_
the	_	_
distribution	_	_
of	_	_
latent	_	_
variables	_	_
.	_	_

#341
Although	_	_
accuracies	_	_
were	_	_
relatively	_	_
good	_	_
for	_	_
smaller	_	_
training	_	_
sets	_	_
,	_	_
the	_	_
Gaussian	_	_
assumptions	_	_
meant	_	_
that	_	_
increasing	_	_
the	_	_
amount	_	_
of	_	_
training	_	_
data	_	_
beyond	_	_
about	_	_
5,000	_	_
examples	_	_
did	_	_
not	_	_
bring	_	_
any	_	_
additional	_	_
accuracy	_	_
.	_	_

#342
One	_	_
example	_	_
of	_	_
where	_	_
the	_	_
Gaussian	_	_
distribution	_	_
fails	_	_
is	_	_
when	_	_
attempting	_	_
to	_	_
deal	_	_
with	_	_
sevens	_	_
written	_	_
either	_	_
with	_	_
or	_	_
without	_	_
a	_	_
bar	_	_
through	_	_
them	_	_
,	_	_
which	_	_
clearly	_	_
requires	_	_
some	_	_
form	_	_
of	_	_
bimodal	_	_
distribution	_	_
to	_	_
describe	_	_
(	_	_
see	_	_
Fig.	_	_
9	_	_
)	_	_
.	_	_

#343
One	_	_
approach	_	_
to	_	_
achieving	_	_
a	_	_
more	_	_
flexible	_	_
model	_	_
of	_	_
the	_	_
latent	_	_
variable	_	_
probability	_	_
density	_	_
would	_	_
to	_	_
use	_	_
a	_	_
Gaussian	_	_
Mixture	_	_
Model	_	_
(	_	_
GMM	_	_
)	_	_
[	_	_
47	_	_
]	_	_
.	_	_

#344
This	_	_
type	_	_
of	_	_

#345
3	_	_
RESULTS	_	_
27	_	_

#346
2nd	_	_
mode	_	_
-0.04	_	_
0	_	_
0.04	_	_
rd	_	_
m	_	_
o	_	_
d	_	_
e	_	_
-0.04	_	_
-0.02	_	_
0.02	_	_
0.04	_	_
Digit	_	_
1	_	_
2nd	_	_
mode	_	_
-0.04	_	_
0	_	_
0.04	_	_
th	_	_
m	_	_
o	_	_
d	_	_
e	_	_
-0.04	_	_
-0.02	_	_
0.02	_	_
0.04	_	_
Digit	_	_
7	_	_
1st	_	_
mode	_	_
-0.04	_	_
0	_	_
0.04	_	_
n	_	_
d	_	_
m	_	_
o	_	_
d	_	_
e	_	_
-0.04	_	_
-0.02	_	_
0.02	_	_
0.04	_	_
Digit	_	_
8	_	_
Figure	_	_
9	_	_
:	_	_
Illustration	_	_
of	_	_
the	_	_
non-Gaussian	_	_
distributions	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
for	_	_
some	_	_
of	_	_
the	_	_
MNIST	_	_
digits	_	_
.	_	_

#347
Plots	_	_
of	_	_
selected	_	_
latent	_	_
variables	_	_
are	_	_
shown	_	_
above	_	_
,	_	_
with	_	_
the	_	_
corresponding	_	_
modes	_	_
of	_	_
variation	_	_
shown	_	_
below	_	_
.	_	_

#348
Gaussian	_	_
mixture	_	_
models	_	_
are	_	_
likely	_	_
to	_	_
provide	_	_
better	_	_
models	_	_
of	_	_
variability	_	_
than	_	_
the	_	_
current	_	_
assumption	_	_
of	_	_
a	_	_
single	_	_
Gaussian	_	_
distribution	_	_
.	_	_

#349
approach	_	_
has	_	_
been	_	_
used	_	_
for	_	_
appearance	_	_
models	_	_
[	_	_
48	_	_
]	_	_
.	_	_

#350
In	_	_
principle	_	_
,	_	_
a	_	_
variational	_	_
Bayesian	_	_
GMM	_	_
[	_	_
43	_	_
]	_	_
could	_	_
be	_	_
used	_	_
to	_	_
automatically	_	_
select	_	_
the	_	_
optimal	_	_
model	_	_
complexity	_	_
,	_	_
leading	_	_
to	_	_
an	_	_
approach	_	_
that	_	_
self-tunes	_	_
its	_	_
complexity	_	_
according	_	_
to	_	_
the	_	_
amount	_	_
and	_	_
quality	_	_
of	_	_
of	_	_
data	_	_
available	_	_
.	_	_

#351
This	_	_
type	_	_
of	_	_
model	_	_
selection	_	_
has	_	_
previously	_	_
been	_	_
used	_	_
for	_	_
shape	_	_
modelling	_	_
[	_	_
49	_	_
,	_	_
50	_	_
]	_	_
,	_	_
as	_	_
well	_	_
as	_	_
for	_	_
other	_	_
aspects	_	_
of	_	_
medical	_	_
image	_	_
computing	_	_
.	_	_

#352
With	_	_
a	_	_
more	_	_
flexible	_	_
model	_	_
,	_	_
it	_	_
may	_	_
be	_	_
possible	_	_
to	_	_
achieve	_	_
accuracies	_	_
similar	_	_
to	_	_
those	_	_
achieved	_	_
by	_	_
deep	_	_
learning	_	_
,	_	_
but	_	_
with	_	_
fewer	_	_
labelled	_	_
training	_	_
examples	_	_
.	_	_

#353
One	_	_
of	_	_
the	_	_
aims	_	_
of	_	_
the	_	_
Medical	_	_
Informatics	_	_
Platform	_	_
of	_	_
the	_	_
HBP	_	_
was	_	_
to	_	_
cluster	_	_
patients	_	_
into	_	_
different	_	_
sub-groups	_	_
.	_	_

#354
In	_	_
addition	_	_
to	_	_
achieving	_	_
greater	_	_
accuracy	_	_
(	_	_
probably	_	_
)	_	_
,	_	_
incorporating	_	_
a	_	_
GMM	_	_
over	_	_
the	_	_
latent	_	_
variables	_	_
could	_	_
also	_	_
lead	_	_
to	_	_
this	_	_
clustering	_	_
goal	_	_
being	_	_
achieved	_	_
.	_	_

#355
3.3	_	_
.	_	_

#356
Experiments	_	_
with	_	_
segmented	_	_
MRI	_	_
Experiments	_	_
were	_	_
performed	_	_
using	_	_
1,913	_	_
T1-weighted	_	_
MR	_	_
images	_	_
from	_	_
the	_	_
following	_	_
datasets	_	_
.	_	_

#357
•	_	_
The	_	_
IXI	_	_
dataset	_	_
,	_	_
which	_	_
is	_	_
available	_	_
under	_	_
the	_	_
Creative	_	_
Commons	_	_
CC	_	_
BYSA	_	_
3.0	_	_
license	_	_
from	_	_
http	_	_
:	_	_
//brain-development.org/ixi-dataset/	_	_
.	_	_

#358
Information	_	_
about	_	_
scanner	_	_
parameters	_	_
and	_	_
subject	_	_
demographics	_	_
are	_	_
also	_	_

#359
3	_	_
RESULTS	_	_
28	_	_

#360
available	_	_
from	_	_
the	_	_
web	_	_
site	_	_
.	_	_

#361
Scans	_	_
were	_	_
collected	_	_
on	_	_
three	_	_
different	_	_
scanners	_	_
using	_	_
a	_	_
variety	_	_
of	_	_
MR	_	_
sequences	_	_
.	_	_

#362
This	_	_
work	_	_
used	_	_
only	_	_
the	_	_
581	_	_
T1-weighted	_	_
scans	_	_
.	_	_

#363
•	_	_
The	_	_
OASIS	_	_
Longitudinal	_	_
dataset	_	_
is	_	_
described	_	_
in	_	_
[	_	_
51	_	_
]	_	_
.	_	_

#364
The	_	_
dataset	_	_
contains	_	_
longitudinal	_	_
T1-weighted	_	_
MRI	_	_
scans	_	_
of	_	_
elderly	_	_
subjects	_	_
,	_	_
some	_	_
of	_	_
whom	_	_
had	_	_
dementia	_	_
.	_	_

#365
Only	_	_
data	_	_
from	_	_
the	_	_
first	_	_
82	_	_
subjects	_	_
of	_	_
this	_	_
dataset	_	_
were	_	_
downloaded	_	_
from	_	_
http	_	_
:	_	_
//www.oasis-brains.org/	_	_
,	_	_
and	_	_
averages	_	_
of	_	_
the	_	_
scans	_	_
acquired	_	_
at	_	_
the	_	_
first	_	_
time	_	_
point	_	_
were	_	_
used	_	_
.	_	_

#366
•	_	_
The	_	_
COBRE	_	_
(	_	_
Centre	_	_
for	_	_
Biomedical	_	_
Research	_	_
Excellence	_	_
)	_	_
dataset	_	_
are	_	_
available	_	_
for	_	_
download	_	_
from	_	_
http	_	_
:	_	_
//fcon_1000.projects.nitrc.org/	_	_
indi/retro/cobre.html	_	_
under	_	_
the	_	_
Creative	_	_
Commons	_	_
CC	_	_
BY-NC	_	_
license	_	_
.	_	_

#367
The	_	_
dataset	_	_
includes	_	_
fMRI	_	_
and	_	_
T1-weighted	_	_
scans	_	_
of	_	_
72	_	_
patients	_	_
with	_	_
Schizophrenia	_	_
and	_	_
74	_	_
healthy	_	_
controls	_	_
.	_	_

#368
Only	_	_
the	_	_
T1-weighted	_	_
scans	_	_
were	_	_
used	_	_
.	_	_

#369
Information	_	_
about	_	_
scanner	_	_
parameters	_	_
and	_	_
subject	_	_
demographics	_	_
is	_	_
available	_	_
from	_	_
the	_	_
web	_	_
site	_	_
.	_	_

#370
•	_	_
The	_	_
ABIDE	_	_
I	_	_
(	_	_
Autism	_	_
Brain	_	_
Imaging	_	_
Date	_	_
Exchange	_	_
)	_	_
dataset	_	_
was	_	_
downloaded	_	_
via	_	_
http	_	_
:	_	_
//fcon_1000.projects.nitrc.org/indi/abide/abide_	_	_
I.html	_	_
and	_	_
is	_	_
available	_	_
under	_	_
the	_	_
Creative	_	_
Commons	_	_
CC	_	_
BY-NC-SA	_	_
license	_	_
.	_	_

#371
There	_	_
were	_	_
scans	_	_
from	_	_
1,102	_	_
subjects	_	_
,	_	_
where	_	_
531	_	_
were	_	_
individuals	_	_
on	_	_
the	_	_
Autism	_	_
Spectrum	_	_
.	_	_

#372
Subjects	_	_
were	_	_
drawn	_	_
from	_	_
a	_	_
wide	_	_
age	_	_
range	_	_
and	_	_
were	_	_
scanned	_	_
at	_	_
17	_	_
different	_	_
sites	_	_
around	_	_
the	_	_
world	_	_
.	_	_

#373
All	_	_
the	_	_
T1-weighted	_	_
scans	_	_
were	_	_
used	_	_
,	_	_
and	_	_
these	_	_
had	_	_
a	_	_
very	_	_
wide	_	_
range	_	_
of	_	_
image	_	_
properties	_	_
,	_	_
resolutions	_	_
and	_	_
fields	_	_
of	_	_
view	_	_
.	_	_

#374
For	_	_
example	_	_
,	_	_
many	_	_
of	_	_
the	_	_
scans	_	_
did	_	_
not	_	_
cover	_	_
the	_	_
cerebellum	_	_
.	_	_

#375
The	_	_
images	_	_
were	_	_
segmented	_	_
using	_	_
the	_	_
algorithm	_	_
in	_	_
SPM12	_	_
,	_	_
which	_	_
uses	_	_
the	_	_
approach	_	_
described	_	_
in	_	_
[	_	_
52	_	_
]	_	_
,	_	_
but	_	_
with	_	_
some	_	_
additional	_	_
modifications	_	_
that	_	_
are	_	_
described	_	_
in	_	_
the	_	_
appendices	_	_
of	_	_
[	_	_
53	_	_
,	_	_
54	_	_
]	_	_
.	_	_

#376
Binary	_	_
maps	_	_
of	_	_
grey	_	_
and	_	_
white	_	_
matter	_	_
were	_	_
approximately	_	_
aligned	_	_
into	_	_
ICBM152	_	_
space	_	_
using	_	_
a	_	_
rigid-body	_	_
transform	_	_
obtained	_	_
from	_	_
a	_	_
weighted	_	_
Procrustes	_	_
analysis	_	_
[	_	_
55	_	_
]	_	_
of	_	_
the	_	_
deformations	_	_
estimated	_	_

#377
3	_	_
RESULTS	_	_
29	_	_

#378
Original	_	_
data	_	_
.	_	_

#379
Shape	_	_
and	_	_
appearance	_	_
model	_	_
fit	_	_
.	_	_

#380
Figure	_	_
10	_	_
:	_	_
A	_	_
random	_	_
selection	_	_
of	_	_
the	_	_
2D	_	_
brain	_	_
image	_	_
data	_	_
,	_	_
showing	_	_
grey	_	_
matter	_	_
(	_	_
red	_	_
)	_	_
,	_	_
white	_	_
matter	_	_
(	_	_
green	_	_
)	_	_
and	_	_
other	_	_
(	_	_
blue	_	_
)	_	_
.	_	_

#381
Black	_	_
regions	_	_
indicate	_	_
missing	_	_
data	_	_
.	_	_

#382
Below	_	_
these	_	_
is	_	_
the	_	_
model	_	_
fit	_	_
to	_	_
the	_	_
images	_	_
.	_	_

#383
by	_	_
the	_	_
segmentation	_	_
algorithm	_	_
.	_	_

#384
These	_	_
approximately	_	_
aligned	_	_
images	_	_
have	_	_
an	_	_
isotropic	_	_
resolution	_	_
of	_	_
2	_	_
mm	_	_
.	_	_

#385
3.3.1	_	_
.	_	_

#386
2D	_	_
experiments	_	_
with	_	_
segmented	_	_
MRI	_	_
It	_	_
is	_	_
generally	_	_
easier	_	_
to	_	_
visualise	_	_
how	_	_
an	_	_
algorithm	_	_
is	_	_
working	_	_
when	_	_
it	_	_
is	_	_
run	_	_
in	_	_
2D	_	_
,	_	_
rather	_	_
than	_	_
3D	_	_
.	_	_

#387
The	_	_
examples	_	_
here	_	_
will	_	_
be	_	_
used	_	_
to	_	_
illustrate	_	_
the	_	_
behaviour	_	_
of	_	_
the	_	_
algorithm	_	_
under	_	_
topological	_	_
changes	_	_
,	_	_
when	_	_
variability	_	_
can	_	_
not	_	_
be	_	_
modelled	_	_
only	_	_
via	_	_
diffeomorphic	_	_
deformations	_	_
.	_	_

#388
A	_	_
single	_	_
slice	_	_
was	_	_
extracted	_	_
from	_	_
the	_	_
grey	_	_
and	_	_
white	_	_
matter	_	_
images	_	_
of	_	_
each	_	_
of	_	_
the	_	_
1,913	_	_
subjects	_	_
,	_	_
and	_	_
the	_	_
joint	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
was	_	_
fit	_	_
to	_	_
the	_	_
data	_	_
using	_	_
the	_	_
settings	_	_
for	_	_
categorical	_	_
image	_	_
data	_	_
.	_	_

#389
This	_	_
assumed	_	_
that	_	_
each	_	_
voxel	_	_
was	_	_
a	_	_
categorical	_	_
variable	_	_
indicating	_	_
one	_	_
of	_	_
three	_	_
tissue	_	_
classes	_	_
(	_	_
grey	_	_
and	_	_
white	_	_
matter	_	_
,	_	_
as	_	_
well	_	_
as	_	_
background	_	_
)	_	_
.	_	_

#390
Each	_	_
2D	_	_
image	_	_
was	_	_
encoded	_	_
by	_	_
100	_	_
latent	_	_
variables	_	_
(	_	_
i.e.	_	_
K	_	_
=	_	_
100	_	_
)	_	_
.	_	_

#391
Eight	_	_
iterations	_	_
of	_	_
the	_	_
algorithm	_	_
were	_	_
used	_	_
,	_	_
with	_	_
λ	_	_
=	_	_
[	_	_
0.9	_	_
0.1	_	_
]	_	_
,	_	_
ωa	_	_
=	_	_
[	_	_
0.1	_	_
16	_	_
128	_	_
]	_	_
,	_	_
ωµ	_	_
=	_	_
N	_	_
[	_	_
0.0001	_	_
0.01	_	_
0.1	_	_
]	_	_
,	_	_
ωv	_	_
=	_	_
[	_	_
0.001	_	_
0	_	_
32	_	_
0.25	_	_
0.5	_	_
]	_	_
and	_	_
ν0	_	_
=	_	_
100	_	_
.	_	_

#392
Some	_	_
model	_	_
fits	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
10	_	_
,	_	_
and	_	_
the	_	_
principal	_	_
modes	_	_
of	_	_
variability	_	_

#393
3	_	_
RESULTS	_	_
30	_	_

#394
Figure	_	_
11	_	_
:	_	_
First	_	_
eight	_	_
(	_	_
out	_	_
of	_	_
a	_	_
total	_	_
of	_	_
100	_	_
)	_	_
modes	_	_
of	_	_
variability	_	_
found	_	_
from	_	_
the	_	_
2D	_	_
brain	_	_
image	_	_
dataset	_	_
,	_	_
shown	_	_
at	_	_
-5	_	_
,	_	_
-3	_	_
,	_	_
-1	_	_
,	_	_
+1	_	_
,	_	_
+3	_	_
&	_	_
+5	_	_
standard	_	_
deviations	_	_
.	_	_

#395
Note	_	_
that	_	_
these	_	_
modes	_	_
encode	_	_
some	_	_
topological	_	_
changes	_	_
,	_	_
in	_	_
addition	_	_
to	_	_
changes	_	_
in	_	_
shape	_	_
.	_	_

#396
are	_	_
shown	_	_
in	_	_
Fig.	_	_
11	_	_
.	_	_

#397
As	_	_
can	_	_
be	_	_
seen	_	_
,	_	_
these	_	_
images	_	_
are	_	_
reasonably	_	_
well	_	_
modelled	_	_
,	_	_
although	_	_
achieving	_	_
a	_	_
similar	_	_
quality	_	_
of	_	_
fit	_	_
for	_	_
the	_	_
full	_	_
3D	_	_
data	_	_
would	_	_
probably	_	_
require	_	_
about	_	_
1,000	_	_
(	_	_
1003/2	_	_
)	_	_
variables	_	_
.	_	_

#398
Fitting	_	_
a	_	_
model	_	_
of	_	_
this	_	_
size	_	_
would	_	_
require	_	_
many	_	_
more	_	_
than	_	_
the	_	_
1,900	_	_
subjects	_	_
in	_	_
this	_	_
dataset	_	_
.	_	_

#399
Note	_	_
that	_	_
the	_	_
topology	_	_
of	_	_
the	_	_
images	_	_
may	_	_
differ	_	_
,	_	_
which	_	_
(	_	_
by	_	_
definition8	_	_
)	_	_
is	_	_
not	_	_
something	_	_
that	_	_
can	_	_
be	_	_
modelled	_	_
by	_	_
diffeomorphisms	_	_
alone	_	_
.	_	_

#400
The	_	_
inclusion	_	_
of	_	_
the	_	_
appearance	_	_
model	_	_
allows	_	_
these	_	_
topology	_	_
differences	_	_
to	_	_
be	_	_
more	_	_
accurately	_	_
captured	_	_
.	_	_

#401
3.3.2	_	_
.	_	_

#402
Imputing	_	_
missing	_	_
data	_	_
The	_	_
ability	_	_
to	_	_
elegantly	_	_
handle	_	_
missing	_	_
data	_	_
is	_	_
a	_	_
useful	_	_
requirement	_	_
for	_	_
mining	_	_
hospital	_	_
scans	_	_
.	_	_

#403
These	_	_
often	_	_
have	_	_
limited	_	_
fields	_	_
of	_	_
view	_	_
,	_	_
and	_	_
may	_	_
miss	_	_
out	_	_
parts	_	_
of	_	_
the	_	_
brain	_	_
that	_	_
are	_	_
present	_	_
in	_	_
other	_	_
images	_	_
.	_	_

#404
The	_	_
objective	_	_
here	_	_
is	_	_
to	_	_
demonstrate	_	_
that	_	_
a	_	_
reasonable	_	_
image	_	_
factorisation	_	_
can	_	_
be	_	_
learned	_	_
,	_	_
even	_	_
when	_	_
some	_	_
images	_	_
in	_	_
the	_	_
dataset	_	_
may	_	_
not	_	_
have	_	_
full	_	_
organ	_	_
coverage	_	_
.	_	_

#405
This	_	_
experiment	_	_
used	_	_
the	_	_
same	_	_
slice	_	_
through	_	_
the	_	_
data	_	_
as	_	_
above	_	_
,	_	_
and	_	_
a	_	_
rectangle	_	_
covering	_	_
25	_	_
%	_	_
of	_	_
the	_	_
area	_	_
of	_	_
the	_	_
images	_	_
was	_	_
placed	_	_
randomly	_	_
in	_	_
each	_	_
and	_	_
8Topology	_	_
is	_	_
concerned	_	_
with	_	_
properties	_	_
that	_	_
are	_	_
preserved	_	_
following	_	_
diffeomorphic	_	_
deformations	_	_
(	_	_
see	_	_
https	_	_
:	_	_
//en.wikipedia.org/wiki/Topology	_	_
)	_	_
.	_	_

#406
3	_	_
RESULTS	_	_
31	_	_

#407
Figure	_	_
12	_	_
:	_	_
Randomly	_	_
generated	_	_
slice	_	_
through	_	_
brain	_	_
images	_	_
.	_	_

#408
These	_	_
images	_	_
were	_	_
constructed	_	_
by	_	_
using	_	_
randomly	_	_
assigned	_	_
latent	_	_
variables	_	_
.	_	_

#409
Note	_	_
that	_	_
the	_	_
top	_	_
set	_	_
of	_	_
images	_	_
uses	_	_
the	_	_
same	_	_
random	_	_
variables	_	_
as	_	_
the	_	_
bottom	_	_
set	_	_
,	_	_
except	_	_
they	_	_
are	_	_
multiplied	_	_
by	_	_
−1	_	_
.	_	_

#410
This	_	_
means	_	_
that	_	_
one	_	_
set	_	_
is	_	_
a	_	_
sort	_	_
of	_	_
“opposite”	_	_
of	_	_
the	_	_
other	_	_
.	_	_

#411
For	_	_
example	_	_
,	_	_
if	_	_
a	_	_
brain	_	_
in	_	_
the	_	_
upper	_	_
set	_	_
has	_	_
large	_	_
ventricles	_	_
,	_	_
then	_	_
the	_	_
corresponding	_	_
brain	_	_
in	_	_
the	_	_
lower	_	_
set	_	_
will	_	_
have	_	_
small	_	_
ventricles	_	_
.	_	_

#412
every	_	_
image	_	_
of	_	_
the	_	_
training	_	_
set	_	_
(	_	_
wrapping	_	_
around	_	_
at	_	_
the	_	_
edge	_	_
of	_	_
the	_	_
field	_	_
of	_	_
view	_	_
)	_	_
,	_	_
and	_	_
the	_	_
intensities	_	_
within	_	_
these	_	_
rectangles	_	_
set	_	_
to	_	_
NaN	_	_
(	_	_
“not	_	_
a	_	_
number”	_	_
in	_	_
the	_	_
IEEE	_	_
754	_	_
floating-point	_	_
standard	_	_
)	_	_
.	_	_

#413
The	_	_
algorithm	_	_
was	_	_
trained	_	_
,	_	_
using	_	_
the	_	_
same	_	_
settings	_	_
as	_	_
described	_	_
previously	_	_
,	_	_
on	_	_
the	_	_
these	_	_
modified	_	_
images	_	_
.	_	_

#414
Although	_	_
imputed	_	_
missing	_	_
values	_	_
may	_	_
not	_	_
be	_	_
explicitly	_	_
required	_	_
,	_	_
they	_	_
do	_	_
provide	_	_
a	_	_
useful	_	_
illustration	_	_
of	_	_
how	_	_
well	_	_
the	_	_
model	_	_
works	_	_
in	_	_
less	_	_
than	_	_
ideal	_	_
situations	_	_
.	_	_

#415
Fig.	_	_
13	_	_
shows	_	_
a	_	_
selection	_	_
of	_	_
the	_	_
images	_	_
with	_	_
regions	_	_
set	_	_
to	_	_
NaN	_	_
,	_	_
and	_	_
the	_	_
same	_	_
images	_	_
with	_	_
the	_	_
missing	_	_
values	_	_
predicted	_	_
by	_	_
the	_	_
algorithm	_	_
.	_	_

#416
The	_	_
ability	_	_
to	_	_
handle	_	_
missing	_	_
data	_	_
allows	_	_
cross-validation	_	_
to	_	_
be	_	_
used	_	_
to	_	_
determine	_	_
the	_	_
accuracy	_	_
of	_	_
a	_	_
model	_	_
,	_	_
and	_	_
how	_	_
well	_	_
it	_	_
generalises	_	_
.	_	_

#417
In	_	_
addition	_	_
to	_	_
the	_	_
joint	_	_
shape	_	_
and	_	_
appearance	_	_
model	_	_
,	_	_
this	_	_
work	_	_
also	_	_
allows	_	_
simplified	_	_
versions	_	_
to	_	_
be	_	_
fitted	_	_
that	_	_
involve	_	_
only	_	_
shape	_	_
(	_	_
i.e.	_	_
,	_	_
not	_	_
using	_	_
Wa	_	_
,	_	_
as	_	_
in	_	_
[	_	_
7	_	_
]	_	_
)	_	_
or	_	_
in	_	_
a	_	_
form	_	_
that	_	_
varies	_	_
only	_	_
the	_	_
appearance	_	_
(	_	_
i.e.	_	_
not	_	_
using	_	_
Wv	_	_
)	_	_
.	_	_

#418
In	_	_
addition	_	_
,	_	_
this	_	_
work	_	_
also	_	_
includes	_	_
a	_	_
version	_	_
where	_	_
different	_	_
sets	_	_
of	_	_
latent	_	_
variables	_	_
control	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
.	_	_

#419
Here	_	_
,	_	_
there	_	_
were	_	_
30	_	_
variables	_	_
to	_	_
control	_	_
appearance	_	_
,	_	_
and	_	_
70	_	_
to	_	_
control	_	_
shape	_	_
.	_	_

#420
The	_	_
aim	_	_
was	_	_
to	_	_
compare	_	_
the	_	_
four	_	_
models	_	_
by	_	_
assessing	_	_
how	_	_
well	_	_
they	_	_
are	_	_
able	_	_
to	_	_
predict	_	_
data	_	_
that	_	_
was	_	_
unavailable	_	_
to	_	_
the	_	_
model	_	_
during	_	_
fitting	_	_
.	_	_

#421
3	_	_
RESULTS	_	_
32	_	_

#422
Original	_	_
data	_	_
.	_	_

#423
Missing	_	_
data	_	_
filled	_	_
in	_	_
.	_	_

#424
Figure	_	_
13	_	_
:	_	_
A	_	_
random	_	_
selection	_	_
of	_	_
the	_	_
2D	_	_
brain	_	_
image	_	_
data	_	_
showing	_	_
the	_	_
location	_	_
of	_	_
missing	_	_
data	_	_
.	_	_

#425
The	_	_
attempt	_	_
to	_	_
fill	_	_
in	_	_
the	_	_
missing	_	_
information	_	_
is	_	_
shown	_	_
below	_	_
.	_	_

#426
These	_	_
may	_	_
be	_	_
compared	_	_
against	_	_
the	_	_
original	_	_
images	_	_
shown	_	_
in	_	_
Fig.	_	_
10	_	_
.	_	_

#427
This	_	_
gives	_	_
us	_	_
ground	_	_
truth	_	_
with	_	_
which	_	_
to	_	_
compare	_	_
the	_	_
models’	_	_
predictions	_	_
,	_	_
and	_	_
is	_	_
essentially	_	_
a	_	_
form	_	_
of	_	_
cross-validation	_	_
procedure	_	_
.	_	_

#428
Accuracy	_	_
was	_	_
measured	_	_
by	_	_
the	_	_
log-likelihood	_	_
of	_	_
the	_	_
ground	_	_
truth	_	_
data	_	_
,	_	_
which	_	_
was	_	_
computed	_	_
only	_	_
for	_	_
pixels	_	_
that	_	_
the	_	_
models	_	_
did	_	_
not	_	_
have	_	_
access	_	_
to	_	_
during	_	_
training	_	_
.	_	_

#429
The	_	_
results	_	_
of	_	_
the	_	_
cross-validation	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
14	_	_
,	_	_
and	_	_
clearly	_	_
show	_	_
that	_	_
the	_	_
models	_	_
that	_	_
combine	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
have	_	_
greater	_	_
predictive	_	_
validity	_	_
than	_	_
either	_	_
the	_	_
shape	_	_
or	_	_
appearance	_	_
models	_	_
alone	_	_
.	_	_

#430
Mean	_	_
squared	_	_
errors	_	_
from	_	_
the	_	_
different	_	_
models	_	_
are	_	_
also	_	_
presented	_	_
,	_	_
and	_	_
these	_	_
exhibit	_	_
the	_	_
same	_	_
general	_	_
pattern	_	_
.	_	_

#431
Although	_	_
the	_	_
difference	_	_
was	_	_
small	_	_
,	_	_
the	_	_
best	_	_
results	_	_
were	_	_
from	_	_
the	_	_
model	_	_
where	_	_
each	_	_
latent	_	_
variable	_	_
controls	_	_
both	_	_
shape	_	_
and	_	_
appearance	_	_
,	_	_
rather	_	_
than	_	_
when	_	_
they	_	_
are	_	_
controlled	_	_
separately	_	_
.	_	_

#432
Both	_	_
of	_	_
these	_	_
combined	_	_
models	_	_
outperformed	_	_
models	_	_
of	_	_
only	_	_
shape	_	_
or	_	_
only	_	_
appearance	_	_
variability	_	_
.	_	_

#433
Note	_	_
that	_	_
changes	_	_
to	_	_
hyper-parameter	_	_
settings	_	_
,	_	_
etc	_	_
.	_	_

#434
may	_	_
improve	_	_
accuracies	_	_
further	_	_
.	_	_

#435
3.3.3	_	_
.	_	_

#436
3D	_	_
experiments	_	_
with	_	_
segmented	_	_
MRI	_	_
The	_	_
aim	_	_
of	_	_
this	_	_
section	_	_
was	_	_
to	_	_
apply	_	_
the	_	_
method	_	_
to	_	_
a	_	_
large	_	_
set	_	_
of	_	_
3D	_	_
images	_	_
,	_	_
and	_	_
use	_	_
the	_	_
resulting	_	_
latent	_	_
variables	_	_
as	_	_
features	_	_
for	_	_
pattern	_	_
recognition	_	_
.	_	_

#437
For	_	_

#438
3	_	_
RESULTS	_	_
33	_	_

#439
K=100	_	_
K	_	_
a	_	_
=0	_	_
,	_	_
K	_	_
v	_	_
=100	_	_
K	_	_
a	_	_
=100	_	_
,	_	_
K	_	_
v	_	_
=0	_	_
K	_	_
a	_	_
=30	_	_
,	_	_
K	_	_
v	_	_
=70	_	_
L	_	_
o	_	_
g	_	_
L	_	_
ik	_	_
e	_	_
lih	_	_
o	_	_
o	_	_
d	_	_
-0.7	_	_
-0.6	_	_
-0.5	_	_
-0.4	_	_
-0.3	_	_
-0.2	_	_
-0.1	_	_
K=100	_	_
Ka=0	_	_
,	_	_
K	_	_
v=100	_	_
Ka=100	_	_
,	_	_
K	_	_
v=0	_	_
Ka=30	_	_
,	_	_
K	_	_
v=70	_	_
M	_	_
e	_	_
a	_	_
n	_	_
S	_	_
q	_	_
u	_	_
a	_	_
re	_	_
d	_	_
D	_	_
if	_	_
fe	_	_
re	_	_
n	_	_
c	_	_
e	_	_
×10	_	_
-6	_	_
Figure	_	_
14	_	_
:	_	_
Cross-validation	_	_
accuracy	_	_
measures	_	_
based	_	_
on	_	_
predicting	_	_
the	_	_
left-out	_	_
patches	_	_
of	_	_
the	_	_
images	_	_
.	_	_

#440
The	_	_
blue	_	_
dots	_	_
show	_	_
the	_	_
mean	_	_
value	_	_
for	_	_
each	_	_
of	_	_
the	_	_
1,913	_	_
images	_	_
,	_	_
whereas	_	_
the	_	_
horizontal	_	_
bars	_	_
show	_	_
the	_	_
mean	_	_
values	_	_
overall	_	_
.	_	_

#441
this	_	_
,	_	_
a	_	_
version	_	_
of	_	_
the	_	_
model	_	_
was	_	_
used	_	_
whereby	_	_
some	_	_
latent	_	_
variables	_	_
controlled	_	_
appearance	_	_
,	_	_
whereas	_	_
others	_	_
controlled	_	_
shape	_	_
.	_	_

#442
The	_	_
motivation	_	_
for	_	_
this	_	_
was	_	_
that	_	_
it	_	_
allows	_	_
the	_	_
different	_	_
types	_	_
of	_	_
features	_	_
to	_	_
be	_	_
differentially	_	_
weighted	_	_
when	_	_
they	_	_
are	_	_
used	_	_
to	_	_
make	_	_
predictions	_	_
.	_	_

#443
The	_	_
algorithm	_	_
was	_	_
run	_	_
on	_	_
the	_	_
full	_	_
3D	_	_
dataset	_	_
,	_	_
using	_	_
70	_	_
variables	_	_
to	_	_
control	_	_
shape	_	_
and	_	_
30	_	_
to	_	_
control	_	_
appearance	_	_
.	_	_

#444
Eight	_	_
iterations	_	_
were	_	_
used	_	_
,	_	_
with	_	_
λ	_	_
=	_	_
[	_	_
1	_	_
1	_	_
]	_	_
,	_	_
ωa	_	_
=	_	_
[	_	_
0.01	_	_
1	_	_
50	_	_
]	_	_
,	_	_
ωµ	_	_
=	_	_
N	_	_
[	_	_
0.00001	_	_
0.01	_	_
0.1	_	_
]	_	_
and	_	_
ωv	_	_
=	_	_
[	_	_
0.001	_	_
0	_	_
10	_	_
0.1	_	_
0.2	_	_
]	_	_
.	_	_

#445
The	_	_
resulting	_	_
model	_	_
fits	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
15	_	_
,	_	_
as	_	_
well	_	_
as	_	_
reconstructions	_	_
using	_	_
only	_	_
the	_	_
appearance	_	_
part	_	_
of	_	_
the	_	_
model	_	_
or	_	_
the	_	_
shape	_	_
model	_	_
part	_	_
.	_	_

#446
This	_	_
result	_	_
can	_	_
be	_	_
compared	_	_
against	_	_
the	_	_
2D	_	_
model	_	_
fit	_	_
shown	_	_
in	_	_
Fig.	_	_
10	_	_
.	_	_

#447
There	_	_
are	_	_
two	_	_
reasons	_	_
why	_	_
the	_	_
3D	_	_
fit	_	_
explains	_	_
a	_	_
smaller	_	_
proportion	_	_
of	_	_
the	_	_
variability	_	_
than	_	_
for	_	_
the	_	_
2D	_	_
examples	_	_
.	_	_

#448
The	_	_
first	_	_
is	_	_
that	_	_
the	_	_
3D	_	_
model	_	_
fit	_	_
uses	_	_
different	_	_
variables	_	_
to	_	_
control	_	_
shape	_	_
and	_	_
appearance	_	_
,	_	_
meaning	_	_
that	_	_
each	_	_
can	_	_
explain	_	_
slightly	_	_
less	_	_
of	_	_
the	_	_
variability	_	_
.	_	_

#449
The	_	_
second	_	_
reason	_	_
is	_	_
simply	_	_
that	_	_
it	_	_
is	_	_
a	_	_
3D	_	_
fit	_	_
,	_	_
so	_	_
that	_	_
there	_	_
is	_	_
a	_	_
great	_	_
deal	_	_
more	_	_
variability	_	_
to	_	_
explain	_	_
.	_	_

#450
The	_	_
main	_	_
objective	_	_
of	_	_
this	_	_
work	_	_
is	_	_
to	_	_
extract	_	_
features	_	_
from	_	_
sets	_	_
of	_	_
medical	_	_
images	_	_
,	_	_
which	_	_
are	_	_
effective	_	_
for	_	_
machine	_	_
learning	_	_
applications	_	_
.	_	_

#451
Here	_	_
,	_	_
a	_	_
five-fold	_	_
cross-validation	_	_
is	_	_
used	_	_
to	_	_
assess	_	_
the	_	_
effectiveness	_	_
of	_	_
these	_	_
features	_	_
.	_	_

#452
Machine	_	_
learning	_	_
used	_	_
a	_	_
linear	_	_
Gaussian	_	_
process	_	_
classification	_	_
procedure	_	_
,	_	_
which	_	_
is	_	_
es3	_	_
RESULTS	_	_
34	_	_
Appearance	_	_
model	_	_
part	_	_
.	_	_

#453
Shape	_	_
model	_	_
part	_	_
.	_	_

#454
Both	_	_
shape	_	_
and	_	_
appearance	_	_
.	_	_

#455
Figure	_	_
15	_	_
:	_	_
An	_	_
illustration	_	_
of	_	_
slice	_	_
40	_	_
(	_	_
one	_	_
out	_	_
of	_	_
91	_	_
slices	_	_
)	_	_
of	_	_
the	_	_
model	_	_
fits	_	_
to	_	_
the	_	_
random	_	_
selection	_	_
of	_	_
3D	_	_
images	_	_
.	_	_

#456
These	_	_
may	_	_
be	_	_
compared	_	_
against	_	_
the	_	_
original	_	_
images	_	_
shown	_	_
in	_	_
Fig.	_	_
10	_	_
.	_	_

#457
3	_	_
RESULTS	_	_
35	_	_

#458
False	_	_
positive	_	_
rate	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
T	_	_
ru	_	_
e	_	_
p	_	_
o	_	_
s	_	_
it	_	_
iv	_	_
e	_	_
r	_	_
a	_	_
te	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
ROC	_	_
Curve	_	_
for	_	_
ABIDE	_	_
data	_	_
(	_	_
AUC=0.6097	_	_
)	_	_
False	_	_
positive	_	_
rate	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
T	_	_
ru	_	_
e	_	_
p	_	_
o	_	_
s	_	_
it	_	_
iv	_	_
e	_	_
r	_	_
a	_	_
te	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
ROC	_	_
Curve	_	_
for	_	_
COBRE	_	_
data	_	_
(	_	_
AUC=0.7825	_	_
)	_	_
Figure	_	_
16	_	_
:	_	_
ROC	_	_
curves	_	_
from	_	_
five-fold	_	_
cross-validation	_	_
accuracies	_	_
from	_	_
the	_	_
ABIDE	_	_
and	_	_
COBRE	_	_
data	_	_
.	_	_

#459
Red	_	_
dots	_	_
show	_	_
the	_	_
point	_	_
on	_	_
the	_	_
curve	_	_
where	_	_
the	_	_
classification	_	_
gives	_	_
probabilities	_	_
of	_	_
0.5.	_	_
sentially	_	_
equivalent	_	_
to	_	_
a	_	_
Bayesian	_	_
approach	_	_
to	_	_
logistic	_	_
regression	_	_
.	_	_

#460
The	_	_
implementation	_	_
was	_	_
based	_	_
on	_	_
the	_	_
method	_	_
for	_	_
binary	_	_
classification	_	_
using	_	_
expectation	_	_
propagation	_	_
described	_	_
in	_	_
[	_	_
18	_	_
]	_	_
.	_	_

#461
For	_	_
the	_	_
COBRE	_	_
dataset	_	_
,	_	_
classification	_	_
involved	_	_
separating	_	_
controls	_	_
from	_	_
patients	_	_
with	_	_
schizophrenia	_	_
.	_	_

#462
Similarly	_	_
,	_	_
the	_	_
analysis	_	_
of	_	_
the	_	_
ABIDE	_	_
dataset	_	_
involved	_	_
identifying	_	_
those	_	_
subjects	_	_
on	_	_
the	_	_
autism	_	_
spectrum	_	_
,	_	_
with	_	_
features	_	_
orthogonalised	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
different	_	_
sites	_	_
.	_	_

#463
Classification	_	_
involved	_	_
three	_	_
hyper-parameters	_	_
,	_	_
which	_	_
weighted	_	_
the	_	_
contributions	_	_
from	_	_
shape	_	_
features	_	_
,	_	_
appearance	_	_
features	_	_
and	_	_
a	_	_
constant	_	_
offset	_	_
.	_	_

#464
ROC	_	_
curves	_	_
are	_	_
shown	_	_
in	_	_
Fig.	_	_
16	_	_
.	_	_

#465
For	_	_
ABIDE	_	_
,	_	_
the	_	_
accuracy	_	_
was	_	_
57.6	_	_
%	_	_
.	_	_

#466
While	_	_
this	_	_
is	_	_
not	_	_
especially	_	_
high	_	_
,	_	_
it	_	_
is	_	_
close	_	_
to	_	_
the	_	_
accuracy	_	_
reported	_	_
by	_	_
others	_	_
who	_	_
have	_	_
applied	_	_
machine	_	_
learning	_	_
to	_	_
the	_	_
T1-weighted	_	_
scans	_	_
.	_	_

#467
For	_	_
example	_	_
,	_	_
[	_	_
56	_	_
]	_	_
reported	_	_
60.1	_	_
%	_	_
classification	_	_
accuracy	_	_
with	_	_
the	_	_
same	_	_
data	_	_
using	_	_
a	_	_
support-vector	_	_
machine	_	_
with	_	_
radial	_	_
basis	_	_
functions	_	_
.	_	_

#468
The	_	_
accuracy	_	_
achieved	_	_
for	_	_
the	_	_
COBRE	_	_
dataset	_	_
was	_	_
74.7	_	_
%	_	_
,	_	_
which	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
69.7	_	_
%	_	_
accuracy	_	_
reported	_	_
by	_	_
[	_	_
57	_	_
]	_	_
using	_	_
COBRE	_	_
.	_	_

#469
[	_	_
58	_	_
]	_	_
achieved	_	_
71.4	_	_
%	_	_
accuracy	_	_
for	_	_
separating	_	_
controls	_	_
from	_	_
subjects	_	_
with	_	_
schizophrenia	_	_
,	_	_
but	_	_
using	_	_
a	_	_
different	_	_
(	_	_
larger	_	_
)	_	_
dataset	_	_
of	_	_
T1-weighted	_	_
scans	_	_
.	_	_

#470
Both	_	_
results	_	_
were	_	_
comparable	_	_
to	_	_
those	_	_
obtained	_	_
by	_	_
[	_	_
59	_	_
]	_	_
.	_	_

#471
4	_	_
DISCUSSION	_	_
36	_	_

#472
4	_	_
.	_	_

#473
Discussion	_	_
This	_	_
work	_	_
presents	_	_
a	_	_
very	_	_
general	_	_
generative	_	_
framework	_	_
that	_	_
may	_	_
have	_	_
widespread	_	_
use	_	_
within	_	_
the	_	_
medical	_	_
imaging	_	_
community	_	_
,	_	_
particularly	_	_
for	_	_
those	_	_
situations	_	_
where	_	_
conventional	_	_
image	_	_
registration	_	_
approaches	_	_
are	_	_
more	_	_
likely	_	_
to	_	_
fail	_	_
.	_	_

#474
As	_	_
in	_	_
many	_	_
image	_	_
analysis	_	_
procedures	_	_
,	_	_
there	_	_
is	_	_
a	_	_
certain	_	_
amount	_	_
of	_	_
circularity	_	_
arising	_	_
from	_	_
the	_	_
dependencies	_	_
among	_	_
variables	_	_
.	_	_

#475
These	_	_
circularities	_	_
are	_	_
not	_	_
handled	_	_
well	_	_
by	_	_
using	_	_
a	_	_
single	_	_
pass	_	_
through	_	_
a	_	_
conventional	_	_
pipeline	_	_
.	_	_

#476
In	_	_
contrast	_	_
,	_	_
by	_	_
formulating	_	_
the	_	_
problem	_	_
within	_	_
a	_	_
generative	_	_
modelling	_	_
framework	_	_
,	_	_
the	_	_
algorithm	_	_
is	_	_
better	_	_
able	_	_
to	_	_
deal	_	_
with	_	_
them	_	_
.	_	_

#477
Because	_	_
of	_	_
its	_	_
generality	_	_
,	_	_
the	_	_
model	_	_
we	_	_
presented	_	_
should	_	_
provide	_	_
a	_	_
good	_	_
starting	_	_
point	_	_
for	_	_
a	_	_
number	_	_
of	_	_
avenues	_	_
of	_	_
further	_	_
development	_	_
,	_	_
in	_	_
addition	_	_
to	_	_
those	_	_
previously	_	_
mentioned	_	_
that	_	_
relate	_	_
to	_	_
going	_	_
beyond	_	_
a	_	_
simple	_	_
Gaussian	_	_
distribution	_	_
for	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#478
Most	_	_
image	_	_
analysis	_	_
applications	_	_
have	_	_
a	_	_
number	_	_
of	_	_
settings	_	_
to	_	_
be	_	_
tuned	_	_
,	_	_
and	_	_
the	_	_
current	_	_
approach	_	_
is	_	_
no	_	_
exception	_	_
.	_	_

#479
Although	_	_
this	_	_
tuning	_	_
is	_	_
rarely	_	_
discussed	_	_
in	_	_
papers	_	_
,	_	_
the	_	_
settings	_	_
can	_	_
have	_	_
quite	_	_
a	_	_
large	_	_
impact	_	_
on	_	_
any	_	_
results	_	_
.	_	_

#480
The	_	_
effects	_	_
of	_	_
different	_	_
forms	_	_
of	_	_
regularisation	_	_
used	_	_
for	_	_
registration	_	_
are	_	_
illustrated	_	_
in	_	_
[	_	_
60	_	_
]	_	_
.	_	_

#481
The	_	_
choice	_	_
of	_	_
settings	_	_
plays	_	_
a	_	_
more	_	_
important	_	_
role	_	_
when	_	_
training	_	_
using	_	_
only	_	_
a	_	_
small	_	_
number	_	_
of	_	_
images	_	_
.	_	_

#482
With	_	_
more	_	_
images	_	_
,	_	_
the	_	_
contribution	_	_
of	_	_
these	_	_
priors	_	_
becomes	_	_
less	_	_
influential	_	_
.	_	_

#483
One	_	_
aspect	_	_
of	_	_
the	_	_
work	_	_
that	_	_
may	_	_
need	_	_
additional	_	_
attention	_	_
is	_	_
the	_	_
setting	_	_
of	_	_
λ1	_	_
and	_	_
λ2	_	_
.	_	_

#484
The	_	_
ideal	_	_
settings	_	_
would	_	_
be	_	_
λ1	_	_
=	_	_
1	_	_
and	_	_
λ2	_	_
=	_	_
0	_	_
,	_	_
but	_	_
in	_	_
practice	_	_
,	_	_
greater	_	_
regularisation	_	_
is	_	_
required	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
good	_	_
results	_	_
.	_	_

#485
A	_	_
plausible	_	_
explanation	_	_
for	_	_
this	_	_
would	_	_
be	_	_
that	_	_
assumptions	_	_
of	_	_
i.i.d	_	_
.	_	_

#486
noise	_	_
are	_	_
not	_	_
generally	_	_
met	_	_
,	_	_
so	_	_
a	_	_
“virtual	_	_
decimation	_	_
factor”	_	_
,	_	_
which	_	_
accounts	_	_
for	_	_
correlations	_	_
among	_	_
residuals	_	_
,	_	_
may	_	_
need	_	_
to	_	_
be	_	_
accounted	_	_
for	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#487
The	_	_
fact	_	_
that	_	_
the	_	_
approach	_	_
is	_	_
not	_	_
fully	_	_
Bayesian	_	_
(	_	_
i.e.	_	_
,	_	_
it	_	_
only	_	_
makes	_	_
point	_	_
estimates	_	_
of	_	_
many	_	_
parameters	_	_
and	_	_
latent	_	_
variables	_	_
,	_	_
rather	_	_
than	_	_
properly	_	_
accounting	_	_
for	_	_
their	_	_
uncertainty	_	_
)	_	_
is	_	_
likely	_	_
to	_	_
be	_	_
another	_	_
reason	_	_
why	_	_
additional	_	_
regularisation	_	_
is	_	_
needed	_	_
.	_	_

#488
Greater	_	_
regularisation	_	_
also	_	_
seems	_	_
to	_	_
help	_	_
avoid	_	_
local	_	_
optima	_	_
,	_	_
suggesting	_	_
that	_	_
further	_	_
investigation	_	_
into	_	_
coarse-to-fine	_	_
strategies	_	_
may	_	_
be	_	_
warranted	_	_
.	_	_

#489
For	_	_
computational	_	_
reasons	_	_
,	_	_
the	_	_
uncertainty	_	_
with	_	_
which	_	_
the	_	_
latent	_	_
variables	_	_

#490
4	_	_
DISCUSSION	_	_
37	_	_

#491
(	_	_
z	_	_
)	_	_
are	_	_
estimated	_	_
is	_	_
not	_	_
treated	_	_
in	_	_
a	_	_
properly	_	_
Bayesian	_	_
way	_	_
.	_	_

#492
In	_	_
theory	_	_
,	_	_
more	_	_
accurate	_	_
results	_	_
would	_	_
be	_	_
achieved	_	_
by	_	_
marginalising	_	_
with	_	_
respect	_	_
to	_	_
these	_	_
latent	_	_
variables	_	_
.	_	_

#493
This	_	_
strategy	_	_
is	_	_
relatively	_	_
straightforward	_	_
for	_	_
PCA	_	_
with	_	_
simple	_	_
Gaussian	_	_
noise	_	_
assumptions	_	_
[	_	_
61	_	_
]	_	_
,	_	_
but	_	_
becomes	_	_
more	_	_
difficult	_	_
for	_	_
other	_	_
types	_	_
of	_	_
data	_	_
.	_	_

#494
Monte-Carlo	_	_
approaches	_	_
[	_	_
62	_	_
,	_	_
32	_	_
,	_	_
63	_	_
]	_	_
are	_	_
currently	_	_
too	_	_
slow	_	_
for	_	_
routine	_	_
use	_	_
on	_	_
large	_	_
datasets	_	_
,	_	_
although	_	_
a	_	_
reasonable	_	_
alternative	_	_
might	speculation	_
be	_	_
to	_	_
use	_	_
variational	_	_
Bayesian	_	_
methods	_	_
.	_	_

#495
For	_	_
example	_	_
,	_	_
[	_	_
64	_	_
]	_	_
used	_	_
a	_	_
local	_	_
variational	_	_
approximation	_	_
of	_	_
the	_	_
log	_	_
partition	_	_
function	_	_
to	_	_
do	_	_
a	_	_
type	_	_
of	_	_
generalised	_	_
PCA	_	_
of	_	_
binary	_	_
data	_	_
.	_	_

#496
[	_	_
65	_	_
]	_	_
used	_	_
a	_	_
related	_	_
approach	_	_
for	_	_
modelling	_	_
the	_	_
shape	_	_
variability	_	_
of	_	_
binary	_	_
images	_	_
,	_	_
although	_	_
this	_	_
work	_	_
used	_	_
a	_	_
different	_	_
local	_	_
approximation	_	_
.	_	_

#497
Local	_	_
variational	_	_
approaches	_	_
appear	_	_
to	_	_
be	_	_
more	_	_
difficult	_	_
to	_	_
extend	_	_
to	_	_
categorical	_	_
data	_	_
with	_	_
more	_	_
than	_	_
two	_	_
classes	_	_
,	_	_
and	_	_
the	_	_
image	_	_
deformation	_	_
part	_	_
of	_	_
the	_	_
model	_	_
would	_	_
probably	_	_
lead	_	_
to	_	_
many	_	_
further	_	_
difficulties	_	_
for	_	_
variational	_	_
Bayesian	_	_
methods	_	_
.	_	_

#498
There	_	_
are	_	_
a	_	_
number	_	_
of	_	_
directions	_	_
in	_	_
which	_	_
the	_	_
current	_	_
work	_	_
could	_	_
be	_	_
extended	_	_
.	_	_

#499
One	_	_
of	_	_
the	_	_
first	_	_
areas	_	_
we	_	_
plan	_	_
to	_	_
investigate	_	_
is	_	_
to	_	_
integrate	_	_
the	_	_
approach	_	_
with	_	_
current	_	_
strategies	_	_
for	_	_
generating	_	_
tissue	_	_
probability	_	_
maps	_	_
[	_	_
66	_	_
,	_	_
67	_	_
,	_	_
22	_	_
]	_	_
.	_	_

#500
Rather	_	_
than	_	_
first	_	_
segmenting	_	_
images	_	_
into	_	_
different	_	_
tissue	_	_
classes	_	_
and	_	_
then	_	_
fitting	_	_
the	_	_
proposed	_	_
model	_	_
to	_	_
these	_	_
,	_	_
it	_	_
would	_	_
be	_	_
more	_	_
elegant	_	_
to	_	_
properly	_	_
combine	_	_
the	_	_
approaches	_	_
into	_	_
a	_	_
single	_	_
unified	_	_
generative	_	_
model	_	_
of	_	_
the	_	_
original	_	_
images	_	_
.	_	_

#501
In	_	_
principle	_	_
,	_	_
this	_	_
strategy	_	_
could	_	_
lead	_	_
to	_	_
more	_	_
robust	_	_
segmentation	_	_
because	_	_
the	_	_
ways	_	_
in	_	_
which	_	_
tissue	_	_
priors	_	_
are	_	_
allowed	_	_
to	_	_
vary	_	_
are	_	_
constrained	_	_
to	_	_
be	_	_
more	_	_
biologically	_	_
plausible	_	_
.	_	_

#502
Another	_	_
avenue	_	_
is	_	_
to	_	_
allow	_	_
some	_	_
shape	_	_
variability	_	_
beyond	_	_
what	_	_
can	_	_
be	_	_
encoded	_	_
by	_	_
the	_	_
first	_	_
few	_	_
eigenmodes	_	_
.	_	_

#503
For	_	_
example	_	_
,	_	_
[	_	_
68	_	_
]	_	_
combined	_	_
the	_	_
eigenmode	_	_
representation	_	_
with	_	_
a	_	_
model	_	_
of	_	_
additional	_	_
shape	_	_
variability	_	_
,	_	_
giving	_	_
a	_	_
framework	_	_
that	_	_
is	_	_
conceptually	_	_
related	_	_
to	_	_
that	_	_
of	_	_
[	_	_
69	_	_
]	_	_
,	_	_
as	_	_
this	_	_
allows	_	_
a	_	_
covariance	_	_
matrix	_	_
over	_	_
velocity	_	_
fields	_	_
to	_	_
be	_	_
defined	_	_
and	_	_
optimised	_	_
.	_	_

#504
Approaches	_	_
building	_	_
on	_	_
[	_	_
70	_	_
]	_	_
,	_	_
who	_	_
proposed	_	_
to	_	_
learn	_	_
the	_	_
residual	_	_
covariance	_	_
by	_	_
assuming	_	_
finite	_	_
support	_	_
(	_	_
and	_	_
thus	_	_
sparsity	_	_
)	_	_
of	_	_
the	_	_
underlying	_	_
covariance	_	_
function	_	_
,	_	_
might	rhetorical-speculation	_
also	_	_
be	_	_
potentially	_	_
interesting	_	_
.	_	_

#505
The	_	_
framework	_	_
would	_	_
also	_	_
generalise	_	_
further	_	_
for	_	_
handling	_	_
paired	_	_
or	_	_
multi-view	_	_
data	_	_
,	_	_
which	_	_
could	_	_
add	_	_
a	_	_
degree	_	_
of	_	_
supervision	_	_
to	_	_
the	_	_
method	_	_
.	_	_

#506
There	_	_
have	_	_
been	_	_
a	_	_
number	_	_
of	_	_
publications	_	_
on	_	_
generating	_	_
age-	_	_
or	_	_
gender-specific	_	_
templates	_	_
,	_	_
or	_	_
on	_	_
geodesic	_	_
regression	_	_
approaches	_	_
[	_	_
71	_	_
,	_	_
72	_	_
]	_	_
for	_	_
modelling	_	_
trajectories	_	_
of	_	_
ageing	_	_
.	_	_

#507
Concepts	_	_
from	_	_
joint	_	_
matrix	_	_
factorisation	_	_
approaches	_	_
,	_	_
such	_	_
as	_	_
canonical	_	_
correlation	_	_
analysis	_	_
[	_	_
73	_	_
,	_	_
74	_	_
]	_	_
,	_	_
could	_	_
be	_	_
integrated	_	_
into	_	_
the	_	_
current	_	_
work	_	_
,	_	_
and	_	_
these	_	_
could	_	_
be	_	_
used	_	_
to	_	_
allow	_	_
the	_	_
model	_	_
fitting	_	_
to	_	_
be	_	_
informed	_	_
by	_	_
age	_	_
,	_	_
gender	_	_
,	_	_
disease	_	_
status	_	_
etc	_	_
.	_	_

#508
Acknowledgements	_	_
This	_	_
project	_	_
has	_	_
received	_	_
funding	_	_
from	_	_
the	_	_
European	_	_
Unions	_	_
Horizon	_	_
2020	_	_
Research	_	_
and	_	_
Innovation	_	_
Programme	_	_
under	_	_
Grant	_	_
Agreement	_	_
No	_	_
.	_	_

#509
720270	_	_
(	_	_
HBP	_	_
SGA1	_	_
)	_	_
.	_	_

#510
The	_	_
Wellcome	_	_
Centre	_	_
for	_	_
Human	_	_
Neuroimaging	_	_
is	_	_
supported	_	_
by	_	_
core	_	_
funding	_	_
from	_	_
the	_	_
Wellcome	_	_
Trust	_	_
[	_	_
grant	_	_
number	_	_
203147/Z/16/Z	_	_
]	_	_
.	_	_

#511
Funding	_	_
for	_	_
the	_	_
OASIS	_	_
dataset	_	_
came	_	_
from	_	_
the	_	_
following	_	_
grants	_	_
:	_	_
P50	_	_
AG05681	_	_
,	_	_
P01	_	_
AG03991	_	_
,	_	_
R01	_	_
AG021910	_	_
,	_	_
P20	_	_
MH071616	_	_
,	_	_
U24	_	_
RR021382	_	_
.	_	_

#512
Funding	_	_
for	_	_
the	_	_
ABIDE	_	_
I	_	_
dataset	_	_
came	_	_
from	_	_
a	_	_
variety	_	_
of	_	_
sources	_	_
,	_	_
which	_	_
include	_	_
NIMH	_	_
(	_	_
K23MH087770	_	_
and	_	_
R03MH096321	_	_
)	_	_
,	_	_
the	_	_
Leon	_	_
Levy	_	_
Foundation	_	_
,	_	_
Joseph	_	_
P.	_	_
Healy	_	_
and	_	_
the	_	_
Stavros	_	_
Niarchos	_	_
Foundation	_	_
to	_	_
the	_	_
Child	_	_
Mind	_	_
Institute	_	_
.	_	_

#513
Appendix	_	_
A.	_	_
Algorithm	_	_
A	_	_
highly	_	_
simplified	_	_
version	_	_
of	_	_
the	_	_
factorisation	_	_
algorithm	_	_
is	_	_
shown	_	_
in	_	_
Algorithm	_	_
1	_	_
.	_	_

#514
It	_	_
is	_	_
an	_	_
expectation	_	_
maximisation	_	_
approach	_	_
,	_	_
which	_	_
involves	_	_
alternating	_	_
between	_	_
computing	_	_
the	_	_
shape	_	_
and	_	_
appearance	_	_
basis	_	_
functions	_	_
(	_	_
plus	_	_
a	_	_
few	_	_
other	_	_
variables	_	_
–	_	_
M-step	_	_
)	_	_
,	_	_
and	_	_
computing	_	_
the	_	_
expectations	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
(	_	_
Estep	_	_
)	_	_
.	_	_

#515
For	_	_
better	_	_
convergence	_	_
of	_	_
the	_	_
basis	_	_
function	_	_
updates	_	_
of	_	_
the	_	_
M-step	_	_
,	_	_
an	_	_
orthogonalisation	_	_
step	_	_
is	_	_
included	_	_
in	_	_
the	_	_
algorithm	_	_
.	_	_

#516
The	_	_
M-step	_	_
relies	_	_
on	_	_
Gauss-Newton	_	_
updates	_	_
of	_	_
three	_	_
elements	_	_
:	_	_
the	_	_
mean	_	_
template	_	_
(	_	_
µ	_	_
)	_	_
,	_	_
shape	_	_
subspace	_	_
(	_	_
Wa	_	_
)	_	_
and	_	_
appearance	_	_
subspace	_	_
(	_	_
Wv	_	_
)	_	_
.	_	_

#517
These	_	_
updates	_	_
have	_	_
the	_	_
general	_	_
form	_	_
of	_	_
w	_	_
←	_	_
w	_	_
−	_	_
(	_	_
H	_	_
+	_	_
L	_	_
)	_	_
−1	_	_
(	_	_
g	_	_
+	_	_
Lw	_	_
)	_	_
,	_	_
where	_	_
L	_	_
is	_	_
a	_	_
very	_	_
sparse	_	_
Toeplitz	_	_
or	_	_
circulant	_	_
matrix	_	_
encoding	_	_
spatial	_	_
regularisation	_	_
,	_	_
and	_	_
H	_	_
encodes	_	_
a	_	_
field	_	_
of	_	_
small	_	_
matrices	_	_
that	_	_
are	_	_
easy	_	_
to	_	_
invert	_	_
.	_	_

#518
The	_	_
full-multigrid	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
39	_	_
method	_	_
,	_	_
described	_	_
in	_	_
[	_	_
75	_	_
]	_	_
,	_	_
is	_	_
particularly	_	_
well	_	_
suited	_	_
to	_	_
solving	_	_
this	_	_
type	_	_
of	_	_
problem	_	_
.	_	_

#519
The	_	_
E-step	_	_
involves	_	_
updating	_	_
the	_	_
distributions	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
(	_	_
Z	_	_
)	_	_
and	_	_
Gaussian	_	_
prior	_	_
(	_	_
A	_	_
)	_	_
.	_	_

#520
To	_	_
break	_	_
the	_	_
initial	_	_
symmetry	_	_
,	_	_
the	_	_
latent	_	_
variables	_	_
are	_	_
all	_	_
initialised	_	_
randomly	_	_
,	_	_
while	_	_
ensuring	_	_
that	_	_
ẐẐT	_	_
is	_	_
orthonormal	_	_
.	_	_

#521
This	_	_
is	_	_
easier	_	_
than	_	_
random	_	_
initialisation	_	_
of	_	_
Ŵa	_	_
and	_	_
Ŵv	_	_
,	_	_
which	_	_
are	_	_
instead	_	_
initialised	_	_
to	_	_
zero	_	_
.	_	_

#522
In	_	_
most	_	_
of	_	_
this	_	_
appendix	_	_
,	_	_
matrices	_	_
are	_	_
written	_	_
in	_	_
bold	_	_
upper-case	_	_
(	_	_
e.g.	_	_
,	_	_
Wa	_	_
,	_	_
Z	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#523
In	_	_
the	_	_
computations	_	_
,	_	_
images	_	_
are	_	_
treated	_	_
as	_	_
vectors	_	_
.	_	_

#524
These	_	_
are	_	_
written	_	_
as	_	_
lower-case	_	_
bold	_	_
,	_	_
which	_	_
includes	_	_
the	_	_
notation	_	_
for	_	_
individual	_	_
columns	_	_
of	_	_
various	_	_
matrices	_	_
(	_	_
e.g.	_	_
,	_	_
wa	_	_
k	_	_
denotes	_	_
the	_	_
kth	_	_
column	_	_
of	_	_
Wa	_	_
,	_	_
zn	_	_
denotes	_	_
the	_	_
nth	_	_
column	_	_
of	_	_
Z	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#525
Scalars	_	_
are	_	_
written	_	_
in	_	_
italic	_	_
,	_	_
with	_	_
dimensions	_	_
in	_	_
upper-case	_	_
.	_	_

#526
Estimates	_	_
or	_	_
expectations	_	_
of	_	_
parameters	_	_
are	_	_
written	_	_
with	_	_
a	_	_
circumflex	_	_
(	_	_
e.g.	_	_
,	_	_
Ẑ	_	_
)	_	_
.	_	_

#527
Collections	_	_
of	_	_
vectors	_	_
may	_	_
be	_	_
conceptualised	_	_
as	_	_
matrices	_	_
,	_	_
so	_	_
are	_	_
written	_	_
in	_	_
bold-upper-case	_	_
(	_	_
e.g.	_	_
,	_	_
Ga	_	_
,	_	_
where	_	_
individual	_	_
vectors	_	_
are	_	_
gak	_	_
)	_	_
.	_	_

#528
Collections	_	_
of	_	_
matrices	_	_
are	_	_
written	_	_
in	_	_
“mathcal”	_	_
font	_	_
(	_	_
e.g.	_	_
,	_	_
Ha	_	_
,	_	_
where	_	_
individual	_	_
matrices	_	_
are	_	_
Ha	_	_
kk	_	_
)	_	_
.	_	_

#529
There	_	_
is	_	_
no	_	_
systematic	_	_
rule	_	_
about	_	_
the	_	_
use	_	_
of	_	_
Greek	_	_
letters	_	_
,	_	_
although	_	_
sometimes	_	_
warping	_	_
an	_	_
image	_	_
may	_	_
be	_	_
written	_	_
as	_	_
a	_	_
(	_	_
ψ	_	_
)	_	_
and	_	_
at	_	_
other	_	_
times	_	_
,	_	_
warping	_	_
may	_	_
be	_	_
conceptualised	_	_
as	_	_
matrix	_	_
multiplication	_	_
and	_	_
written	_	_
as	_	_
Ψa	_	_
.	_	_

#530
The	_	_
matrix	_	_
transpose	_	_
operation	_	_
is	_	_
denoted	_	_
by	_	_
the	_	_
“T	_	_
”	_	_
superscript	_	_
(	_	_
as	_	_
in	_	_
ΨT	_	_
)	_	_
.	_	_

#531
Creating	_	_
a	_	_
diagonal	_	_
matrix	_	_
from	_	_
a	_	_
vector	_	_
(	_	_
as	_	_
in	_	_
diag	_	_
(	_	_
exp	_	_
q	_	_
)	_	_
)	_	_
,	_	_
as	_	_
well	_	_
as	_	_
treating	_	_
the	_	_
diagonal	_	_
elements	_	_
of	_	_
a	_	_
matrix	_	_
as	_	_
a	_	_
vector	_	_
(	_	_
as	_	_
in	_	_
diag	_	_
(	_	_
Q	_	_
)	_	_
)	_	_
are	_	_
both	_	_
denoted	_	_
by	_	_
“diag”	_	_
.	_	_

#532
The	_	_
trace	_	_
of	_	_
a	_	_
matrix	_	_
(	_	_
sum	_	_
of	_	_
diagonal	_	_
elements	_	_
)	_	_
is	_	_
denoted	_	_
by	_	_
“Tr”	_	_
.	_	_

#533
Sometimes	_	_
,	_	_
gradients	_	_
of	_	_
an	_	_
image	_	_
are	_	_
required	_	_
.	_	_

#534
In	_	_
3D	_	_
,	_	_
the	_	_
three	_	_
components	_	_
of	_	_
the	_	_
spatial	_	_
gradient	_	_
of	_	_
a	_	_
would	_	_
be	_	_
denoted	_	_
by	_	_
∇1a	_	_
,	_	_
∇2a	_	_
and	_	_
∇3a	_	_
.	_	_

#535
Comments	_	_
in	_	_
Algorithm	_	_
1	_	_
saying	_	_
“Dist”	_	_
indicate	_	_
which	_	_
steps	_	_
should	_	_
be	_	_
modified	_	_
for	_	_
running	_	_
within	_	_
a	_	_
distributed	_	_
privacy-preserving	_	_
framework	_	_
.	_	_

#536
The	_	_
idea	_	_
here	_	_
is	_	_
that	_	_
the	_	_
main	_	_
procedure	_	_
would	_	_
be	_	_
run	_	_
on	_	_
the	_	_
“master”	_	_
computer	_	_
,	_	_
whereas	_	_
various	_	_
functions	_	_
would	_	_
be	_	_
run	_	_
on	_	_
the	_	_
“worker”	_	_
machines	_	_
on	_	_
which	_	_
the	_	_
data	_	_
reside	_	_
.	_	_

#537
These	_	_
workers	_	_
would	_	_
only	_	_
pass	_	_
aggregate	_	_
data	_	_
back	_	_
to	_	_
the	_	_
master	_	_
,	_	_
whereas	_	_
the	_	_
latent	_	_
variables	_	_
,	_	_
which	_	_
explicitly	_	_
encode	_	_
information	_	_
about	_	_
individuals	_	_
,	_	_
would	_	_
remain	_	_
on	_	_
the	_	_
workers	_	_
.	_	_

#538
As	_	_
the	_	_
algorithm	_	_
is	_	_
described	_	_
here	_	_
,	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
40	_	_
the	_	_
images	_	_
(	_	_
F	_	_
)	_	_
and	_	_
expectations	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
Ẑ	_	_
are	_	_
passed	_	_
back	_	_
and	_	_
forth	_	_
between	_	_
the	_	_
master	_	_
and	_	_
workers	_	_
,	_	_
but	_	_
this	_	_
need	_	_
not	_	_
be	_	_
the	_	_
case	_	_
.	_	_

#539
If	_	_
these	_	_
data	_	_
and	_	_
variables	_	_
were	_	_
all	_	_
to	_	_
reside	_	_
on	_	_
the	_	_
worker	_	_
machines	_	_
,	_	_
the	_	_
master	_	_
machine	_	_
would	_	_
still	_	_
be	_	_
able	_	_
to	_	_
run	_	_
using	_	_
only	_	_
the	_	_
aggregate	_	_
data	_	_
.	_	_

#540
For	_	_
simplicity	_	_
,	_	_
Algorithm	_	_
1	_	_
does	_	_
not	_	_
include	_	_
functions	_	_
for	_	_
computing	_	_
variances	_	_
(	_	_
σ2	_	_
used	_	_
by	_	_
the	_	_
Gaussian	_	_
noise	_	_
model	_	_
)	_	_
,	_	_
etc.	_	_
,	_	_
and	_	_
these	_	_
variables	_	_
are	_	_
not	_	_
shown	_	_
to	_	_
be	_	_
passed	_	_
to	_	_
the	_	_
various	_	_
functions	_	_
that	_	_
use	_	_
them	_	_
.	_	_

#541
However	_	_
,	_	_
it	_	_
should	_	_
be	_	_
easy	_	_
to	_	_
see	_	_
how	_	_
these	_	_
changes	_	_
would	_	_
be	_	_
incorporated	_	_
in	_	_
practice	_	_
.	_	_

#542
Also	_	_
,	_	_
the	_	_
illustration	_	_
does	_	_
not	_	_
show	_	_
any	_	_
steps	_	_
requiring	_	_
the	_	_
objective	_	_
function	_	_
,	_	_
which	_	_
include	_	_
various	_	_
backtracking	_	_
line-searches	_	_
to	_	_
ensure	_	_
that	_	_
parameter	_	_
updates	_	_
cause	_	_
the	_	_
objective	_	_
function	_	_
to	_	_
improve	_	_
each	_	_
time	_	_
.	_	_

#543
In	_	_
practice	_	_
,	_	_
the	_	_
algorithm	_	_
is	_	_
run	_	_
for	_	_
a	_	_
fixed	_	_
number	_	_
of	_	_
iterations	_	_
,	_	_
although	_	_
the	_	_
log-likelihood	_	_
could	_	_
be	_	_
used	_	_
to	_	_
determine	_	_
when	_	_
to	_	_
stop	_	_
.	_	_

#544
Appendix	_	_
A.1	_	_
.	_	_

#545
Updating	_	_
the	_	_
mean	_	_
(	_	_
µ̂	_	_
)	_	_
From	_	_
Eqn	_	_
.	_	_

#546
12	_	_
,	_	_
we	_	_
see	_	_
that	_	_
a	_	_
point	_	_
estimate	_	_
of	_	_
the	_	_
mean	_	_
(	_	_
µ	_	_
)	_	_
may	_	_
be	_	_
computed	_	_
by	_	_
µ̂	_	_
=	_	_
arg	_	_
min	_	_
µ	_	_
(	_	_
2µ	_	_
TLµµ+	_	_
N∑	_	_
n=1	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
ẑn	_	_
,	_	_
µ	_	_
,	_	_
Ŵ	_	_
a	_	_
,	_	_
Ŵv	_	_
)	_	_
)	_	_
.	_	_

#547
(	_	_
A.1	_	_
)	_	_
In	_	_
practice	_	_
,	_	_
this	_	_
log	_	_
probability	_	_
is	_	_
not	_	_
fully	_	_
maximised	_	_
with	_	_
respect	_	_
to	_	_
µ	_	_
at	_	_
each	_	_
iteration	_	_
.	_	_

#548
Instead	_	_
,	_	_
µ̂	_	_
is	_	_
updated	_	_
by	_	_
a	_	_
single	_	_
Gauss-Newton	_	_
iteration	_	_
.	_	_

#549
This	_	_
requires	_	_
gradients	_	_
and	_	_
Hessians	_	_
computed	_	_
as	_	_
shown	_	_
in	_	_
Algorithm	_	_
2	_	_
,	_	_
which	_	_
simply	_	_
involves	_	_
summing	_	_
over	_	_
those	_	_
computed	_	_
for	_	_
the	_	_
individual	_	_
images	_	_
.	_	_

#550
A	_	_
small	_	_
amount	_	_
of	_	_
regularisation	_	_
is	_	_
used	_	_
for	_	_
the	_	_
estimate	_	_
of	_	_
the	_	_
mean	_	_
.	_	_

#551
One	_	_
of	_	_
the	_	_
reasons	_	_
for	_	_
using	_	_
this	_	_
is	_	_
that	_	_
it	_	_
alleviates	_	_
some	_	_
of	_	_
the	_	_
problems	_	_
with	_	_
the	_	_
gradients	_	_
of	_	_
Jacobian-weighted	_	_
averaging	_	_
that	_	_
was	_	_
pointed	_	_
out	_	_
in	_	_
Appendix	_	_
B	_	_
of	_	_
[	_	_
60	_	_
]	_	_
,	_	_
thus	_	_
leading	_	_
to	_	_
better	_	_
convergence	_	_
.	_	_

#552
This	_	_
is	_	_
especially	_	_
important	_	_
in	_	_
situations	_	_
where	_	_
it	_	_
can	_	_
help	_	_
to	_	_
smooth	_	_
over	_	_
some	_	_
of	_	_
the	_	_
effects	_	_
of	_	_
missing	_	_
data	_	_
.	_	_

#553
Appendix	_	_
A.2	_	_
.	_	_

#554
Likelihood	_	_
derivatives	_	_
The	_	_
algorithm	_	_
can	_	_
be	_	_
run	_	_
using	_	_
a	_	_
number	_	_
of	_	_
different	_	_
appearance	_	_
models	_	_
,	_	_
and	_	_
the	_	_
gradients	_	_
and	_	_
Hessians	_	_
involved	_	_
in	_	_
the	_	_
Gauss-Newton	_	_
updates	_	_
depend	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
41	_	_
Algorithm	_	_
1	_	_
Shape	_	_
and	_	_
appearance	_	_
model	_	_
Initialize	_	_
variables	_	_
.	_	_

#555
repeat	_	_
gµ	_	_
,	_	_
Hµ	_	_
←	_	_
MeanDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
.	_	_

#556
Dist	_	_
µ̂←	_	_
µ̂−	_	_
(	_	_
Hµ	_	_
+	_	_
Lµ	_	_
)	_	_
−1	_	_
(	_	_
gµ	_	_
+	_	_
Lµµ̂	_	_
)	_	_
Gv	_	_
,	_	_
Hv	_	_
←	_	_
ShapeDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
.	_	_

#557
Dist	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Kv	_	_
do	_	_
ŵv	_	_
k	_	_
←	_	_
ŵv	_	_
k	_	_
−	_	_
(	_	_
Hv	_	_
kk	_	_
+	_	_
(	_	_
λ1N	_	_
+	_	_
λ2c	_	_
z	_	_
kk	_	_
)	_	_
Lv	_	_
)	_	_
−1	_	_
(	_	_
gvk	_	_
+	_	_
(	_	_
λ1N	_	_
+	_	_
λ2c	_	_
z	_	_
kk	_	_
)	_	_
Lvŵv	_	_
k	_	_
)	_	_
end	_	_
for	_	_
Ga	_	_
,	_	_
Ha	_	_
←	_	_
AppearanceDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
.	_	_

#558
Dist	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Ka	_	_
do	_	_
ŵa	_	_
k	_	_
←	_	_
ŵa	_	_
k	_	_
−	_	_
(	_	_
Ha	_	_
kk	_	_
+	_	_
(	_	_
λ1N	_	_
+	_	_
λ2c	_	_
z	_	_
kk	_	_
)	_	_
La	_	_
)	_	_
−1	_	_
(	_	_
gak	_	_
+	_	_
(	_	_
λ1N	_	_
+	_	_
λ2c	_	_
z	_	_
kk	_	_
)	_	_
Laŵa	_	_
k	_	_
)	_	_
end	_	_
for	_	_
C←	_	_
(	_	_
Ŵv	_	_
)	_	_
TLvŴv	_	_
+	_	_
(	_	_
Ŵa	_	_
)	_	_
TLaŴa	_	_
Ẑ	_	_
,	_	_
S	_	_
,	_	_
Cz	_	_
←	_	_
UpdateLatentVariables	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
,	_	_
λ1Â	_	_
+	_	_
λ2C	_	_
)	_	_
.	_	_

#559
Dist	_	_
T←	_	_
OrthogonalisationMatrix	_	_
(	_	_
C	_	_
,	_	_
Cz	_	_
,	_	_
S	_	_
,	_	_
N	_	_
)	_	_
Ŵa	_	_
←	_	_
ŴaT−1	_	_
Ŵv	_	_
←	_	_
ŴvT−1	_	_
Cz	_	_
←	_	_
TCzTT	_	_
S←	_	_
TSTT	_	_
Ẑ←	_	_
TẐ	_	_
.	_	_

#560
Dist	_	_
Â←	_	_
(	_	_
N	_	_
+	_	_
ν0	_	_
)	_	_
(	_	_
Cz	_	_
+	_	_
S	_	_
+	_	_
Λ−1	_	_
0	_	_
)	_	_
−1	_	_
until	_	_
convergence	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
42	_	_
Algorithm	_	_
2	_	_
Computing	_	_
gradients	_	_
and	_	_
Hessians	_	_
for	_	_
mean	_	_
function	_	_
MeanDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
gµ	_	_
=	_	_
0	_	_
,	_	_
Hµ	_	_
=	_	_
0	_	_
for	_	_
n	_	_
=	_	_
1	_	_
...	_	_
N	_	_
do	_	_
a←	_	_
µ̂+	_	_
Ŵaẑn	_	_
Ψ←	_	_
Shoot	_	_
(	_	_
Ŵvẑn	_	_
)	_	_
g′	_	_
,	_	_
H′	_	_
←	_	_
LikelihoodDerivatives	_	_
(	_	_
fn	_	_
,	_	_
a	_	_
,	_	_
Ψ	_	_
)	_	_
gµ	_	_
←	_	_
gµ	_	_
+	_	_
g′	_	_
Hµ	_	_
←	_	_
Hµ	_	_
+	_	_
H′	_	_
end	_	_
for	_	_
return	_	_
gµ	_	_
,	_	_
Hµ	_	_
end	_	_
function	_	_
upon	_	_
the	_	_
one	_	_
used	_	_
.	_	_

#561
Appendix	_	_
A.2.1	_	_
.	_	_

#562
Gaussian	_	_
model	_	_
Algorithm	_	_
3	_	_
shows	_	_
derivatives	_	_
for	_	_
the	_	_
Gaussian	_	_
noise	_	_
model	_	_
.	_	_

#563
For	_	_
a	_	_
single	_	_
voxel	_	_
,	_	_
this	_	_
is	_	_
based	_	_
on	_	_
JL2	_	_
=	_	_
1	_	_
2	_	_
ln	_	_
(	_	_
2π	_	_
)	_	_
+	_	_
1	_	_
2	_	_
lnσ2	_	_
+	_	_
1	_	_
2σ2	_	_
||f	_	_
−	_	_
a′||22	_	_
(	_	_
A.2	_	_
)	_	_
dJL2	_	_
da′	_	_
=	_	_
1	_	_
σ2	_	_
(	_	_
a′	_	_
−	_	_
f	_	_
)	_	_
and	_	_
d2JL2	_	_
da′2	_	_
=	_	_
1	_	_
σ2	_	_
(	_	_
A.3	_	_
)	_	_
For	_	_
voxels	_	_
where	_	_
data	_	_
is	_	_
missing	_	_
,	_	_
both	_	_
JL2	_	_
and	_	_
dJL2	_	_
da′	_	_
are	_	_
assumed	_	_
to	_	_
be	_	_
zero	_	_
.	_	_

#564
Using	_	_
matrix	_	_
notation	_	_
,	_	_
the	_	_
objective	_	_
function	_	_
for	_	_
an	_	_
image	_	_
is	_	_
therefore	_	_
J	_	_
′	_	_
=	_	_
1	_	_
2σ2	_	_
(	_	_
Ψa−	_	_
f	_	_
)	_	_
T	_	_
(	_	_
Ψa−	_	_
f	_	_
)	_	_
+	_	_
M	_	_
2	_	_
(	_	_
ln	_	_
(	_	_
σ2	_	_
)	_	_
+	_	_
ln	_	_
(	_	_
2π	_	_
)	_	_
)	_	_
.	_	_

#565
(	_	_
A.4	_	_
)	_	_
The	_	_
gradients	_	_
and	_	_
Hessians	_	_
,	_	_
with	_	_
respect	_	_
to	_	_
variations	_	_
in	_	_
a	_	_
,	_	_
are	_	_
g′	_	_
=	_	_
ΨT	_	_
(	_	_
σ2	_	_
(	_	_
Ψa−	_	_
f	_	_
)	_	_
)	_	_
(	_	_
A.5	_	_
)	_	_
H′	_	_
=	_	_
1	_	_
σ2	_	_
ΨTΨ	_	_
(	_	_
A.6	_	_
)	_	_
In	_	_
practice	_	_
,	_	_
the	_	_
Hessian	_	_
(	_	_
H′	_	_
)	_	_
is	_	_
approximated	_	_
by	_	_
a	_	_
diagonal	_	_
matrix	_	_
H′	_	_
'	_	_
diag	_	_
(	_	_
ΨT1	_	_
1	_	_
σ2	_	_
)	_	_
(	_	_
A.7	_	_
)	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
43	_	_
where	_	_
1	_	_
is	_	_
a	_	_
vector	_	_
of	_	_
ones	_	_
.	_	_

#566
This	_	_
approximation	_	_
works	_	_
in	_	_
the	_	_
optimisation	_	_
because	_	_
all	_	_
rows	_	_
of	_	_
Ψ	_	_
sum	_	_
to	_	_
1	_	_
,	_	_
so	_	_
for	_	_
any	_	_
vector	_	_
d	_	_
of	_	_
the	_	_
right	_	_
dimension	_	_
,	_	_
the	_	_
rows	_	_
of	_	_
ΨT	_	_
diag	_	_
(	_	_
d	_	_
)	_	_
Ψ	_	_
sum	_	_
to	_	_
ΨTd	_	_
.	_	_

#567
Because	_	_
(	_	_
for	_	_
trilinear	_	_
interpolation	_	_
)	_	_
all	_	_
elements	_	_
of	_	_
Ψ	_	_
are	_	_
greater	_	_
than	_	_
or	_	_
equal	_	_
to	_	_
zero	_	_
,	_	_
so	_	_
if	_	_
all	_	_
elements	_	_
of	_	_
d	_	_
are	_	_
non-negative	_	_
,	_	_
then	_	_
all	_	_
eigenvalues	_	_
of	_	_
diag	_	_
(	_	_
ΨTd	_	_
)	_	_
−	_	_
ΨT	_	_
diag	_	_
(	_	_
d	_	_
)	_	_
Ψ	_	_
are	_	_
greater	_	_
than	_	_
or	_	_
equal	_	_
to	_	_
zero	_	_
.	_	_

#568
Non-negative	_	_
eigenvalues	_	_
ensure	_	_
that	_	_
the	_	_
approximation	_	_
to	_	_
the	_	_
Hessian	_	_
is	_	_
positive	_	_
semi-definite	_	_
,	_	_
and	_	_
thus	_	_
the	_	_
optimisation	_	_
aims	_	_
for	_	_
a	_	_
minimum	_	_
,	_	_
rather	_	_
than	_	_
a	_	_
maximum	_	_
.	_	_

#569
Algorithm	_	_
3	_	_
Likelihood	_	_
derivatives	_	_
for	_	_
Gaussian	_	_
noise	_	_
model	_	_
function	_	_
LikelihoodDerivatives	_	_
(	_	_
f	_	_
,	_	_
a	_	_
,	_	_
Ψ	_	_
)	_	_
J	_	_
′	_	_
←	_	_
1	_	_
2σ2	_	_
||Ψa−	_	_
f	_	_
||2	_	_
+	_	_
M	_	_
2	_	_
(	_	_
ln	_	_
(	_	_
σ2	_	_
)	_	_
+	_	_
ln	_	_
(	_	_
2π	_	_
)	_	_
)	_	_
.	_	_

#570
If	_	_
needed	_	_
g′	_	_
←	_	_
ΨT	_	_
(	_	_
σ2	_	_
(	_	_
Ψa−	_	_
f	_	_
)	_	_
)	_	_
H′	_	_
←	_	_
diag	_	_
(	_	_
ΨT	_	_
(	_	_
σ2	_	_
1	_	_
)	_	_
)	_	_
.	_	_

#571
where	_	_
1	_	_
is	_	_
an	_	_
array	_	_
of	_	_
ones	_	_
return	_	_
J	_	_
′	_	_
,	_	_
g′	_	_
,	_	_
H′	_	_
end	_	_
function	_	_
Appendix	_	_
A.2.2	_	_
.	_	_

#572
Binary	_	_
model	_	_
Slight	_	_
modifications	_	_
are	_	_
made	_	_
to	_	_
the	_	_
algorithm	_	_
in	_	_
order	_	_
to	_	_
use	_	_
the	_	_
Bernoulli	_	_
noise	_	_
model	_	_
with	_	_
the	_	_
sigmoidal	_	_
squashing	_	_
function	_	_
.	_	_

#573
For	_	_
each	_	_
voxel	_	_
,	_	_
the	_	_
negative	_	_
log-likelihood	_	_
is	_	_
JBern	_	_
=	_	_
−	_	_
(	_	_
fa′	_	_
+	_	_
ln	_	_
s	_	_
(	_	_
−a′	_	_
)	_	_
)	_	_
,	_	_
where	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
=	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
−a′	_	_
)	_	_
.	_	_

#574
(	_	_
A.8	_	_
)	_	_
Modifications	_	_
are	_	_
made	_	_
to	_	_
the	_	_
gradient	_	_
and	_	_
Hessian	_	_
of	_	_
Algorithm	_	_
3	_	_
,	_	_
based	_	_
on	_	_
the	_	_
derivatives	_	_
dJBern	_	_
da′	_	_
=	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
−	_	_
f	_	_
and	_	_
d2JBern	_	_
da′2	_	_
=	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
(	_	_
1−	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
)	_	_
.	_	_

#575
(	_	_
A.9	_	_
)	_	_
Using	_	_
matrix	_	_
notation	_	_
(	_	_
where	_	_
s	_	_
≡	_	_
s	_	_
(	_	_
a	_	_
)	_	_
)	_	_
,	_	_
the	_	_
gradients	_	_
and	_	_
Hessians	_	_
are	_	_
g′	_	_
=	_	_
ΨT	_	_
(	_	_
Ψs−	_	_
f	_	_
)	_	_
(	_	_
A.10	_	_
)	_	_
H′	_	_
=	_	_
ΨT	_	_
diag	_	_
(	_	_
s	_	_
)	_	_
diag	_	_
(	_	_
1−	_	_
s	_	_
)	_	_
Ψ	_	_
'	_	_
diag	_	_
(	_	_
ΨT	_	_
diag	_	_
(	_	_
s	_	_
)	_	_
(	_	_
1−	_	_
s	_	_
)	_	_
)	_	_
(	_	_
A.11	_	_
)	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
44	_	_
Appendix	_	_
A.2.3	_	_
.	_	_

#576
Categorical	_	_
model	_	_
For	_	_
the	_	_
categorical	_	_
model	_	_
with	_	_
a	_	_
softmax	_	_
squashing	_	_
function	_	_
,	_	_
the	_	_
negative	_	_
log-likelihood	_	_
of	_	_
a	_	_
single	_	_
voxel	_	_
is	_	_
Jcat	_	_
=	_	_
−	_	_
K∑	_	_
k=1	_	_
a′kfk	_	_
+	_	_
log	_	_
(	_	_
K∑	_	_
k=1	_	_
exp	_	_
a′k	_	_
)	_	_
(	_	_
A.12	_	_
)	_	_
This	_	_
would	_	_
use	_	_
the	_	_
gradients	_	_
and	_	_
Hessians	_	_
dJcat	_	_
da′k	_	_
=	_	_
sk	_	_
(	_	_
a′	_	_
)	_	_
−	_	_
fk	_	_
,	_	_
where	_	_
s	_	_
(	_	_
a′	_	_
)	_	_
=	_	_
exp	_	_
a′	_	_
)	_	_
∑K	_	_
k=1	_	_
exp	_	_
a′k	_	_
(	_	_
A.13	_	_
)	_	_
d2Jcat	_	_
da′ka	_	_
′	_	_
l	_	_
=	_	_
sk	_	_
(	_	_
a′	_	_
)	_	_
(	_	_
δjk	_	_
−	_	_
sj	_	_
(	_	_
a′	_	_
)	_	_
)	_	_
(	_	_
A.14	_	_
)	_	_
where	_	_
δjk	_	_
is	_	_
the	_	_
Kronecker	_	_
delta	_	_
function	_	_
.	_	_

#577
Computation	_	_
of	_	_
the	_	_
gradients	_	_
and	_	_
the	_	_
approximation	_	_
of	_	_
the	_	_
Hessian	_	_
follow	_	_
similar	_	_
lines	_	_
to	_	_
those	_	_
for	_	_
the	_	_
binary	_	_
and	_	_
Gaussian	_	_
models	_	_
.	_	_

#578
Appendix	_	_
A.3	_	_
.	_	_

#579
Geodesic	_	_
shooting	_	_
The	_	_
algorithm	_	_
requires	_	_
a	_	_
means	_	_
of	_	_
computing	_	_
diffeomorphic	_	_
deformations	_	_
from	_	_
the	_	_
initial	_	_
velocities	_	_
via	_	_
a	_	_
Geodesic	_	_
shooting	_	_
procedure	_	_
.	_	_

#580
Algorithm	_	_
4	_	_
shows	_	_
how	_	_
this	_	_
is	_	_
achieved	_	_
.	_	_

#581
In	_	_
the	_	_
presented	_	_
algorithm	_	_
,	_	_
Dψ	_	_
denotes	_	_
the	_	_
Jacobian	_	_
tensor	_	_
field	_	_
of	_	_
ψ	_	_
,	_	_
and	_	_
(	_	_
Dψ	_	_
)	_	_
Tu	_	_
indicates	_	_
a	_	_
pointwise	_	_
multiplication	_	_
with	_	_
the	_	_
transpose	_	_
of	_	_
the	_	_
Jacobian	_	_
.	_	_

#582
|Dψ|	_	_
denotes	_	_
the	_	_
field	_	_
of	_	_
Jacobian	_	_
determinants	_	_
.	_	_

#583
Lv	_	_
in	_	_
the	_	_
continuous	_	_
framework	_	_
is	_	_
equivalent	_	_
to	_	_
the	_	_
matrix	_	_
multiplication	_	_
Lvv	_	_
in	_	_
the	_	_
discrete	_	_
framework	_	_
.	_	_

#584
The	_	_
operation	_	_
Lgu	_	_
denotes	_	_
applying	_	_
the	_	_
inverse	_	_
of	_	_
L	_	_
to	_	_
u	_	_
,	_	_
such	_	_
that	_	_
LLgu	_	_
=	_	_
u	_	_
.	_	_

#585
In	_	_
practice	_	_
,	_	_
this	_	_
is	_	_
a	_	_
deconvolution	_	_
,	_	_
which	_	_
is	_	_
computed	_	_
using	_	_
fast	_	_
Fourier	_	_
transforms	_	_
.	_	_

#586
Much	_	_
has	_	_
already	_	_
been	_	_
written	_	_
about	_	_
the	_	_
geodesic	_	_
shooting	_	_
procedure	_	_
,	_	_
so	_	_
the	_	_
reader	_	_
is	_	_
referred	_	_
to	_	_
[	_	_
24	_	_
,	_	_
76	_	_
]	_	_
for	_	_
further	_	_
information	_	_
.	_	_

#587
Appendix	_	_
A.4	_	_
.	_	_

#588
Updating	_	_
appearance	_	_
basis	_	_
functions	_	_
(	_	_
Ŵa	_	_
)	_	_
Appearance	_	_
basis	_	_
functions	_	_
are	_	_
optimised	_	_
by	_	_
Ŵa	_	_
=	_	_
arg	_	_
min	_	_
Wa	_	_
(	_	_

#589
2	_	_
Tr	_	_

#590
(	_	_
(	_	_
λ1NI	_	_
+	_	_
λ2ẐẐT	_	_
)	_	_
(	_	_
Wa	_	_
)	_	_
TLaWa	_	_
)	_	_
+	_	_
N∑	_	_
n=1	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
ẑn	_	_
,	_	_
µ̂	_	_
,	_	_
W	_	_
a	_	_
,	_	_
Ŵv	_	_
)	_	_
)	_	_
.	_	_

#591
(	_	_
A.15	_	_
)	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
45	_	_
Algorithm	_	_
4	_	_
Geodesic	_	_
shooting	_	_
via	_	_
Euler	_	_
integration	_	_
function	_	_
Shoot	_	_
(	_	_
v0	_	_
)	_	_
u0	_	_
←	_	_
Lv0	_	_
.	_	_

#592
Lvv	_	_
≡	_	_
Lv	_	_
ψ	_	_
←	_	_
id	_	_
for	_	_
t	_	_
=	_	_
1	_	_
...	_	_
T	_	_
do	_	_
u←	_	_
|Dψ|	_	_
(	_	_
Dψ	_	_
)	_	_
Tu0	_	_
(	_	_
ψ	_	_
)	_	_
v	_	_
←	_	_
Lgu	_	_
.	_	_

#593
Convolution	_	_
using	_	_
FFT	_	_
ψ	_	_
←	_	_
ψ	_	_
(	_	_
id−	_	_
1	_	_
T	_	_
v	_	_
)	_	_
end	_	_
for	_	_
return	_	_
ψ	_	_
end	_	_
function	_	_
The	_	_
first	_	_
step	_	_
involves	_	_
computing	_	_
the	_	_
gradients	_	_
and	_	_
Hessians	_	_
,	_	_
which	_	_
can	_	_
be	_	_
done	_	_
in	_	_
a	_	_
distributed	_	_
way	_	_
and	_	_
is	_	_
shown	_	_
in	_	_
Algorithm	_	_
5	_	_
.	_	_

#594
Note	_	_
that	_	_
Algorithm	_	_
5	_	_
only	_	_
shows	_	_
the	_	_
computation	_	_
of	_	_
gradients	_	_
and	_	_
Hessians	_	_
for	_	_
the	_	_
Gaussian	_	_
noise	_	_
model	_	_
,	_	_
and	_	_
that	_	_
slight	_	_
modifications	_	_
are	_	_
required	_	_
when	_	_
using	_	_
other	_	_
forms	_	_
of	_	_
appearance	_	_
model	_	_
.	_	_

#595
Gradients	_	_
and	_	_
Hessians	_	_
for	_	_
updating	_	_
these	_	_
basis	_	_
functions	_	_
(	_	_
Wa	_	_
)	_	_
are	_	_
similar	_	_
to	_	_
those	_	_
for	_	_
the	_	_
mean	_	_
updates	_	_
,	_	_
except	_	_
for	_	_
weighting	_	_
based	_	_
on	_	_
the	_	_
current	_	_
estimates	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
.	_	_

#596
Note	_	_
that	_	_
for	_	_
this	_	_
approach	_	_
to	_	_
work	_	_
effectively	_	_
,	_	_
the	_	_
rows	_	_
of	_	_
Ẑ	_	_
should	_	_
be	_	_
orthogonal	_	_
to	_	_
each	_	_
other	_	_
,	_	_
which	_	_
is	_	_
explained	_	_
further	_	_
in	_	_
Section	_	_
Appendix	_	_
A.8	_	_
.	_	_

#597
Note	_	_
that	_	_
only	_	_
a	_	_
single	_	_
Gauss-Newton	_	_
iteration	_	_
is	_	_
performed	_	_
,	_	_
so	_	_
the	_	_
objective	_	_
function	_	_
in	_	_
Eqn	_	_
.	_	_

#598
A.15	_	_
is	_	_
not	_	_
fully	_	_
optimised	_	_
,	_	_
but	_	_
merely	_	_
improved	_	_
over	_	_
its	_	_
previous	_	_
value	_	_
.	_	_

#599
Appendix	_	_
A.5	_	_
.	_	_

#600
Updating	_	_
shape	_	_
basis	_	_
functions	_	_
(	_	_
Ŵv	_	_
)	_	_
Shape	_	_
basis	_	_
functions	_	_
are	_	_
optimised	_	_
by	_	_
Ŵv	_	_
=	_	_
arg	_	_
min	_	_
Wv	_	_
(	_	_

#601
2	_	_
Tr	_	_

#602
(	_	_
(	_	_
λ1NI	_	_
+	_	_
λ2ẐẐT	_	_
)	_	_
(	_	_
Wv	_	_
)	_	_
TLvWv	_	_
)	_	_
+	_	_
N∑	_	_
n=1	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
ẑn	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵ	_	_
a	_	_
,	_	_
Wv	_	_
)	_	_
)	_	_
.	_	_

#603
(	_	_
A.16	_	_
)	_	_
A	_	_
single	_	_
Gauss-Newton	_	_
iteration	_	_
is	_	_
used	_	_
to	_	_
update	_	_
the	_	_
basis	_	_
functions	_	_
of	_	_
the	_	_
shape	_	_
model	_	_
(	_	_
Wv	_	_
)	_	_
,	_	_
which	_	_
is	_	_
done	_	_
in	_	_
such	_	_
a	_	_
way	_	_
that	_	_
changes	_	_
to	_	_
Wv	_	_
improve	_	_
the	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
46	_	_
Algorithm	_	_
5	_	_
Computing	_	_
gradients	_	_
and	_	_
Hessians	_	_
for	_	_
appearance	_	_
function	_	_
AppearanceDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Kv	_	_
do	_	_
gak	_	_
←	_	_
0	_	_
,	_	_
Ha	_	_
kk	_	_
←	_	_
0	_	_
end	_	_
for	_	_
for	_	_
n	_	_
=	_	_
1	_	_
...	_	_
N	_	_
do	_	_
a←	_	_
µ̂+	_	_
Ŵaẑn	_	_
Ψ←	_	_
Shoot	_	_
(	_	_
Ŵvẑn	_	_
)	_	_
g′	_	_
,	_	_
H′	_	_
←	_	_
LikelihoodDerivatives	_	_
(	_	_
fn	_	_
,	_	_
a	_	_
,	_	_
Ψ	_	_
)	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Kv	_	_
do	_	_
gak	_	_
←	_	_
gak	_	_
+	_	_
ẑkng′	_	_
Ha	_	_
kk	_	_
←	_	_
Ha	_	_
kk	_	_
+	_	_
ẑ2	_	_
knH′	_	_
end	_	_
for	_	_
end	_	_
for	_	_
return	_	_
Ga	_	_
,	_	_
Ha	_	_
.	_	_

#604
Where	_	_
Ga	_	_
=	_	_
{	_	_
ga1	_	_
,	_	_
ga2	_	_
,	_	_
.	_	_

#605
.	_	_

#606
.	_	_

#607
,	_	_
gaK	_	_
}	_	_
and	_	_
Ha	_	_
=	_	_
{	_	_
Ha	_	_
1,1	_	_
,	_	_
H	_	_
a	_	_
2,2	_	_
,	_	_
.	_	_

#608
.	_	_

#609
.	_	_

#610
,	_	_
H	_	_
a	_	_
K	_	_
,	_	_
K	_	_
}	_	_
end	_	_
function	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
47	_	_
objective	_	_
function	_	_
with	_	_
respect	_	_
to	_	_
its	_	_
previous	_	_
value	_	_
,	_	_
rather	_	_
than	_	_
fully	_	_
optimise	_	_
Eqn	_	_
.	_	_

#611
A.16	_	_
.	_	_

#612
While	_	_
most	_	_
Gauss-Newton	_	_
iterations	_	_
improve	_	_
the	_	_
fit	_	_
,	_	_
a	_	_
backtracking	_	_
line	_	_
search	_	_
is	_	_
included	_	_
to	_	_
ensure	_	_
that	_	_
they	_	_
do	_	_
not	_	_
overshoot	_	_
.	_	_

#613
For	_	_
simplicity	_	_
,	_	_
this	_	_
aspect	_	_
of	_	_
the	_	_
algorithm	_	_
is	_	_
not	_	_
shown	_	_
in	_	_
Algorithm	_	_
1	_	_
.	_	_

#614
As	_	_
for	_	_
updating	_	_
Wa	_	_
,	_	_
this	_	_
requires	_	_
the	_	_
rows	_	_
of	_	_
Ẑ	_	_
to	_	_
be	_	_
orthogonal	_	_
to	_	_
each	_	_
other	_	_
.	_	_

#615
As	_	_
for	_	_
updating	_	_
the	_	_
appearance	_	_
basis	_	_
functions	_	_
,	_	_
computing	_	_
the	_	_
gradients	_	_
and	_	_
Hessians	_	_
for	_	_
the	_	_
shape	_	_
bases	_	_
can	_	_
be	_	_
also	_	_
done	_	_
in	_	_
a	_	_
distributed	_	_
way	_	_
.	_	_

#616
The	_	_
strategy	_	_
for	_	_
computing	_	_
gradients	_	_
and	_	_
Hessians	_	_
is	_	_
shown	_	_
in	_	_
Algorithm	_	_
6	_	_
.	_	_

#617
Algorithm	_	_
6	_	_
Computing	_	_
gradients	_	_
and	_	_
Hessians	_	_
for	_	_
shape	_	_
function	_	_
ShapeDerivatives	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
)	_	_
.	_	_

#618
Various	_	_
settings	_	_
(	_	_
eg	_	_
Lv	_	_
)	_	_
are	_	_
not	_	_
passed	_	_
as	_	_
arguments	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Kv	_	_
do	_	_
gvk	_	_
←	_	_
0	_	_
,	_	_
Hv	_	_
kk	_	_
←	_	_
0	_	_
end	_	_
for	_	_
for	_	_
n	_	_
=	_	_
1	_	_
...	_	_
N	_	_
do	_	_
a←	_	_
µ̂+	_	_
Ŵaẑn	_	_
Ψ←	_	_
Shoot	_	_
(	_	_
Ŵvẑn	_	_
)	_	_
g′	_	_
,	_	_
H′	_	_
←	_	_
LikelihoodDerivatives	_	_
(	_	_
fn	_	_
,	_	_
a	_	_
,	_	_
Ψ	_	_
)	_	_
D←	_	_
[	_	_
diag	_	_
(	_	_
∇1a	_	_
)	_	_
diag	_	_
(	_	_
∇2a	_	_
)	_	_
diag	_	_
(	_	_
∇3a	_	_
)	_	_
]	_	_
g′	_	_
←	_	_
DTg′	_	_
H′	_	_
←	_	_
DTH′D	_	_
for	_	_
k	_	_
=	_	_
1	_	_
...	_	_
Kv	_	_
do	_	_
.	_	_

#619
Update	_	_
gradients	_	_
and	_	_
Hessians	_	_
gvk	_	_
←	_	_
gvk	_	_
+	_	_
ẑkng′n	_	_
Hv	_	_
kk	_	_
←	_	_
Hv	_	_
kk	_	_
+	_	_
ẑ2	_	_
knH′n	_	_
end	_	_
for	_	_
end	_	_
for	_	_
return	_	_
Gv	_	_
,	_	_
Hv	_	_
end	_	_
function	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
48	_	_
Appendix	_	_
A.6	_	_
.	_	_

#620
Updating	_	_
latent	_	_
variables	_	_
(	_	_
ẑn	_	_
)	_	_
Within	_	_
a	_	_
distributed	_	_
multi-centre	_	_
privacy-preserving	_	_
framework	_	_
,	_	_
the	_	_
strategy	_	_
would	_	_
be	_	_
to	_	_
have	_	_
the	_	_
basis	_	_
functions	_	_
shared	_	_
across	_	_
sites	_	_
,	_	_
whereas	_	_
the	_	_
image	_	_
data	_	_
and	_	_
latent	_	_
variables	_	_
would	_	_
remain	_	_
hidden	_	_
within	_	_
each	_	_
of	_	_
the	_	_
sites	_	_
.	_	_

#621
All	_	_
computations	_	_
that	_	_
use	_	_
the	_	_
image	_	_
data	_	_
and	_	_
latent	_	_
variables	_	_
would	_	_
be	_	_
done	_	_
at	_	_
each	_	_
site	_	_
,	_	_
with	_	_
only	_	_
aggregate	_	_
data	_	_
shared	_	_
across	_	_
them	_	_
.	_	_

#622
The	_	_
modes	_	_
of	_	_
the	_	_
latent	_	_
variables	_	_
are	_	_
updated	_	_
via	_	_
a	_	_
Gauss-Newton	_	_
scheme	_	_
,	_	_
similar	_	_
to	_	_
that	_	_
used	_	_
by	_	_
[	_	_
77	_	_
,	_	_
10	_	_
,	_	_
11	_	_
]	_	_
.	_	_

#623
ẑn	_	_
=	_	_
arg	_	_
min	_	_
zn	_	_
(	_	_
J	_	_
(	_	_
fn	_	_
,	_	_
zn	_	_
,	_	_
µ	_	_
,	_	_
Ŵ	_	_
a	_	_
,	_	_
Ŵv	_	_
)	_	_
+	_	_
1	_	_
2zTn	_	_
(	_	_
λ1Â	_	_
+	_	_
λ2	_	_
(	_	_
Ŵa	_	_
)	_	_
TLaŴa	_	_
+	_	_
λ2	_	_
(	_	_
Ŵv	_	_
)	_	_
TLvŴv	_	_
)	_	_
zn	_	_
)	_	_
(	_	_
A.17	_	_
)	_	_
Algorithm	_	_
7	_	_
Updating	_	_
latent	_	_
variables	_	_
function	_	_
UpdateLatentVariables	_	_
(	_	_
F	_	_
,	_	_
Ẑ	_	_
,	_	_
µ̂	_	_
,	_	_
Ŵa	_	_
,	_	_
Ŵv	_	_
,	_	_
A	_	_
)	_	_
S←	_	_
0	_	_
for	_	_
n	_	_
=	_	_
1	_	_
...	_	_
N	_	_
do	_	_
a←	_	_
µ̂+	_	_
Ŵaẑn	_	_
Ψ←	_	_
Shoot	_	_
(	_	_
Ŵvẑn	_	_
)	_	_
g′	_	_
,	_	_
H′	_	_
←	_	_
LikelihoodDerivatives	_	_
(	_	_
fn	_	_
,	_	_
a	_	_
,	_	_
Ψ	_	_
)	_	_
D←	_	_
(	_	_
diag	_	_
(	_	_
∇1a	_	_
)	_	_
diag	_	_
(	_	_
∇2a	_	_
)	_	_
diag	_	_
(	_	_
∇3a	_	_
)	_	_
)	_	_
B←	_	_
DTWv	_	_
+	_	_
Wa	_	_
g←	_	_
BTg′	_	_
H←	_	_
BTH′B	_	_
ẑn	_	_
←	_	_
ẑn	_	_
−	_	_
(	_	_
H	_	_
+	_	_
A	_	_
)	_	_
−1	_	_
(	_	_
g	_	_
+	_	_
Aẑn	_	_
)	_	_
S←	_	_
S	_	_
+	_	_
(	_	_
H	_	_
+	_	_
A	_	_
)	_	_
−1	_	_
end	_	_
for	_	_
Cz	_	_
←	_	_
ẐẐT	_	_
return	_	_
Ẑ	_	_
,	_	_
S	_	_
,	_	_
Cz	_	_
end	_	_
function	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
49	_	_
The	_	_
inverse	_	_
of	_	_
the	_	_
(	_	_
approximate	_	_
)	_	_
Hessians	_	_
allows	_	_
a	_	_
Gaussian	_	_
approximation	_	_
of	_	_
the	_	_
uncertainty	_	_
with	_	_
which	_	_
the	_	_
latent	_	_
variables	_	_
are	_	_
updated	_	_
to	_	_
be	_	_
computed	_	_
(	_	_
“Laplace	_	_
approximation”	_	_
)	_	_
.	_	_

#624
This	_	_
is	_	_
the	_	_
S	_	_
matrix	_	_
(	_	_
returned	_	_
by	_	_
Algorithm	_	_
7	_	_
)	_	_
,	_	_
which	_	_
is	_	_
combined	_	_
with	_	_
ẐẐT	_	_
(	_	_
returned	_	_
as	_	_
Cz	_	_
)	_	_
and	_	_
used	_	_
to	_	_
re-compute	_	_
Â	_	_
.	_	_

#625
Appendix	_	_
A.7	_	_
.	_	_

#626
Expectation	_	_
of	_	_
the	_	_
precision	_	_
matrix	_	_
(	_	_
Â	_	_
)	_	_
This	_	_
work	_	_
uses	_	_
a	_	_
variational	_	_
Bayesian	_	_
approach	_	_
for	_	_
approximating	_	_
the	_	_
distribution	_	_
of	_	_
A	_	_
,	_	_
which	_	_
is	_	_
a	_	_
method	_	_
described	_	_
in	_	_
more	_	_
detail	_	_
by	_	_
textbooks	_	_
,	_	_
such	_	_
as	_	_
[	_	_
43	_	_
]	_	_
or	_	_
[	_	_
44	_	_
]	_	_
.	_	_

#627
Briefly	_	_
,	_	_
it	_	_
involves	_	_
taking	_	_
the	_	_
joint	_	_
probability	_	_
of	_	_
Eqn	_	_
.	_	_

#628
12	_	_
,	_	_
discarding	_	_
terms	_	_
that	_	_
do	_	_
not	_	_
involve	_	_
A	_	_
,	_	_
and	_	_
substituting	_	_
the	_	_
expectations	_	_
of	_	_
the	_	_
other	_	_
parameters	_	_
into	_	_
the	_	_
expression	_	_
.	_	_

#629
This	_	_
leads	_	_
to	_	_
the	_	_
following	_	_
approximating	_	_
distribution	_	_
,	_	_
which	_	_
can	_	_
be	_	_
recognised	_	_
as	_	_
Wishart	_	_
.	_	_

#630
ln	_	_
q	_	_
(	_	_
A	_	_
)	_	_
=	_	_
1	_	_
2	_	_
(	_	_
N	_	_
+	_	_
ν0	_	_
−K	_	_
−	_	_
1	_	_
)	_	_
ln	_	_
det	_	_
|A|	_	_
−	_	_
1	_	_

#631
2	_	_
Tr	_	_

#632
(	_	_
(	_	_
E	_	_
[	_	_
ZZT	_	_
]	_	_
+	_	_
Λ−1	_	_
0	_	_
)	_	_
A	_	_
)	_	_
+	_	_
const	_	_
=	_	_
lnW	_	_
(	_	_
A|Λ	_	_
,	_	_
ν	_	_
)	_	_
(	_	_
A.18	_	_
)	_	_
where	_	_
Λ	_	_
=	_	_
(	_	_
E	_	_
[	_	_
ZZT	_	_
]	_	_
+	_	_
Λ−1	_	_
0	_	_
)	_	_
−1	_	_
and	_	_
ν	_	_
=	_	_
ν0	_	_
+N	_	_
.	_	_

#633
In	_	_
practice	_	_
,	_	_
E	_	_
[	_	_
ZZT	_	_
]	_	_
is	_	_
approximated	_	_
by	_	_
Cz	_	_
+	_	_
S	_	_
,	_	_
described	_	_
previously	_	_
.	_	_

#634
Other	_	_
steps	_	_
in	_	_
the	_	_
algorithm	_	_
use	_	_
the	_	_
expectation	_	_
of	_	_
A	_	_
,	_	_
which	_	_
(	_	_
see	_	_
Appendix	_	_
B	_	_
of	_	_
[	_	_
43	_	_
]	_	_
)	_	_
is	_	_
Â	_	_
=	_	_
E	_	_
[	_	_
A	_	_
]	_	_
=	_	_
νΛ	_	_
.	_	_

#635
(	_	_
A.19	_	_
)	_	_
Appendix	_	_
A.8	_	_
.	_	_

#636
Orthogonalisation	_	_
The	_	_
strategy	_	_
for	_	_
updating	_	_
Ŵa	_	_
and	_	_
Ŵv	_	_
involves	_	_
some	_	_
approximations	_	_
,	_	_
which	_	_
are	_	_
needed	_	_
in	_	_
order	_	_
to	_	_
save	_	_
memory	_	_
and	_	_
computation	_	_
.	_	_

#637
This	_	_
approximation	_	_
is	_	_
related	_	_
to	_	_
the	_	_
Jacobi	_	_
iterative	_	_
method	_	_
for	_	_
determining	_	_
the	_	_
solutions	_	_
to	_	_
linear	_	_
equations	_	_
,	_	_
which	_	_
is	_	_
only	_	_
guaranteed	_	_
to	_	_
converge	_	_
for	_	_
diagonally	_	_
dominant	_	_
matrices	_	_
.	_	_

#638
Rather	_	_
than	_	_
work	_	_
with	_	_
the	_	_
Hessian	_	_
for	_	_
the	_	_
entire	_	_
W	_	_
matrix	_	_
together	_	_
,	_	_
only	_	_
the	_	_
Hessians	_	_
for	_	_
each	_	_
column	_	_
of	_	_
W	_	_
are	_	_
computed	_	_
by	_	_
Algorithms	_	_
5	_	_
and	_	_
6	_	_
.	_	_

#639
This	_	_
corresponds	_	_
with	_	_
a	_	_
block	_	_
diagonal	_	_
Hessian	_	_
matrix	_	_
for	_	_
the	_	_
entire	_	_
W	_	_
,	_	_
which	_	_
has	_	_
APPENDIX	_	_
A	_	_
ALGORITHM	_	_
50	_	_
the	_	_
form	_	_
H	_	_
=	_	_
	_	_
H11	_	_
0	_	_
.	_	_

#640
.	_	_

#641
.	_	_

#642
0	_	_
0	_	_
H22	_	_
.	_	_

#643
.	_	_

#644
.	_	_

#645
0	_	_
...	_	_
...	_	_
.	_	_

#646
.	_	_

#647
.	_	_

#648
...	_	_
0	_	_
0	_	_
.	_	_

#649
.	_	_

#650
.	_	_

#651
HKK	_	_
	_	_
.	_	_

#652
(	_	_
A.20	_	_
)	_	_
More	_	_
stable	_	_
convergence	_	_
can	_	_
be	_	_
achieved	_	_
by	_	_
transforming	_	_
the	_	_
basis	_	_
functions	_	_
and	_	_
latent	_	_
variables	_	_
in	_	_
order	_	_
to	_	_
minimise	_	_
the	_	_
amount	_	_
of	_	_
signal	_	_
that	_	_
would	_	_
be	_	_
in	_	_
the	_	_
off-diagonal	_	_
blocks	_	_
,	_	_
thus	_	_
increasing	_	_
the	_	_
diagonal	_	_
dominance	_	_
of	_	_
the	_	_
system	_	_
of	_	_
equations	_	_
.	_	_

#653
In	_	_
situations	_	_
where	_	_
diagonal	_	_
dominance	_	_
is	_	_
violated	_	_
,	_	_
convergence	_	_
can	_	_
still	_	_
be	_	_
achieved	_	_
by	_	_
decreasing	_	_
the	_	_
update	_	_
step	_	_
size	_	_
.	_	_

#654
This	_	_
is	_	_
analogous	_	_
to	_	_
using	_	_
a	_	_
weighted	_	_
Jacobi	_	_
iteration	_	_
,	_	_
where	_	_
in	_	_
practice	_	_
the	_	_
weights	_	_
are	_	_
found	_	_
using	_	_
a	_	_
backtracking	_	_
line-search	_	_
.	_	_

#655
Signal	_	_
in	_	_
the	_	_
off-diagonal	_	_
blocks	_	_
is	_	_
reduced	_	_
by	_	_
orthogonalising	_	_
the	_	_
rows	_	_
of	_	_
Ẑ	_	_
.	_	_

#656
This	_	_
is	_	_
achieved	_	_
by	_	_
finding	_	_
a	_	_
transformation	_	_
,	_	_
T	_	_
,	_	_
such	_	_
that	_	_
TẐ	_	_
(	_	_
TẐ	_	_
)	_	_
T	_	_
and	_	_
(	_	_
ŴvT−1	_	_
)	_	_
TLvŴvT−1	_	_
+	_	_
(	_	_
ŴaT−1	_	_
)	_	_
TLaŴaT−1	_	_
are	_	_
both	_	_
diagonal	_	_
matrices	_	_
.	_	_

#657
Transformation	_	_
T	_	_
is	_	_
derived	_	_
from	_	_
an	_	_
eigendecomposition	_	_
of	_	_
the	_	_
sufficient	_	_
statistics	_	_
,	_	_
whereby	_	_
the	_	_
symmetric	_	_
positive	_	_
definite	_	_
matrices	_	_
are	_	_
decomposed	_	_
into	_	_
diagonal	_	_
(	_	_
Dz	_	_
and	_	_
Dw	_	_
)	_	_
and	_	_
orthonormal	_	_
(	_	_
Vz	_	_
and	_	_
Vw	_	_
)	_	_
matrices	_	_
,	_	_
such	_	_
that	_	_
VzDz	_	_
(	_	_
Vz	_	_
)	_	_
T	_	_
=	_	_
Cz	_	_
,	_	_
(	_	_
A.21	_	_
)	_	_
VwDw	_	_
(	_	_
Vw	_	_
)	_	_
T	_	_
=	_	_
C	_	_
,	_	_
(	_	_
A.22	_	_
)	_	_
where	_	_
Cz	_	_
=	_	_
ẐẐT	_	_
and	_	_
C	_	_
=	_	_
(	_	_
Ŵv	_	_
)	_	_
TLvŴv	_	_
+	_	_
(	_	_
Ŵa	_	_
)	_	_
TLaŴa	_	_
.	_	_

#658
A	_	_
further	_	_
singular	_	_
value	_	_
decomposition	_	_
is	_	_
then	_	_
used	_	_
,	_	_
giving	_	_
UDVT	_	_
=	_	_
(	_	_
Dw	_	_
)	_	_
2	_	_
(	_	_
Vw	_	_
)	_	_
TVz	_	_
(	_	_
Dz	_	_
)	_	_
2	_	_
.	_	_

#659
(	_	_
A.23	_	_
)	_	_
The	_	_
combination	_	_
of	_	_
various	_	_
matrices	_	_
is	_	_
used	_	_
to	_	_
give	_	_
an	_	_
initial	_	_
estimate	_	_
of	_	_
the	_	_
transform	_	_
T	_	_
=	_	_
DVT	_	_
(	_	_
Dz	_	_
)	_	_
−	_	_
2	_	_
(	_	_
Vz	_	_
)	_	_
T	_	_
.	_	_

#660
(	_	_
A.24	_	_
)	_	_
REFERENCES	_	_
51	_	_
The	_	_
above	_	_
T	_	_
matrix	_	_
could	_	_
be	_	_
used	_	_
to	_	_
render	_	_
the	_	_
matrices	_	_
orthogonal	_	_
,	_	_
but	_	_
their	_	_
relative	_	_
scalings	_	_
would	_	_
not	_	_
be	_	_
optimal	_	_
.	_	_

#661
The	_	_
remainder	_	_
of	_	_
the	_	_
orthogonalisation	_	_
procedure	_	_
involves	_	_
an	_	_
iterative	_	_
strategy	_	_
similar	_	_
to	_	_
expectation	_	_
maximisation	_	_
,	_	_
where	_	_
the	_	_
aim	_	_
is	_	_
to	_	_
estimate	_	_
some	_	_
diagonal	_	_
scaling	_	_
matrix	_	_
Q	_	_
with	_	_
which	_	_
to	_	_
multiply	_	_
T.	_	_
This	_	_
matrix	_	_
is	_	_
parameterised	_	_
by	_	_
a	_	_
set	_	_
of	_	_
parameters	_	_
q	_	_
,	_	_
such	_	_
that	_	_
Q	_	_
=	_	_
diag	_	_
(	_	_
exp	_	_
q	_	_
)	_	_
.	_	_

#662
(	_	_
A.25	_	_
)	_	_
The	_	_
first	_	_
step	_	_
of	_	_
the	_	_
iterative	_	_
scheme	_	_
involves	_	_
re-computing	_	_
Â	_	_
,	_	_
as	_	_
described	_	_
in	_	_
Section	_	_
Appendix	_	_
A.7	_	_
,	_	_
but	_	_
incorporating	_	_
the	_	_
current	_	_
estimates	_	_
of	_	_
QT	_	_
.	_	_

#663
Â	_	_
=	_	_
νΛ	_	_
=	_	_
(	_	_
N	_	_
+	_	_
ν0	_	_
)	_	_
(	_	_
QT	_	_
(	_	_
Cz	_	_
+	_	_
S	_	_
)	_	_
(	_	_
QT	_	_
)	_	_
T	_	_
+	_	_
Λ−1	_	_
0	_	_
)	_	_
−1	_	_
.	_	_

#664
(	_	_
A.26	_	_
)	_	_
The	_	_
next	_	_
step	_	_
in	_	_
the	_	_
iterative	_	_
scheme	_	_
is	_	_
to	_	_
re-estimate	_	_
q	_	_
,	_	_
such	_	_
that	_	_
q̂	_	_
=	_	_
arg	_	_
min	_	_
q	_	_
(	_	_
Tr	_	_
(	_	_
diag	_	_
(	_	_
exp	_	_
(	_	_
−q	_	_
)	_	_
)	_	_
(	_	_
T−1	_	_
)	_	_
TCT−1	_	_
diag	_	_
(	_	_
exp	_	_
(	_	_
−q	_	_
)	_	_
)	_	_
)	_	_
+	_	_
Tr	_	_
(	_	_
diag	_	_
(	_	_
exp	_	_
q	_	_
)	_	_
TCzTT	_	_
diag	_	_
(	_	_
exp	_	_
q	_	_
)	_	_
Â	_	_
)	_	_
)	_	_
.	_	_

#665
(	_	_
A.27	_	_
)	_	_
This	_	_
can	_	_
be	_	_
achieved	_	_
via	_	_
a	_	_
Gauss-Newton	_	_
update	_	_
,	_	_
which	_	_
uses	_	_
first	_	_
and	_	_
second	_	_
derivatives	_	_
with	_	_
respect	_	_
to	_	_
q	_	_
.	_	_

#666
The	_	_
overall	_	_
strategy	_	_
is	_	_
illustrated	_	_
in	_	_
Algorithm	_	_
8	_	_
,	_	_
which	_	_
empirically	_	_
is	_	_
found	_	_
to	_	_
converge	_	_
well	_	_
.	_	_