#0
Imperfect	_	_
Segmentation	_	_
Labels	_	_
:	_	_
How	_	_
Much	_	_
Do	_	_
They	_	_
Matter	_	_
?	_	_

#1
Nicholas	_	_
Heller	_	_
,	_	_
Joshua	_	_
Dean	_	_
,	_	_
and	_	_
Nikolaos	_	_
Papanikolopoulos	_	_
Computer	_	_
Science	_	_
and	_	_
Engineering	_	_
,	_	_
University	_	_
of	_	_
Minnesota	_	_
–	_	_
Twin	_	_
Cities	_	_
{	_	_
helle246	_	_
,	_	_
deanx252	_	_
,	_	_
papan001	_	_
}	_	_
@	_	_
umn.edu	_	_
Abstract	_	_
.	_	_

#2
Labeled	_	_
datasets	_	_
for	_	_
semantic	_	_
segmentation	_	_
are	_	_
imperfect	_	_
,	_	_
especially	_	_
in	_	_
medical	_	_
imaging	_	_
where	_	_
borders	_	_
are	_	_
often	_	_
subtle	_	_
or	_	_
ill-defined	_	_
.	_	_

#3
Little	_	_
work	_	_
has	_	_
been	_	_
done	_	_
to	_	_
analyze	_	_
the	_	_
effect	_	_
that	_	_
label	_	_
errors	_	_
have	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
segmentation	_	_
methodologies	_	_
.	_	_

#4
Here	_	_
we	_	_
present	_	_
a	_	_
large-scale	_	_
study	_	_
of	_	_
model	_	_
performance	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
varying	_	_
types	_	_
and	_	_
degrees	_	_
of	_	_
error	_	_
in	_	_
training	_	_
data	_	_
.	_	_

#5
We	_	_
trained	_	_
U-Net	_	_
,	_	_
SegNet	_	_
,	_	_
and	_	_
FCN32	_	_
several	_	_
times	_	_
for	_	_
liver	_	_
segmentation	_	_
with	_	_
10	_	_
different	_	_
modes	_	_
of	_	_
ground-truth	_	_
perturbation	_	_
.	_	_

#6
Our	_	_
results	_	_
show	_	_
that	_	_
for	_	_
each	_	_
architecture	_	_
,	_	_
performance	_	_
steadily	_	_
declines	_	_
with	_	_
boundary-localized	_	_
errors	_	_
,	_	_
however	_	_
,	_	_
U-Net	_	_
was	_	_
significantly	_	_
more	_	_
robust	_	_
to	_	_
jagged	_	_
boundary	_	_
errors	_	_
than	_	_
the	_	_
other	_	_
architectures	_	_
.	_	_

#7
We	_	_
also	_	_
found	_	_
that	_	_
each	_	_
architecture	_	_
was	_	_
very	_	_
robust	_	_
to	_	_
non-boundary-localized	_	_
errors	_	_
,	_	_
suggesting	_	_
that	_	_
boundary-localized	_	_
errors	_	_
are	_	_
fundamentally	_	_
different	_	_
and	_	_
more	_	_
challenging	_	_
problem	_	_
than	_	_
random	_	_
label	_	_
errors	_	_
in	_	_
a	_	_
classification	_	_
setting	_	_
.	_	_

#8
1	_	_
Introduction	_	_

#9
Automatic	_	_
semantic	_	_
segmentation	_	_
has	_	_
wide	_	_
applications	_	_
in	_	_
medicine	_	_
,	_	_
including	_	_
new	_	_
visualization	_	_
techniques	_	_
[	_	_
19	_	_
]	_	_
,	_	_
surgical	_	_
simulation	_	_
[	_	_
10	_	_
]	_	_
,	_	_
and	_	_
larger	_	_
studies	_	_
of	_	_
morphological	_	_
features	_	_
[	_	_
5	_	_
]	_	_
,	_	_
all	_	_
of	_	_
which	_	_
would	_	_
remain	_	_
prohibitively	_	_
expensive	_	_
if	_	_
segmentations	_	_
were	_	_
provided	_	_
manually	_	_
.	_	_

#10
In	_	_
the	_	_
past	_	_
4	_	_
years	_	_
,	_	_
Deep	_	_
Learning	_	_
(	_	_
DL	_	_
)	_	_
has	_	_
risen	_	_
to	_	_
the	_	_
forefront	_	_
of	_	_
semantic	_	_
segmentation	_	_
techniques	_	_
with	_	_
virtually	_	_
all	_	_
segmentation	_	_
challenges	_	_
currently	_	_
dominated	_	_
by	_	_
DL-based	_	_
entries	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#11
Deep	_	_
learning	_	_
is	_	_
a	_	_
subfield	_	_
of	_	_
machine	_	_
learning	_	_
which	_	_
uses	_	_
labeled	_	_
input	_	_
,	_	_
or	_	_
training	_	_
data	_	_
to	_	_
learn	_	_
functions	_	_
that	_	_
map	_	_
unlabeled	_	_
input	_	_
data	_	_
to	_	_
its	_	_
correct	_	_
response	_	_
.	_	_

#12
In	_	_
the	_	_
case	_	_
of	_	_
semantic	_	_
segmentation	_	_
,	_	_
the	_	_
model	_	_
learns	_	_
from	_	_
image	_	_
and	_	_
mask	_	_
pairs	_	_
,	_	_
where	_	_
the	_	_
mask	_	_
assigns	_	_
each	_	_
pixel	_	_
or	_	_
voxel	_	_
to	_	_
one	_	_
of	_	_
a	_	_
set	_	_
number	_	_
of	_	_
classes	_	_
.	_	_

#13
These	_	_
masks	_	_
are	_	_
typically	_	_
provided	_	_
manually	_	_
by	_	_
a	_	_
domain	_	_
expert	_	_
and	_	_
often	_	_
contain	_	_
some	_	_
errors	_	_
.	_	_

#14
Typically	_	_
the	_	_
most	_	_
challenging	_	_
and	_	_
expensive	_	_
task	_	_
in	_	_
using	_	_
deep	_	_
learning	_	_
for	_	_
semantic	_	_
segmentation	_	_
is	_	_
curating	_	_
a	_	_
ground-truth	_	_
dataset	_	_
that	_	_
is	_	_
sufficiently	_	_
large	_	_
for	_	_
the	_	_
trained	_	_
model	_	_
to	_	_
effectively	_	_
generalize	_	_
to	_	_
unseen	_	_
data	_	_
.	_	_

#15
Practitioners	_	_
are	_	_
often	_	_
faced	_	_
with	_	_
a	_	_
tradeoff	_	_
between	_	_
the	_	_
quantity	_	_
of	_	_
ground-truth	_	_
masks	_	_
and	_	_
their	_	_
quality	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#16
We	_	_
categorize	_	_
ground	_	_
truth	_	_
errors	_	_
to	_	_
be	_	_
either	_	_
biased	_	_
or	_	_
unbiased	_	_
.	_	_

#17
Biased	_	_
errors	_	_
stem	_	_
from	_	_
errors	_	_
of	_	_
intention	_	_
,	_	_
where	_	_
the	_	_
expert	_	_
creating	_	_
the	_	_
labels	_	_
would	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
6	_	_
.	_	_

#18
8v	_	_
3	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
4	_	_
Se	_	_
p	_	_
2	_	_
N.	_	_
Heller	_	_
et	_	_
al.	_	_
repeat	_	_
the	_	_
error	_	_
if	_	_
asked	_	_
to	_	_
label	_	_
the	_	_
instance	_	_
again	_	_
.	_	_

#19
These	_	_
errors	_	_
are	_	_
pernicious	_	_
because	_	_
they	_	_
can	_	_
result	_	_
in	_	_
systemic	_	_
inaccuracies	_	_
in	_	_
the	_	_
dataset	_	_
that	_	_
may	_	_
then	_	_
be	_	_
imparted	_	_
to	_	_
the	_	_
learned	_	_
model	_	_
.	_	_

#20
These	_	_
errors	_	_
can	_	_
often	_	_
be	_	_
mitigated	_	_
by	_	_
giving	_	_
clear	_	_
and	_	_
unambiguous	_	_
instructions	_	_
for	_	_
those	_	_
performing	_	_
the	_	_
labeling	_	_
.	_	_

#21
Unbiased	_	_
errors	_	_
are	_	_
all	_	_
other	_	_
types	_	_
of	_	_
errors	_	_
.	_	_

#22
For	_	_
instance	_	_
,	_	_
if	_	_
an	_	_
annotator’s	_	_
hand	_	_
shakes	_	_
when	_	_
performing	_	_
labeling	_	_
,	_	_
this	_	_
would	_	_
be	_	_
an	_	_
unbiased	_	_
error	_	_
so	_	_
long	_	_
as	_	_
his	_	_
hand	_	_
is	_	_
not	_	_
more	_	_
likely	_	_
to	_	_
shake	_	_
on	_	_
certain	_	_
features	_	_
than	_	_
on	_	_
others	_	_
.	_	_

#23
We	_	_
define	_	_
the	_	_
gold	_	_
standard	_	_
ground	_	_
truth	_	_
to	_	_
be	_	_
what	_	_
an	_	_
unbiased	_	_
annotator	_	_
would	_	_
produce	_	_
if	_	_
he	_	_
were	_	_
to	_	_
annotate	_	_
every	_	_
instance	_	_
an	_	_
infinite	_	_
number	_	_
of	_	_
times	_	_
and	_	_
then	_	_
take	_	_
plurality	_	_
votes	_	_
to	_	_
produce	_	_
the	_	_
final	_	_
labels	_	_
.	_	_

#24
For	_	_
semantic	_	_
segmentation	_	_
,	_	_
each	_	_
pixel	_	_
would	_	_
be	_	_
an	_	_
instance	_	_
in	_	_
this	_	_
example	_	_
.	_	_

#25
Errors	_	_
can	_	_
be	_	_
difficult	_	_
to	_	_
recognize	_	_
in	_	_
annotated	_	_
images	_	_
,	_	_
but	_	_
in	_	_
medical	_	_
imaging	_	_
,	_	_
3D	_	_
imaging	_	_
modalities	_	_
such	_	_
as	_	_
Computed	_	_
Tomography	_	_
(	_	_
CT	_	_
)	_	_
allow	_	_
us	_	_
to	_	_
scrutinize	_	_
the	_	_
annotations	_	_
from	_	_
the	_	_
other	_	_
anatomical	_	_
planes	_	_
.	_	_

#26
In	_	_
Fig.	_	_
1	_	_
we	_	_
can	_	_
clearly	_	_
see	_	_
that	_	_
the	_	_
expert	_	_
is	_	_
somewhat	_	_
inconsistent	_	_
in	_	_
his	_	_
treatment	_	_
of	_	_
the	_	_
region	_	_
boundary	_	_
in	_	_
the	_	_
axial	_	_
plane	_	_
,	_	_
since	_	_
there	_	_
are	_	_
clear	_	_
discontinuities	_	_
in	_	_
the	_	_
contour	_	_
when	_	_
viewed	_	_
from	_	_
the	_	_
saggital	_	_
plane	_	_
.	_	_

#27
This	_	_
is	_	_
important	_	_
in	_	_
medical	_	_
image	_	_
processing	_	_
because	_	_
often	_	_
models	_	_
are	_	_
trained	_	_
on	_	_
all	_	_
three	_	_
anatomical	_	_
planes	_	_
to	_	_
produce	_	_
a	_	_
more	_	_
robust	_	_
final	_	_
segmentation	_	_
[	_	_
15	_	_
]	_	_
,	_	_
or	_	_
volumetric	_	_
models	_	_
are	_	_
used	_	_
[	_	_
13	_	_
]	_	_
.	_	_

#28
It’s	_	_
conceivable	_	_
that	_	_
this	_	_
jagged	_	_
boundary	_	_
might	capability-speculation	_
confuse	_	_
a	_	_
learned	_	_
model	_	_
by	_	_
suggesting	_	_
that	_	_
the	_	_
predicted	_	_
segmentations	_	_
should	_	_
also	_	_
have	_	_
jagged	_	_
boundaries	_	_
.	_	_

#29
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
study	_	_
how	_	_
errors	_	_
in	_	_
ground	_	_
truth	_	_
masks	_	_
affect	_	_
the	_	_
performance	_	_
of	_	_
trained	_	_
models	_	_
for	_	_
the	_	_
task	_	_
of	_	_
semantic	_	_
segmentation	_	_
in	_	_
medical	_	_
imaging	_	_
.	_	_

#30
In	_	_
particular	_	_
,	_	_
we	_	_
simulate	_	_
ground	_	_
truth	_	_
errors	_	_
in	_	_
the	_	_
widely	_	_
used	_	_
Liver	_	_
Segmentation	_	_
Dataset1	_	_
by	_	_
perturbing	_	_
the	_	_
training	_	_
annotations	_	_
to	_	_
various	_	_
degrees	_	_
in	_	_
a	_	_
”natural”	_	_
,	_	_
”choppy”	_	_
,	_	_
and	_	_
”random”	_	_
way	_	_
.	_	_

#31
The	_	_
validation	_	_
and	_	_
testing	_	_
annotations	_	_
were	_	_
left	_	_
untouched	_	_
.	_	_

#32
We	_	_
repeatedly	_	_
train	_	_
three	_	_
widely	_	_
used	_	_
DL-based	_	_
segmentation	_	_
models	_	_
(	_	_
U-Net	_	_
[	_	_
17	_	_
]	_	_
,	_	_
SegNet	_	_
[	_	_
3	_	_
]	_	_
,	_	_
and	_	_
FCN32	_	_
[	_	_
20	_	_
]	_	_
)	_	_
on	_	_
the	_	_
perturbed	_	_
training	_	_
data	_	_
and	_	_
report	_	_
the	_	_
corresponding	_	_
degradation	_	_
in	_	_
performance	_	_
.	_	_

#33
2	_	_
Related	_	_
Work	_	_

#34
In	_	_
[	_	_
2	_	_
]	_	_
Angluin	_	_
and	_	_
Laird	_	_
analyzed	_	_
mislabeled	_	_
examples	_	_
from	_	_
the	_	_
standpoint	_	_
of	_	_
Probably	_	_
Approximately	_	_
Correct	_	_
learning	_	_
.	_	_

#35
They	_	_
show	_	_
that	_	_
the	_	_
learning	_	_
problem	_	_
remains	_	_
feasible	_	_
so	_	_
long	_	_
as	_	_
the	_	_
noise	_	_
affects	_	_
less	_	_
than	_	_
half	_	_
of	_	_
the	_	_
instances	_	_
on	_	_
average	_	_
,	_	_
although	_	_
sample	_	_
complexity	_	_
increases	_	_
with	_	_
label	_	_
noise	_	_
.	_	_

#36
Considerable	_	_
work	_	_
has	_	_
been	_	_
done	_	_
to	_	_
characterize	_	_
the	_	_
effect	_	_
of	_	_
label	_	_
noise	_	_
on	_	_
classical	_	_
algorithms	_	_
such	_	_
as	_	_
Decision	_	_
Trees	_	_
,	_	_
Support	_	_
Vector	_	_
Machines	_	_
,	_	_
and	_	_
k-Nearest	_	_
Neighbors	_	_
,	_	_
and	_	_
robust	_	_
variants	_	_
of	_	_
these	_	_
have	_	_
been	_	_
proposed	_	_
.	_	_

#37
For	_	_
a	_	_
detailed	_	_
survey	_	_
,	_	_
see	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#38
Many	_	_
data-cleansing	_	_
algorithms	_	_
have	_	_
been	_	_
proposed	_	_
to	_	_
reduce	_	_
the	_	_
incidence	_	_
of	_	_
mislabeled	_	_
data	_	_
in	_	_
datasets	_	_
[	_	_
14	_	_
]	_	_
[	_	_
4	_	_
]	_	_
[	_	_
21	_	_
]	_	_
but	_	_
challenges	_	_
arise	_	_
in	_	_
distinguishing	_	_
mislabeled	_	_
instances	_	_
from	_	_
instances	_	_
that	_	_
are	_	_
difficult	_	_
but	_	_
informative	_	_
.	_	_

#39
With	_	_
the	_	_
rise	_	_
to	_	_
prominence	_	_
of	_	_
deep	_	_
learning	_	_
for	_	_
computer	_	_
vision	_	_
tasks	_	_
,	_	_
the	_	_
ready	_	_
availability	_	_
of	_	_
vast	_	_
quantities	_	_
of	_	_
noisily	_	_
labeled	_	_
data	_	_
on	_	_
the	_	_
internet	_	_
,	_	_
and	_	_
the	_	_
lack	_	_
of	_	_
sufficient	_	_
data-cleansing	_	_
algorithms	_	_
,	_	_
many	_	_
have	_	_
turned	_	_
their	_	_
attention	_	_
to	_	_
studying	_	_
the	_	_
pitfalls	_	_
of	_	_
training	_	_
Deep	_	_
Neural	_	_
Networks	_	_
for	_	_
image	_	_
recognition	_	_
,	_	_
attribute	_	_
learning	_	_
,	_	_
and	_	_
scene	_	_
classification	_	_
using	_	_
noisy	_	_
labels	_	_
.	_	_

#40
In	_	_
[	_	_
22	_	_
]	_	_
the	_	_
authors	_	_
find	_	_
that	_	_
transfer	_	_
learning	_	_
from	_	_
a	_	_
noisy	_	_
dataset	_	_
to	_	_
a	_	_
smaller	_	_
but	_	_
clean	_	_
dataset	_	_
for	_	_
the	_	_
same	_	_
task	_	_
does	_	_
better	_	_
than	_	_
fine-tuning	_	_
on	_	_
the	_	_
clean	_	_
dataset	_	_
alone	_	_
.	_	_

#41
They	_	_
go	_	_
on	_	_
to	_	_
extend	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
with	_	_
a	_	_
probabilistic	_	_
framework	_	_
to	_	_
model	_	_
how	_	_
mislabelings	_	_
occur	_	_
and	_	_
infer	_	_
true	_	_
labels	_	_
with	_	_
the	_	_
Expectation	_	_
Maximization	_	_
algorithm	_	_
.	_	_

#42
In	_	_
[	_	_
16	_	_
]	_	_
the	_	_
authors	_	_
utilize	_	_
what	_	_
they	_	_
call	_	_
”perceptual	_	_
consistency”	_	_
.	_	_

#43
They	_	_
argue	_	_
this	_	_
is	_	_
implicit	_	_
in	_	_
the	_	_
network	_	_
parameters	_	_
and	_	_
that	_	_
it	_	_
holds	_	_
an	_	_
internal	_	_
representation	_	_
of	_	_
the	_	_
world	_	_
.	_	_

#44
Thus	_	_
,	_	_
it	_	_
can	_	_
serve	_	_
as	_	_
a	_	_
basis	_	_
for	_	_
the	_	_
network	_	_
to	_	_
”disagree”	_	_
with	_	_
the	_	_
provided	_	_
labels	_	_
and	_	_
relabel	_	_
data	_	_
during	_	_
training	_	_
.	_	_

#45
The	_	_
network	_	_
then	_	_
”bootstraps”	_	_
itself	_	_
in	_	_
this	_	_
way	_	_
,	_	_
using	_	_
what	_	_
it	_	_
learns	_	_
from	_	_
the	_	_
relabeling	_	_
as	_	_
a	_	_
basis	_	_
to	_	_
relabel	_	_
more	_	_
data	_	_
,	_	_
and	_	_
so	_	_
on	_	_
.	_	_

#46
These	_	_
techniques	_	_
are	_	_
very	_	_
robust	_	_
to	_	_
label	_	_
noise	_	_
in	_	_
the	_	_
image	_	_
recognition	_	_
but	_	_
label	_	_
errors	_	_
in	_	_
semantic	_	_
segmentation	_	_
present	_	_
a	_	_
fundamentally	_	_
different	_	_
problem	_	_
,	_	_
since	_	_
label	_	_
errors	_	_
overwhelmingly	_	_
occur	_	_
at	_	_
region	_	_
boundaries	_	_
,	_	_
and	_	_
no	_	_
such	_	_
concept	_	_
exists	_	_
for	_	_
holistic	_	_
image	_	_
analysis	_	_
.	_	_

#47
In	_	_
addition	_	_
,	_	_
learning	_	_
in	_	_
semantic	_	_
segmentation	_	_
is	_	_
done	_	_
with	_	_
fixed	_	_
cohorts	_	_
of	_	_
pixels	_	_
(	_	_
images	_	_
)	_	_
within	_	_
random	_	_
batches	_	_
.	_	_

#48
Therefore	_	_
,	_	_
a	_	_
DL	_	_
model	_	_
may	_	_
learn	_	_
a	_	_
general	_	_
rule	_	_
about	_	_
feasible	_	_
region	_	_
size	_	_
and	_	_
discourage	_	_
an	_	_
otherwise	_	_
positive	_	_
prediction	_	_
for	_	_
a	_	_
pixel	_	_
in	_	_
the	_	_
absence	_	_
of	_	_
positive	_	_
predictions	_	_
for	_	_
its	_	_
neighbors	_	_
.	_	_

#49
1	_	_
https	_	_
:	_	_
//competitions.codalab.org/competitions/17094	_	_
4	_	_
N.	_	_
Heller	_	_
et	_	_
al.	_	_
3	_	_
Methods	_	_

#50
3.1	_	_
Perturbations	_	_

#51
We	_	_
attempted	_	_
to	_	_
perturb	_	_
ground	_	_
truth	_	_
masks	_	_
such	_	_
that	_	_
they	_	_
closely	_	_
mimicked	_	_
the	_	_
sorts	_	_
of	_	_
errors	_	_
that	_	_
human	_	_
experts	_	_
often	_	_
make	_	_
when	_	_
drawing	_	_
freehand	_	_
contours	_	_
.	_	_

#52
In	_	_
order	_	_
to	_	_
achieve	_	_
this	_	_
,	_	_
we	_	_
first	_	_
retrieved	_	_
the	_	_
contours	_	_
from	_	_
an	_	_
existing	_	_
binary	_	_
mask	_	_
using	_	_
OpenCV’s	_	_
findContours	_	_
(	_	_
)	_	_
function	_	_
.	_	_

#53
We	_	_
then	_	_
sampled	_	_
points	_	_
from	_	_
this	_	_
contour	_	_
and	_	_
moved	_	_
them	_	_
a	_	_
random	_	_
offset	_	_
either	_	_
towards	_	_
or	_	_
away	_	_
from	_	_
the	_	_
contour’s	_	_
center	_	_
.	_	_

#54
We	_	_
used	_	_
a	_	_
simple	_	_
fill	_	_
to	_	_
produce	_	_
the	_	_
perturbed	_	_
annotation	_	_
.	_	_

#55
The	_	_
offsets	_	_
were	_	_
produced	_	_
by	_	_
a	_	_
normal	_	_
distribution	_	_
with	_	_
a	_	_
given	_	_
variance	_	_
and	_	_
zero	_	_
mean	_	_
.	_	_

#56
We	_	_
call	_	_
these	_	_
offsets	_	_
natural	_	_
perturbations	_	_
.	_	_

#57
A	_	_
natural	_	_
perturbation	_	_
applied	_	_
to	_	_
a	_	_
circle	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
2	_	_
(	_	_
middle-left	_	_
)	_	_
.	_	_

#58
In	_	_
addition	_	_
,	_	_
we	_	_
wanted	_	_
to	_	_
mimic	_	_
the	_	_
sort	_	_
of	_	_
errors	_	_
that	_	_
occur	_	_
when	_	_
natural	_	_
errors	_	_
are	_	_
made	_	_
in	_	_
a	_	_
single	_	_
plane	_	_
of	_	_
a	_	_
volume	_	_
and	_	_
data	_	_
is	_	_
viewed	_	_
from	_	_
an	_	_
orthogonal	_	_
plane	_	_
,	_	_
as	_	_
seen	_	_
in	_	_
Fig.	_	_
1	_	_
.	_	_

#59
For	_	_
this	_	_
,	_	_
we	_	_
iterated	_	_
over	_	_
every	_	_
row	_	_
in	_	_
the	_	_
masks	_	_
,	_	_
found	_	_
each	_	_
block	_	_
of	_	_
consecutive	_	_
positive	_	_
labels	_	_
,	_	_
and	_	_
shifted	_	_
the	_	_
block’s	_	_
starting	_	_
and	_	_
end	_	_
points	_	_
by	_	_
some	_	_
amount	_	_
that	_	_
was	_	_
once	_	_
again	_	_
sampled	_	_
from	_	_
a	_	_
normal	_	_
distribution	_	_
with	_	_
zero	_	_
mean	_	_
and	_	_
provided	_	_
variance	_	_
.	_	_

#60
We	_	_
call	_	_
these	_	_
choppy	_	_
perturbations	_	_
.	_	_

#61
A	_	_
choppy	_	_
perturbation	_	_
applied	_	_
to	_	_
a	_	_
circle	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
2	_	_
(	_	_
middle-right	_	_
)	_	_
.	_	_

#62
Finally	_	_
,	_	_
in	_	_
order	_	_
to	_	_
simulate	_	_
random	_	_
errors	_	_
in	_	_
a	_	_
classification	_	_
setting	_	_
,	_	_
we	_	_
randomly	_	_
chose	_	_
an	_	_
equal	_	_
proportion	_	_
of	_	_
voxels	_	_
from	_	_
both	_	_
the	_	_
negative	_	_
and	_	_
positive	_	_
classes	_	_
and	_	_
flipped	_	_
their	_	_
values	_	_
.	_	_

#63
We	_	_
call	_	_
these	_	_
random	_	_
perturbations	_	_
(	_	_
Fig.	_	_
2	_	_
,	_	_
right	_	_
)	_	_
.	_	_

#64
Three	_	_
parameter	_	_
settings	_	_
were	_	_
chosen	_	_
for	_	_
each	_	_
perturbation	_	_
mode	_	_
in	_	_
order	_	_
to	_	_
produce	_	_
perturbed	_	_
ground	_	_
truth	_	_
with	_	_
0.95	_	_
,	_	_
0.90	_	_
,	_	_
and	_	_
0.85	_	_
Dice-Sorensen	_	_
agreement	_	_
with	_	_
the	_	_
original	_	_
ground	_	_
truth	_	_
,	_	_
i.e.	_	_
we	_	_
chose	_	_
9	_	_
total	_	_
parameter	_	_
setting	_	_
.	_	_

#65
Each	_	_
was	_	_
tuned	_	_
by	_	_
randomly	_	_
choosing	_	_
1000	_	_
slices	_	_
and	_	_
using	_	_
bisection	_	_
with	_	_
the	_	_
terminal	_	_
condition	_	_
that	_	_
the	_	_
upper	_	_
and	_	_
lower	_	_
bounds	_	_
each	_	_
producing	_	_
Dice	_	_
scores	_	_
within	_	_
0.005	_	_
of	_	_
the	_	_
target	_	_
.	_	_

#66
Fig.	_	_
2	_	_
.	_	_

#67
From	_	_
left	_	_
to	_	_
right	_	_
:	_	_
unperturbed	_	_
,	_	_
natural	_	_
perturbations	_	_
,	_	_
choppy	_	_
perturbations	_	_
,	_	_
and	_	_
random	_	_
perturbations	_	_
.	_	_

#68
Imperfect	_	_
Segmentation	_	_
Labels	_	_
:	_	_
How	_	_
Much	_	_
Do	_	_
They	_	_
Matter	_	_
?	_	_

#69
5	_	_

#70
3.2	_	_
Training	_	_

#71
We	_	_
ran	_	_
the	_	_
experiments	_	_
using	_	_
the	_	_
Keras	_	_
[	_	_
6	_	_
]	_	_
framework	_	_
with	_	_
a	_	_
TensorFlow	_	_
[	_	_
1	_	_
]	_	_
back-end	_	_
.	_	_

#72
We	_	_
optimized	_	_
our	_	_
models	_	_
using	_	_
the	_	_
Adam	_	_
algorithm	_	_
[	_	_
11	_	_
]	_	_
with	_	_
the	_	_
default	_	_
parameter	_	_
values	_	_
.	_	_

#73
We	_	_
addressed	_	_
the	_	_
imbalance	_	_
of	_	_
the	_	_
problem	_	_
by	_	_
equally	_	_
sampling	_	_
from	_	_
each	_	_
class	_	_
,	_	_
and	_	_
we	_	_
used	_	_
mini-batches	_	_
of	_	_
20	_	_
slices	_	_
,	_	_
where	_	_
each	_	_
slice	_	_
is	_	_
a	_	_
512x512	_	_
array	_	_
of	_	_
Hounsfield	_	_
Units	_	_
from	_	_
the	_	_
axial	_	_
plane	_	_
.	_	_

#74
For	_	_
each	_	_
model	_	_
,	_	_
we	_	_
started	_	_
with	_	_
6	_	_
initial	_	_
convolutional	_	_
kernels	_	_
and	_	_
the	_	_
number	_	_
doubled	_	_
with	_	_
each	_	_
down-sampling	_	_
.	_	_

#75
Each	_	_
model	_	_
was	_	_
trained	_	_
for	_	_
100	_	_
epochs	_	_
with	_	_
35	_	_
steps	_	_
per	_	_
epoch	_	_
.	_	_

#76
For	_	_
each	_	_
architecture	_	_
and	_	_
perturbation	_	_
pair	_	_
,	_	_
we	_	_
trained	_	_
five	_	_
times	_	_
in	_	_
order	_	_
to	_	_
improve	_	_
statistical	_	_
power	_	_
,	_	_
resulting	_	_
in	_	_
150	_	_
total	_	_
training	_	_
sessions	_	_
.	_	_

#77
4	_	_
Results	_	_

#78
Our	_	_
results	_	_
show	_	_
that	_	_
the	_	_
performance	_	_
of	_	_
each	_	_
model	_	_
steadily	_	_
declined	_	_
with	_	_
the	_	_
extent	_	_
of	_	_
boundary-localized	_	_
perturbations	_	_
,	_	_
but	_	_
that	_	_
model	_	_
performance	_	_
was	_	_
very	_	_
robust	_	_
to	_	_
random	_	_
perturbations	_	_
.	_	_

#79
This	_	_
suggests	_	_
that	_	_
flawed	_	_
ground	_	_
truth	_	_
labels	_	_
,	_	_
particularly	_	_
in	_	_
border	_	_
regions	_	_
,	_	_
are	_	_
hindering	_	_
the	_	_
performance	_	_
of	_	_
DL-based	_	_
models	_	_
for	_	_
semantic	_	_
segmentation	_	_
.	_	_

#80
As	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
3	_	_
,	_	_
other	_	_
than	_	_
the	_	_
large	_	_
choppy	_	_
perturbations	_	_
for	_	_
U-Net	_	_
,	_	_
the	_	_
responses	_	_
of	_	_
each	_	_
architecture	_	_
to	_	_
the	_	_
different	_	_
degrees	_	_
of	_	_
boundary-localized	_	_
perturbation	_	_
were	_	_
surprisingly	_	_
uniform	_	_
.	_	_

#81
This	_	_
suggests	_	_
that	_	_
there	_	_
may	_	_
be	_	_
a	_	_
general	_	_
predictive	_	_
relationship	_	_
between	_	_
the	_	_
incidence	_	_
of	_	_
ground-truth	_	_
errors	_	_
and	_	_
the	_	_
expected	_	_
performance	_	_
of	_	_
these	_	_
models	_	_
.	_	_

#82
Additionally	_	_
,	_	_
it	_	_
appears	_	_
that	_	_
each	_	_
of	_	_
the	_	_
models	_	_
are	_	_
very	_	_
resilient	_	_
to	_	_
random	_	_
perturbations	_	_
in	_	_
ground	_	_
truth	_	_
,	_	_
in	_	_
some	_	_
cases	_	_
outperforming	_	_
the	_	_
Dice-Sorenson	_	_
score	_	_
of	_	_
the	_	_
training	_	_
data	_	_
itself	_	_
by	_	_
more	_	_
than	_	_
5	_	_
%	_	_
.	_	_

#83
U-Net	_	_
SegNet	_	_
FCN32	_	_
Control	_	_
0.9134	_	_
0.8993	_	_
0.8870	_	_
Natural	_	_
0.95	_	_
0.8880	_	_
0.8640	_	_
0.8587	_	_
Natural	_	_
0.90	_	_
0.8193	_	_
0.8265	_	_
0.8250	_	_
Natural	_	_
0.85	_	_
0.7521	_	_
0.7717	_	_
0.7581	_	_
Choppy	_	_
0.95	_	_
0.8928	_	_
0.8799	_	_
0.8660	_	_
Choppy	_	_
0.90	_	_
0.8321	_	_
0.8268	_	_
0.8202	_	_
Choppy	_	_
0.85	_	_
0.8058	_	_
0.7823	_	_
0.7782	_	_
Random	_	_
0.95	_	_
0.9124	_	_
0.9050	_	_
0.8881	_	_
Random	_	_
0.90	_	_
0.9213	_	_
0.9013	_	_
0.8751	_	_
Random	_	_
0.85	_	_
0.9182	_	_
0.9068	_	_
0.8676	_	_
Table	_	_
1	_	_
.	_	_

#84
Mean	_	_
Dice-Sorensen	_	_
score	_	_
for	_	_
each	_	_
model-perturbation	_	_
pair	_	_
for	_	_
Liver	_	_
Segmentation	_	_
6	_	_
N.	_	_
Heller	_	_
et	_	_
al.	_	_
Fig.	_	_
3	_	_
.	_	_

#85
The	_	_
results	_	_
for	_	_
liver	_	_
segmentation	_	_
for	_	_
each	_	_
model	_	_
with	_	_
each	_	_
type	_	_
of	_	_
mode	_	_
of	_	_
ground-truth	_	_
perturbation	_	_
(	_	_
best	_	_
viewed	_	_
in	_	_
color	_	_
)	_	_
.	_	_

#86
U-Net’s	_	_
anomalously	_	_
good	_	_
performance	_	_
in	_	_
the	_	_
presence	_	_
of	_	_
large	_	_
choppy	_	_
perturbations	_	_
is	_	_
interesting	_	_
.	_	_

#87
We	_	_
hypothesize	_	_
that	_	_
this	_	_
is	_	_
because	_	_
U-Net’s	_	_
”skip	_	_
connections”	_	_
allow	_	_
it	_	_
to	_	_
very	_	_
effectively	_	_
preserve	_	_
border	_	_
information	_	_
from	_	_
activation	_	_
functions	_	_
early	_	_
on	_	_
in	_	_
the	_	_
network	_	_
.	_	_

#88
Thus	_	_
,	_	_
borders	_	_
are	_	_
likely	_	_
still	_	_
emphasized	_	_
because	_	_
even	_	_
though	_	_
the	_	_
contour	_	_
has	_	_
become	_	_
jagged	_	_
,	_	_
the	_	_
region	_	_
edges	_	_
are	_	_
centered	_	_
on	_	_
the	_	_
true	_	_
contour	_	_
.	_	_

#89
This	_	_
is	_	_
not	_	_
the	_	_
case	_	_
for	_	_
the	_	_
”natural”	_	_
perturbations	_	_
.	_	_

#90
5	_	_
Limitations	_	_

#91
Better	_	_
performance	_	_
has	_	_
been	_	_
reported	_	_
for	_	_
the	_	_
liver	_	_
segmentation	_	_
problem	_	_
[	_	_
12	_	_
]	_	_
,	_	_
but	_	_
that	_	_
is	_	_
due	_	_
to	_	_
the	_	_
use	_	_
of	_	_
ensembles	_	_
and	_	_
hyperparameter	_	_
tuning	_	_
.	_	_

#92
It	_	_
would	_	_
not	_	_
be	_	_
feasible	_	_
to	_	_
engineer	_	_
and	_	_
train	_	_
such	_	_
techniques	_	_
for	_	_
each	_	_
and	_	_
every	_	_
data	_	_
point	_	_
.	_	_

#93
It	_	_
is	_	_
possible	_	_
(	_	_
although	_	_
we	_	_
believe	_	_
unlikely	_	_
)	_	_
that	_	_
these	_	_
findings	_	_
do	_	_
not	_	_
translate	_	_
to	_	_
large	_	_
ensemble	_	_
settings	_	_
,	_	_
but	_	_
this	_	_
must	deontic	_
be	_	_
the	_	_
subject	_	_
for	_	_
future	_	_
work	_	_
.	_	_

#94
Additionally	_	_
,	_	_
these	_	_
experiments	_	_
were	_	_
all	_	_
run	_	_
on	_	_
a	_	_
single	_	_
dataset	_	_
with	_	_
binary	_	_
labels	_	_
.	_	_

#95
More	_	_
work	_	_
must	deontic	_
be	_	_
done	_	_
to	_	_
study	_	_
whether	_	_
these	_	_
results	_	_
generalize	_	_
to	_	_
different	_	_
problems	_	_
,	_	_
and	_	_
problems	_	_
with	_	_
many	_	_
class	_	_
labels	_	_
.	_	_

#96
Finally	_	_
,	_	_
this	_	_
study	_	_
did	_	_
not	_	_
examine	_	_
the	_	_
effect	_	_
of	_	_
biased	_	_
labels	_	_
,	_	_
which	_	_
are	_	_
also	_	_
likely	_	_
to	_	_
exist	_	_
in	_	_
semantic	_	_
segmentation	_	_
datasets	_	_
.	_	_

#97
Our	_	_
intuition	_	_
is	_	_
that	_	_
the	_	_
models	_	_
will	_	_
tend	_	_
to	_	_
exhibit	_	_
the	_	_
same	_	_
bias	_	_
as	_	_
the	_	_
expert	_	_
,	_	_
but	_	_
it’s	_	_
unclear	_	_
what	_	_
the	_	_
effect	_	_
on	_	_
performance	_	_
would	_	_
be	_	_
when	_	_
there	_	_
are	_	_
multiple	_	_
experts	_	_
,	_	_
each	_	_
with	_	_
different	_	_
biases	_	_
.	_	_

#98
This	_	_
,	_	_
too	_	_
,	_	_
must	deontic	_
be	_	_
the	_	_
subject	_	_
for	_	_
future	_	_
work	_	_
.	_	_

#99
6	_	_
Conclusion	_	_

#100
In	_	_
this	_	_
work	_	_
we	_	_
tested	_	_
how	_	_
three	_	_
widely-used	_	_
deep	_	_
learning	_	_
based	_	_
models	_	_
responded	_	_
to	_	_
various	_	_
modes	_	_
of	_	_
errors	_	_
in	_	_
ground-truth	_	_
labels	_	_
for	_	_
semantic	_	_
segmentation	_	_
of	_	_
the	_	_
liver	_	_
in	_	_
abdominal	_	_
CT	_	_
scans	_	_
.	_	_

#101
We	_	_
found	_	_
that	_	_
in	_	_
general	_	_
,	_	_
these	_	_
models	_	_
each	_	_
experience	_	_
relatively	_	_
uniform	_	_
performance	_	_
degradation	_	_
with	_	_
increased	_	_
incidence	_	_
of	_	_
label	_	_
errors	_	_
,	_	_
but	_	_
that	_	_
U-Net	_	_
was	_	_
especially	_	_
robust	_	_
to	_	_
large	_	_
amounts	_	_
of	_	_
”choppy”	_	_
noise	_	_
on	_	_
the	_	_
liver	_	_
regions	_	_
.	_	_

#102
There	_	_
are	_	_
many	_	_
opportunities	_	_
to	_	_
continue	_	_
this	_	_
work	_	_
.	_	_

#103
In	_	_
particular	_	_
,	_	_
we	_	_
would	_	_
like	_	_
to	_	_
expand	_	_
the	_	_
scope	_	_
of	_	_
this	_	_
study	_	_
to	_	_
look	_	_
also	_	_
at	_	_
how	_	_
the	_	_
hyperparameters	_	_
of	_	_
the	_	_
architectures	_	_
and	_	_
training	_	_
procedures	_	_
affect	_	_
its	_	_
sensitivity	_	_
.	_	_

#104
We	_	_
also	_	_
believe	_	_
it	_	_
would	_	_
be	_	_
useful	_	_
to	_	_
explore	_	_
the	_	_
effect	_	_
of	_	_
dataset	_	_
size	_	_
on	_	_
sensitivity	_	_
,	_	_
since	_	_
it’s	_	_
possible	_	_
that	_	_
models	_	_
will	_	_
have	_	_
a	_	_
more	_	_
difficult	_	_
time	_	_
coping	_	_
with	_	_
noisy	_	_
data	_	_
when	_	_
they	_	_
have	_	_
less	_	_
data	_	_
to	_	_
look	_	_
at	_	_
.	_	_

#105
Finally	_	_
,	_	_
we	_	_
plan	_	_
to	_	_
study	_	_
how	_	_
deep-learning-based	_	_
architectures	_	_
for	_	_
semantic	_	_
segmentation	_	_
can	_	_
be	_	_
modified	_	_
in	_	_
order	_	_
to	_	_
be	_	_
more	_	_
robust	_	_
to	_	_
ground	_	_
truth	_	_
errors	_	_
at	_	_
region	_	_
boundaries	_	_
.	_	_

#106
The	_	_
code	_	_
for	_	_
our	_	_
experiments	_	_
has	_	_
been	_	_
made	_	_
available	_	_
at	_	_
https	_	_
:	_	_
//github.com/neheller/labels18	_	_
.	_	_

#107
Acknowledgements	_	_
Research	_	_
reported	_	_
in	_	_
this	_	_
publication	_	_
was	_	_
supported	_	_
by	_	_
the	_	_
National	_	_
Cancer	_	_
Institute	_	_
of	_	_
the	_	_
National	_	_
Institutes	_	_
of	_	_
Health	_	_
under	_	_
Award	_	_
Number	_	_
R01CA225435	_	_
.	_	_

#108
The	_	_
content	_	_
is	_	_
solely	_	_
the	_	_
responsibility	_	_
of	_	_
the	_	_
authors	_	_
and	_	_
does	_	_
not	_	_
necessarily	_	_
represent	_	_
the	_	_
official	_	_
views	_	_
of	_	_
the	_	_
National	_	_
Institutes	_	_
of	_	_
Health	_	_
.	_	_