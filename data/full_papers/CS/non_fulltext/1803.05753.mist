#0
What	_	_
Catches	_	_
the	_	_
Eye	_	_
?	_	_

#1
Visualizing	_	_
and	_	_
Understanding	_	_
Deep	_	_
Saliency	_	_
Models	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#2
1	_	_
Department	_	_
of	_	_
Computer	_	_
Science	_	_

#3
University	_	_
of	_	_
Exeter	_	_

#4
2	_	_
Center	_	_
for	_	_
Research	_	_
in	_	_
Computer	_	_
Vision	_	_

#5
University	_	_
of	_	_
Central	_	_
Florida	_	_
Abstract	_	_
.	_	_

#6
Deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
have	_	_
demonstrated	_	_
high	_	_
performances	_	_
for	_	_
fixation	_	_
prediction	_	_
in	_	_
recent	_	_
years	_	_
.	_	_

#7
How	_	_
they	_	_
achieve	_	_
this	_	_
,	_	_
however	_	_
,	_	_
is	_	_
less	_	_
explored	_	_
and	_	_
they	_	_
remain	_	_
to	_	_
be	_	_
black	_	_
box	_	_
models	_	_
.	_	_

#8
Here	_	_
,	_	_
we	_	_
attempt	_	_
to	_	_
shed	_	_
light	_	_
on	_	_
the	_	_
internal	_	_
structure	_	_
of	_	_
deep	_	_
saliency	_	_
models	_	_
and	_	_
study	_	_
what	_	_
features	_	_
they	_	_
extract	_	_
for	_	_
fixation	_	_
prediction	_	_
.	_	_

#9
Specifically	_	_
,	_	_
we	_	_
use	_	_
a	_	_
simple	_	_
yet	_	_
powerful	_	_
architecture	_	_
,	_	_
consisting	_	_
of	_	_
only	_	_
one	_	_
CNN	_	_
and	_	_
a	_	_
single	_	_
resolution	_	_
input	_	_
,	_	_
combined	_	_
with	_	_
a	_	_
new	_	_
loss	_	_
function	_	_
for	_	_
pixel-wise	_	_
fixation	_	_
prediction	_	_
during	_	_
free	_	_
viewing	_	_
of	_	_
natural	_	_
scenes	_	_
.	_	_

#10
We	_	_
show	_	_
that	_	_
our	_	_
simple	_	_
method	_	_
is	_	_
on	_	_
par	_	_
or	_	_
better	_	_
than	_	_
state-of-the-art	_	_
complicated	_	_
saliency	_	_
models	_	_
.	_	_

#11
Furthermore	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
method	_	_
,	_	_
related	_	_
to	_	_
saliency	_	_
model	_	_
evaluation	_	_
metrics	_	_
,	_	_
to	_	_
visualize	_	_
deep	_	_
models	_	_
for	_	_
fixation	_	_
prediction	_	_
.	_	_

#12
Our	_	_
method	_	_
reveals	_	_
the	_	_
inner	_	_
representations	_	_
of	_	_
deep	_	_
models	_	_
for	_	_
fixation	_	_
prediction	_	_
and	_	_
provides	_	_
evidence	_	_
that	_	_
saliency	_	_
,	_	_
as	_	_
experienced	_	_
by	_	_
humans	_	_
,	_	_
is	_	_
likely	_	_
to	_	_
involve	_	_
high-level	_	_
semantic	_	_
knowledge	_	_
in	_	_
addition	_	_
to	_	_
low-level	_	_
perceptual	_	_
cues	_	_
.	_	_

#13
Our	_	_
results	_	_
can	_	_
be	_	_
useful	_	_
to	_	_
measure	_	_
the	_	_
gap	_	_
between	_	_
current	_	_
saliency	_	_
models	_	_
and	_	_
the	_	_
human	_	_
inter-observer	_	_
model	_	_
and	_	_
to	_	_
build	_	_
new	_	_
models	_	_
to	_	_
close	_	_
this	_	_
gap	_	_
.	_	_

#14
Keywords	_	_
:	_	_
Deep	_	_
Neural	_	_
Network	_	_
,	_	_
Saliency	_	_
,	_	_
Eye	_	_
Fixation	_	_
Prediction	_	_
,	_	_
Model	_	_
Visualization	_	_

#15
1	_	_
Introduction	_	_

#16
The	_	_
human	_	_
visual	_	_
system	_	_
receives	_	_
a	_	_
large	_	_
amount	_	_
of	_	_
information	_	_
every	_	_
second	_	_
(	_	_
about	_	_
108	_	_
to	_	_
109	_	_
bits	_	_
)	_	_
.	_	_

#17
An	_	_
essential	_	_
mechanism	_	_
that	_	_
allows	_	_
the	_	_
human	_	_
visual	_	_
system	_	_
to	_	_
process	_	_
such	_	_
a	_	_
vast	_	_
amount	_	_
of	_	_
information	_	_
in	_	_
real	_	_
time	_	_
is	_	_
its	_	_
capacity	_	_
to	_	_
selectively	_	_
focus	_	_
attention	_	_
on	_	_
parts	_	_
of	_	_
the	_	_
scene	_	_
.	_	_

#18
This	_	_
process	_	_
has	_	_
been	_	_
extensively	_	_
studied	_	_
by	_	_
Psychologists	_	_
to	_	_
discover	_	_
which	_	_
visual	_	_
patterns	_	_
capture	_	_
human	_	_
attention	_	_
.	_	_

#19
Desimone	_	_
&	_	_
Duncan	_	_
[	_	_
1	_	_
]	_	_
found	_	_
that	_	_
parts	_	_
of	_	_
an	_	_
image	_	_
that	_	_
differ	_	_
from	_	_
their	_	_
surroundings	_	_
stand	_	_
out	_	_
.	_	_

#20
This	_	_
paradigm	_	_
is	_	_
called	_	_
center-surround	_	_
difference	_	_
in	_	_
early	_	_
computational	_	_
modeling	_	_
of	_	_
visual	_	_
attention	_	_
.	_	_

#21
Based	_	_
on	_	_
the	_	_
center-surround	_	_
difference	_	_
and	_	_
the	_	_
feature	_	_
integration	_	_
theory	_	_
proposed	_	_
by	_	_
Treisman	_	_
&	_	_
Gelade	_	_
[	_	_
2	_	_
]	_	_
,	_	_
many	_	_
computational	_	_
models	_	_
of	_	_
visual	_	_
attention	_	_
have	_	_
been	_	_
proposed	_	_
[	_	_
3,4,5	_	_
]	_	_
.	_	_

#22
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#23
3v	_	_
3	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
2	_	_
M	_	_
ar	_	_
2	_	_

#24
2	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#25
In	_	_
recent	_	_
years	_	_
,	_	_
with	_	_
the	_	_
availability	_	_
of	_	_
large	_	_
scale	_	_
datasets	_	_
recording	_	_
mouse	_	_
movements	_	_
of	_	_
human	_	_
subjects	_	_
as	_	_
a	_	_
proxy	_	_
of	_	_
gaze	_	_
(	_	_
e.g.	_	_
,	_	_
[	_	_
6	_	_
]	_	_
)	_	_
and	_	_
of	_	_
powerful	_	_
parallel	_	_
hardware	_	_
,	_	_
the	_	_
development	_	_
of	_	_
data	_	_
driven	_	_
approaches	_	_
based	_	_
on	_	_
deep	_	_
learning	_	_
have	_	_
demonstrated	_	_
significantly	_	_
higher	_	_
performance	_	_
than	_	_
previous	_	_
models	_	_
on	_	_
all	_	_
benchmarks	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#26
Currently	_	_
,	_	_
almost	_	_
all	_	_
deep	_	_
saliency	_	_
models	_	_
treat	_	_
the	_	_
gaze	_	_
map	_	_
as	_	_
a	_	_
small	_	_
scale	_	_
map	_	_
recording	_	_
the	_	_
density	_	_
of	_	_
fixations	_	_
at	_	_
every	_	_
image	_	_
location	_	_
(	_	_
downsampled	_	_
from	_	_
the	_	_
ground	_	_
truth	_	_
[	_	_
8,9	_	_
]	_	_
)	_	_
.	_	_

#27
Such	_	_
models	_	_
are	_	_
almost	_	_
invariably	_	_
trained	_	_
by	_	_
minimizing	_	_
the	_	_
distance	_	_
between	_	_
the	_	_
predicted	_	_
saliency	_	_
maps	_	_
and	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#28
At	_	_
inference	_	_
time	_	_
,	_	_
the	_	_
saliency	_	_
map	_	_
is	_	_
then	_	_
upsampled	_	_
to	_	_
the	_	_
input’s	_	_
image	_	_
size	_	_
.	_	_

#29
Such	_	_
deep	_	_
saliency	_	_
models	_	_
have	_	_
achieved	_	_
much	_	_
better	_	_
performance	_	_
than	_	_
models	_	_
based	_	_
on	_	_
hand-crafted	_	_
features	_	_
or	_	_
psychological	_	_
assumptions	_	_
,	_	_
but	_	_
unlike	_	_
for	_	_
the	_	_
task	_	_
of	_	_
image	_	_
recognition	_	_
,	_	_
where	_	_
the	_	_
representations	_	_
learned	_	_
by	_	_
deep	_	_
neurons	_	_
have	_	_
been	_	_
studied	_	_
and	_	_
visualized	_	_
[	_	_
10,11	_	_
]	_	_
,	_	_
it	_	_
remains	_	_
unclear	_	_
why	_	_
deep	_	_
saliency	_	_
models	_	_
perform	_	_
so	_	_
well	_	_
or	_	_
what	_	_
salient	_	_
patterns	_	_
have	_	_
deep	_	_
neurons	_	_
attuned	_	_
to	_	_
in	_	_
the	_	_
process	_	_
.	_	_

#30
The	_	_
complexity	_	_
of	_	_
some	_	_
of	_	_
the	_	_
proposed	_	_
architectures	_	_
make	_	_
them	_	_
even	_	_
more	_	_
inscrutable	_	_
.	_	_

#31
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
use	_	_
a	_	_
simple	_	_
yet	_	_
powerful	_	_
residual-like	_	_
decoder	_	_
with	_	_
a	_	_
new	_	_
loss	_	_
function	_	_
for	_	_
pixel-wise	_	_
gaze	_	_
prediction	_	_
.	_	_

#32
The	_	_
architecture	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
architecture	_	_
in	_	_
[	_	_
12	_	_
]	_	_
,	_	_
but	_	_
we	_	_
dispense	_	_
with	_	_
the	_	_
GAN	_	_
training	_	_
and	_	_
instead	_	_
propose	_	_
a	_	_
simpler	_	_
,	_	_
residual	_	_
decoder	_	_
.	_	_

#33
We	_	_
demonstrate	_	_
that	_	_
the	_	_
model	_	_
although	_	_
simpler	_	_
,	_	_
achieves	_	_
better	_	_
performance	_	_
on	_	_
most	_	_
metrics	_	_
and	_	_
datasets	_	_
.	_	_

#34
Additionally	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
method	_	_
to	_	_
visualize	_	_
and	_	_
analyze	_	_
the	_	_
representations	_	_
learnt	_	_
by	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#35
To	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
this	_	_
is	_	_
the	_	_
first	_	_
work	_	_
which	_	_
looks	_	_
inside	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#36
The	_	_
rest	_	_
of	_	_
the	_	_
paper	_	_
is	_	_
organized	_	_
as	_	_
follows	_	_
.	_	_

#37
Section	_	_
2	_	_
reviews	_	_
the	_	_
state-of-the-art	_	_
gaze	_	_
prediction	_	_
models	_	_
as	_	_
well	_	_
as	_	_
visualization	_	_
methods	_	_
for	_	_
deep	_	_
convolutional	_	_
networks	_	_
.	_	_

#38
Section	_	_
3	_	_
introduces	_	_
the	_	_
proposed	_	_
deep	_	_
saliency	_	_
model	_	_
and	_	_
the	_	_
model	_	_
visualization	_	_
method	_	_
.	_	_

#39
Experimental	_	_
results	_	_
and	_	_
benchmarks	_	_
are	_	_
presented	_	_
in	_	_
Sections	_	_
4	_	_
and	_	_
5	_	_
.	_	_

#40
2	_	_
Related	_	_
work	_	_

#41
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
first	_	_
review	_	_
the	_	_
state-of-the-art	_	_
deep	_	_
gaze	_	_
prediction	_	_
models	_	_
before	_	_
introducing	_	_
visualization	_	_
methods	_	_
for	_	_
deep	_	_
convolutional	_	_
networks	_	_
.	_	_

#42
2.1	_	_
Deep	_	_
saliency	_	_
models	_	_

#43
The	_	_
release	_	_
of	_	_
SALICON	_	_
dataset	_	_
[	_	_
6	_	_
]	_	_
,	_	_
offered	_	_
for	_	_
the	_	_
first	_	_
time	_	_
a	_	_
large	_	_
scale	_	_
dataset	_	_
for	_	_
saliency	_	_
,	_	_
which	_	_
spurred	_	_
the	_	_
development	_	_
of	_	_
a	_	_
number	_	_
of	_	_
saliency	_	_
models	_	_
.	_	_

#44
For	_	_
example	_	_
,	_	_
Deepnet	_	_
[	_	_
13	_	_
]	_	_
learns	_	_
saliency	_	_
using	_	_
8	_	_
convolutional	_	_
layers	_	_
,	_	_
where	_	_
only	_	_
the	_	_
first	_	_
3	_	_
layers	_	_
were	_	_
initialized	_	_
from	_	_
the	_	_
pre-trained	_	_
image	_	_
classification	_	_
model	_	_
.	_	_

#45
PDP	_	_
[	_	_
9	_	_
]	_	_
treats	_	_
the	_	_
gaze	_	_
map	_	_
as	_	_
a	_	_
small	_	_
scale	_	_
probability	_	_
map	_	_
.	_	_

#46
Authors	_	_
investigated	_	_
different	_	_
loss	_	_
functions	_	_
for	_	_
training	_	_
their	_	_
gaze	_	_
prediction	_	_
model	_	_
and	_	_
found	_	_
that	_	_
Bhattacharyya	_	_
distance	_	_
is	_	_
the	_	_
best	_	_
loss	_	_
function	_	_
when	_	_
the	_	_
gaze	_	_
map	_	_
is	_	_
Title	_	_
Suppressed	_	_
Due	_	_
to	_	_
Excessive	_	_
Length	_	_
3	_	_
treated	_	_
as	_	_
a	_	_
small	_	_
scale	_	_
probability	_	_
map	_	_
.	_	_

#47
The	_	_
Salicon	_	_
[	_	_
8	_	_
]	_	_
model	_	_
uses	_	_
multi	_	_
resolution	_	_
inputs	_	_
,	_	_
and	_	_
combines	_	_
feature	_	_
representations	_	_
in	_	_
the	_	_
deep	_	_
layers	_	_
for	_	_
gaze	_	_
prediction	_	_
.	_	_

#48
Deepfix	_	_
[	_	_
14	_	_
]	_	_
combined	_	_
the	_	_
deep	_	_
architectures	_	_
of	_	_
VGG	_	_
[	_	_
15	_	_
]	_	_
,	_	_
Googlenet	_	_
[	_	_
16	_	_
]	_	_
,	_	_
and	_	_
Dilated	_	_
convolutions	_	_
[	_	_
17	_	_
]	_	_
in	_	_
their	_	_
network	_	_
as	_	_
well	_	_
as	_	_
adding	_	_
a	_	_
central	_	_
bias	_	_
,	_	_
to	_	_
achieve	_	_
a	_	_
higher	_	_
performance	_	_
than	_	_
previous	_	_
models	_	_
.	_	_

#49
SalGAN	_	_
[	_	_
12	_	_
]	_	_
uses	_	_
an	_	_
encoder-decoder	_	_
architecture	_	_
and	_	_
proposes	_	_
the	_	_
binary	_	_
cross	_	_
entropy	_	_
(	_	_
BCE	_	_
)	_	_
loss	_	_
function	_	_
to	_	_
perform	_	_
pixel-wise	_	_
(	_	_
rather	_	_
than	_	_
image-wise	_	_
)	_	_
saliency	_	_
estimation	_	_
.	_	_

#50
After	_	_
pre-training	_	_
the	_	_
encoder-decoder	_	_
,	_	_
they	_	_
use	_	_
a	_	_
Generative	_	_
Adversarial	_	_
Network	_	_
(	_	_
GAN	_	_
)	_	_
to	_	_
boost	_	_
their	_	_
model’s	_	_
performance	_	_
.	_	_

#51
DVA	_	_
[	_	_
18	_	_
]	_	_
uses	_	_
multiple	_	_
deep	_	_
layer	_	_
representations	_	_
,	_	_
builds	_	_
a	_	_
decoder	_	_
for	_	_
each	_	_
layer	_	_
,	_	_
and	_	_
fuses	_	_
them	_	_
at	_	_
the	_	_
final	_	_
stage	_	_
for	_	_
pixel-wise	_	_
gaze	_	_
prediction	_	_
.	_	_

#52
DSCLRCN	_	_
[	_	_
19	_	_
]	_	_
also	_	_
uses	_	_
multiple	_	_
inputs	_	_
by	_	_
adding	_	_
a	_	_
contextual	_	_
information	_	_
stream	_	_
,	_	_
and	_	_
concatenates	_	_
the	_	_
original	_	_
representation	_	_
and	_	_
the	_	_
contextual	_	_
representation	_	_
into	_	_
a	_	_
LSTM	_	_
network	_	_
for	_	_
the	_	_
final	_	_
prediction	_	_
.	_	_

#53
Table	_	_
1	_	_
provides	_	_
a	_	_
comparison	_	_
of	_	_
state-of-the-art	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#54
Complex	_	_
architectures	_	_
[	_	_
8,14,18,20,19	_	_
]	_	_
are	_	_
intrinsically	_	_
inscrutable	_	_
and	_	_
difficult	_	_
to	_	_
interpret	_	_
,	_	_
hence	_	_
in	_	_
this	_	_
article	_	_
we	_	_
propose	_	_
to	_	_
use	_	_
a	_	_
simple	_	_
fully	_	_
convolutional	_	_
encoder	_	_
with	_	_
a	_	_
residual	_	_
decoder	_	_
,	_	_
using	_	_
the	_	_
exponential	_	_
absolute	_	_
distance	_	_
(	_	_
EAD	_	_
)	_	_
to	_	_
do	_	_
pixel-wise	_	_
gaze	_	_
prediction	_	_
.	_	_

#55
We	_	_
demonstrate	_	_
that	_	_
despite	_	_
its	_	_
simplicity	_	_
,	_	_
this	_	_
architecture	_	_
can	_	_
compete	_	_
,	_	_
or	_	_
even	_	_
outperform	_	_
,	_	_
more	_	_
complex	_	_
state-of-the-art	_	_
architectures	_	_
.	_	_

#56
Model	_	_
Input	_	_
CNN	_	_
LSTM	_	_
CB	_	_
Loss	_	_
pixel/PD	_	_
DSCLRCN	_	_
[	_	_
19	_	_
]	_	_
multi	_	_
inputs	_	_
Resnet	_	_
[	_	_
21	_	_
]	_	_
,	_	_
Places	_	_
[	_	_
22	_	_
]	_	_
Yes	_	_
no	_	_
NSS	_	_
pixel	_	_
Deepfix	_	_
[	_	_
14	_	_
]	_	_
single	_	_
input	_	_
MA	_	_
(	_	_
VGG	_	_
,	_	_
Googlenet	_	_
,	_	_
Dilated	_	_
)	_	_
no	_	_
yes	_	_
L2	_	_
pixel	_	_
Salicon	_	_
[	_	_
8	_	_
]	_	_
MR	_	_
inputs	_	_
VGG	_	_
no	_	_
no	_	_
K-L	_	_
PD	_	_
SalGAN	_	_
[	_	_
12	_	_
]	_	_
single	_	_
input	_	_
VGG	_	_
,	_	_
GAN	_	_
no	_	_
no	_	_
BCE	_	_
pixel	_	_
PDP	_	_
[	_	_
9	_	_
]	_	_
single	_	_
input	_	_
VGG	_	_
no	_	_
no	_	_
Bha	_	_
PD	_	_
DVA	_	_
[	_	_
18	_	_
]	_	_
single	_	_
input	_	_
VGG	_	_
,	_	_
MD	_	_
no	_	_
no	_	_
BCE	_	_
pixel	_	_
Deepnet	_	_
[	_	_
13	_	_
]	_	_
single	_	_
input	_	_
Custom	_	_
8-layers	_	_
no	_	_
no	_	_
L2	_	_
pixel	_	_
Ours	_	_
single	_	_
input	_	_
VGG	_	_
no	_	_
no	_	_
EAD	_	_
pixel	_	_
Table	_	_
1	_	_
:	_	_
Comparison	_	_
of	_	_
saliency	_	_
prediction	_	_
models	_	_
.	_	_

#57
MA	_	_
:	_	_
multi-architecture	_	_
,	_	_
MR	_	_
:	_	_
multi-resolution	_	_
,	_	_
PD	_	_
:	_	_
probability	_	_
distribution	_	_
based	_	_
,	_	_
Bha	_	_
:	_	_
Bhattacharyya	_	_
distance	_	_
,	_	_
MD	_	_
:	_	_
multiple	_	_
decoders	_	_
,	_	_
CB	_	_
:	_	_
central	_	_
bias	_	_
,	_	_
K-L	_	_
:	_	_
Kullback-Leibler	_	_
divergence	_	_
.	_	_

#58
2.2	_	_
Visualizing	_	_
deep	_	_
neural	_	_
networks	_	_

#59
The	_	_
success	_	_
of	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
has	_	_
raised	_	_
the	_	_
question	_	_
of	_	_
what	_	_
representations	_	_
are	_	_
learned	_	_
by	_	_
neurons	_	_
located	_	_
in	_	_
deep	_	_
layers	_	_
.	_	_

#60
One	_	_
approach	_	_
towards	_	_
understand	_	_
how	_	_
CNNs	_	_
work	_	_
and	_	_
learn	_	_
is	_	_
to	_	_
visualize	_	_
individual	_	_
neurons’	_	_
activations	_	_
and	_	_
receptive	_	_
fields	_	_
.	_	_

#61
For	_	_
example	_	_
,	_	_
Zeiler	_	_
&	_	_
Fergus	_	_
[	_	_
10	_	_
]	_	_
proposed	_	_
a	_	_
deconvolution	_	_
network	_	_
in	_	_
order	_	_
to	_	_
visualize	_	_
the	_	_
original	_	_
patterns	_	_
that	_	_
activate	_	_
the	_	_
corresponding	_	_
activation	_	_
maps	_	_
.	_	_

#62
In	_	_
the	_	_
forward	_	_
pass	_	_
of	_	_
a	_	_
convolutional	_	_
neural	_	_
network	_	_
the	_	_
main	_	_
operations	_	_
are	_	_
convolution	_	_
,	_	_
ReLU	_	_
(	_	_
or	_	_
another	_	_
nonlinearity	_	_
)	_	_
and	_	_
pooling	_	_
.	_	_

#63
Conversely	_	_
,	_	_
a	_	_
deconvolution	_	_
network	_	_
is	_	_
consists	_	_
of	_	_
the	_	_
three	_	_
steps	_	_
of	_	_
unpooling	_	_
,	_	_
transposed	_	_
convolution	_	_
(	_	_
using	_	_
the	_	_
pre-trained	_	_
weights	_	_
in	_	_
the	_	_
forward	_	_

#64
4	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#65
pass	_	_
,	_	_
and	_	_
transposing	_	_
them	_	_
for	_	_
convolution	_	_
)	_	_
,	_	_
and	_	_
the	_	_
ReLU	_	_
operation	_	_
.	_	_

#66
Yosinski	_	_
et	_	_
al.	_	_
[	_	_
23	_	_
]	_	_
developed	_	_
two	_	_
tools	_	_
for	_	_
understanding	_	_
the	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
.	_	_

#67
The	_	_
first	_	_
of	_	_
these	_	_
tools	_	_
is	_	_
designed	_	_
to	_	_
visualize	_	_
the	_	_
activation	_	_
maps	_	_
at	_	_
different	_	_
layers	_	_
for	_	_
a	_	_
given	_	_
input	_	_
image	_	_
.	_	_

#68
The	_	_
second	_	_
tool	_	_
aims	_	_
to	_	_
estimate	_	_
the	_	_
input	_	_
pattern	_	_
which	_	_
a	_	_
network	_	_
is	_	_
maximally	_	_
attuned	_	_
to	_	_
for	_	_
a	_	_
given	_	_
object	_	_
class	_	_
.	_	_

#69
In	_	_
practice	_	_
,	_	_
the	_	_
last	_	_
layer	_	_
of	_	_
a	_	_
classification	_	_
deep	_	_
neural	_	_
network	_	_
typically	_	_
features	_	_
one	_	_
neuron	_	_
per	_	_
object	_	_
class	_	_
.	_	_

#70
Yosinski	_	_
et	_	_
al.	_	_
propose	_	_
to	_	_
use	_	_
gradient	_	_
ascent	_	_
(	_	_
with	_	_
regularization	_	_
)	_	_
to	_	_
find	_	_
the	_	_
input	_	_
image	_	_
that	_	_
maximizes	_	_
the	_	_
output	_	_
of	_	_
a	_	_
specific	_	_
neuron	_	_
(	_	_
i.e.	_	_
,	_	_
for	_	_
a	_	_
specific	_	_
object	_	_
class	_	_
)	_	_
.	_	_

#71
Hence	_	_
,	_	_
it	_	_
derives	_	_
the	_	_
optimum	_	_
input	_	_
that	_	_
appeals	_	_
to	_	_
the	_	_
network	_	_
for	_	_
a	_	_
specific	_	_
class	_	_
.	_	_

#72
Both	_	_
visualization	_	_
methods	_	_
discussed	_	_
above	_	_
are	_	_
essentially	_	_
qualitative	_	_
.	_	_

#73
In	_	_
contrast	_	_
,	_	_
Bau	_	_
et	_	_
al.	_	_
[	_	_
11	_	_
]	_	_
proposed	_	_
a	_	_
quantitative	_	_
method	_	_
to	_	_
give	_	_
each	_	_
activation	_	_
map	_	_
a	_	_
semantic	_	_
meaning	_	_
.	_	_

#74
In	_	_
their	_	_
work	_	_
,	_	_
they	_	_
proposed	_	_
a	_	_
dataset	_	_
with	_	_
6	_	_
image	_	_
categories	_	_
and	_	_
63,305	_	_
images	_	_
for	_	_
network	_	_
dissection	_	_
,	_	_
where	_	_
each	_	_
image	_	_
is	_	_
labeled	_	_
with	_	_
pixel-wise	_	_
semantic	_	_
meaning	_	_
.	_	_

#75
At	_	_
first	_	_
,	_	_
they	_	_
forward	_	_
all	_	_
images	_	_
in	_	_
the	_	_
dataset	_	_
into	_	_
a	_	_
pre-trained	_	_
deep	_	_
model	_	_
.	_	_

#76
For	_	_
each	_	_
activation	_	_
map	_	_
inside	_	_
the	_	_
model	_	_
,	_	_
different	_	_
inputs	_	_
have	_	_
different	_	_
patterns	_	_
.	_	_

#77
Then	_	_
,	_	_
they	_	_
compute	_	_
the	_	_
distribution	_	_
of	_	_
each	_	_
unit	_	_
activation	_	_
map	_	_
over	_	_
the	_	_
whole	_	_
dataset	_	_
,	_	_
and	_	_
determine	_	_
a	_	_
threshold	_	_
for	_	_
each	_	_
unit	_	_
based	_	_
on	_	_
its	_	_
activation	_	_
distribution	_	_
.	_	_

#78
With	_	_
the	_	_
threshold	_	_
for	_	_
each	_	_
unit	_	_
,	_	_
the	_	_
activation	_	_
map	_	_
for	_	_
each	_	_
input	_	_
image	_	_
is	_	_
quantized	_	_
into	_	_
a	_	_
binary	_	_
map	_	_
.	_	_

#79
Finally	_	_
,	_	_
they	_	_
compute	_	_
the	_	_
intersection	_	_
over	_	_
union	_	_
(	_	_
IOU	_	_
)	_	_
between	_	_
the	_	_
quantized	_	_
activation	_	_
map	_	_
and	_	_
the	_	_
labeled	_	_
ground	_	_
truth	_	_
to	_	_
determine	_	_
what	_	_
objects	_	_
or	_	_
object	_	_
parts	_	_
that	_	_
unit	_	_
is	_	_
detecting	_	_
.	_	_

#80
Although	_	_
these	_	_
approaches	_	_
provide	_	_
useful	_	_
insight	_	_
into	_	_
the	_	_
workings	_	_
of	_	_
deep	_	_
neural	_	_
networks	_	_
,	_	_
they	_	_
are	_	_
ill-suited	_	_
for	_	_
understanding	_	_
deep	_	_
saliency	_	_
networks	_	_
:	_	_
If	_	_
it	_	_
is	_	_
reasonable	_	_
to	_	_
expect	_	_
that	_	_
neurons	_	_
in	_	_
a	_	_
dog/cat	_	_
classifier	_	_
will	_	_
encode	_	_
patterns	_	_
characteristic	_	_
of	_	_
dogs	_	_
and	_	_
cats	_	_
,	_	_
a	_	_
saliency	_	_
model	_	_
is	_	_
expected	_	_
to	_	_
encode	_	_
both	_	_
salient	_	_
patterns	_	_
but	_	_
also	_	_
non-salient	_	_
ones	_	_
.	_	_

#81
For	_	_
this	_	_
reason	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
use	_	_
the	_	_
normalized	_	_
scan-path	_	_
saliency	_	_
(	_	_
NSS	_	_
)	_	_
[	_	_
24	_	_
]	_	_
score	_	_
to	_	_
determine	_	_
whether	_	_
individual	_	_
neurons	_	_
act	_	_
as	_	_
negative	_	_
or	_	_
positive	_	_
predictors	_	_
of	_	_
gaze	_	_
in	_	_
the	_	_
network	_	_
.	_	_

#82
Moreover	_	_
,	_	_
in	_	_
order	_	_
to	_	_
interpret	_	_
what	_	_
has	_	_
been	_	_
learnt	_	_
as	_	_
salient	_	_
by	_	_
the	_	_
model	_	_
,	_	_
we	_	_
use	_	_
the	_	_
network	_	_
dissection	_	_
approach	_	_
of	_	_
Bau	_	_
et	_	_
al.	_	_
[	_	_
11	_	_
]	_	_
to	_	_
highlight	_	_
what	_	_
objects	_	_
or	_	_
object	_	_
parts	_	_
neurons	_	_
in	_	_
our	_	_
saliency	_	_
models	_	_
are	_	_
implicitly	_	_
attuned	_	_
to	_	_
.	_	_

#83
3	_	_
Methodology	_	_

#84
This	_	_
section	_	_
will	_	_
first	_	_
introduce	_	_
the	_	_
proposed	_	_
simplified	_	_
architecture	_	_
for	_	_
saliency	_	_
estimation	_	_
.	_	_

#85
In	_	_
a	_	_
second	_	_
part	_	_
,	_	_
we	_	_
then	_	_
describe	_	_
how	_	_
to	_	_
visualize	_	_
and	_	_
analyze	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#86
3.1	_	_
Gaze	_	_
prediction	_	_

#87
The	_	_
whole	_	_
architecture	_	_
of	_	_
our	_	_
network	_	_
is	_	_
illustrated	_	_
in	_	_
Figure	_	_
1	_	_
.	_	_

#88
The	_	_
input	_	_
is	_	_
first	_	_
processed	_	_
by	_	_
encoder	_	_
network	_	_
,	_	_
and	_	_
represented	_	_
by	_	_
a	_	_
feature	_	_
tensor	_	_
(	_	_
F	_	_
0	_	_
)	_	_
of	_	_
Title	_	_
Suppressed	_	_
Due	_	_
to	_	_
Excessive	_	_
Length	_	_
5	_	_
Input	_	_
Ground	_	_
Truth	_	_
Loss	_	_
Fig.	_	_
1	_	_
:	_	_
The	_	_
encoder	_	_
and	_	_
residual-decoder	_	_
architecture	_	_
of	_	_
our	_	_
network	_	_
.	_	_

#89
shape	_	_
M	_	_
×N	_	_
×K	_	_
.	_	_

#90
F	_	_
0	_	_
=	_	_
CNN	_	_
(	_	_
Image	_	_
)	_	_
=	_	_
[	_	_
X1	_	_
,	_	_
X2	_	_
,	_	_
·	_	_
·	_	_
·	_	_
,	_	_
XM×N	_	_
]	_	_
,	_	_
(	_	_
1	_	_
)	_	_
where	_	_
M	_	_
×	_	_
N	_	_
is	_	_
the	_	_
number	_	_
of	_	_
locations	_	_
in	_	_
the	_	_
feature	_	_
tensor	_	_
,	_	_
and	_	_
K	_	_
is	_	_
the	_	_
dimension	_	_
of	_	_
each	_	_
location	_	_
.	_	_

#91
In	_	_
our	_	_
model	_	_
,	_	_
we	_	_
use	_	_
the	_	_
first	_	_
5	_	_
convolutional	_	_
blocks	_	_
(	_	_
we	_	_
removed	_	_
the	_	_
last	_	_
pooling	_	_
layer	_	_
,	_	_
and	_	_
kept	_	_
the	_	_
first	_	_
4	_	_
pooling	_	_
layers	_	_
in	_	_
the	_	_
encoder	_	_
)	_	_
of	_	_
a	_	_
pre-trained	_	_
VGG16	_	_
[	_	_
15	_	_
]	_	_
network	_	_
to	_	_
initialize	_	_
the	_	_
feature	_	_
extraction	_	_
part	_	_
and	_	_
fine-tune	_	_
it	_	_
during	_	_
training	_	_
.	_	_

#92
The	_	_
input	_	_
was	_	_
resized	_	_
to	_	_
240×	_	_
320	_	_
,	_	_
hence	_	_
the	_	_
shape	_	_
of	_	_
the	_	_
feature	_	_
tensor	_	_
is	_	_
15×	_	_
20×	_	_
512	_	_
.	_	_

#93
After	_	_
feature	_	_
extraction	_	_
,	_	_
the	_	_
feature	_	_
tensor	_	_
was	_	_
then	_	_
fed	_	_
into	_	_
the	_	_
residualdecoder	_	_
.	_	_

#94
The	_	_
decoder	_	_
is	_	_
consists	_	_
of	_	_
four	_	_
blocks	_	_
,	_	_
where	_	_
each	_	_
block	_	_
upsamples	_	_
the	_	_
feature	_	_
tensor	_	_
once	_	_
to	_	_
recover	_	_
the	_	_
resolution	_	_
lost	_	_
in	_	_
the	_	_
encoding	_	_
stage	_	_
.	_	_

#95
Each	_	_
block	_	_
shares	_	_
three	_	_
similar	_	_
processes	_	_
:	_	_
convolution	_	_
for	_	_
dimension	_	_
reduction	_	_
,	_	_
normal	_	_
convolution	_	_
,	_	_
and	_	_
deconvolution	_	_
to	_	_
recover	_	_
the	_	_
resolution	_	_
lost	_	_
in	_	_
the	_	_
encoder	_	_
due	_	_
to	_	_
pooling	_	_
.	_	_

#96
In	_	_
each	_	_
block	_	_
,	_	_
the	_	_
feature	_	_
tensor	_	_
from	_	_
the	_	_
previous	_	_
block	_	_
Fn−1	_	_
is	_	_
first	_	_
processed	_	_
by	_	_
a	_	_
dimension	_	_
reduction	_	_
convolutional	_	_
layer	_	_
Cn	_	_
1	_	_
to	_	_
reduce	_	_
the	_	_
number	_	_
of	_	_
feature	_	_
maps	_	_
K.	_	_
In	_	_
our	_	_
model	_	_
,	_	_
we	_	_
halve	_	_
the	_	_
number	_	_
of	_	_
feature	_	_
maps	_	_
K	_	_
in	_	_
each	_	_
block	_	_
of	_	_
the	_	_
decoder	_	_
.	_	_

#97
Fn	_	_
1	_	_
=	_	_
Cn	_	_
1	_	_
(	_	_
Fn−1	_	_
)	_	_
(	_	_
2	_	_
)	_	_
Then	_	_
,	_	_
the	_	_
processed	_	_
feature	_	_
tensor	_	_
(	_	_
Fn	_	_
1	_	_
)	_	_
is	_	_
processed	_	_
by	_	_
a	_	_
conventional	_	_
convolutional	_	_
layer	_	_
Cn	_	_
2	_	_
for	_	_
further	_	_
processing	_	_
.	_	_

#98
Fn	_	_
2	_	_
=	_	_
Cn	_	_
2	_	_
(	_	_
Fn	_	_
1	_	_
)	_	_
(	_	_
3	_	_
)	_	_
Finally	_	_
,	_	_
the	_	_
two	_	_
processed	_	_
tensors	_	_
(	_	_
Fn	_	_
1	_	_
and	_	_
Fn	_	_
2	_	_
)	_	_
are	_	_
added	_	_
together	_	_
and	_	_
then	_	_
sent	_	_
to	_	_
a	_	_
deconvolutional	_	_
layer	_	_
Dn	_	_
to	_	_
increase	_	_
the	_	_
tensor	_	_
resolution	_	_
and	_	_
generate	_	_
the	_	_
block’s	_	_
output	_	_
tensor	_	_
Fn	_	_
.	_	_

#99
Fn	_	_
=	_	_
Dn	_	_
(	_	_
Fn	_	_
1	_	_
+	_	_
Fn	_	_
2	_	_
)	_	_
(	_	_
4	_	_
)	_	_

#100
6	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#101
The	_	_
kernel	_	_
size	_	_
was	_	_
set	_	_
to	_	_
3×	_	_
3	_	_
for	_	_
convolutional	_	_
layers	_	_
and	_	_
2×	_	_
2	_	_
for	_	_
deconvolutional	_	_
layers	_	_
.	_	_

#102
Zero-padding	_	_
was	_	_
used	_	_
to	_	_
preserve	_	_
the	_	_
input’s	_	_
scale	_	_
.	_	_

#103
The	_	_
last	_	_
layer	_	_
of	_	_
the	_	_
decoder	_	_
is	_	_
a	_	_
3	_	_
×	_	_
3	_	_
convolutional	_	_
layer	_	_
,	_	_
which	_	_
transforms	_	_
the	_	_
64	_	_
(	_	_
output	_	_
of	_	_
last	_	_
deconvolutional	_	_
layer	_	_
)	_	_
activation	_	_
maps	_	_
into	_	_
the	_	_
saliency	_	_
map	_	_
.	_	_

#104
No	_	_
further	_	_
processing	_	_
was	_	_
implemented	_	_
.	_	_

#105
To	_	_
train	_	_
our	_	_
model	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
new	_	_
pixel-wise	_	_
loss	_	_
function	_	_
,	_	_
the	_	_
exponential	_	_
absolute	_	_
distance	_	_
(	_	_
EAD	_	_
)	_	_
,	_	_
formulated	_	_
as	_	_
follows	_	_
:	_	_
EAD	_	_
=	_	_
N∑	_	_
i=1	_	_
(	_	_
exp	_	_
|pi	_	_
−	_	_
gi|	_	_
−	_	_
1	_	_
)	_	_
(	_	_
5	_	_
)	_	_
where	_	_
,	_	_
N	_	_
is	_	_
the	_	_
number	_	_
of	_	_
pixels	_	_
in	_	_
the	_	_
gaze	_	_
map	_	_
,	_	_
pi	_	_
and	_	_
gi	_	_
is	_	_
the	_	_
prediction	_	_
and	_	_
ground	_	_
truth	_	_
at	_	_
the	_	_
ith	_	_
pixel	_	_
.	_	_

#106
Compared	_	_
to	_	_
the	_	_
L2	_	_
distance	_	_
,	_	_
the	_	_
EAD	_	_
has	_	_
a	_	_
better	_	_
(	_	_
a	_	_
)	_	_
3	_	_
loss	_	_
functions	_	_
(	_	_
b	_	_
)	_	_
EAD	_	_
loss	_	_
map	_	_
(	_	_
c	_	_
)	_	_
BCE	_	_
loss	_	_
map	_	_
Fig.	_	_
2	_	_
:	_	_
Properties	_	_
of	_	_
different	_	_
loss	_	_
functions	_	_
.	_	_

#107
gradient	_	_
when	_	_
the	_	_
absolute	_	_
difference	_	_
is	_	_
small	_	_
.	_	_

#108
Compared	_	_
to	_	_
the	_	_
L1	_	_
distance	_	_
,	_	_
which	_	_
is	_	_
linear	_	_
in	_	_
the	_	_
absolute	_	_
difference	_	_
,	_	_
the	_	_
EAD	_	_
gives	_	_
a	_	_
larger	_	_
punishment	_	_
when	_	_
the	_	_
difference	_	_
is	_	_
large	_	_
.	_	_

#109
In	_	_
contrast	_	_
to	_	_
EAD	_	_
,	_	_
the	_	_
BCE	_	_
loss	_	_
proposed	_	_
in	_	_
[	_	_
12	_	_
]	_	_
yields	_	_
a	_	_
non-zero	_	_
loss	_	_
even	_	_
for	_	_
perfect	_	_
predictions	_	_
(	_	_
as	_	_
illustrated	_	_
in	_	_
Figure	_	_
2	_	_
)	_	_
.	_	_

#110
The	_	_
unbounded	_	_
nature	_	_
of	_	_
the	_	_
BCE	_	_
requires	_	_
the	_	_
application	_	_
of	_	_
an	_	_
additional	_	_
sigmoid	_	_
function	_	_
to	_	_
produce	_	_
pixel-wise	_	_
saliency	_	_
values	_	_
in	_	_
the	_	_
range	_	_
[	_	_
0,1	_	_
]	_	_
.	_	_

#111
The	_	_
model	_	_
is	_	_
trained	_	_
using	_	_
Tensorflow	_	_
[	_	_
25	_	_
]	_	_
with	_	_
the	_	_
Adam	_	_
[	_	_
26	_	_
]	_	_
optimizer	_	_
.	_	_

#112
We	_	_
set	_	_
the	_	_
initial	_	_
leaning	_	_
rate	_	_
as	_	_
5	_	_
×	_	_
10−5	_	_
,	_	_
and	_	_
decay	_	_
it	_	_
with	_	_
a	_	_
factor	_	_
of	_	_
0.1	_	_
after	_	_
each	_	_
training	_	_
epoch	_	_
.	_	_

#113
3.2	_	_
Model	_	_
visualization	_	_

#114
As	_	_
discussed	_	_
before	_	_
,	_	_
one	_	_
important	_	_
question	_	_
is	_	_
what	_	_
is	_	_
learnt	_	_
by	_	_
a	_	_
deep	_	_
saliency	_	_
model	_	_
that	_	_
allows	_	_
it	_	_
to	_	_
outperform	_	_
hand-crafted	_	_
shallow	_	_
models	_	_
based	_	_
on	_	_
psychological	_	_
theories	_	_
?	_	_

#115
In	_	_
other	_	_
words	_	_
,	_	_
what	_	_
specific	_	_
salient	_	_
patterns	_	_
are	_	_
learnt	_	_
by	_	_
the	_	_
model	_	_
?	_	_

#116
One	_	_
hypothesis	_	_
is	_	_
that	_	_
such	_	_
deep	_	_
network	_	_
encode	_	_
semantic	_	_
information	_	_
about	_	_
saliency	_	_
going	_	_
beyond	_	_
classical	_	_
centre-surround	_	_
assumptions	_	_
.	_	_

#117
Here	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
use	_	_
the	_	_
actual	_	_
saliency	_	_
evaluation	_	_
metric	_	_
,	_	_
the	_	_
normalized	_	_
scan-path	_	_
saliency	_	_
(	_	_
NSS	_	_
)	_	_
score	_	_
,	_	_
to	_	_
visualize	_	_
and	_	_
understand	_	_
inner	_	_
representations	_	_
Title	_	_
Suppressed	_	_
Due	_	_
to	_	_
Excessive	_	_
Length	_	_
7	_	_
Model	_	_
Feature	_	_
Extraction	_	_
Activation	_	_
map	_	_
i	_	_
Activation	_	_
map	_	_
j	_	_
Activation	_	_
map	_	_
k	_	_
..	_	_
..	_	_
..	_	_
..	_	_
Fixation	_	_
Maps	_	_
Inputs	_	_
High	_	_
Mean	_	_
NSS	_	_
Medium	_	_
Mean	_	_
NSS	_	_
Low	_	_
Mean	_	_
NSS	_	_
Network	_	_
Dissection	_	_
Activated	_	_
Patterns	_	_
Activation	_	_
Maps	_	_
Fig.	_	_
3	_	_
:	_	_
The	_	_
visualization	_	_
method	_	_
to	_	_
compute	_	_
the	_	_
NSS	_	_
score	_	_
for	_	_
each	_	_
unit	_	_
activation	_	_
map	_	_
.	_	_

#118
of	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#119
At	_	_
first	_	_
,	_	_
we	_	_
feed	_	_
all	_	_
images	_	_
in	_	_
the	_	_
dataset	_	_
with	_	_
fixations	_	_
(	_	_
MIT1003	_	_
dataset	_	_
[	_	_
27	_	_
]	_	_
)	_	_
to	_	_
the	_	_
pre-trained	_	_
deep	_	_
saliency	_	_
model	_	_
.	_	_

#120
For	_	_
each	_	_
single	_	_
image	_	_
,	_	_
it	_	_
produces	_	_
a	_	_
set	_	_
of	_	_
activation	_	_
maps	_	_
as	_	_
the	_	_
output	_	_
of	_	_
the	_	_
feature	_	_
extraction	_	_
part	_	_
(	_	_
in	_	_
our	_	_
model	_	_
,	_	_
this	_	_
is	_	_
the	_	_
output	_	_
of	_	_
the	_	_
encoder	_	_
)	_	_
,	_	_
one	_	_
per	_	_
neuron	_	_
.	_	_

#121
Each	_	_
activation	_	_
map	_	_
has	_	_
a	_	_
unique	_	_
pattern	_	_
for	_	_
a	_	_
given	_	_
input	_	_
image	_	_
.	_	_

#122
We	_	_
rescale	_	_
the	_	_
activation	_	_
map	_	_
to	_	_
the	_	_
input’s	_	_
scale	_	_
and	_	_
use	_	_
the	_	_
activation	_	_
to	_	_
compute	_	_
the	_	_
NSS	_	_
score	_	_
for	_	_
each	_	_
neuron	_	_
over	_	_
the	_	_
whole	_	_
dataset	_	_
.	_	_

#123
We	_	_
use	_	_
the	_	_
top	_	_
5	_	_
NSS	_	_
scores	_	_
for	_	_
each	_	_
unit	_	_
activation	_	_
map	_	_
,	_	_
and	_	_
compute	_	_
their	_	_
mean	_	_
as	_	_
the	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
each	_	_
unit	_	_
activation	_	_
map	_	_
(	_	_
As	_	_
a	_	_
convolutional	_	_
feature	_	_
channel	_	_
can	_	_
only	_	_
correspond	_	_
to	_	_
a	_	_
certain	_	_
type	_	_
visual	_	_
pattern	_	_
[	_	_
28,29	_	_
]	_	_
)	_	_
.	_	_

#124
Therefore	_	_
,	_	_
each	_	_
neuron’s	_	_
activation	_	_
map	_	_
has	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
,	_	_
which	_	_
indicates	_	_
its	_	_
correlation	_	_
with	_	_
human	_	_
gaze	_	_
locations	_	_
.	_	_

#125
Using	_	_
the	_	_
mean	_	_
NSS	_	_
score	_	_
for	_	_
each	_	_
neuron’s	_	_
activation	_	_
map	_	_
,	_	_
we	_	_
normalize	_	_
the	_	_
mean	_	_
NSS	_	_
score	_	_
across	_	_
all	_	_
activation	_	_
maps	_	_
between	_	_
[	_	_
0	_	_
,	_	_
1	_	_
]	_	_
,	_	_
and	_	_
set	_	_
a	_	_
threshold	_	_
T	_	_
(	_	_
we	_	_
choose	_	_
T	_	_
=	_	_
0.9	_	_
in	_	_
our	_	_
experiment	_	_
)	_	_
.	_	_

#126
Neurons	_	_
with	_	_
mean	_	_
NSS	_	_
score	_	_
above	_	_
threshold	_	_
T	_	_
are	_	_
identified	_	_
as	_	_
positive	_	_
fixation	_	_
detectors	_	_
.	_	_

#127
After	_	_
selecting	_	_
positive	_	_
fixation	_	_
detectors	_	_
,	_	_
we	_	_
use	_	_
network	_	_
dissection	_	_
[	_	_
11	_	_
]	_	_
(	_	_
using	_	_
the	_	_
same	_	_
method	_	_
and	_	_
dataset	_	_
as	_	_
the	_	_
authors	_	_
)	_	_
to	_	_
reveal	_	_
what	_	_
kind	_	_
of	_	_
object	_	_
or	_	_
object	_	_
part	_	_
are	_	_
those	_	_
positive	_	_
fixation	_	_
detectors	_	_
attuned	_	_
to	_	_
.	_	_

#128
We	_	_
proceed	_	_
as	_	_
follows	_	_
:	_	_
For	_	_
every	_	_
image	_	_
in	_	_
the	_	_
Broden	_	_
dataset	_	_
[	_	_
11	_	_
]	_	_
,	_	_
there	_	_
is	_	_
a	_	_
unique	_	_
pattern	_	_
for	_	_
each	_	_
unit	_	_
activation	_	_
map	_	_
.	_	_

#129
For	_	_
each	_	_
neuron’s	_	_
activation	_	_
map	_	_
,	_	_
we	_	_
compute	_	_
the	_	_
distribution	_	_
of	_	_
its	_	_
values	_	_
over	_	_
the	_	_
whole	_	_
dataset	_	_
and	_	_
find	_	_
the	_	_
threshold	_	_
Tk	_	_
such	_	_
that	_	_
the	_	_
value	_	_
larger	_	_
than	_	_
Tk	_	_
with	_	_
a	_	_
probability	_	_
>	_	_
0.005	_	_
.	_	_

#130
Then	_	_
,	_	_
all	_	_
activation	_	_
maps	_	_
for	_	_
all	_	_
images	_	_
are	_	_
scaled	_	_
to	_	_
the	_	_
input	_	_
size	_	_
and	_	_
are	_	_
quantized	_	_
to	_	_
a	_	_
binary	_	_
map	_	_
.	_	_

#131
Finally	_	_
,	_	_
the	_	_
IOU	_	_
[	_	_
11	_	_
]	_	_
is	_	_
computed	_	_
for	_	_
each	_	_
activation	_	_
map	_	_
to	_	_
determine	_	_
what	_	_
sort	_	_
of	_	_
objects	_	_
or	_	_
object	_	_
parts	_	_
they	_	_
are	_	_
attuned	_	_
to	_	_
detecting	_	_
(	_	_
more	_	_
details	_	_
are	_	_
in	_	_
the	_	_
[	_	_
11	_	_
]	_	_
)	_	_
.	_	_

#132
In	_	_
our	_	_
work	_	_
,	_	_
we	_	_
only	_	_
show	_	_
the	_	_
objects	_	_
or	_	_
object	_	_
parts	_	_
for	_	_
the	_	_
positive	_	_
fixation	_	_
detectors	_	_
.	_	_

#133
8	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#134
4	_	_
Saliency	_	_
prediction	_	_
performance	_	_

#135
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
first	_	_
introduce	_	_
the	_	_
datasets	_	_
used	_	_
in	_	_
the	_	_
experiments	_	_
and	_	_
then	_	_
show	_	_
the	_	_
performance	_	_
of	_	_
different	_	_
pixel-wise	_	_
loss	_	_
functions	_	_
as	_	_
well	_	_
as	_	_
the	_	_
comparison	_	_
between	_	_
our	_	_
model	_	_
and	_	_
other	_	_
state-of-the-art	_	_
models	_	_
.	_	_

#136
4.1	_	_
Datasets	_	_

#137
SALICON	_	_
[	_	_
6	_	_
]	_	_
:	_	_
The	_	_
SALICON	_	_
dataset	_	_
is	_	_
the	_	_
largest	_	_
dataset	_	_
in	_	_
the	_	_
field	_	_
of	_	_
visual	_	_
saliency	_	_
.	_	_

#138
Saliency	_	_
maps	_	_
are	_	_
estimated	_	_
from	_	_
human	_	_
observers’	_	_
mouse	_	_
clicks	_	_
gathered	_	_
over	_	_
20,000	_	_
images	_	_
,	_	_
with	_	_
10,000	_	_
images	_	_
in	_	_
the	_	_
training	_	_
set	_	_
,	_	_
5,000	_	_
images	_	_
in	_	_
the	_	_
validation	_	_
dataset	_	_
,	_	_
and	_	_
another	_	_
5,000	_	_
images	_	_
in	_	_
the	_	_
testing	_	_
dataset	_	_
.	_	_

#139
We	_	_
use	_	_
the	_	_
SALICON	_	_
training	_	_
and	_	_
validation	_	_
datasets	_	_
to	_	_
train	_	_
and	_	_
validate	_	_
our	_	_
model	_	_
.	_	_

#140
MIT1003	_	_
[	_	_
27	_	_
]	_	_
:	_	_
This	_	_
dataset	_	_
includes	_	_
gaze	_	_
data	_	_
of	_	_
15	_	_
subjects	_	_
using	_	_
an	_	_
eye	_	_
tracker	_	_
over	_	_
1,003	_	_
images	_	_
.	_	_

#141
It	_	_
is	_	_
used	_	_
in	_	_
the	_	_
visualization	_	_
part	_	_
.	_	_

#142
To	_	_
compare	_	_
our	_	_
model	_	_
with	_	_
other	_	_
state-of-the-art	_	_
models	_	_
on	_	_
MIT300	_	_
benchmark	_	_
,	_	_
we	_	_
also	_	_
randomly	_	_
choose	_	_
900	_	_
images	_	_
from	_	_
this	_	_
dataset	_	_
to	_	_
fine-tune	_	_
our	_	_
model	_	_
and	_	_
another	_	_
103	_	_
images	_	_
for	_	_
testing	_	_
the	_	_
performance	_	_
of	_	_
different	_	_
loss	_	_
functions	_	_
.	_	_

#143
MIT300	_	_
[	_	_
30	_	_
]	_	_
:	_	_
This	_	_
dataset	_	_
is	_	_
the	_	_
standard	_	_
benchmark	_	_
dataset	_	_
for	_	_
human	_	_
gaze	_	_
prediction	_	_
.	_	_

#144
It	_	_
includes	_	_
the	_	_
gaze	_	_
data	_	_
of	_	_
39	_	_
subjects	_	_
over	_	_
300	_	_
images	_	_
.	_	_

#145
Broden	_	_
[	_	_
11	_	_
]	_	_
:	_	_
This	_	_
dataset	_	_
contains	_	_
63,305	_	_
images	_	_
.	_	_

#146
With	_	_
four	_	_
subsets	_	_
,	_	_
ADE20K	_	_
(	_	_
22,211	_	_
images	_	_
)	_	_
,	_	_
Opensurfaces	_	_
(	_	_
25,351	_	_
images	_	_
)	_	_
,	_	_
DTD	_	_
(	_	_
5,639	_	_
images	_	_
)	_	_
,	_	_
and	_	_
PASCAL	_	_
(	_	_
10,104	_	_
images	_	_
)	_	_
,	_	_
with	_	_
pixel-wise	_	_
semantic	_	_
labels	_	_
.	_	_

#147
It	_	_
is	_	_
used	_	_
for	_	_
network	_	_
dissection	_	_
.	_	_

#148
4.2	_	_
Model	_	_
performance	_	_

#149
Table	_	_
2	_	_
records	_	_
the	_	_
accuracy	_	_
of	_	_
predicted	_	_
saliency	_	_
maps	_	_
according	_	_
to	_	_
a	_	_
range	_	_
of	_	_
standard	_	_
error	_	_
measures	_	_
:	_	_
Normalised	_	_
Scan-path	_	_
(	_	_
NSS	_	_
)	_	_
,	_	_
Cross	_	_
Correlation	_	_
(	_	_
CC	_	_
)	_	_
,	_	_
Area	_	_
Under	_	_
ROC	_	_
curve	_	_
(	_	_
AUC	_	_
)	_	_
and	_	_
Similarity	_	_
(	_	_
Sim	_	_
)	_	_
(	_	_
we	_	_
refer	_	_
to	_	_
[	_	_
24	_	_
]	_	_
for	_	_
a	_	_
discussion	_	_
of	_	_
saliency	_	_
metrics	_	_
)	_	_
.	_	_

#150
The	_	_
accuracy	_	_
is	_	_
recorded	_	_
for	_	_
different	_	_
pixel-wise	_	_
loss	_	_
functions	_	_
.	_	_

#151
We	_	_
can	_	_
see	_	_
that	_	_
our	_	_
proposed	_	_
EAD	_	_
loss	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
among	_	_
all	_	_
pixel-wise	_	_
loss	_	_
functions	_	_
.	_	_

#152
Furthermore	_	_
,	_	_
the	_	_
L2	_	_
loss	_	_
function	_	_
,	_	_
which	_	_
is	_	_
used	_	_
as	_	_
a	_	_
baseline	_	_
loss	_	_
function	_	_
in	_	_
many	_	_
deep	_	_
gaze	_	_
prediction	_	_
models	_	_
[	_	_
9,12	_	_
]	_	_
,	_	_
also	_	_
shows	_	_
good	_	_
performance	_	_
,	_	_
which	_	_
demonstrates	_	_
that	_	_
the	_	_
proposed	_	_
architecture	_	_
is	_	_
competitive	_	_
regardless	_	_
of	_	_
the	_	_
loss	_	_
function	_	_
and	_	_
despite	_	_
its	_	_
comparative	_	_
simplicity	_	_
.	_	_

#153
This	_	_
experiment	_	_
was	_	_
performed	_	_
on	_	_
the	_	_
MIT1003	_	_
dataset	_	_
.	_	_

#154
Tables	_	_
3	_	_
and	_	_
4	_	_
compare	_	_
our	_	_
model’s	_	_
performance	_	_
with	_	_
state-of-the-art	_	_
models	_	_
.	_	_

#155
One	_	_
can	_	_
see	_	_
that	_	_
our	_	_
model	_	_
performs	_	_
on	_	_
par	_	_
or	_	_
better	_	_
than	_	_
all	_	_
single	_	_
architecture	_	_
models	_	_
(	_	_
Table	_	_
1	_	_
)	_	_
,	_	_
especially	_	_
when	_	_
considering	_	_
the	_	_
NSS	_	_
score	_	_
,	_	_
which	_	_
is	_	_
the	_	_
metric	_	_
of	_	_
choice	_	_
for	_	_
ranking	_	_
saliency	_	_
models	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#156
In	_	_
Table	_	_
4	_	_
,	_	_
we	_	_
see	_	_
that	_	_
our	_	_
model’s	_	_
performance	_	_
come	_	_
close	_	_
to	_	_
considerably	_	_
more	_	_
complex	_	_
,	_	_
multi-architecture	_	_
approaches	_	_
such	_	_
as	_	_
Deepfix	_	_
[	_	_
14	_	_
]	_	_
and	_	_
DSCLRCN	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#157
Figure	_	_
4	_	_
is	_	_
a	_	_
qualitative	_	_
comparison	_	_
of	_	_
the	_	_
saliency	_	_
predicted	_	_
by	_	_
different	_	_
models	_	_
on	_	_
some	_	_
example	_	_
images	_	_
.	_	_

#158
Title	_	_
Suppressed	_	_
Due	_	_
to	_	_
Excessive	_	_
Length	_	_
9	_	_
Loss	_	_
function	_	_
NSS	_	_
CC	_	_
AUC	_	_
Sim	_	_
L1	_	_
2.388	_	_
0.684	_	_
0.855	_	_
0.556	_	_
L2	_	_
2.389	_	_
0.686	_	_
0.881	_	_
0.532	_	_
BCE	_	_
2.083	_	_
0.614	_	_
0.851	_	_
0.488	_	_
EAD	_	_
(	_	_
proposed	_	_
)	_	_
2.404	_	_
0.701	_	_
0.869	_	_
0.570	_	_
Table	_	_
2	_	_
:	_	_
The	_	_
performance	_	_
of	_	_
different	_	_
loss	_	_
functions	_	_
on	_	_
MIT1003	_	_
testing	_	_
dataset	_	_
.	_	_

#159
Model	_	_
NSS	_	_
CC	_	_
AUC	_	_
Sim	_	_
Salicon*	_	_
[	_	_
8	_	_
]	_	_
1.557	_	_
0.659	_	_
0.808	_	_
0.600	_	_
SalGAN	_	_
[	_	_
12	_	_
]	_	_
1.816	_	_
0.844	_	_
0.857	_	_
0.728	_	_
Deepnet	_	_
[	_	_
13	_	_
]	_	_
1.555	_	_
0.763	_	_
0.840	_	_
0.639	_	_
proposed	_	_
1.896	_	_
0.871	_	_
0.852	_	_
0.760	_	_
Table	_	_
3	_	_
:	_	_
Comparison	_	_
of	_	_
different	_	_
models	_	_
on	_	_
LSUN	_	_
2017	_	_
saliency	_	_
prediction	_	_
challenge	_	_
[	_	_
31	_	_
]	_	_
(	_	_
SALICON	_	_
testing	_	_
dataset	_	_
)	_	_
.	_	_

#160
*As	_	_
the	_	_
code	_	_
for	_	_
Salicon	_	_
is	_	_
not	_	_
available	_	_
,	_	_
we	_	_
use	_	_
the	_	_
open	_	_
source	_	_
implementation	_	_
[	_	_
32	_	_
]	_	_
.	_	_

#161
5	_	_
Visualizing	_	_
salient	_	_
patterns	_	_

#162
This	_	_
section	_	_
analyses	_	_
what	_	_
is	_	_
learnt	_	_
by	_	_
state-of-the-art	_	_
deep	_	_
saliency	_	_
models	_	_
using	_	_
the	_	_
visualization	_	_
tools	_	_
discussed	_	_
in	_	_
section	_	_
3.2	_	_
.	_	_

#163
The	_	_
model	_	_
proposed	_	_
in	_	_
section	_	_
3.1	_	_
is	_	_
trained	_	_
on	_	_
the	_	_
SALICON	_	_
training	_	_
dataset	_	_
as	_	_
before	_	_
,	_	_
but	_	_
without	_	_
fine-tuning	_	_
it	_	_
on	_	_
MIT1003	_	_
dataset	_	_
to	_	_
avoid	_	_
overfitting	_	_
when	_	_
computing	_	_
the	_	_
NSS	_	_
score	_	_
for	_	_
each	_	_
activation	_	_
map	_	_
.	_	_

#164
In	_	_
addition	_	_
to	_	_
the	_	_
proposed	_	_
model	_	_
,	_	_
we	_	_
apply	_	_
a	_	_
similar	_	_
analysis	_	_
on	_	_
three	_	_
deep	_	_
saliency	_	_
models	_	_
for	_	_
which	_	_
the	_	_
code	_	_
is	_	_
publicly	_	_
available	_	_
:	_	_
Deepnet	_	_
,	_	_
SalGAN	_	_
and	_	_
OpenSalicon	_	_
.	_	_

#165
In	_	_
Deepnet	_	_
[	_	_
13	_	_
]	_	_
,	_	_
the	_	_
first	_	_
five	_	_
convolutional	_	_
layers	_	_
were	_	_
determined	_	_
as	_	_
the	_	_
feature	_	_
extraction	_	_
part	_	_
.	_	_

#166
In	_	_
SalGAN	_	_
[	_	_
12	_	_
]	_	_
,	_	_
the	_	_
encoder	_	_
was	_	_
treated	_	_
as	_	_
the	_	_
feature	_	_
extraction	_	_
part	_	_
.	_	_

#167
In	_	_
OpenSalicon	_	_
[	_	_
32	_	_
]	_	_
,	_	_
both	_	_
coarse	_	_
scale	_	_
(	_	_
Saliconc	_	_
)	_	_
and	_	_
fine	_	_
scale	_	_
(	_	_
Saliconf	_	_
)	_	_
were	_	_
visualized	_	_
.	_	_

#168
Figures	_	_
5	_	_
to	_	_
9	_	_
show	_	_
example	_	_
patterns	_	_
for	_	_
activation	_	_
maps	_	_
with	_	_
high	_	_
mean	_	_
NSS	_	_
scores	_	_
(	_	_
far	_	_
beyond	_	_
the	_	_
model’s	_	_
performance	_	_
)	_	_
in	_	_
different	_	_
models	_	_
.	_	_

#169
The	_	_
patterns	_	_
in	_	_
these	_	_
figures	_	_
are	_	_
generated	_	_
as	_	_
the	_	_
product	_	_
of	_	_
an	_	_
input	_	_
image	_	_
with	_	_
the	_	_
activation	_	_
map	_	_
,	_	_
cropped	_	_
to	_	_
the	_	_
active	_	_
areas	_	_
for	_	_
legibility	_	_
.	_	_

#170
From	_	_
those	_	_
figures	_	_
,	_	_
we	_	_
can	_	_
see	_	_
Model	_	_
NSS	_	_
CC	_	_
AUC	_	_
Sim	_	_
DSCLRCN	_	_
[	_	_
19	_	_
]	_	_
2.35	_	_
0.80	_	_
0.87	_	_
0.68	_	_
Deepfix	_	_
[	_	_
14	_	_
]	_	_
2.26	_	_
0.78	_	_
0.87	_	_
0.67	_	_
proposed	_	_
2.17	_	_
0.74	_	_
0.83	_	_
0.60	_	_
Salicon	_	_
[	_	_
8	_	_
]	_	_
2.12	_	_
0.74	_	_
0.87	_	_
0.67	_	_
SalGAN	_	_
[	_	_
12	_	_
]	_	_
2.04	_	_
0.73	_	_
0.86	_	_
0.63	_	_
PDP	_	_
[	_	_
9	_	_
]	_	_
2.05	_	_
0.70	_	_
0.85	_	_
0.60	_	_
DVA	_	_
[	_	_
18	_	_
]	_	_
1.98	_	_
0.68	_	_
0.85	_	_
0.58	_	_
Table	_	_
4	_	_
:	_	_
The	_	_
comparison	_	_
of	_	_
different	_	_
models	_	_
on	_	_
MIT300	_	_
dataset	_	_
.	_	_

#171
10	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#172
Input	_	_
GT	_	_
Ours	_	_
SalGAN	_	_
Salicon*	_	_
Deepnet	_	_
Fig.	_	_
4	_	_
:	_	_
Qualitative	_	_
comparisons	_	_
of	_	_
different	_	_
models	_	_
(	_	_
GT	_	_
stands	_	_
for	_	_
Ground	_	_
Truth	_	_
)	_	_
.	_	_

#173
*As	_	_
the	_	_
code	_	_
for	_	_
Salicon	_	_
is	_	_
not	_	_
available	_	_
,	_	_
we	_	_
use	_	_
the	_	_
open	_	_
source	_	_
implementation	_	_
[	_	_
32	_	_
]	_	_
.	_	_

#174
Fig.	_	_
5	_	_
:	_	_
Examples	_	_
of	_	_
patterns	_	_
produced	_	_
for	_	_
the	_	_
activation	_	_
map	_	_
115	_	_
of	_	_
the	_	_
proposed	_	_
model	_	_
,	_	_
with	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
4.5808	_	_
.	_	_

#175
Fig.	_	_
6	_	_
:	_	_
Example	_	_
of	_	_
patterns	_	_
produced	_	_
for	_	_
the	_	_
activation	_	_
map	_	_
221	_	_
of	_	_
SalGAN	_	_
,	_	_
with	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
4.6019	_	_
.	_	_

#176
Fig.	_	_
7	_	_
:	_	_
Example	_	_
of	_	_
patterns	_	_
produced	_	_
for	_	_
the	_	_
activation	_	_
map	_	_
434	_	_
of	_	_
Salicon	_	_
at	_	_
fine	_	_
resolution	_	_
(	_	_
Saliconf	_	_
)	_	_
,	_	_
with	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
5.3637	_	_
.	_	_

#177
Title	_	_
Suppressed	_	_
Due	_	_
to	_	_
Excessive	_	_
Length	_	_
11	_	_
Fig.	_	_
8	_	_
:	_	_
Example	_	_
of	_	_
patterns	_	_
produced	_	_
for	_	_
the	_	_
activation	_	_
map	_	_
232	_	_
of	_	_
Salicon	_	_
at	_	_
coarse	_	_
resolution	_	_
(	_	_
Saliconc	_	_
)	_	_
,	_	_
with	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
5.0027	_	_
.	_	_

#178
Fig.	_	_
9	_	_
:	_	_
Example	_	_
of	_	_
patterns	_	_
produced	_	_
for	_	_
the	_	_
activation	_	_
map	_	_
162	_	_
of	_	_
Deepnet	_	_
,	_	_
with	_	_
a	_	_
mean	_	_
NSS	_	_
score	_	_
of	_	_
4.0101.	_	_
that	_	_
most	_	_
activation	_	_
maps	_	_
with	_	_
high	_	_
mean	_	_
NSS	_	_
score	_	_
focus	_	_
on	_	_
a	_	_
unique	_	_
object	_	_
or	_	_
part	_	_
of	_	_
an	_	_
object	_	_
(	_	_
head	_	_
or	_	_
face	_	_
,	_	_
etc	_	_
)	_	_
.	_	_

#179
Figure	_	_
10	_	_
shows	_	_
example	_	_
patterns	_	_
for	_	_
activation	_	_
maps	_	_
with	_	_
medium	_	_
mean	_	_
NSS	_	_
scores	_	_
.	_	_

#180
In	_	_
these	_	_
examples	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
active	_	_
regions	_	_
are	_	_
less	_	_
clearly	_	_
focused	_	_
on	_	_
a	_	_
single	_	_
object	_	_
or	_	_
part	_	_
.	_	_

#181
More	_	_
importantly	_	_
,	_	_
for	_	_
activation	_	_
maps	_	_
with	_	_
low	_	_
mean	_	_
NSS	_	_
score	_	_
,	_	_
shown	_	_
in	_	_
Figure	_	_
11	_	_
,	_	_
the	_	_
patterns	_	_
show	_	_
a	_	_
negative	_	_
central	_	_
bias	_	_
,	_	_
clearly	_	_
inhibiting	_	_
the	_	_
central	_	_
part	_	_
of	_	_
the	_	_
image	_	_
.	_	_

#182
Note	_	_
that	_	_
the	_	_
models	_	_
analyzed	_	_
in	_	_
this	_	_
figure	_	_
do	_	_
not	_	_
include	_	_
an	_	_
explicit	_	_
central	_	_
bias	_	_
constraint	_	_
,	_	_
therefore	_	_
this	_	_
bias	_	_
has	_	_
been	_	_
learnt	_	_
solely	_	_
from	_	_
the	_	_
training	_	_
data	_	_
and	_	_
appears	_	_
to	_	_
be	_	_
encoded	_	_
by	_	_
low	_	_
NSS	_	_
neurons	_	_
.	_	_

#183
(	_	_
a	_	_
)	_	_
Deepnet	_	_
(	_	_
b	_	_
)	_	_
SalGAN	_	_
(	_	_
c	_	_
)	_	_
Saliconc	_	_
(	_	_
d	_	_
)	_	_
Saliconf	_	_
(	_	_
e	_	_
)	_	_
ours	_	_
Fig.	_	_
10	_	_
:	_	_
Example	_	_
patterns	_	_
for	_	_
activation	_	_
maps	_	_
with	_	_
medium	_	_
mean	_	_
NSS	_	_
score	_	_
,	_	_
drawn	_	_
for	_	_
different	_	_
models	_	_
.	_	_

#184
Since	_	_
some	_	_
activation	_	_
maps	_	_
(	_	_
positive	_	_
fixation	_	_
detectors	_	_
)	_	_
have	_	_
very	_	_
high	_	_
mean	_	_
NSS	_	_
scores	_	_
,	_	_
we	_	_
investigate	_	_
the	_	_
relationships	_	_
between	_	_
the	_	_
model	_	_
performance	_	_
(	_	_
here	_	_
we	_	_
use	_	_
the	_	_
NSS	_	_
score	_	_
in	_	_
MIT1003	_	_
dataset	_	_
as	_	_
the	_	_
model	_	_
performance	_	_
)	_	_
and	_	_
the	_	_
proportion	_	_
of	_	_
positive	_	_
fixation	_	_
detectors	_	_
.	_	_

#185
Table	_	_
5	_	_
records	_	_
this	_	_
ratio	_	_
for	_	_
all	_	_
analyzed	_	_
models	_	_
.	_	_

#186
We	_	_
can	_	_
see	_	_
that	_	_
models	_	_
with	_	_
better	_	_
overall	_	_
performance	_	_
have	_	_
higher	_	_
proportions	_	_
of	_	_
fixation	_	_
detectors	_	_
(	_	_
with	_	_
a	_	_
correlation	_	_
coefficient	_	_
of	_	_

#187
12	_	_
Sen	_	_
He	_	_
1	_	_
,	_	_
Ali	_	_
Borji	_	_
2	_	_
,	_	_
Yang	_	_
Mi	_	_
1	_	_
,	_	_
Nicolas	_	_
Pugeault	_	_
1	_	_

#188
(	_	_
a	_	_
)	_	_
Deepnet	_	_
(	_	_
b	_	_
)	_	_
SalGAN	_	_
(	_	_
c	_	_
)	_	_
Saliconc	_	_
(	_	_
d	_	_
)	_	_
Saliconf	_	_
(	_	_
e	_	_
)	_	_
ours	_	_
Fig.	_	_
11	_	_
:	_	_
Example	_	_
patterns	_	_
for	_	_
activation	_	_
maps	_	_
with	_	_
low	_	_
mean	_	_
NSS	_	_
score	_	_
,	_	_
drawn	_	_
for	_	_
different	_	_
models	_	_
.	_	_

#189
r	_	_
=	_	_
0.94	_	_
)	_	_
,	_	_
and	_	_
our	_	_
model	_	_
has	_	_
the	_	_
highest	_	_
ratio	_	_
of	_	_
positive	_	_
fixation	_	_
detectors	_	_
amongst	_	_
all	_	_
analyzed	_	_
models	_	_
.	_	_

#190
Note	_	_
that	_	_
in	_	_
all	_	_
cases	_	_
,	_	_
the	_	_
ratio	_	_
of	_	_
positive	_	_
fixation	_	_
detectors	_	_
remains	_	_
small	_	_
.	_	_

#191
Model	_	_
#	_	_
activation	_	_
maps	_	_
#	_	_
positive	_	_
detectors	_	_
ratio	_	_
NSS	_	_
Deepnet	_	_
[	_	_
13	_	_
]	_	_
512	_	_
5	_	_
∼1	_	_
%	_	_
1.68	_	_
SalGAN	_	_
[	_	_
12	_	_
]	_	_
512	_	_
14	_	_
2.7	_	_
%	_	_
2.15	_	_
OpenSalicon	_	_
[	_	_
8,32	_	_
]	_	_
1024	_	_
19	_	_
1.8	_	_
%	_	_
1.92	_	_
proposed	_	_
512	_	_
21	_	_
4.1	_	_
%	_	_
2.21	_	_
Table	_	_
5	_	_
:	_	_
The	_	_
relationship	_	_
between	_	_
the	_	_
model	_	_
performance	_	_
and	_	_
the	_	_
number	_	_
of	_	_
positive	_	_
fixation	_	_
detectors	_	_
inside	_	_
the	_	_
model	_	_
.	_	_

#192
The	_	_
correlation	_	_
between	_	_
NSS	_	_
and	_	_
ratio	_	_
is	_	_
0.94	_	_
.	_	_

#193
After	_	_
determining	_	_
which	_	_
activation	_	_
maps	_	_
are	_	_
positive	_	_
fixation	_	_
detectors	_	_
inside	_	_
the	_	_
deep	_	_
models	_	_
,	_	_
the	_	_
question	_	_
remains	_	_
of	_	_
what	_	_
are	_	_
those	_	_
detectors	_	_
are	_	_
attuned	_	_
to	_	_
(	_	_
i.e.	_	_
,	_	_
objects	_	_
and	_	_
object	_	_
parts	_	_
)	_	_
.	_	_

#194
For	_	_
this	_	_
purpose	_	_
we	_	_
use	_	_
measure	_	_
of	_	_
the	_	_
normalized	_	_
mean	_	_
detection	_	_
frequency	_	_
fn	_	_
(	_	_
c	_	_
)	_	_
of	_	_
a	_	_
class	_	_
c	_	_
as	_	_
the	_	_
ratio	_	_
:	_	_
fn	_	_
(	_	_
c	_	_
)	_	_
=	_	_
fd	_	_
(	_	_
c	_	_
)	_	_
ft	_	_
(	_	_
c	_	_
)	_	_
(	_	_
6	_	_
)	_	_
where	_	_
ft	_	_
is	_	_
the	_	_
total	_	_
number	_	_
of	_	_
occurrence	_	_
of	_	_
the	_	_
class	_	_
in	_	_
the	_	_
dataset	_	_
and	_	_
fd	_	_
is	_	_
the	_	_
number	_	_
of	_	_
detected	_	_
instances	_	_
.	_	_

#195
Figure	_	_
12	_	_
records	_	_
the	_	_
normalized	_	_
mean	_	_
detection	_	_
frequency	_	_
for	_	_
the	_	_
analyzed	_	_
models	_	_
and	_	_
for	_	_
classes	_	_
(	_	_
left	_	_
)	_	_
and	_	_
parts	_	_
(	_	_
right	_	_
)	_	_
in	_	_
the	_	_
Broden	_	_
dataset	_	_
.	_	_

#196
On	_	_
the	_	_
left	_	_
hand	_	_
side	_	_
of	_	_
the	_	_
figure	_	_
shows	_	_
the	_	_
results	_	_
of	_	_
this	_	_
analysis	_	_
for	_	_
object	_	_
labels	_	_
:	_	_
Most	_	_
positive	_	_
fixation	_	_
detectors	_	_
are	_	_
attuned	_	_
to	_	_
common	_	_
animals	_	_
(	_	_
dog	_	_
,	_	_
cat	_	_
,	_	_
cow	_	_
,	_	_
sheep	_	_
,	_	_
and	_	_
person	_	_
)	_	_
.	_	_

#197
The	_	_
reason	_	_
might	speculation	_
be	_	_
that	_	_
they	_	_
are	_	_
fine-tuned	_	_
from	_	_
the	_	_
image	_	_
recognition	_	_
models	_	_
,	_	_
which	_	_
have	_	_
already	_	_
learnt	_	_
rich	_	_
object	_	_
classes	_	_
.	_	_

#198
This	_	_
assumption	_	_
is	_	_
supported	_	_
by	_	_
the	_	_
results	_	_
on	_	_
Deepnet	_	_
,	_	_
that	_	_
has	_	_
been	_	_
trained	_	_
from	_	_
scratch	_	_
without	_	_
pre-training	_	_
,	_	_
and	_	_
for	_	_
which	_	_
the	_	_
first	_	_
four	_	_
object	_	_
classes	_	_
(	_	_
motorbike	_	_
,	_	_
ball	_	_
,	_	_
bus	_	_
,	_	_
airplane	_	_
)	_	_
are	_	_
not	_	_
those	_	_
animals	_	_
.	_	_

#199
Interestingly	_	_
,	_	_
the	_	_
detectors	_	_
on	_	_
Saliconc	_	_
and	_	_
Saliconf	_	_
are	_	_
attuned	_	_
to	_	_
different	_	_
visual	_	_
classes	_	_
;	_	_
the	_	_
coarse	_	_
model	_	_
(	_	_
Saliconc	_	_
)	_	_
appears	_	_
to	_	_
capture	_	_
more	_	_
common	_	_
object	_	_
classes	_	_
than	_	_
the	_	_
fine	_	_
model	_	_
,	_	_
as	_	_
evidenced	_	_
by	_	_
higher	_	_
fn	_	_
scores	_	_
.	_	_

#200
The	_	_
right	_	_
hand	_	_
side	_	_
shows	_	_
similar	_	_
results	_	_
but	_	_
using	_	_
parts	_	_
labels	_	_
instead	_	_
of	_	_
object	_	_
labels	_	_
.	_	_

#201
In	_	_
these	_	_
graphs	_	_
,	_	_
we	_	_
see	_	_
that	_	_
almost	_	_
all	_	_
positive	_	_
fixation	_	_
detectors	_	_
focus	_	_
on	_	_
the	_	_
head	_	_
or	_	_
head	_	_
parts	_	_
(	_	_
i.e.	_	_
,	_	_
head	_	_
,	_	_
hair	_	_
,	_	_
torso	_	_
,	_	_
ear	_	_
,	_	_
and	_	_
neck	_	_
)	_	_
.	_	_

#202
6	_	_
Conclusion	_	_

#203
This	_	_
paper	_	_
set	_	_
out	_	_
to	_	_
investigate	_	_
the	_	_
reason	_	_
behind	_	_
the	_	_
high	_	_
performance	_	_
achieved	_	_
by	_	_
deep	_	_
saliency	_	_
models	_	_
,	_	_
compared	_	_
to	_	_
shallow	_	_
models	_	_
using	_	_
hand-crafted	_	_
features	_	_
based	_	_
on	_	_
theoretical	_	_
considerations	_	_
about	_	_
saliency	_	_
(	_	_
e.g.	_	_
,	_	_
center-surround	_	_
difference	_	_
)	_	_
.	_	_

#204
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
proposed	_	_
a	_	_
simple	_	_
residual-like	_	_
decoder	_	_
combined	_	_
with	_	_
a	_	_
pixel-wise	_	_
exponential	_	_
absolute	_	_
distance	_	_
loss	_	_
function	_	_
.	_	_

#205
The	_	_
proposed	_	_
loss	_	_
function	_	_
achieves	_	_
best	_	_
results	_	_
among	_	_
all	_	_
pixel-wise	_	_
loss	_	_
functions	_	_
and	_	_
the	_	_
model	_	_
performance	_	_
is	_	_
on	_	_
par	_	_
or	_	_
better	_	_
than	_	_
those	_	_
state-of-the-art	_	_
saliency	_	_
models	_	_
,	_	_
despite	_	_
being	_	_
based	_	_
on	_	_
a	_	_
simple	_	_
architecture	_	_
.	_	_

#206
Furthermore	_	_
,	_	_
we	_	_
proposed	_	_
a	_	_
visualization	_	_
method	_	_
for	_	_
deep	_	_
gaze	_	_
prediction	_	_
models	_	_
,	_	_
and	_	_
did	_	_
a	_	_
comprehensive	_	_
study	_	_
to	_	_
reveal	_	_
the	_	_
inner	_	_
representations	_	_
inside	_	_
those	_	_
models	_	_
.	_	_

#207
Our	_	_
analyses	_	_
allow	_	_
us	_	_
to	_	_
draw	_	_
three	_	_
conclusions	_	_
about	_	_
what	_	_
is	_	_
learned	_	_
by	_	_
deep	_	_
saliency	_	_
models	_	_
.	_	_

#208
First	_	_
,	_	_
better	_	_
performing	_	_
models	_	_
have	_	_
developed	_	_
higher	_	_
proportions	_	_
of	_	_
deep	_	_
neurons	_	_
highly	_	_
predictive	_	_
of	_	_
human	_	_
gaze	_	_
,	_	_
and	_	_
those	_	_
neurons	_	_
are	_	_
attuned	_	_
to	_	_
very	_	_
specific	_	_
visual	_	_
patterns	_	_
.	_	_

#209
Second	_	_
,	_	_
another	_	_
category	_	_
of	_	_
neurons	_	_
,	_	_
which	_	_
are	_	_
not	_	_
predictive	_	_
of	_	_
human	_	_
gaze	_	_
,	_	_
appear	_	_
to	_	_
encode	_	_
a	_	_
form	_	_
of	_	_
negative	_	_
central	_	_
bias	_	_
into	_	_
the	_	_
model	_	_
.	_	_

#210
Third	_	_
,	_	_
we	_	_
have	_	_
demonstrated	_	_
that	_	_
the	_	_
predictive	_	_
neurons	_	_
are	_	_
attuned	_	_
to	_	_
clear	_	_
semantic	_	_
categories	_	_
such	_	_
as	_	_
animals	_	_
(	_	_
dogs	_	_
,	_	_
cats	_	_
)	_	_
,	_	_
objects	_	_
(	_	_
motorbike	_	_
,	_	_
ball	_	_
)	_	_
and	_	_
parts	_	_
(	_	_
head	_	_
,	_	_
hair	_	_
)	_	_
.	_	_

#211
These	_	_
results	_	_
provide	_	_
evidence	_	_
that	_	_
the	_	_
higher	_	_
prediction	_	_
performance	_	_
achieved	_	_
by	_	_
deep	_	_
saliency	_	_
models	_	_
is	_	_
likely	_	_
caused	_	_
by	_	_
the	_	_
additional	_	_
semantic	_	_
content	_	_
encoded	_	_
by	_	_
such	_	_
networks	_	_
,	_	_
allowing	_	_
the	_	_
models	_	_
to	_	_
capture	_	_
the	_	_
fact	_	_
that	_	_
specific	_	_
visual	_	_
classes	_	_
are	_	_
salient	_	_
in	_	_
their	_	_
own	_	_
right	_	_
,	_	_
in	_	_
contrast	_	_
to	_	_
shallow	_	_
saliency	_	_
models	_	_
that	_	_
rely	_	_
on	_	_
low	_	_
level	_	_
perceptual	_	_
patterns	_	_
(	_	_
such	_	_
as	_	_
center-surround	_	_
difference	_	_
)	_	_
.	_	_

#212
This	_	_
hints	_	_
that	_	_
saliency	_	_
,	_	_
as	_	_
experienced	_	_
by	_	_
humans	_	_
,	_	_
is	_	_
a	_	_
process	_	_
that	_	_
likely	_	_
involves	_	_
high-level	_	_
world	_	_
knowledge	_	_
in	_	_
addition	_	_
to	_	_
low-level	_	_
perceptual	_	_
cues	_	_
.	_	_

#213
We	_	_
believe	_	_
that	_	_
our	_	_
results	_	_
can	_	_
be	_	_
useful	_	_
to	_	_
measure	_	_
the	_	_
gap	_	_
between	_	_
current	_	_
saliency	_	_
models	_	_
and	_	_
the	_	_
human	_	_
inter-observer	_	_
model	_	_
and	_	_
to	_	_
build	_	_
new	_	_
models	_	_
to	_	_
close	_	_
this	_	_
gap	_	_
.	_	_

#214
We	_	_
will	_	_
share	_	_
our	_	_
code	_	_
to	_	_
facilitate	_	_
future	_	_
research	_	_
in	_	_
this	_	_
direction	_	_
.	_	_