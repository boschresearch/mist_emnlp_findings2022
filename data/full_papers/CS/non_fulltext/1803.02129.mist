#0
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#1
9v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
6	_	_
M	_	_
ar	_	_
2	_	_
A	_	_
Non-Technical	_	_
Survey	_	_
on	_	_
Deep	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
Architectures	_	_
Felix	_	_
Altenberger	_	_
Technical	_	_
University	_	_
of	_	_
Munich	_	_

#2
85748	_	_
Garching	_	_
,	_	_
Germany	_	_

#3
Email	_	_
:	_	_
felix.altenberger	_	_
@	_	_
tum.de	_	_
Claus	_	_
Lenz	_	_
Cognition	_	_
Factory	_	_
GmbH	_	_

#4
80797	_	_
Munich	_	_
,	_	_
Germany	_	_

#5
Email	_	_
:	_	_
lenz	_	_
@	_	_
cognitionfactory.de	_	_
Abstract—Artificial	_	_
neural	_	_
networks	_	_
have	_	_
recently	_	_
shown	_	_
great	_	_
results	_	_
in	_	_
many	_	_
disciplines	_	_
and	_	_
a	_	_
variety	_	_
of	_	_
applications	_	_
,	_	_
including	_	_
natural	_	_
language	_	_
understanding	_	_
,	_	_
speech	_	_
processing	_	_
,	_	_
games	_	_
and	_	_
image	_	_
data	_	_
generation	_	_
.	_	_

#6
One	_	_
particular	_	_
application	_	_
in	_	_
which	_	_
the	_	_
strong	_	_
performance	_	_
of	_	_
artificial	_	_
neural	_	_
networks	_	_
was	_	_
demonstrated	_	_
is	_	_
the	_	_
recognition	_	_
of	_	_
objects	_	_
in	_	_
images	_	_
,	_	_
where	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
are	_	_
commonly	_	_
applied	_	_
.	_	_

#7
In	_	_
this	_	_
survey	_	_
,	_	_
we	_	_
give	_	_
a	_	_
comprehensive	_	_
introduction	_	_
to	_	_
this	_	_
topic	_	_
(	_	_
object	_	_
recognition	_	_
with	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
)	_	_
,	_	_
with	_	_
a	_	_
strong	_	_
focus	_	_
on	_	_
the	_	_
evolution	_	_
of	_	_
network	_	_
architectures	_	_
.	_	_

#8
Therefore	_	_
,	_	_
we	_	_
aim	_	_
to	_	_
compress	_	_
the	_	_
most	_	_
important	_	_
concepts	_	_
in	_	_
this	_	_
field	_	_
in	_	_
a	_	_
simple	_	_
and	_	_
non-technical	_	_
manner	_	_
to	_	_
allow	_	_
for	_	_
future	_	_
researchers	_	_
to	_	_
have	_	_
a	_	_
quick	_	_
general	_	_
understanding	_	_
.	_	_

#9
This	_	_
work	_	_
is	_	_
structured	_	_
as	_	_
follows	_	_
:	_	_
1	_	_
)	_	_
We	_	_
will	_	_
explain	_	_
the	_	_
basic	_	_
ideas	_	_
of	_	_
(	_	_
convolutional	_	_
)	_	_
neural	_	_
networks	_	_
and	_	_
deep	_	_
learning	_	_
and	_	_
examine	_	_
their	_	_
usage	_	_
for	_	_
three	_	_
object	_	_
recognition	_	_
tasks	_	_
:	_	_
image	_	_
classification	_	_
,	_	_
object	_	_
localization	_	_
and	_	_
object	_	_
detection	_	_
.	_	_

#10
2	_	_
)	_	_
We	_	_
give	_	_
a	_	_
review	_	_
on	_	_
the	_	_
evolution	_	_
of	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
by	_	_
providing	_	_
an	_	_
extensive	_	_
overview	_	_
of	_	_
the	_	_
most	_	_
important	_	_
network	_	_
architectures	_	_
presented	_	_
in	_	_
chronological	_	_
order	_	_
of	_	_
their	_	_
appearances	_	_
.	_	_

#11
Index	_	_
Terms—Deep	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
,	_	_
Network	_	_
Architectures	_	_
,	_	_
Object	_	_
Recognition	_	_
,	_	_
Object	_	_
Detection	_	_
,	_	_
Neural	_	_
Networks	_	_
,	_	_
Deep	_	_
Learning	_	_
I	_	_
.	_	_

#12
INTRODUCTION	_	_
During	_	_
the	_	_
last	_	_
years	_	_
,	_	_
artificial	_	_
agents	_	_
have	_	_
been	_	_
increasingly	_	_
able	_	_
to	_	_
outperform	_	_
humans	_	_
in	_	_
a	_	_
variety	_	_
of	_	_
challenges	_	_
across	_	_
many	_	_
different	_	_
domains	_	_
[	_	_
1	_	_
]	_	_
,	_	_
[	_	_
2	_	_
]	_	_
,	_	_
[	_	_
3	_	_
]	_	_
,	_	_
[	_	_
4	_	_
]	_	_
,	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#13
While	_	_
it	_	_
seems	_	_
that	_	_
computers	_	_
have	_	_
obvious	_	_
advantages	_	_
over	_	_
humans	_	_
in	_	_
many	_	_
areas	_	_
,	_	_
such	_	_
as	_	_
calculus	_	_
or	_	_
industrial	_	_
assembly	_	_
,	_	_
they	_	_
have	_	_
very	_	_
recently	_	_
also	_	_
managed	_	_
to	_	_
outperform	_	_
humans	_	_
on	_	_
tasks	_	_
that	_	_
are	_	_
comparatively	_	_
easy	_	_
for	_	_
humans	_	_
while	_	_
being	_	_
utterly	_	_
complex	_	_
problems	_	_
for	_	_
artificial	_	_
agents	_	_
.	_	_

#14
Recognizing	_	_
faces	_	_
in	_	_
an	_	_
image	_	_
is	_	_
a	_	_
good	_	_
example	_	_
for	_	_
such	_	_
a	_	_
task	_	_
,	_	_
as	_	_
every	_	_
human	_	_
is	_	_
able	_	_
to	_	_
do	_	_
it	_	_
within	_	_
the	_	_
fraction	_	_
of	_	_
a	_	_
second	_	_
.	_	_

#15
In	_	_
computer	_	_
vision	_	_
,	_	_
face	_	_
recognition	_	_
is	_	_
a	_	_
very	_	_
difficult	_	_
challenge	_	_
,	_	_
where	_	_
a	_	_
lot	_	_
of	_	_
research	_	_
is	_	_
still	_	_
being	_	_
conducted	_	_
.	_	_

#16
Until	_	_
recently	_	_
,	_	_
artificial	_	_
agents	_	_
were	_	_
not	_	_
able	_	_
to	_	_
achieve	_	_
results	_	_
comparable	_	_
to	_	_
those	_	_
of	_	_
humans	_	_
,	_	_
even	_	_
with	_	_
the	_	_
most	_	_
advanced	_	_
approaches	_	_
and	_	_
the	_	_
best	_	_
available	_	_
hardware	_	_
.	_	_

#17
The	_	_
methodology	_	_
that	_	_
finally	_	_
allowed	_	_
computer	_	_
vision	_	_
to	_	_
outperform	_	_
humans	_	_
on	_	_
object	_	_
recognition	_	_
tasks	_	_
[	_	_
6	_	_
]	_	_
is	_	_
the	_	_
Deep	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
(	_	_
DCNN	_	_
)	_	_
,	_	_
which	_	_
we	_	_
will	_	_
inspect	_	_
more	_	_
closely	_	_
in	_	_
this	_	_
survey	_	_
.	_	_

#18
Almost	_	_
20	_	_
years	_	_
ago	_	_
,	_	_
[	_	_
7	_	_
]	_	_
already	_	_
proposed	_	_
the	_	_
LeNet	_	_
,	_	_
a	_	_
novel	_	_
DCNN	_	_
architecture	_	_
for	_	_
object	_	_
recognition	_	_
,	_	_
but	_	_
only	_	_
in	_	_
2012	_	_
an	_	_
implementation	_	_
by	_	_
[	_	_
8	_	_
]	_	_
,	_	_
the	_	_
AlexNet	_	_
,	_	_
was	_	_
first	_	_
able	_	_
to	_	_
beat	_	_
more	_	_
traditional	_	_
geometrical	_	_
approaches	_	_
on	_	_
the	_	_
most	_	_
popular	_	_
object	_	_
recognition	_	_
contest	_	_
-	_	_
the	_	_
ILSVRC	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#19
Ever	_	_
since	_	_
,	_	_
DCNNs	_	_
have	_	_
been	_	_
achieving	_	_
state-of-the-art	_	_
results	_	_
on	_	_
object	_	_
recognition	_	_
tasks	_	_
.	_	_

#20
This	_	_
paper	_	_
will	_	_
give	_	_
a	_	_
detailed	_	_
overview	_	_
of	_	_
the	_	_
evolution	_	_
of	_	_
DCNN	_	_
architectures	_	_
and	_	_
how	_	_
they	_	_
are	_	_
applied	_	_
to	_	_
object	_	_
recognition	_	_
challenges	_	_
.	_	_

#21
The	_	_
paper	_	_
is	_	_
structured	_	_
as	_	_
follows	_	_
:	_	_
In	_	_
Sect.	_	_
II	_	_
,	_	_
we	_	_
will	_	_
take	_	_
a	_	_
closer	_	_
look	_	_
at	_	_
the	_	_
deep	_	_
convolutional	_	_
neural	_	_
network	_	_
and	_	_
how	_	_
it	_	_
works	_	_
.	_	_

#22
Afterwards	_	_
,	_	_
in	_	_
Sect.	_	_
III	_	_
,	_	_
we	_	_
will	_	_
inspect	_	_
how	_	_
DCNNs	_	_
are	_	_
used	_	_
for	_	_
three	_	_
different	_	_
object	_	_
recognition	_	_
tasks	_	_
:	_	_
classification	_	_
,	_	_
localization	_	_
and	_	_
detection	_	_
.	_	_

#23
In	_	_
Sect.	_	_
IV	_	_
,	_	_
the	_	_
most	_	_
influential	_	_
DCNN	_	_
architectures	_	_
,	_	_
including	_	_
the	_	_
LeNet	_	_
and	_	_
AlexNet	_	_
we	_	_
mentioned	_	_
earlier	_	_
,	_	_
are	_	_
presented	_	_
in	_	_
chronological	_	_
order	_	_
and	_	_
explained	_	_
.	_	_

#24
Finally	_	_
,	_	_
in	_	_
Sect.	_	_
V	_	_
,	_	_
we	_	_
will	_	_
sum	_	_
up	_	_
the	_	_
key	_	_
aspects	_	_
covered	_	_
in	_	_
this	_	_
paper	_	_
and	_	_
list	_	_
selected	_	_
resources	_	_
for	_	_
further	_	_
research	_	_
.	_	_

#25
II	_	_
.	_	_

#26
DEEP	_	_
CONVOLUTIONAL	_	_
NEURAL	_	_
NETWORKS	_	_
In	_	_
this	_	_
section	_	_
the	_	_
basics	_	_
of	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
will	_	_
be	_	_
explained	_	_
.	_	_

#27
Therefore	_	_
,	_	_
general	_	_
artificial	_	_
neural	_	_
networks	_	_
and	_	_
deep	_	_
learning	_	_
will	_	_
be	_	_
introduced	_	_
first	_	_
,	_	_
before	_	_
diving	_	_
deeper	_	_
into	_	_
convolutional	_	_
layers	_	_
and	_	_
operations	_	_
.	_	_

#28
A.	_	_
Artificial	_	_
Neural	_	_
Networks	_	_

#29
Nature	_	_
has	_	_
always	_	_
been	_	_
a	_	_
source	_	_
of	_	_
inspiration	_	_
for	_	_
scientific	_	_
advances	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#30
When	_	_
given	_	_
the	_	_
task	_	_
of	_	_
designing	_	_
an	_	_
algorithm	_	_
for	_	_
object	_	_
recognition	_	_
without	_	_
having	_	_
much	_	_
prior	_	_
knowledge	_	_
of	_	_
the	_	_
field	_	_
,	_	_
one	_	_
might	options	_
attempt	_	_
replicating	_	_
some	_	_
object	_	_
recognition	_	_
system	_	_
that	_	_
can	_	_
be	_	_
found	_	_
in	_	_
nature	_	_
.	_	_

#31
Within	_	_
the	_	_
human	_	_
brain	_	_
,	_	_
the	_	_
neocortex	_	_
is	_	_
responsible	_	_
for	_	_
recognizing	_	_
very	_	_
high-level	_	_
patterns	_	_
,	_	_
such	_	_
as	_	_
abstract	_	_
concepts	_	_
or	_	_
complicated	_	_
implications	_	_
,	_	_
which	_	_
is	_	_
performed	_	_
by	_	_
around	_	_
20	_	_
billion	_	_
small	_	_
processing	_	_
units	_	_
,	_	_
called	_	_
Neurons	_	_
,	_	_
that	_	_
are	_	_
connected	_	_
with	_	_
each	_	_
other	_	_
and	_	_
organized	_	_
hierarchically	_	_
[	_	_
10	_	_
]	_	_
,	_	_
[	_	_
11	_	_
]	_	_
.	_	_

#32
In	_	_
the	_	_
field	_	_
of	_	_
artificial	_	_
intelligence	_	_
,	_	_
Artificial	_	_
Neural	_	_
Networks	_	_
(	_	_
ANNs	_	_
)	_	_
have	_	_
been	_	_
built	_	_
in	_	_
a	_	_
way	_	_
that	_	_
mimics	_	_
their	_	_
biological	_	_
counterparts	_	_
,	_	_
but	_	_
the	_	_
reproduction	_	_
of	_	_
this	_	_
processing	_	_
functionality	_	_
of	_	_
the	_	_
brain	_	_
has	_	_
been	_	_
made	_	_
with	_	_
abstractions	_	_
and	_	_
major	_	_
simplifications	_	_
.	_	_

#33
Some	_	_
of	_	_
the	_	_
biological	_	_
details	_	_
have	_	_
been	_	_
omitted	_	_
either	_	_
for	_	_
simplification	_	_
and	_	_
computational	_	_
cost	_	_
reduction	_	_
or	_	_
because	_	_
of	_	_
the	_	_
lack	_	_
of	_	_
knowledge	_	_
about	_	_
their	_	_
role	_	_
.	_	_

#34
Fig.	_	_
1	_	_
.	_	_

#35
A	_	_
simple	_	_
artificial	_	_
neural	_	_
network	_	_
,	_	_
consisting	_	_
of	_	_
an	_	_
input	_	_
layer	_	_
,	_	_
an	_	_
output	_	_
layer	_	_
and	_	_
two	_	_
hidden	_	_
layers	_	_
On	_	_
a	_	_
high-level	_	_
perspective	_	_
,	_	_
artificial	_	_
neural	_	_
networks	_	_
can	_	_
be	_	_
divided	_	_
into	_	_
Layers	_	_
.	_	_

#36
We	_	_
can	_	_
distinguish	_	_
three	_	_
types	_	_
of	_	_
layers	_	_
:	_	_
the	_	_
Input	_	_
Layer	_	_
and	_	_
the	_	_
Output	_	_
Layer	_	_
,	_	_
which	_	_
are	_	_
representations	_	_
of	_	_
the	_	_
input	_	_
and	_	_
output	_	_
respectively	_	_
,	_	_
as	_	_
well	_	_
as	_	_
optional	_	_
Hidden	_	_
Layers	_	_
.	_	_

#37
Hidden	_	_
layers	_	_
are	_	_
simply	_	_
all	_	_
the	_	_
other	_	_
layers	_	_
in	_	_
between	_	_
,	_	_
which	_	_
are	_	_
performing	_	_
complementary	_	_
computations	_	_
,	_	_
resulting	_	_
in	_	_
intermediate	_	_
feature	_	_
representation	_	_
of	_	_
the	_	_
input	_	_
.	_	_

#38
A	_	_
simple	_	_
example	_	_
of	_	_
such	_	_
an	_	_
architecture	_	_
with	_	_
two	_	_
hidden	_	_
layers	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
1	_	_
.	_	_

#39
Neurons	_	_
that	_	_
belong	_	_
to	_	_
the	_	_
same	_	_
layer	_	_
of	_	_
a	_	_
network	_	_
do	_	_
generally	_	_
recognize	_	_
similar	_	_
patterns	_	_
.	_	_

#40
In	_	_
the	_	_
first	_	_
hidden	_	_
layers	_	_
,	_	_
which	_	_
are	_	_
close	_	_
to	_	_
the	_	_
input	_	_
layer	_	_
,	_	_
Low	_	_
Level	_	_
features	_	_
like	_	_
lines	_	_
and	_	_
edges	_	_
can	_	_
be	_	_
recognized	_	_
.	_	_

#41
As	_	_
for	_	_
deeper	_	_
layers	_	_
,	_	_
they	_	_
are	_	_
supposed	_	_
to	_	_
find	_	_
High	_	_
Level	_	_
patterns	_	_
,	_	_
e.g.	_	_
eyes	_	_
,	_	_
noses	_	_
and	_	_
mouths	_	_
for	_	_
the	_	_
task	_	_
of	_	_
face	_	_
recognition	_	_
.	_	_

#42
In	_	_
the	_	_
following	_	_
,	_	_
we	_	_
will	_	_
also	_	_
use	_	_
the	_	_
terms	_	_
low	_	_
level	_	_
and	_	_
high	_	_
level	_	_
to	_	_
describe	_	_
the	_	_
location	_	_
of	_	_
neurons	_	_
within	_	_
a	_	_
network	_	_
.	_	_

#43
In	_	_
general	_	_
,	_	_
ANNs	_	_
can	_	_
be	_	_
divided	_	_
into	_	_
two	_	_
main	_	_
categories	_	_
.	_	_

#44
In	_	_
the	_	_
first	_	_
one	_	_
,	_	_
which	_	_
is	_	_
called	_	_
Feedforward	_	_
Neural	_	_
Networks	_	_
(	_	_
FFNN	_	_
)	_	_
,	_	_
neurons	_	_
are	_	_
only	_	_
forwarding	_	_
their	_	_
output	_	_
to	_	_
neurons	_	_
of	_	_
subsequent	_	_
layers	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
1	_	_
.	_	_

#45
The	_	_
second	_	_
type	_	_
of	_	_
ANN	_	_
is	_	_
called	_	_
Recurrent	_	_
Neural	_	_
Network	_	_
(	_	_
RNN	_	_
)	_	_
,	_	_
where	_	_
neurons	_	_
can	_	_
also	_	_
transmit	_	_
information	_	_
to	_	_
other	_	_
neurons	_	_
within	_	_
the	_	_
same	_	_
layer	_	_
,	_	_
to	_	_
neurons	_	_
of	_	_
previous	_	_
layers	_	_
and	_	_
even	_	_
to	_	_
themselves	_	_
.	_	_

#46
For	_	_
object	_	_
recognition	_	_
purposes	_	_
,	_	_
FFNNs	_	_
are	_	_
usually	_	_
used	_	_
.	_	_

#47
Thus	_	_
,	_	_
RNNs	_	_
will	_	_
not	_	_
be	_	_
further	_	_
elaborated	_	_
on	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#48
Furthermore	_	_
,	_	_
the	_	_
neurons	_	_
of	_	_
a	_	_
given	_	_
layer	_	_
in	_	_
Fig.	_	_
1	_	_
are	_	_
always	_	_
connected	_	_
to	_	_
all	_	_
neurons	_	_
of	_	_
the	_	_
previous	_	_
and	_	_
subsequent	_	_
layers	_	_
.	_	_

#49
This	_	_
is	_	_
the	_	_
most	_	_
basic	_	_
kind	_	_
of	_	_
layer	_	_
,	_	_
called	_	_
Fully-Connected	_	_
Layer	_	_
,	_	_
which	_	_
can	_	_
be	_	_
used	_	_
for	_	_
many	_	_
different	_	_
tasks	_	_
across	_	_
many	_	_
different	_	_
domains	_	_
.	_	_

#50
As	_	_
we	_	_
will	_	_
see	_	_
later	_	_
,	_	_
there	_	_
also	_	_
exist	_	_
other	_	_
types	_	_
of	_	_
layers	_	_
,	_	_
which	_	_
are	_	_
better	_	_
suited	_	_
for	_	_
certain	_	_
challenges	_	_
.	_	_

#51
In	_	_
an	_	_
artificial	_	_
neural	_	_
network	_	_
,	_	_
connections	_	_
from	_	_
one	_	_
neuron	_	_
to	_	_
another	_	_
are	_	_
called	_	_
Synapses	_	_
,	_	_
which	_	_
each	_	_
contain	_	_
a	_	_
Weight	_	_
.	_	_

#52
This	_	_
weight	_	_
determines	_	_
how	_	_
important	_	_
the	_	_
result	_	_
of	_	_
the	_	_
lower	_	_
level	_	_
neuron	_	_
is	_	_
for	_	_
the	_	_
outcome	_	_
of	_	_
the	_	_
higher	_	_
level	_	_
neuron	_	_
.	_	_

#53
Recognizing	_	_
eyes	_	_
might	_	_
,	_	_
for	_	_
instance	_	_
,	_	_
be	_	_
an	_	_
important	_	_
prerequisite	_	_
for	_	_
recognizing	_	_
a	_	_
face	_	_
,	_	_
so	_	_
the	_	_
corresponding	_	_
weight	_	_
should	deontic	_
be	_	_
high	_	_
.	_	_

#54
In	_	_
addition	_	_
to	_	_
that	_	_
,	_	_
each	_	_
neuron	_	_
contains	_	_
a	_	_
Bias	_	_
that	_	_
reflects	_	_
how	_	_
likely	_	_
it	_	_
is	_	_
in	_	_
general	_	_
that	_	_
the	_	_
corresponding	_	_
pattern	_	_
is	_	_
present	_	_
.	_	_

#55
The	_	_
values	_	_
of	_	_
these	_	_
weights	_	_
and	_	_
biases	_	_
are	_	_
the	_	_
Parameters	_	_
of	_	_
the	_	_
network	_	_
,	_	_
which	_	_
will	_	_
be	_	_
learned	_	_
during	_	_
the	_	_
training	_	_
process	_	_
.	_	_

#56
Other	_	_
attributes	_	_
of	_	_
the	_	_
training	_	_
procedure	_	_
exist	_	_
and	_	_
are	_	_
manually	_	_
chosen	_	_
.	_	_

#57
They	_	_
are	_	_
referred	_	_
to	_	_
as	_	_
Hyperparameters	_	_
.	_	_

#58
The	_	_
output	_	_
of	_	_
a	_	_
neuron	_	_
is	_	_
a	_	_
real	_	_
value	_	_
.	_	_

#59
This	_	_
value	_	_
is	_	_
obtained	_	_
by	_	_
first	_	_
performing	_	_
a	_	_
linear	_	_
combination	_	_
of	_	_
the	_	_
inputs	_	_
,	_	_
which	_	_
are	_	_
the	_	_
outputs	_	_
of	_	_
the	_	_
previous	_	_
layer	_	_
,	_	_
with	_	_
the	_	_
corresponding	_	_
weights	_	_
and	_	_
bias	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
the	_	_
following	_	_
equation	_	_
:	_	_
output	_	_
=	_	_
(	_	_
∑	_	_
i	_	_
inputi	_	_
∗	_	_
weighti	_	_
)	_	_
+	_	_
bias	_	_
(	_	_
1	_	_
)	_	_
Afterwards	_	_
,	_	_
an	_	_
Activation	_	_
Function	_	_
is	_	_
applied	_	_
to	_	_
the	_	_
result	_	_
,	_	_
which	_	_
is	_	_
enabling	_	_
the	_	_
artificial	_	_
neural	_	_
network	_	_
to	_	_
also	_	_
approximate	_	_
very	_	_
complex	_	_
functions	_	_
by	_	_
performing	_	_
non-linear	_	_
transformations	_	_
.	_	_

#60
An	_	_
example	_	_
of	_	_
such	_	_
an	_	_
activation	_	_
function	_	_
is	_	_
the	_	_
Rectified	_	_
Linear	_	_
Unit	_	_
(	_	_
ReLU	_	_
)	_	_
[	_	_
12	_	_
]	_	_
,	_	_
which	_	_
simply	_	_
discards	_	_
negative	_	_
values	_	_
,	_	_
as	_	_
shown	_	_
by	_	_
Equation	_	_
2	_	_
,	_	_
and	_	_
is	_	_
very	_	_
popular	_	_
due	_	_
to	_	_
its	_	_
simplicity	_	_
and	_	_
effectiveness	_	_
in	_	_
practice	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#61
f	_	_
(	_	_
x	_	_
)	_	_
=	_	_
max	_	_
(	_	_
x	_	_
,	_	_
0	_	_
)	_	_
(	_	_
2	_	_
)	_	_
B	_	_
.	_	_

#62
Deep	_	_
Learning	_	_
In	_	_
order	_	_
to	_	_
train	_	_
a	_	_
network	_	_
,	_	_
a	_	_
long	_	_
repetitive	_	_
learning	_	_
procedure	_	_
is	_	_
applied	_	_
.	_	_

#63
At	_	_
each	_	_
iteration	_	_
of	_	_
the	_	_
training	_	_
,	_	_
a	_	_
Loss	_	_
Function	_	_
is	_	_
evaluated	_	_
.	_	_

#64
It	_	_
determines	_	_
the	_	_
network’s	_	_
prediction	_	_
quality	_	_
and	_	_
is	_	_
dependent	_	_
on	_	_
the	_	_
type	_	_
of	_	_
training	_	_
.	_	_

#65
For	_	_
instance	_	_
,	_	_
in	_	_
Unsupervised	_	_
Learning	_	_
,	_	_
the	_	_
loss	_	_
aims	_	_
to	_	_
put	_	_
an	_	_
emphasis	_	_
on	_	_
constraints	_	_
that	_	_
the	_	_
network	_	_
should	deontic	_
have	_	_
e.g.	_	_
ability	_	_
to	_	_
reconstruct	_	_
its	_	_
input	_	_
,	_	_
as	_	_
performed	_	_
in	_	_
Autoencoders	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#66
In	_	_
Supervised	_	_
Learning	_	_
,	_	_
where	_	_
pairs	_	_
of	_	_
inputs	_	_
and	_	_
corresponding	_	_
target	_	_
outputs	_	_
are	_	_
available	_	_
,	_	_
the	_	_
loss	_	_
can	_	_
be	_	_
defined	_	_
as	_	_
the	_	_
measure	_	_
of	_	_
similarity	_	_
between	_	_
the	_	_
network’s	_	_
predictions	_	_
and	_	_
the	_	_
target	_	_
outputs	_	_
.	_	_

#67
For	_	_
object	_	_
recognition	_	_
,	_	_
supervised	_	_
learning	_	_
is	_	_
usually	_	_
used	_	_
,	_	_
for	_	_
which	_	_
reason	_	_
it	_	_
will	_	_
be	_	_
the	_	_
only	_	_
learning	_	_
type	_	_
considered	_	_
in	_	_
the	_	_
remainder	_	_
of	_	_
this	_	_
paper	_	_
.	_	_

#68
In	_	_
both	_	_
cases	_	_
,	_	_
a	_	_
Parameter	_	_
Update	_	_
Scheme	_	_
is	_	_
used	_	_
to	_	_
alter	_	_
the	_	_
network	_	_
parameters	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
defined	_	_
loss	_	_
.	_	_

#69
Most	_	_
of	_	_
these	_	_
update	_	_
schemes	_	_
are	_	_
based	_	_
on	_	_
computing	_	_
the	_	_
gradient	_	_
of	_	_
the	_	_
loss	_	_
function	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
model	_	_
parameters	_	_
and	_	_
transmitting	_	_
the	_	_
updates	_	_
from	_	_
the	_	_
output	_	_
layer	_	_
to	_	_
earlier	_	_
layers	_	_
with	_	_
a	_	_
method	_	_
named	_	_
Backpropagation	_	_
.	_	_

#70
When	_	_
the	_	_
term	_	_
Deep	_	_
is	_	_
used	_	_
to	_	_
describe	_	_
a	_	_
network	_	_
,	_	_
it	_	_
refers	_	_
to	_	_
the	_	_
number	_	_
of	_	_
layers	_	_
that	_	_
comprise	_	_
the	_	_
network	_	_
.	_	_

#71
The	_	_
1A	_	_
comparison	_	_
of	_	_
popular	_	_
loss	_	_
functions	_	_
has	_	_
been	_	_
made	_	_
by	_	_
[	_	_
13	_	_
]	_	_
term	_	_
Deep	_	_
Learning	_	_
is	_	_
describing	_	_
the	_	_
procedure	_	_
of	_	_
performing	_	_
machine	_	_
learning	_	_
tasks	_	_
with	_	_
deep	_	_
artificial	_	_
neural	_	_
networks	_	_
[	_	_
15	_	_
]	_	_
.	_	_

#72
In	_	_
reality	_	_
,	_	_
the	_	_
best	_	_
performing	_	_
deep	_	_
neural	_	_
networks	_	_
are	_	_
nowadays	_	_
consisting	_	_
of	_	_
hundreds	_	_
of	_	_
layers	_	_
.	_	_

#73
Since	_	_
it	_	_
is	_	_
often	_	_
hard	_	_
to	_	_
understand	_	_
what	_	_
a	_	_
specific	_	_
neuron	_	_
is	_	_
recognizing	_	_
,	_	_
it	_	_
is	_	_
difficult	_	_
to	_	_
tell	_	_
how	_	_
or	_	_
why	_	_
a	_	_
given	_	_
deep	_	_
neural	_	_
network	_	_
is	_	_
working	_	_
(	_	_
or	_	_
not	_	_
)	_	_
.	_	_

#74
Another	_	_
challenge	_	_
of	_	_
using	_	_
deep	_	_
architectures	_	_
is	_	_
that	_	_
different	_	_
layers	_	_
might	speculation	_
learn	_	_
and	_	_
adapt	_	_
at	_	_
a	_	_
different	_	_
pace	_	_
.	_	_

#75
Especially	_	_
the	_	_
earlier	_	_
hidden	_	_
layers	_	_
,	_	_
which	_	_
are	_	_
close	_	_
to	_	_
the	_	_
input	_	_
layer	_	_
,	_	_
do	_	_
frequently	_	_
either	_	_
learn	_	_
much	_	_
slower	_	_
(	_	_
caused	_	_
by	_	_
very	_	_
low	_	_
gradients	_	_
)	_	_
or	_	_
much	_	_
faster	_	_
(	_	_
caused	_	_
by	_	_
high	_	_
gradients	_	_
)	_	_
also	_	_
leading	_	_
to	_	_
an	_	_
oscillatory	_	_
behavior	_	_
.	_	_

#76
These	_	_
two	_	_
problems	_	_
are	_	_
referred	_	_
to	_	_
as	_	_
Vanishing	_	_
Gradient	_	_
Problem	_	_
and	_	_
Exploding	_	_
Gradient	_	_
Problem	_	_
respectively	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#77
These	_	_
can	_	_
nowadays	_	_
be	_	_
prevented	_	_
quite	_	_
well	_	_
by	_	_
a	_	_
variety	_	_
of	_	_
techniques	_	_
,	_	_
including	_	_
gradient	_	_
norm	_	_
clipping	_	_
[	_	_
17	_	_
]	_	_
,	_	_
proper	_	_
network	_	_
parameter	_	_
initialization	_	_
,	_	_
proper	_	_
choice	_	_
of	_	_
activation	_	_
functions	_	_
,	_	_
as	_	_
well	_	_
as	_	_
input	_	_
normalization	_	_
[	_	_
18	_	_
]	_	_
,	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#78
C.	_	_
Convolutional	_	_
Layers	_	_
and	_	_
CNNs	_	_

#79
When	_	_
tackling	_	_
deep	_	_
learning	_	_
tasks	_	_
,	_	_
it	_	_
is	_	_
generally	_	_
recommended	_	_
to	_	_
train	_	_
and	_	_
run	_	_
the	_	_
models	_	_
on	_	_
raw	_	_
inputs	_	_
,	_	_
without	_	_
manually	_	_
extracting	_	_
any	_	_
features	_	_
before	_	_
.	_	_

#80
The	_	_
reason	_	_
for	_	_
this	_	_
is	_	_
that	_	_
a	_	_
network	_	_
trained	_	_
on	_	_
the	_	_
raw	_	_
input	_	_
could	capability-speculation	_
learn	_	_
to	_	_
extract	_	_
these	_	_
features	_	_
on	_	_
its	_	_
own	_	_
,	_	_
but	_	_
in	_	_
contrast	_	_
to	_	_
working	_	_
with	_	_
prebuilt	_	_
features	_	_
,	_	_
it	_	_
would	_	_
also	_	_
be	_	_
able	_	_
to	_	_
further	_	_
optimize	_	_
the	_	_
feature	_	_
extraction	_	_
as	_	_
the	_	_
network	_	_
improves	_	_
.	_	_

#81
If	_	_
the	_	_
input	_	_
is	_	_
an	_	_
image	_	_
,	_	_
it	_	_
would	_	_
,	_	_
therefore	_	_
,	_	_
be	_	_
desirable	_	_
to	_	_
work	_	_
directly	_	_
with	_	_
its	_	_
raw	_	_
pixel	_	_
values	_	_
.	_	_

#82
Since	_	_
an	_	_
image	_	_
consists	_	_
of	_	_
many	_	_
pixels	_	_
and	_	_
each	_	_
pixel	_	_
is	_	_
possibly	_	_
represented	_	_
by	_	_
multiple	_	_
color	_	_
values	_	_
,	_	_
the	_	_
representation	_	_
of	_	_
that	_	_
image	_	_
in	_	_
the	_	_
input	_	_
layer	_	_
can	_	_
become	_	_
highly	_	_
complex	_	_
.	_	_

#83
A	_	_
full	_	_
HD	_	_
RGB	_	_
image	_	_
with	_	_
1920×	_	_
1080	_	_
pixels	_	_
would	_	_
,	_	_
for	_	_
instance	_	_
,	_	_
require	_	_
an	_	_
input	_	_
layer	_	_
consisting	_	_
of	_	_
about	_	_
six	_	_
million	_	_
neurons	_	_
.	_	_

#84
If	_	_
one	_	_
would	_	_
use	_	_
the	_	_
simple	_	_
fully-connected	_	_
network	_	_
architecture	_	_
described	_	_
in	_	_
Sect.	_	_
II-A	_	_
,	_	_
each	_	_
neuron	_	_
in	_	_
the	_	_
subsequent	_	_
layer	_	_
would	_	_
then	_	_
be	_	_
connected	_	_
to	_	_
about	_	_
six	_	_
million	_	_
neurons	_	_
and	_	_
if	_	_
the	_	_
first	_	_
fully-connected	_	_
layer	_	_
would	_	_
contain	_	_
just	_	_
1000	_	_
neurons	_	_
,	_	_
the	_	_
total	_	_
number	_	_
of	_	_
parameters	_	_
would	_	_
amount	_	_
to	_	_
over	_	_
six	_	_
billion	_	_
.	_	_

#85
Since	_	_
the	_	_
network	_	_
has	_	_
to	_	_
optimize	_	_
all	_	_
of	_	_
these	_	_
parameters	_	_
,	_	_
the	_	_
training	_	_
process	_	_
could	speculation	_
then	_	_
become	_	_
very	_	_
time	_	_
and	_	_
storage	_	_
intensive	_	_
.	_	_

#86
In	_	_
order	_	_
to	_	_
solve	_	_
this	_	_
computational	_	_
problem	_	_
,	_	_
a	_	_
different	_	_
kind	_	_
of	_	_
network	_	_
architecture	_	_
is	_	_
used	_	_
,	_	_
called	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
(	_	_
CNN	_	_
)	_	_
.	_	_

#87
CNNs	_	_
are	_	_
specifically	_	_
designed	_	_
for	_	_
working	_	_
with	_	_
images	_	_
.	_	_

#88
For	_	_
this	_	_
reason	_	_
,	_	_
the	_	_
neurons	_	_
of	_	_
a	_	_
layer	_	_
are	_	_
organized	_	_
across	_	_
the	_	_
three	_	_
dimensions	_	_
,	_	_
height	_	_
,	_	_
width	_	_
and	_	_
depth	_	_
,	_	_
just	_	_
like	_	_
the	_	_
pixels	_	_
in	_	_
an	_	_
image	_	_
where	_	_
the	_	_
depth	_	_
dimension	_	_
would	_	_
differentiate	_	_
the	_	_
different	_	_
color	_	_
values	_	_
.	_	_

#89
In	_	_
addition	_	_
to	_	_
that	_	_
,	_	_
CNNs	_	_
introduce	_	_
two	_	_
new	_	_
types	_	_
of	_	_
hidden	_	_
layers	_	_
in	_	_
which	_	_
each	_	_
neuron	_	_
is	_	_
only	_	_
connected	_	_
to	_	_
a	_	_
small	_	_
subset	_	_
of	_	_
the	_	_
neurons	_	_
in	_	_
the	_	_
previous	_	_
layer	_	_
,	_	_
to	_	_
prevent	_	_
the	_	_
aforementioned	_	_
problem	_	_
.	_	_

#90
How	_	_
these	_	_
layers	_	_
work	_	_
and	_	_
what	_	_
they	_	_
are	_	_
used	_	_
for	_	_
will	_	_
be	_	_
explained	_	_
in	_	_
the	_	_
following	_	_
.	_	_

#91
1	_	_
)	_	_
Convolutional	_	_
Layer	_	_
:	_	_
In	_	_
order	_	_
to	_	_
deal	_	_
with	_	_
the	_	_
problems	_	_
that	_	_
fully-connected	_	_
layers	_	_
faced	_	_
when	_	_
processing	_	_
images	_	_
,	_	_
Fig.	_	_
2	_	_
.	_	_

#92
A	_	_
neuron	_	_
of	_	_
a	_	_
convolutional	_	_
layer	_	_
performing	_	_
a	_	_
convolution	_	_
operation	_	_
with	_	_
a	_	_
3×	_	_
3	_	_
receptive	_	_
field	_	_
on	_	_
an	_	_
RGB	_	_
image	_	_
Convolutional	_	_
Layers	_	_
are	_	_
used	_	_
in	_	_
CNNs	_	_
instead	_	_
.	_	_

#93
What	_	_
separates	_	_
a	_	_
convolutional	_	_
layer	_	_
from	_	_
a	_	_
fully-connected	_	_
one	_	_
is	_	_
that	_	_
each	_	_
neuron	_	_
is	_	_
only	_	_
connected	_	_
to	_	_
a	_	_
small	_	_
,	_	_
local	_	_
subset	_	_
of	_	_
the	_	_
neurons	_	_
in	_	_
the	_	_
previous	_	_
layer	_	_
,	_	_
which	_	_
is	_	_
a	_	_
square	_	_
sized	_	_
region	_	_
across	_	_
the	_	_
height	_	_
and	_	_
width	_	_
dimensions	_	_
.	_	_

#94
The	_	_
size	_	_
of	_	_
this	_	_
square	_	_
is	_	_
a	_	_
hyperparameter	_	_
named	_	_
Receptive	_	_
Field	_	_
.	_	_

#95
For	_	_
the	_	_
depth	_	_
dimension	_	_
,	_	_
there	_	_
is	_	_
no	_	_
hyperparameter	_	_
that	_	_
has	_	_
to	_	_
be	_	_
defined	_	_
,	_	_
as	_	_
the	_	_
convolutions	_	_
are	_	_
by	_	_
default	_	_
always	_	_
performed	_	_
across	_	_
the	_	_
whole	_	_
depth	_	_
.	_	_

#96
The	_	_
reason	_	_
for	_	_
this	_	_
is	_	_
that	_	_
the	_	_
depth	_	_
dimension	_	_
of	_	_
the	_	_
input	_	_
does	_	_
typically	_	_
define	_	_
the	_	_
different	_	_
colors	_	_
of	_	_
the	_	_
image	_	_
and	_	_
it	_	_
is	_	_
usually	_	_
necessary	_	_
to	_	_
combine	_	_
them	_	_
in	_	_
order	_	_
to	_	_
extract	_	_
any	_	_
useful	_	_
information	_	_
.	_	_

#97
Neurons	_	_
of	_	_
the	_	_
convolution	_	_
operator	_	_
can	_	_
recognize	_	_
certain	_	_
local	_	_
patterns	_	_
of	_	_
the	_	_
previous	_	_
layer’s	_	_
output	_	_
.	_	_

#98
Since	_	_
the	_	_
patterns	_	_
that	_	_
are	_	_
recognized	_	_
should	deontic	_
be	_	_
independent	_	_
of	_	_
their	_	_
position	_	_
in	_	_
the	_	_
image	_	_
,	_	_
all	_	_
neurons	_	_
will	_	_
be	_	_
forced	_	_
to	_	_
recognize	_	_
the	_	_
same	_	_
pattern	_	_
by	_	_
making	_	_
all	_	_
of	_	_
them	_	_
share	_	_
one	_	_
single	_	_
set	_	_
of	_	_
parameters	_	_
.	_	_

#99
This	_	_
concept	_	_
is	_	_
referred	_	_
to	_	_
as	_	_
Parameter	_	_
Sharing	_	_
.	_	_

#100
In	_	_
order	_	_
to	_	_
now	_	_
recognize	_	_
multiple	_	_
different	_	_
features	_	_
within	_	_
one	_	_
layer	_	_
,	_	_
it	_	_
is	_	_
required	_	_
to	_	_
have	_	_
several	_	_
Filters	_	_
,	_	_
where	_	_
each	_	_
filter	_	_
is	_	_
a	_	_
group	_	_
of	_	_
neurons	_	_
that	_	_
recognize	_	_
a	_	_
certain	_	_
pattern	_	_
at	_	_
different	_	_
locations	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#101
In	_	_
the	_	_
convolutional	_	_
layer	_	_
,	_	_
the	_	_
depth	_	_
dimension	_	_
is	_	_
then	_	_
specifying	_	_
to	_	_
which	_	_
filter	_	_
a	_	_
given	_	_
neuron	_	_
belongs	_	_
.	_	_

#102
Another	_	_
reason	_	_
for	_	_
why	_	_
the	_	_
convolution	_	_
operations	_	_
are	_	_
performed	_	_
across	_	_
all	_	_
depth	_	_
values	_	_
is	_	_
that	_	_
neurons	_	_
in	_	_
a	_	_
convolutional	_	_
layer	_	_
,	_	_
which	_	_
are	_	_
stacked	_	_
on	_	_
top	_	_
of	_	_
others	_	_
,	_	_
should	deontic	_
have	_	_
their	_	_
features	_	_
jointly	_	_
considered	_	_
in	_	_
the	_	_
next	_	_
layer	_	_
.	_	_

#103
A	_	_
neuron	_	_
in	_	_
a	_	_
convolutional	_	_
layer	_	_
will	_	_
,	_	_
therefore	_	_
,	_	_
be	_	_
connected	_	_
to	_	_
r	_	_
∗	_	_
d	_	_
neurons	_	_
of	_	_
the	_	_
underlying	_	_
layer	_	_
,	_	_
where	_	_
r	_	_
is	_	_
the	_	_
size	_	_
of	_	_
the	_	_
receptive	_	_
field	_	_
and	_	_
d	_	_
is	_	_
the	_	_
depth	_	_
of	_	_
the	_	_
previous	_	_
layer	_	_
.	_	_

#104
When	_	_
performing	_	_
a	_	_
convolution	_	_
directly	_	_
on	_	_
the	_	_
input	_	_
layer	_	_
of	_	_
an	_	_
RGB	_	_
image	_	_
with	_	_
a	_	_
receptive	_	_
field	_	_
of	_	_
3×3	_	_
,	_	_
each	_	_
neuron	_	_
in	_	_
the	_	_
layer	_	_
would	_	_
,	_	_
for	_	_
instance	_	_
,	_	_
be	_	_
connected	_	_
to	_	_
27	_	_
input	_	_
neurons	_	_
,	_	_
consisting	_	_
of	_	_
a	_	_
3	_	_
×	_	_
3	_	_
square	_	_
of	_	_
pixels	_	_
with	_	_
three	_	_
neurons	_	_
per	_	_
pixel	_	_
,	_	_
as	_	_
illustrated	_	_
in	_	_
Fig.	_	_
2	_	_
.	_	_

#105
How	_	_
many	_	_
convolutions	_	_
are	_	_
being	_	_
conducted	_	_
is	_	_
defined	_	_
Fig.	_	_
3	_	_
.	_	_

#106
A	_	_
visualization	_	_
of	_	_
the	_	_
different	_	_
hyperparameters	_	_
of	_	_
convolutional	_	_
layers	_	_
:	_	_
receptive	_	_
field	_	_
,	_	_
stride	_	_
and	_	_
padding	_	_
by	_	_
another	_	_
hyperparameter	_	_
,	_	_
called	_	_
Stride	_	_
,	_	_
which	_	_
determines	_	_
how	_	_
big	_	_
the	_	_
gap	_	_
between	_	_
two	_	_
scanned	_	_
regions	_	_
is	_	_
.	_	_

#107
Without	_	_
using	_	_
any	_	_
further	_	_
hyperparameters	_	_
we	_	_
would	_	_
always	_	_
perform	_	_
fewer	_	_
convolutions	_	_
on	_	_
inputs	_	_
close	_	_
to	_	_
the	_	_
borders	_	_
.	_	_

#108
Adjusting	_	_
the	_	_
Padding	_	_
hyperparameter	_	_
can	_	_
make	_	_
that	_	_
more	_	_
even	_	_
,	_	_
as	_	_
it	_	_
adds	_	_
an	_	_
additional	_	_
border	_	_
of	_	_
0	_	_
values	_	_
around	_	_
the	_	_
original	_	_
input	_	_
.	_	_

#109
Another	_	_
reason	_	_
for	_	_
why	_	_
padding	_	_
is	_	_
applied	_	_
in	_	_
some	_	_
implementations	_	_
is	_	_
to	_	_
make	_	_
the	_	_
convolution	_	_
result	_	_
have	_	_
a	_	_
certain	_	_
width	_	_
and	_	_
height	_	_
,	_	_
e.g.	_	_
making	_	_
the	_	_
output	_	_
have	_	_
the	_	_
same	_	_
size	_	_
as	_	_
the	_	_
input	_	_
.	_	_

#110
A	_	_
visualization	_	_
of	_	_
the	_	_
three	_	_
hyperparameters	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
3	_	_
and	_	_
the	_	_
output	_	_
size	_	_
of	_	_
a	_	_
convolutional	_	_
layer	_	_
can	_	_
be	_	_
calculated	_	_
by	_	_
the	_	_
following	_	_
equation	_	_
:	_	_
out	_	_
=	_	_
in−	_	_
receptivefield+	_	_
2	_	_
∗	_	_
padding	_	_
stride	_	_
+	_	_
1	_	_
(	_	_
3	_	_
)	_	_
2	_	_
)	_	_
Pooling	_	_
Layer	_	_
:	_	_
The	_	_
third	_	_
kind	_	_
of	_	_
layer	_	_
,	_	_
which	_	_
has	_	_
the	_	_
purpose	_	_
of	_	_
decreasing	_	_
the	_	_
complexity	_	_
of	_	_
CNNs	_	_
,	_	_
is	_	_
the	_	_
Pooling	_	_
Layer	_	_
.	_	_

#111
Similarly	_	_
to	_	_
the	_	_
convolutional	_	_
layer	_	_
,	_	_
neurons	_	_
in	_	_
the	_	_
pooling	_	_
layer	_	_
are	_	_
connected	_	_
to	_	_
a	_	_
square	_	_
sized	_	_
region	_	_
across	_	_
the	_	_
width	_	_
and	_	_
height	_	_
dimensions	_	_
of	_	_
the	_	_
previous	_	_
layer	_	_
.	_	_

#112
The	_	_
main	_	_
difference	_	_
between	_	_
convolution	_	_
and	_	_
pooling	_	_
is	_	_
that	_	_
a	_	_
pooling	_	_
layer	_	_
is	_	_
not	_	_
Parametrized2	_	_
.	_	_

#113
This	_	_
means	_	_
that	_	_
neurons	_	_
in	_	_
the	_	_
pooling	_	_
layer	_	_
do	_	_
not	_	_
have	_	_
weights	_	_
or	_	_
biases	_	_
that	_	_
will	_	_
be	_	_
learned	_	_
during	_	_
the	_	_
training	_	_
process	_	_
but	_	_
instead	_	_
perform	_	_
some	_	_
fixed	_	_
function	_	_
on	_	_
its	_	_
inputs	_	_
.	_	_

#114
Additionally	_	_
,	_	_
the	_	_
pooling	_	_
operation	_	_
does	_	_
not	_	_
combine	_	_
neurons	_	_
with	_	_
different	_	_
depth	_	_
values	_	_
.	_	_

#115
Instead	_	_
,	_	_
the	_	_
resulting	_	_
pooling	_	_
layer	_	_
will	_	_
have	_	_
the	_	_
same	_	_
depth	_	_
as	_	_
the	_	_
previous	_	_
layer	_	_
and	_	_
it	_	_
will	_	_
only	_	_
combine	_	_
local	_	_
regions	_	_
within	_	_
a	_	_
filter	_	_
.	_	_

#116
One	_	_
common	_	_
type	_	_
of	_	_
pooling	_	_
is	_	_
Max	_	_
Pooling	_	_
,	_	_
where	_	_
the	_	_
result	_	_
of	_	_
combining	_	_
a	_	_
number	_	_
of	_	_
neurons	_	_
is	_	_
the	_	_
maximum	_	_
value	_	_
that	_	_
any	_	_
of	_	_
them	_	_
returned	_	_
,	_	_
which	_	_
is	_	_
illustrated	_	_
in	_	_
Fig.	_	_
4	_	_
.	_	_

#117
Since	_	_
all	_	_
neurons	_	_
in	_	_
the	_	_
convolutional	_	_
layer	_	_
recognize	_	_
the	_	_
2The	_	_
only	_	_
parametrized	_	_
layers	_	_
we	_	_
will	_	_
inspect	_	_
in	_	_
the	_	_
scope	_	_
of	_	_
this	_	_
paper	_	_
are	_	_
the	_	_
fully-connected	_	_
and	_	_
the	_	_
convolutional	_	_
layers	_	_
.	_	_

#118
Fig.	_	_
4	_	_
.	_	_

#119
A	_	_
neuron	_	_
of	_	_
a	_	_
pooling	_	_
layer	_	_
performing	_	_
a	_	_
max	_	_
pooling	_	_
operation	_	_
with	_	_
a	_	_
3×	_	_
3	_	_
receptive	_	_
field	_	_
on	_	_
neurons	_	_
of	_	_
an	_	_
underlying	_	_
layer	_	_
same	_	_
pattern	_	_
,	_	_
the	_	_
result	_	_
of	_	_
the	_	_
max	_	_
pooling	_	_
operation	_	_
can	_	_
be	_	_
interpreted	_	_
as	_	_
whether	_	_
that	_	_
pattern	_	_
has	_	_
been	_	_
recognized	_	_
in	_	_
the	_	_
pooling	_	_
area	_	_
or	_	_
not	_	_
,	_	_
but	_	_
the	_	_
exact	_	_
location	_	_
will	_	_
not	_	_
be	_	_
relevant	_	_
anymore	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#120
Other	_	_
pooling	_	_
variants	_	_
,	_	_
such	_	_
as	_	_
Average	_	_
Pooling	_	_
,	_	_
perform	_	_
a	_	_
different	_	_
function	_	_
but	_	_
can	_	_
be	_	_
interpreted	_	_
similarly	_	_
.	_	_

#121
How	_	_
many	_	_
neurons	_	_
are	_	_
combined	_	_
across	_	_
each	_	_
of	_	_
the	_	_
two	_	_
dimensions	_	_
and	_	_
how	_	_
large	_	_
the	_	_
gap	_	_
between	_	_
two	_	_
pooling	_	_
operations	_	_
should	deontic	_
be	_	_
is	_	_
again	_	_
defined	_	_
by	_	_
the	_	_
receptive	_	_
field	_	_
and	_	_
stride	_	_
hyperparameters	_	_
that	_	_
were	_	_
used	_	_
for	_	_
defining	_	_
convolutional	_	_
layers	_	_
.	_	_

#122
III.	_	_
APPLICATIONS	_	_
OF	_	_
DCNNS	_	_
FOR	_	_
OBJECT	_	_
RECOGNITION	_	_
TASKS	_	_

#123
After	_	_
having	_	_
described	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
in	_	_
the	_	_
previous	_	_
section	_	_
,	_	_
we	_	_
will	_	_
now	_	_
turn	_	_
our	_	_
focus	_	_
to	_	_
how	_	_
these	_	_
are	_	_
used	_	_
for	_	_
object	_	_
recognition	_	_
purposes	_	_
.	_	_

#124
More	_	_
specifically	_	_
,	_	_
we	_	_
will	_	_
learn	_	_
about	_	_
three	_	_
object	_	_
recognition	_	_
tasks	_	_
-	_	_
classification	_	_
,	_	_
localization	_	_
and	_	_
detection	_	_
-	_	_
and	_	_
how	_	_
each	_	_
of	_	_
them	_	_
can	_	_
be	_	_
tackled	_	_
with	_	_
DCNNs	_	_
.	_	_

#125
A.	_	_
Classification	_	_

#126
The	_	_
task	_	_
of	_	_
Image	_	_
Classification	_	_
describes	_	_
the	_	_
challenge	_	_
of	_	_
categorizing	_	_
a	_	_
given	_	_
image	_	_
into	_	_
one	_	_
of	_	_
several	_	_
classes	_	_
.	_	_

#127
A	_	_
possible	_	_
application	_	_
of	_	_
this	_	_
could	speculation	_
be	_	_
the	_	_
recognition	_	_
of	_	_
hand-written	_	_
digits	_	_
,	_	_
where	_	_
the	_	_
input	_	_
image	_	_
is	_	_
classified	_	_
as	_	_
one	_	_
of	_	_
the	_	_
ten	_	_
classes	_	_
.	_	_

#128
For	_	_
this	_	_
task	_	_
[	_	_
7	_	_
]	_	_
developed	_	_
a	_	_
DCNN	_	_
architecture	_	_
in	_	_
1998	_	_
,	_	_
the	_	_
LeNet-5	_	_
,	_	_
which	_	_
we	_	_
will	_	_
inspect	_	_
more	_	_
closely	_	_
later	_	_
.	_	_

#129
This	_	_
network	_	_
defined	_	_
the	_	_
default	_	_
classification	_	_
approach	_	_
,	_	_
where	_	_
the	_	_
DCNN	_	_
first	_	_
performs	_	_
several	_	_
convolutions	_	_
and	_	_
pooling	_	_
operations	_	_
in	_	_
order	_	_
to	_	_
extract	_	_
high-level	_	_
features	_	_
.	_	_

#130
We	_	_
will	_	_
refer	_	_
to	_	_
this	_	_
part	_	_
as	_	_
Network	_	_
Stem	_	_
in	_	_
the	_	_
remainder	_	_
.	_	_

#131
The	_	_
network	_	_
stem	_	_
is	_	_
then	_	_
followed	_	_
by	_	_
some	_	_
fully-connected	_	_
layers	_	_
,	_	_
which	_	_
we	_	_
call	_	_
the	_	_
Fully-Connected	_	_
Module	_	_
,	_	_
that	_	_
is	_	_
connecting	_	_
the	_	_
stem	_	_
to	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#132
The	_	_
whole	_	_
architecture	_	_
is	_	_
illustrated	_	_
in	_	_
Fig.	_	_
5	_	_
.	_	_

#133
The	_	_
output	_	_
layer	_	_
for	_	_
classification	_	_
tasks	_	_
consists	_	_
of	_	_
one	_	_
neuron	_	_
per	_	_
class	_	_
and	_	_
the	_	_
values	_	_
of	_	_
these	_	_
neurons	_	_
are	_	_
representing	_	_
the	_	_
score	_	_
of	_	_
each	_	_
class	_	_
.	_	_

#134
If	_	_
we	_	_
choose	_	_
a	_	_
score	_	_
distribution	_	_
,	_	_
where	_	_
every	_	_
score	_	_
is	_	_
between	_	_
zero	_	_
and	_	_
one	_	_
and	_	_
where	_	_
all	_	_
class	_	_
scores	_	_
add	_	_
up	_	_
to	_	_
one	_	_
,	_	_
the	_	_
values	_	_
of	_	_
each	_	_
neuron	_	_
can	_	_
then	_	_
be	_	_
interpreted	_	_
as	_	_
the	_	_
probability	_	_
of	_	_
whether	_	_
the	_	_
class	_	_
is	_	_
present	_	_
.	_	_

#135
Fig.	_	_
5	_	_
.	_	_

#136
The	_	_
default	_	_
DCNN	_	_
architecture	_	_
for	_	_
image	_	_
classification	_	_
.	_	_

#137
Multiple	_	_
convolutional	_	_
and	_	_
pooling	_	_
layers	_	_
first	_	_
extract	_	_
abstract	_	_
features	_	_
,	_	_
which	_	_
are	_	_
forwarded	_	_
to	_	_
a	_	_
classifier	_	_
by	_	_
fully-connected	_	_
layers	_	_
B.	_	_
Localization	_	_
For	_	_
Localization	_	_
,	_	_
the	_	_
information	_	_
about	_	_
which	_	_
category	_	_
an	_	_
image	_	_
belongs	_	_
to	_	_
is	_	_
already	_	_
available	_	_
and	_	_
the	_	_
task	_	_
is	_	_
to	_	_
instead	_	_
figure	_	_
out	_	_
where	_	_
exactly	_	_
the	_	_
object	_	_
is	_	_
located	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#138
This	_	_
location	_	_
is	_	_
typically	_	_
specified	_	_
by	_	_
a	_	_
two-dimensional	_	_
bounding	_	_
box	_	_
,	_	_
which	_	_
consists	_	_
of	_	_
four	_	_
values	_	_
that	_	_
describe	_	_
the	_	_
location	_	_
of	_	_
two	_	_
opposite	_	_
couples	_	_
of	_	_
corners	_	_
.	_	_

#139
Finding	_	_
these	_	_
four	_	_
values	_	_
is	_	_
the	_	_
main	_	_
challenge	_	_
of	_	_
localization	_	_
and	_	_
is	_	_
commonly	_	_
referred	_	_
to	_	_
as	_	_
Bounding	_	_
Box	_	_
Regression	_	_
.	_	_

#140
To	_	_
perform	_	_
a	_	_
localization	_	_
task	_	_
,	_	_
we	_	_
can	_	_
use	_	_
a	_	_
similar	_	_
architecture	_	_
as	_	_
the	_	_
one	_	_
we	_	_
defined	_	_
for	_	_
classification	_	_
.	_	_

#141
The	_	_
only	_	_
thing	_	_
that	_	_
has	_	_
to	_	_
be	_	_
modified	_	_
is	_	_
the	_	_
final	_	_
output	_	_
layer	_	_
,	_	_
which	_	_
can	_	_
simply	_	_
be	_	_
replaced	_	_
by	_	_
another	_	_
output	_	_
layer	_	_
that	_	_
performs	_	_
the	_	_
bounding	_	_
box	_	_
regression	_	_
instead	_	_
.	_	_

#142
Classification	_	_
and	_	_
localization	_	_
can	_	_
also	_	_
be	_	_
combined	_	_
so	_	_
that	_	_
a	_	_
fixed	_	_
amount	_	_
of	_	_
objects	_	_
in	_	_
an	_	_
image	_	_
will	_	_
be	_	_
classified	_	_
and	_	_
also	_	_
located	_	_
.	_	_

#143
This	_	_
task	_	_
,	_	_
called	_	_
Multi-Class	_	_
Localization	_	_
,	_	_
can	_	_
be	_	_
tackled	_	_
with	_	_
DCNNs	_	_
by	_	_
fusing	_	_
the	_	_
two	_	_
architectures	_	_
that	_	_
we	_	_
have	_	_
seen	_	_
previously	_	_
.	_	_

#144
The	_	_
resulting	_	_
architecture	_	_
will	_	_
then	_	_
consist	_	_
of	_	_
the	_	_
network	_	_
stem	_	_
,	_	_
as	_	_
well	_	_
as	_	_
one	_	_
output	_	_
layer	_	_
and	_	_
corresponding	_	_
fully-connected	_	_
layers	_	_
for	_	_
each	_	_
of	_	_
the	_	_
two	_	_
subtasks	_	_
,	_	_
which	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
6	_	_
.	_	_

#145
C.	_	_
Detection	_	_
In	_	_
contrast	_	_
to	_	_
multi-class	_	_
localization	_	_
,	_	_
the	_	_
number	_	_
of	_	_
objects	_	_
in	_	_
a	_	_
given	_	_
image	_	_
is	_	_
not	_	_
known	_	_
prior	_	_
to	_	_
the	_	_
execution	_	_
,	_	_
when	_	_
performing	_	_
Object	_	_
Detection	_	_
.	_	_

#146
In	_	_
order	_	_
to	_	_
use	_	_
DCNNs	_	_
for	_	_
such	_	_
object	_	_
detection	_	_
tasks	_	_
,	_	_
the	_	_
architecture	_	_
needs	_	_
to	_	_
be	_	_
extended	_	_
to	_	_
handle	_	_
the	_	_
flexible	_	_
amount	_	_
of	_	_
detections	_	_
.	_	_

#147
For	_	_
this	_	_
purpose	_	_
,	_	_
multiple	_	_
detection	_	_
methods	_	_
have	_	_
been	_	_
developed	_	_
,	_	_
which	_	_
will	_	_
be	_	_
inspected	_	_
in	_	_
the	_	_
following	_	_
.	_	_

#148
1	_	_
)	_	_
R-CNNs	_	_
:	_	_
One	_	_
possible	_	_
way	_	_
of	_	_
predicting	_	_
object	_	_
detections	_	_
in	_	_
an	_	_
image	_	_
is	_	_
by	_	_
feeding	_	_
a	_	_
large	_	_
number	_	_
of	_	_
image	_	_
parts	_	_
from	_	_
the	_	_
original	_	_
image	_	_
into	_	_
a	_	_
DCNN	_	_
,	_	_
which	_	_
is	_	_
then	_	_
performing	_	_
multi-class	_	_
localization	_	_
to	_	_
locate	_	_
and	_	_
classify	_	_
the	_	_
main	_	_
object	_	_
in	_	_
it	_	_
.	_	_

#149
Instead	_	_
of	_	_
selecting	_	_
these	_	_
image	_	_
parts	_	_
randomly	_	_
,	_	_
Region-based	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
(	_	_
RCNNs	_	_
)	_	_
[	_	_
20	_	_
]	_	_
are	_	_
using	_	_
Region	_	_
Proposal	_	_
Networks	_	_
to	_	_
only	_	_
extract	_	_
potentially	_	_
interesting	_	_
regions	_	_
.	_	_

#150
These	_	_
regions	_	_
are	_	_
called	_	_
Regions	_	_
of	_	_
Interest	_	_
(	_	_
ROIs	_	_
)	_	_
.	_	_

#151
They	_	_
are	_	_
obtained	_	_
by	_	_
running	_	_
a	_	_
quick	_	_
segmentation	_	_
to	_	_
spot	_	_
blob-like	_	_
structures	_	_
.	_	_

#152
[	_	_
21	_	_
]	_	_
proposed	_	_
methods	_	_
for	_	_
drastically	_	_
reducing	_	_
the	_	_
execution	_	_
times	_	_
of	_	_
R-CNNs	_	_
,	_	_
which	_	_
was	_	_
achieved	_	_
by	_	_
performing	_	_
only	_	_
one	_	_
forward	_	_
pass	_	_
through	_	_
the	_	_
network	_	_
for	_	_
each	_	_
image	_	_
and	_	_
by	_	_
merging	_	_
the	_	_
modules	_	_
for	_	_
classification	_	_
and	_	_
bounding	_	_
box	_	_
regression	_	_
into	_	_
one	_	_
single	_	_
network	_	_
.	_	_

#153
[	_	_
22	_	_
]	_	_
managed	_	_
to	_	_
further	_	_
improve	_	_
the	_	_
R-CNN	_	_
execution	_	_
times	_	_
by	_	_
integrating	_	_
the	_	_
region	_	_
proposal	_	_
network	_	_
into	_	_
the	_	_
remaining	_	_
network	_	_
as	_	_
well	_	_
,	_	_
making	_	_
the	_	_
resulting	_	_
Faster	_	_
R-CNN	_	_
network	_	_
able	_	_
to	_	_
learn	_	_
end-to-end	_	_
from	_	_
the	_	_
raw	_	_
pixels	_	_
of	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#154
The	_	_
structure	_	_
of	_	_
such	_	_
a	_	_
faster	_	_
R-CNN	_	_
model	_	_
is	_	_
displayed	_	_
in	_	_
Fig.	_	_
7	_	_
.	_	_

#155
2	_	_
)	_	_
R-FCN	_	_
:	_	_
Faster	_	_
R-CNN	_	_
architectures	_	_
are	_	_
able	_	_
to	_	_
achieve	_	_
strong	_	_
detection	_	_
results	_	_
but	_	_
are	_	_
also	_	_
very	_	_
complex	_	_
.	_	_

#156
Since	_	_
the	_	_
fully-connected	_	_
layers	_	_
at	_	_
the	_	_
end	_	_
of	_	_
the	_	_
network	_	_
were	_	_
shown	_	_
to	_	_
have	_	_
a	_	_
particularly	_	_
strong	_	_
impact	_	_
on	_	_
the	_	_
training	_	_
and	_	_
execution	_	_
time	_	_
,	_	_
a	_	_
new	_	_
ROI-based	_	_
detection	_	_
architecture	_	_
was	_	_
proposed	_	_
by	_	_
[	_	_
23	_	_
]	_	_
,	_	_
called	_	_
Region-based	_	_
Fully	_	_
Convolutional	_	_
Networks	_	_
(	_	_
R-FCN	_	_
)	_	_
.	_	_

#157
These	_	_
networks	_	_
are	_	_
structured	_	_
very	_	_
similarly	_	_
to	_	_
the	_	_
faster	_	_
R-CNN	_	_
architecture	_	_
,	_	_
but	_	_
instead	_	_
of	_	_
using	_	_
fully-connected	_	_
modules	_	_
to	_	_
predict	_	_
classes	_	_
and	_	_
bounding	_	_
boxes	_	_
for	_	_
each	_	_
ROI	_	_
,	_	_
R-FCNs	_	_
use	_	_
Position-Sensitive	_	_
Convolutional	_	_
Fig.	_	_
6	_	_
.	_	_

#158
A	_	_
DCNN	_	_
architecture	_	_
for	_	_
multi-class	_	_
localization	_	_
.	_	_

#159
The	_	_
extracted	_	_
features	_	_
of	_	_
the	_	_
convolutional	_	_
and	_	_
pooling	_	_
layers	_	_
in	_	_
the	_	_
network	_	_
stem	_	_
are	_	_
used	_	_
to	_	_
simultaneously	_	_
locate	_	_
the	_	_
object	_	_
by	_	_
bounding	_	_
box	_	_
regression	_	_
and	_	_
classify	_	_
it	_	_
Modules	_	_
.	_	_

#160
Such	_	_
a	_	_
module	_	_
consists	_	_
of	_	_
a	_	_
convolutional	_	_
layer	_	_
with	_	_
k2	_	_
depth	_	_
slices	_	_
,	_	_
called	_	_
Score	_	_
Maps	_	_
,	_	_
where	_	_
each	_	_
score	_	_
map	_	_
represents	_	_
the	_	_
corresponding	_	_
part	_	_
of	_	_
the	_	_
ROI	_	_
and	_	_
computes	_	_
prediction	_	_
scores	_	_
for	_	_
each	_	_
class	_	_
,	_	_
as	_	_
well	_	_
as	_	_
a	_	_
pooling	_	_
layer	_	_
that	_	_
is	_	_
combining	_	_
the	_	_
information	_	_
of	_	_
the	_	_
convolutional	_	_
layer	_	_
into	_	_
a	_	_
k×	_	_
k×	_	_
1	_	_
set	_	_
of	_	_
neurons	_	_
that	_	_
each	_	_
correspond	_	_
to	_	_
one	_	_
of	_	_
the	_	_
k2	_	_
ROI	_	_
parts	_	_
and	_	_
contain	_	_
prediction	_	_
scores	_	_
for	_	_
all	_	_
classes	_	_
.	_	_

#161
At	_	_
the	_	_
end	_	_
,	_	_
the	_	_
scores	_	_
of	_	_
the	_	_
neurons	_	_
are	_	_
averaged	_	_
in	_	_
order	_	_
to	_	_
retrieve	_	_
the	_	_
final	_	_
prediction	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#162
The	_	_
structure	_	_
of	_	_
such	_	_
a	_	_
position-sensitive	_	_
convolutional	_	_
module	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
8	_	_
.	_	_

#163
3	_	_
)	_	_
YOLO	_	_
:	_	_
In	_	_
contrast	_	_
with	_	_
region	_	_
proposal	_	_
based	_	_
techniques	_	_
,	_	_
Single-Shot	_	_
detection	_	_
architectures	_	_
do	_	_
not	_	_
predict	_	_
any	_	_
regions	_	_
of	_	_
interests	_	_
,	_	_
but	_	_
instead	_	_
,	_	_
a	_	_
fixed	_	_
amount	_	_
of	_	_
detections	_	_
on	_	_
the	_	_
image	_	_
directly	_	_
,	_	_
which	_	_
are	_	_
then	_	_
filtered	_	_
to	_	_
contain	_	_
only	_	_
the	_	_
actual	_	_
detections	_	_
.	_	_

#164
These	_	_
networks	_	_
do	_	_
therefore	_	_
have	_	_
much	_	_
faster	_	_
execution	_	_
times	_	_
than	_	_
region-based	_	_
architectures	_	_
but	_	_
are	_	_
found	_	_
to	_	_
also	_	_
have	_	_
a	_	_
lower	_	_
detection	_	_
accuracy	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#165
YOLO	_	_
,	_	_
short	_	_
for	_	_
You	_	_
Only	_	_
Look	_	_
Once	_	_
,	_	_
is	_	_
a	_	_
very	_	_
simple	_	_
single-shot	_	_
detection	_	_
architecture	_	_
that	_	_
replaces	_	_
ROIs	_	_
by	_	_
performing	_	_
a	_	_
multi-box	_	_
bounding	_	_
box	_	_
regression	_	_
on	_	_
the	_	_
input	_	_
image	_	_
directly	_	_
[	_	_
24	_	_
]	_	_
.	_	_

#166
In	_	_
order	_	_
to	_	_
do	_	_
so	_	_
,	_	_
the	_	_
image	_	_
is	_	_
overlayed	_	_
by	_	_
a	_	_
grid	_	_
,	_	_
and	_	_
for	_	_
each	_	_
grid	_	_
cell	_	_
,	_	_
a	_	_
fixed	_	_
amount	_	_
of	_	_
detections	_	_
are	_	_
predicted	_	_
,	_	_
as	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
9	_	_
.	_	_

#167
Therefore	_	_
,	_	_
the	_	_
whole	_	_
network	_	_
has	_	_
to	_	_
be	_	_
evaluated	_	_
only	_	_
once	_	_
per	_	_
image	_	_
leading	_	_
to	_	_
very	_	_
fast	_	_
execution	_	_
times	_	_
that	_	_
are	_	_
well	_	_
suited	_	_
for	_	_
real-time	_	_
applications	_	_
.	_	_

#168
Additionally	_	_
,	_	_
since	_	_
the	_	_
model	_	_
works	_	_
on	_	_
whole	_	_
images	_	_
it	_	_
can	_	_
also	_	_
make	_	_
more	_	_
use	_	_
out	_	_
of	_	_
contextual	_	_
information	_	_
.	_	_

#169
However	_	_
,	_	_
since	_	_
YOLO	_	_
computes	_	_
a	_	_
fixed	_	_
amount	_	_
of	_	_
predictions	_	_
per	_	_
region	_	_
,	_	_
it	_	_
is	_	_
not	_	_
well	_	_
suited	_	_
for	_	_
tasks	_	_
where	_	_
many	_	_
objects	_	_
can	_	_
be	_	_
located	_	_
very	_	_
close	_	_
to	_	_
each	_	_
other	_	_
and	_	_
struggles	_	_
with	_	_
the	_	_
detection	_	_
of	_	_
objects	_	_
that	_	_
have	_	_
a	_	_
strong	_	_
variance	_	_
in	_	_
their	_	_
aspect	_	_
ratios	_	_
.	_	_

#170
4	_	_
)	_	_
SSD	_	_
:	_	_
In	_	_
order	_	_
to	_	_
handle	_	_
the	_	_
problems	_	_
of	_	_
YOLO	_	_
that	_	_
arise	_	_
due	_	_
to	_	_
the	_	_
fixed	_	_
amount	_	_
of	_	_
predictions	_	_
and	_	_
cell	_	_
sizes	_	_
,	_	_
Single	_	_
Shot	_	_
MultiBox	_	_
Detectors	_	_
(	_	_
SSD	_	_
)	_	_
have	_	_
been	_	_
developed	_	_
,	_	_
which	_	_
predict	_	_
detections	_	_
of	_	_
different	_	_
scales	_	_
and	_	_
also	_	_
make	_	_
predictions	_	_
for	_	_
multiple	_	_
different	_	_
aspect	_	_
ratios	_	_
.	_	_

#171
Therefore	_	_
,	_	_
SSD	_	_
detectors	_	_
can	_	_
make	_	_
finer	_	_
predictions	_	_
,	_	_
leading	_	_
to	_	_
significantly	_	_
better	_	_
results	_	_
.	_	_

#172
Similarly	_	_
to	_	_
YOLO	_	_
,	_	_
the	_	_
input	_	_
image	_	_
is	_	_
first	_	_
fed	_	_
into	_	_
a	_	_
convolutional	_	_
neural	_	_
network	_	_
,	_	_
but	_	_
instead	_	_
of	_	_
performing	_	_
bounding	_	_
box	_	_
regression	_	_
on	_	_
the	_	_
final	_	_
layer	_	_
,	_	_
SSDs	_	_
append	_	_
additional	_	_
convolutional	_	_
layers	_	_
that	_	_
gradually	_	_
decrease	_	_
in	_	_
size	_	_
.	_	_

#173
For	_	_
each	_	_
Fig.	_	_
7	_	_
.	_	_

#174
A	_	_
faster	_	_
R-CNN	_	_
architecture	_	_
for	_	_
object	_	_
detection	_	_
.	_	_

#175
A	_	_
region	_	_
proposal	_	_
network	_	_
is	_	_
using	_	_
the	_	_
features	_	_
computed	_	_
in	_	_
the	_	_
network	_	_
stem	_	_
to	_	_
produce	_	_
ROIs	_	_
,	_	_
on	_	_
which	_	_
classification	_	_
and	_	_
bounding	_	_
box	_	_
regression	_	_
are	_	_
performed	_	_
Fig.	_	_
8	_	_
.	_	_

#176
A	_	_
position-sensitive	_	_
convolutional	_	_
module	_	_
of	_	_
an	_	_
R-FCN	_	_
with	_	_
k	_	_
=	_	_
3	_	_
.	_	_

#177
The	_	_
first	_	_
convolutional	_	_
layer	_	_
consists	_	_
of	_	_
k2	_	_
=	_	_
9	_	_
depth	_	_
slices	_	_
corresponding	_	_
to	_	_
certain	_	_
locations	_	_
.	_	_

#178
A	_	_
given	_	_
ROI	_	_
is	_	_
then	_	_
split	_	_
into	_	_
k2	_	_
=	_	_
9	_	_
parts	_	_
and	_	_
for	_	_
each	_	_
part	_	_
information	_	_
from	_	_
the	_	_
corresponding	_	_
depth	_	_
slice	_	_
is	_	_
retrieved	_	_
and	_	_
pooled	_	_
.	_	_

#179
The	_	_
resulting	_	_
values	_	_
are	_	_
averaged	_	_
in	_	_
the	_	_
end	_	_
to	_	_
obtain	_	_
the	_	_
final	_	_
prediction	_	_
of	_	_
these	_	_
additional	_	_
layers	_	_
,	_	_
a	_	_
fixed	_	_
amount	_	_
of	_	_
predictions	_	_
with	_	_
diverse	_	_
aspect	_	_
ratios	_	_
are	_	_
computed	_	_
,	_	_
resulting	_	_
in	_	_
a	_	_
large	_	_
number	_	_
of	_	_
predictions	_	_
that	_	_
differ	_	_
heavily	_	_
across	_	_
size	_	_
and	_	_
aspect	_	_
ratio	_	_
.	_	_

#180
Therefore	_	_
SSDs	_	_
are	_	_
less	_	_
vulnerable	_	_
to	_	_
varying	_	_
occurrences	_	_
of	_	_
objects	_	_
,	_	_
leading	_	_
to	_	_
significantly	_	_
better	_	_
detections	_	_
than	_	_
YOLO	_	_
while	_	_
preserving	_	_
a	_	_
similarly	_	_
fast	_	_
execution	_	_
time	_	_
[	_	_
25	_	_
]	_	_
.	_	_

#181
An	_	_
example	_	_
SSD	_	_
architecture	_	_
,	_	_
differentiating	_	_
the	_	_
same	_	_
amount	_	_
of	_	_
aspect	_	_
ratios	_	_
as	_	_
the	_	_
original	_	_
paper	_	_
[	_	_
25	_	_
]	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
10	_	_
.	_	_

#182
5	_	_
)	_	_
YOLOv2	_	_
:	_	_
The	_	_
basic	_	_
YOLO	_	_
was	_	_
improved	_	_
on	_	_
by	_	_
[	_	_
26	_	_
]	_	_
,	_	_
who	_	_
released	_	_
the	_	_
second	_	_
version	_	_
of	_	_
YOLO	_	_
,	_	_
called	_	_
YOLOv2	_	_
.	_	_

#183
YOLOv2	_	_
contains	_	_
various	_	_
improvements	_	_
in	_	_
comparison	_	_
to	_	_
the	_	_
first	_	_
version	_	_
,	_	_
such	_	_
as	_	_
the	_	_
ability	_	_
to	_	_
predict	_	_
objects	_	_
at	_	_
different	_	_
resolutions	_	_
and	_	_
computing	_	_
first	_	_
bounding	_	_
box	_	_
predictions	_	_
by	_	_
clustering	_	_
.	_	_

#184
Additionally	_	_
,	_	_
the	_	_
input	_	_
size	_	_
is	_	_
repeatedly	_	_
changed	_	_
to	_	_
a	_	_
random	_	_
value	_	_
during	_	_
training	_	_
,	_	_
which	_	_
is	_	_
enabling	_	_
YOLOv2	_	_
to	_	_
perform	_	_
good	_	_
predictions	_	_
across	_	_
various	_	_
resolutions	_	_
.	_	_

#185
As	_	_
a	_	_
result	_	_
,	_	_
YOLOv2	_	_
is	_	_
able	_	_
to	_	_
achieve	_	_
significantly	_	_
better	_	_
detection	_	_
results	_	_
than	_	_
YOLO	_	_
and	_	_
was	_	_
reported	_	_
[	_	_
26	_	_
]	_	_
to	_	_
have	_	_
an	_	_
even	_	_
better	_	_
performance	_	_
than	_	_
the	_	_
SSD	_	_
detector	_	_
.	_	_

#186
IV	_	_
.	_	_

#187
DCNN	_	_
ARCHITECTURES	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
present	_	_
the	_	_
most	_	_
influential	_	_
DCNN	_	_
architectures	_	_
that	_	_
have	_	_
shaped	_	_
the	_	_
current	_	_
state-of-the-art	_	_
in	_	_
object	_	_
recognition	_	_
.	_	_

#188
Most	_	_
of	_	_
the	_	_
architectures	_	_
became	_	_
famous	_	_
by	_	_
winning	_	_
the	_	_
ImageNet	_	_
Large	_	_
Scale	_	_
Visual	_	_
Recognition	_	_
Competition	_	_
(	_	_
ILSVRC	_	_
)	_	_
3	_	_
at	_	_
some	_	_
point	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#189
By	_	_
inspecting	_	_
the	_	_
very	_	_
best	_	_
architectures	_	_
of	_	_
each	_	_
year	_	_
in	_	_
a	_	_
chronological	_	_
order	_	_
,	_	_
3More	_	_
information	_	_
on	_	_
the	_	_
ILSVRC	_	_
can	_	_
be	_	_
found	_	_
here	_	_
:	_	_
http	_	_
:	_	_
//image-net.org/challenges/LSVRC	_	_
Fig.	_	_
9	_	_
.	_	_

#190
The	_	_
YOLO	_	_
detection	_	_
architecture	_	_
.	_	_

#191
The	_	_
input	_	_
image	_	_
is	_	_
overlayed	_	_
by	_	_
a	_	_
grid	_	_
consisting	_	_
of	_	_
S	_	_
×	_	_
S	_	_
cells	_	_
.	_	_

#192
Afterwards	_	_
,	_	_
a	_	_
DCNN	_	_
is	_	_
predicting	_	_
B	_	_
bounding	_	_
boxes	_	_
and	_	_
C	_	_
classes	_	_
per	_	_
grid	_	_
cell	_	_
.	_	_

#193
The	_	_
results	_	_
are	_	_
thresholded	_	_
to	_	_
obtain	_	_
the	_	_
final	_	_
predictions	_	_
Fig.	_	_
10	_	_
.	_	_

#194
The	_	_
SSD	_	_
detection	_	_
architecture	_	_
.	_	_

#195
Multiple	_	_
convolutional	_	_
layers	_	_
(	_	_
feature	_	_
maps	_	_
)	_	_
of	_	_
decreasing	_	_
size	_	_
are	_	_
appended	_	_
to	_	_
the	_	_
DCNN	_	_
.	_	_

#196
For	_	_
each	_	_
feature	_	_
map	_	_
,	_	_
a	_	_
certain	_	_
amount	_	_
of	_	_
detections	_	_
per	_	_
class	_	_
are	_	_
made	_	_
with	_	_
varying	_	_
aspect	_	_
ratios	_	_
and	_	_
the	_	_
results	_	_
are	_	_
thresholded	_	_
we	_	_
will	_	_
also	_	_
understand	_	_
how	_	_
fast	_	_
the	_	_
field	_	_
is	_	_
advancing	_	_
and	_	_
which	_	_
trends	_	_
and	_	_
new	_	_
approaches	_	_
have	_	_
been	_	_
developed	_	_
in	_	_
each	_	_
year	_	_
.	_	_

#197
A.	_	_
LeNet-5	_	_
(	_	_
1998	_	_
)	_	_
Most	_	_
of	_	_
the	_	_
DCNNs	_	_
that	_	_
are	_	_
being	_	_
used	_	_
for	_	_
object	_	_
recognition	_	_
today	_	_
are	_	_
based	_	_
on	_	_
the	_	_
basic	_	_
architecture	_	_
that	_	_
was	_	_
developed	_	_
by	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#198
This	_	_
architecture	_	_
is	_	_
known	_	_
as	_	_
LeNet-5	_	_
,	_	_
which	_	_
was	_	_
used	_	_
to	_	_
read	_	_
digits	_	_
from	_	_
32×	_	_
32	_	_
pixel	_	_
images	_	_
.	_	_

#199
The	_	_
basic	_	_
architecture	_	_
of	_	_
the	_	_
LeNet-5	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
11	_	_
.	_	_

#200
As	_	_
can	_	_
be	_	_
seen	_	_
,	_	_
the	_	_
network	_	_
architecture	_	_
is	_	_
relatively	_	_
simple	_	_
,	_	_
as	_	_
it	_	_
only	_	_
consists	_	_
of	_	_
an	_	_
input	_	_
layer	_	_
of	_	_
size	_	_
32×	_	_
32	_	_
,	_	_
an	_	_
output	_	_
layer	_	_
of	_	_
size	_	_
10	_	_
,	_	_
as	_	_
well	_	_
as	_	_
three	_	_
5	_	_
×	_	_
5	_	_
convolutional	_	_
,	_	_
two	_	_
2×2	_	_
pooling	_	_
and	_	_
one	_	_
fully-connected	_	_
layer	_	_
in	_	_
between	_	_
,	_	_
making	_	_
it	_	_
a	_	_
total	_	_
of	_	_
six	_	_
hidden	_	_
layers	_	_
.	_	_

#201
Since	_	_
all	_	_
convolutional	_	_
and	_	_
pooling	_	_
layers	_	_
use	_	_
a	_	_
stride	_	_
of	_	_
one	_	_
and	_	_
no	_	_
padding	_	_
,	_	_
the	_	_
size	_	_
of	_	_
each	_	_
dimension	_	_
is	_	_
reduced	_	_
by	_	_
4	_	_
during	_	_
each	_	_
convolution	_	_
and	_	_
is	_	_
halved	_	_
by	_	_
each	_	_
pooling	_	_
operation	_	_
.	_	_

#202
The	_	_
general	_	_
idea	_	_
behind	_	_
the	_	_
design	_	_
is	_	_
to	_	_
perform	_	_
multiple	_	_
convolutions	_	_
with	_	_
max	_	_
pooling	_	_
between	_	_
two	_	_
operations	_	_
and	_	_
connecting	_	_
the	_	_
final	_	_
convolutional	_	_
layer	_	_
via	_	_
fully-connected	_	_
layers	_	_
to	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#203
This	_	_
is	_	_
exactly	_	_
the	_	_
default	_	_
classification	_	_
architecture	_	_
that	_	_
was	_	_
presented	_	_
in	_	_
the	_	_
previous	_	_
section	_	_
and	_	_
,	_	_
as	_	_
we’ll	_	_
show	_	_
later	_	_
,	_	_
this	_	_
idea	_	_
has	_	_
been	_	_
the	_	_
basis	_	_
for	_	_
most	_	_
of	_	_
the	_	_
other	_	_
networks	_	_
in	_	_
this	_	_
section	_	_
as	_	_
well	_	_
.	_	_

#204
B.	_	_
AlexNet	_	_
(	_	_
2012	_	_
)	_	_
The	_	_
AlexNet	_	_
,	_	_
developed	_	_
by	_	_
[	_	_
8	_	_
]	_	_
,	_	_
is	_	_
potentially	_	_
the	_	_
most	_	_
influential	_	_
implementation	_	_
of	_	_
DCNNs	_	_
up	_	_
to	_	_
date	_	_
.	_	_

#205
It	_	_
was	_	_
the	_	_
first	_	_
DCNN	_	_
that	_	_
managed	_	_
to	_	_
beat	_	_
more	_	_
traditional	_	_
object	_	_
recognition	_	_
approaches	_	_
in	_	_
the	_	_
ILSVRC	_	_
.	_	_

#206
Moreover	_	_
,	_	_
the	_	_
AlexNet	_	_
proved	_	_
the	_	_
viability	_	_
of	_	_
DCNN	_	_
approaches	_	_
for	_	_
object	_	_
recognition	_	_
tasks	_	_
.	_	_

#207
The	_	_
corresponding	_	_
network	_	_
architecture	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
12	_	_
.	_	_

#208
As	_	_
we	_	_
can	_	_
see	_	_
,	_	_
the	_	_
AlexNet	_	_
is	_	_
not	_	_
much	_	_
different	_	_
from	_	_
the	_	_
LeNet-	_	_
,	_	_
as	_	_
it	_	_
also	_	_
consists	_	_
of	_	_
only	_	_
the	_	_
input	_	_
layer	_	_
,	_	_
a	_	_
few	_	_
convolutional	_	_
layers	_	_
with	_	_
occasional	_	_
pooling	_	_
afterward	_	_
,	_	_
as	_	_
well	_	_
as	_	_
some	_	_
fully-connected	_	_
layers	_	_
right	_	_
before	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#209
However	_	_
,	_	_
the	_	_
AlexNet	_	_
has	_	_
more	_	_
layers	_	_
and	_	_
neurons	_	_
per	_	_
layer	_	_
and	_	_
it	_	_
also	_	_
uses	_	_
different	_	_
hyperparameters	_	_
,	_	_
as	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
12	_	_
.	_	_

#210
C.	_	_
ZFNet	_	_
and	_	_
OverFeat	_	_
(	_	_
2013	_	_
)	_	_
One	_	_
year	_	_
after	_	_
the	_	_
AlexNet	_	_
won	_	_
the	_	_
ILSVRC	_	_
in	_	_
2012	_	_
,	_	_
a	_	_
modification	_	_
of	_	_
it	_	_
,	_	_
the	_	_
ZFNet	_	_
,	_	_
still	_	_
achieved	_	_
best	_	_
results	_	_
,	_	_
winning	_	_
the	_	_
ILSVRC	_	_
again	_	_
.	_	_

#211
[	_	_
27	_	_
]	_	_
developed	_	_
a	_	_
novel	_	_
technique	_	_
for	_	_
visualizing	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
called	_	_
Deconvolutional	_	_
Network	_	_
.	_	_

#212
This	_	_
network	_	_
does	_	_
the	_	_
exact	_	_
opposite	_	_
of	_	_
a	_	_
CNN	_	_
,	_	_
mapping	_	_
features	_	_
to	_	_
pixels	_	_
,	_	_
and	_	_
is	_	_
nowadays	_	_
also	_	_
frequently	_	_
used	_	_
in	_	_
combination	_	_
with	_	_
CNNs	_	_
for	_	_
generative	_	_
tasks	_	_
[	_	_
28	_	_
]	_	_
.	_	_

#213
Visualizing	_	_
the	_	_
AlexNet	_	_
with	_	_
it	_	_
enabled	_	_
them	_	_
to	_	_
improve	_	_
it	_	_
by	_	_
tuning	_	_
its	_	_
hyperparameters	_	_
and	_	_
increasing	_	_
the	_	_
number	_	_
of	_	_
filters	_	_
in	_	_
the	_	_
later	_	_
convolutional	_	_
layers	_	_
.	_	_

#214
The	_	_
winner	_	_
of	_	_
the	_	_
2013	_	_
ILSVRC	_	_
localization	_	_
challenge	_	_
was	_	_
an	_	_
architecture	_	_
named	_	_
OverFeat	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#215
OverFeat	_	_
used	_	_
the	_	_
exact	_	_
version	_	_
of	_	_
the	_	_
AlexNet	_	_
that	_	_
won	_	_
the	_	_
classification	_	_
challenge	_	_
in	_	_
2012	_	_
and	_	_
altered	_	_
it	_	_
in	_	_
a	_	_
novel	_	_
way	_	_
to	_	_
perform	_	_
the	_	_
bounding	_	_
box	_	_
regression	_	_
.	_	_

#216
Instead	_	_
of	_	_
only	_	_
predicting	_	_
bounding	_	_
boxes	_	_
once	_	_
per	_	_
image	_	_
,	_	_
as	_	_
was	_	_
suggested	_	_
in	_	_
Sect.	_	_
III-B	_	_
,	_	_
OverFeat	_	_
tried	_	_
to	_	_
localize	_	_
a	_	_
given	_	_
object	_	_
at	_	_
multiple	_	_
locations	_	_
and	_	_
scales	_	_
and	_	_
merges	_	_
these	_	_
outputs	_	_
to	_	_
obtain	_	_
the	_	_
final	_	_
results	_	_
.	_	_

#217
D.	_	_
VGGNet	_	_
and	_	_
GoogLeNet	_	_
(	_	_
2014	_	_
)	_	_
Another	_	_
influential	_	_
network	_	_
was	_	_
developed	_	_
by	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#218
Their	_	_
implementation	_	_
,	_	_
the	_	_
VGGNet	_	_
,	_	_
scored	_	_
second	_	_
place	_	_
in	_	_
the	_	_
ILSVRC	_	_
and	_	_
influenced	_	_
the	_	_
deep	_	_
learning	_	_
scene	_	_
in	_	_
an	_	_
important	_	_
way	_	_
,	_	_
as	_	_
they	_	_
showed	_	_
that	_	_
using	_	_
a	_	_
deeper	_	_
architecture	_	_
does	_	_
generally	_	_
lead	_	_
to	_	_
better	_	_
results	_	_
,	_	_
which	_	_
was	_	_
not	_	_
obvious	_	_
at	_	_
that	_	_
time	_	_
.	_	_

#219
The	_	_
VGGNet	_	_
that	_	_
was	_	_
submitted	_	_
for	_	_
the	_	_
ILSVRC	_	_
contained	_	_
19	_	_
parametrized	_	_
hidden	_	_
layers	_	_
,	_	_
which	_	_
was	_	_
much	_	_
more	_	_
than	_	_
what	_	_
previous	_	_
architectures	_	_
had	_	_
used	_	_
.	_	_

#220
Apart	_	_
from	_	_
its	_	_
size	_	_
,	_	_
the	_	_
VGGNet	_	_
was	_	_
very	_	_
simple	_	_
.	_	_

#221
It	_	_
only	_	_
consisted	_	_
of	_	_
convolutional	_	_
layers	_	_
with	_	_
a	_	_
3×	_	_
3	_	_
receptive	_	_
field	_	_
,	_	_
which	_	_
is	_	_
the	_	_
smallest	_	_
size	_	_
that	_	_
can	_	_
differentiate	_	_
basic	_	_
directions	_	_
,	_	_
as	_	_
well	_	_
as	_	_
2×	_	_
2	_	_
max	_	_
pooling	_	_
layers	_	_
,	_	_
and	_	_
three	_	_
fully-connected	_	_
layers	_	_
at	_	_
the	_	_
end	_	_
.	_	_

#222
A	_	_
scheme	_	_
of	_	_
the	_	_
VGGNet	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
13	_	_
.	_	_

#223
In	_	_
the	_	_
same	_	_
year	_	_
,	_	_
Szegedy	_	_
et	_	_
al.	_	_
from	_	_
Google	_	_
won	_	_
the	_	_
ILSVRC	_	_
with	_	_
a	_	_
different	_	_
very	_	_
deep	_	_
network	_	_
that	_	_
had	_	_
22	_	_
parametrized	_	_
layers	_	_
-	_	_
even	_	_
more	_	_
than	_	_
the	_	_
VGGNet	_	_
-	_	_
and	_	_
was	_	_
named	_	_
GoogLeNet	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#224
This	_	_
network	_	_
was	_	_
an	_	_
improvement	_	_
of	_	_
the	_	_
AlexNet	_	_
that	_	_
was	_	_
not	_	_
only	_	_
much	_	_
deeper	_	_
but	_	_
also	_	_
reduced	_	_
the	_	_
number	_	_
of	_	_
parameters	_	_
.	_	_

#225
The	_	_
latter	_	_
was	_	_
achieved	_	_
by	_	_
replacing	_	_
the	_	_
first	_	_
fully-connected	_	_
layer	_	_
,	_	_
which	_	_
is	_	_
typically	_	_
accountable	_	_
for	_	_
the	_	_
highest	_	_
number	_	_
of	_	_
parameters	_	_
,	_	_
by	_	_
another	_	_
convolutional	_	_
layer	_	_
.	_	_

#226
In	_	_
addition	_	_
to	_	_
that	_	_
,	_	_
they	_	_
also	_	_
implemented	_	_
the	_	_
so-called	_	_
Inception	_	_
Modules	_	_
,	_	_
which	_	_
enable	_	_
a	_	_
network	_	_
to	_	_
recognize	_	_
patterns	_	_
of	_	_
different	_	_
sizes	_	_
within	_	_
the	_	_
same	_	_
layer	_	_
.	_	_

#227
In	_	_
order	_	_
to	_	_
do	_	_
so	_	_
,	_	_
the	_	_
inception	_	_
module	_	_
performs	_	_
several	_	_
convolutions	_	_
with	_	_
different	_	_
receptive	_	_
fields	_	_
in	_	_
parallel	_	_
and	_	_
combines	_	_
the	_	_
results	_	_
by	_	_
merging	_	_
the	_	_
depth	_	_
slices	_	_
of	_	_
the	_	_
different	_	_
filters	_	_
into	_	_
one	_	_
single	_	_
layer	_	_
.	_	_

#228
Such	_	_
an	_	_
inception	_	_
module	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
14	_	_
.	_	_

#229
The	_	_
final	_	_
GoogLeNet	_	_
consisted	_	_
of	_	_
several	_	_
such	_	_
inception	_	_
modules	_	_
stacked	_	_
on	_	_
top	_	_
of	_	_
each	_	_
other	_	_
with	_	_
occasional	_	_
pooling	_	_
layers	_	_
in	_	_
between	_	_
,	_	_
a	_	_
few	_	_
additional	_	_
convolutional	_	_
layers	_	_
in	_	_
the	_	_
beginning	_	_
of	_	_
the	_	_
network	_	_
and	_	_
a	_	_
few	_	_
fully-connected	_	_
layers	_	_
right	_	_
before	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#230
This	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
15	_	_
.	_	_

#231
The	_	_
GoogLeNet	_	_
also	_	_
contained	_	_
additional	_	_
output	_	_
layers	_	_
closer	_	_
to	_	_
the	_	_
middle	_	_
of	_	_
the	_	_
network	_	_
and	_	_
their	_	_
outputs	_	_
were	_	_
combined	_	_
with	_	_
the	_	_
output	_	_
of	_	_
the	_	_
final	_	_
layer	_	_
of	_	_
the	_	_
network	_	_
to	_	_
obtain	_	_
the	_	_
total	_	_
prediction	_	_
.	_	_

#232
This	_	_
had	_	_
some	_	_
minor	_	_
influence	_	_
on	_	_
the	_	_
overall	_	_
result	_	_
but	_	_
was	_	_
mainly	_	_
intended	_	_
to	_	_
accelerate	_	_
the	_	_
training	_	_
of	_	_
earlier	_	_
layers	_	_
.	_	_

#233
E.	_	_
ResNet	_	_
(	_	_
2015	_	_
)	_	_
[	_	_
32	_	_
]	_	_
developed	_	_
a	_	_
new	_	_
kind	_	_
of	_	_
network	_	_
architecture	_	_
,	_	_
which	_	_
pushed	_	_
the	_	_
depth	_	_
boundaries	_	_
of	_	_
DCNNs	_	_
even	_	_
further	_	_
.	_	_

#234
Their	_	_
network	_	_
,	_	_
called	_	_
Deep	_	_
Residual	_	_
Network	_	_
,	_	_
or	_	_
ResNet	_	_
for	_	_
short	_	_
,	_	_
is	_	_
able	_	_
to	_	_
perform	_	_
much	_	_
better	_	_
with	_	_
very	_	_
deep	_	_
architectures	_	_
.	_	_

#235
In	_	_
ResNets	_	_
,	_	_
convolutional	_	_
layers	_	_
are	_	_
divided	_	_
into	_	_
Residual	_	_
Blocks	_	_
and	_	_
for	_	_
each	_	_
block	_	_
a	_	_
Residual	_	_
Connections	_	_
is	_	_
added	_	_
,	_	_
which	_	_
is	_	_
bypassing	_	_
the	_	_
corresponding	_	_
block	_	_
.	_	_

#236
Afterwards	_	_
,	_	_
the	_	_
output	_	_
of	_	_
the	_	_
residual	_	_
block	_	_
is	_	_
merged	_	_
by	_	_
summation	_	_
with	_	_
the	_	_
original	_	_
Fig.	_	_
11	_	_
.	_	_

#237
The	_	_
architecture	_	_
of	_	_
the	_	_
LeNet-5	_	_
.	_	_

#238
The	_	_
network	_	_
consists	_	_
of	_	_
an	_	_
input	_	_
layer	_	_
,	_	_
a	_	_
convolutional	_	_
layer	_	_
,	_	_
a	_	_
pooling	_	_
layer	_	_
,	_	_
a	_	_
second	_	_
convolutional	_	_
layer	_	_
,	_	_
another	_	_
pooling	_	_
layer	_	_
,	_	_
another	_	_
convolutional	_	_
layer	_	_
,	_	_
a	_	_
fully-connected	_	_
layer	_	_
and	_	_
an	_	_
output	_	_
layer	_	_
from	_	_
left	_	_
to	_	_
right	_	_
Fig.	_	_
12	_	_
.	_	_

#239
The	_	_
AlexNet	_	_
,	_	_
containing	_	_
five	_	_
convolutional	_	_
,	_	_
three	_	_
pooling	_	_
and	_	_
two	_	_
fully-connected	_	_
layers	_	_
between	_	_
input	_	_
and	_	_
output	_	_
layers	_	_
input	_	_
that	_	_
was	_	_
forwarded	_	_
by	_	_
the	_	_
residual	_	_
connection	_	_
,	_	_
as	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Fig.	_	_
16	_	_
.	_	_

#240
By	_	_
adding	_	_
these	_	_
residual	_	_
connections	_	_
,	_	_
the	_	_
result	_	_
of	_	_
a	_	_
training	_	_
step	_	_
can	_	_
be	_	_
backpropagated	_	_
to	_	_
the	_	_
earlier	_	_
layers	_	_
directly	_	_
,	_	_
without	_	_
any	_	_
interference	_	_
from	_	_
subsequent	_	_
layers	_	_
.	_	_

#241
Therefore	_	_
,	_	_
residual	_	_
connections	_	_
enable	_	_
the	_	_
training	_	_
of	_	_
even	_	_
deeper	_	_
networks	_	_
.	_	_

#242
Previously	_	_
,	_	_
having	_	_
more	_	_
layers	_	_
did	_	_
not	_	_
guarantee	_	_
a	_	_
better	_	_
accuracy	_	_
,	_	_
which	_	_
was	_	_
caused	_	_
by	_	_
earlier	_	_
layers	_	_
not	_	_
adapting	_	_
properly	_	_
if	_	_
a	_	_
network	_	_
was	_	_
getting	_	_
too	_	_
big	_	_
.	_	_

#243
As	_	_
a	_	_
result	_	_
,	_	_
He	_	_
et	_	_
al.	_	_
won	_	_
both	_	_
the	_	_
ILSVRC	_	_
localization	_	_
and	_	_
classification	_	_
contests	_	_
,	_	_
as	_	_
well	_	_
as	_	_
the	_	_
COCO	_	_
detection	_	_
and	_	_
segmentation	_	_
challenges	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#244
They	_	_
also	_	_
managed	_	_
to	_	_
improve	_	_
on	_	_
the	_	_
previous	_	_
error	_	_
rates	_	_
by	_	_
a	_	_
big	_	_
margin	_	_
.	_	_

#245
The	_	_
ResNet	_	_
version	_	_
that	_	_
was	_	_
submitted	_	_
to	_	_
these	_	_
contests	_	_
was	_	_
the	_	_
Resnet101	_	_
,	_	_
which	_	_
consists	_	_
of	_	_
101	_	_
parametrized	_	_
layers	_	_
.	_	_

#246
These	_	_
101	_	_
layers	_	_
consist	_	_
of	_	_
an	_	_
initial	_	_
7×7	_	_
convolutional	_	_
layer	_	_
with	_	_
a	_	_
2×2	_	_
stride	_	_
,	_	_
33	_	_
residual	_	_
building	_	_
blocks	_	_
with	_	_
decreasing	_	_
output	_	_
size	_	_
and	_	_
increasing	_	_
depth	_	_
and	_	_
a	_	_
final	_	_
1000	_	_
neuron	_	_
fully-connected	_	_
layer	_	_
[	_	_
32	_	_
]	_	_
.	_	_

#247
In	_	_
addition	_	_
to	_	_
the	_	_
101	_	_
parametrized	_	_
layers	_	_
,	_	_
the	_	_
ResNet101	_	_
also	_	_
includes	_	_
one	_	_
max	_	_
pooling	_	_
layer	_	_
after	_	_
the	_	_
first	_	_
convolutional	_	_
layer	_	_
and	_	_
an	_	_
average	_	_
pooling	_	_
layer	_	_
before	_	_
the	_	_
final	_	_
fully-connected	_	_
layer	_	_
.	_	_

#248
The	_	_
overall	_	_
structure	_	_
of	_	_
the	_	_
ResNet101	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
17	_	_
.	_	_

#249
Note	_	_
that	_	_
the	_	_
first	_	_
convolutional	_	_
layer	_	_
of	_	_
each	_	_
group	_	_
of	_	_
blocks	_	_
uses	_	_
stride	_	_
two	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
the	_	_
output	_	_
size	_	_
reduction	_	_
.	_	_

#250
F.	_	_
Inception-v4	_	_
,	_	_
Inception-ResNet-v1	_	_
and	_	_
ResNeXt	_	_
(	_	_
2016	_	_
)	_	_
Early	_	_
in	_	_
2016	_	_
,	_	_
a	_	_
new	_	_
modification	_	_
of	_	_
the	_	_
GoogLeNet	_	_
has	_	_
been	_	_
released	_	_
by	_	_
[	_	_
34	_	_
]	_	_
.	_	_

#251
This	_	_
network	_	_
,	_	_
called	_	_
Inception-v4	_	_
,	_	_
is	_	_
the	_	_
fourth	_	_
iteration	_	_
of	_	_
the	_	_
GoogLeNet	_	_
and	_	_
consisted	_	_
of	_	_
many	_	_
more	_	_
layers	_	_
than	_	_
the	_	_
original	_	_
version	_	_
.	_	_

#252
During	_	_
the	_	_
continuous	_	_
improvements	_	_
on	_	_
the	_	_
inception	_	_
architectures	_	_
,	_	_
the	_	_
inception	_	_
modules	_	_
,	_	_
as	_	_
introduced	_	_
in	_	_
Sect.	_	_
IV-D	_	_
,	_	_
have	_	_
been	_	_
vastly	_	_
imFig.	_	_
13	_	_
.	_	_

#253
The	_	_
VGGNet	_	_
,	_	_
containing	_	_
a	_	_
total	_	_
of	_	_
24	_	_
hidden	_	_
layers	_	_
,	_	_
consisting	_	_
of	_	_
16	_	_
3	_	_
×	_	_
3	_	_
convolutional	_	_
layers	_	_
,	_	_
five	_	_
2	_	_
×	_	_
2	_	_
max	_	_
pooling	_	_
layers	_	_
,	_	_
as	_	_
well	_	_
as	_	_
three	_	_
fully-connected	_	_
layers	_	_
Fig.	_	_
14	_	_
.	_	_

#254
An	_	_
inception	_	_
module	_	_
,	_	_
performing	_	_
three	_	_
convolutions	_	_
with	_	_
different	_	_
receptive	_	_
fields	_	_
,	_	_
as	_	_
well	_	_
as	_	_
a	_	_
pooling	_	_
operation	_	_
in	_	_
parallel	_	_
,	_	_
as	_	_
first	_	_
presented	_	_
by	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#255
The	_	_
results	_	_
of	_	_
the	_	_
different	_	_
operations	_	_
are	_	_
combined	_	_
by	_	_
a	_	_
depthwise	_	_
filter	_	_
concatenation	_	_
to	_	_
obtain	_	_
the	_	_
final	_	_
output	_	_
of	_	_
the	_	_
module	_	_
proved	_	_
as	_	_
well	_	_
and	_	_
the	_	_
Inception-v4	_	_
uses	_	_
three	_	_
different	_	_
kinds	_	_
of	_	_
inception	_	_
modules	_	_
.	_	_

#256
In	_	_
addition	_	_
to	_	_
the	_	_
Inception-v4	_	_
,	_	_
the	_	_
corresponding	_	_
paper	_	_
also	_	_
introduced	_	_
a	_	_
new	_	_
type	_	_
of	_	_
network	_	_
,	_	_
named	_	_
Inception-ResNet	_	_
,	_	_
which	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
an	_	_
inception	_	_
network	_	_
and	_	_
a	_	_
ResNet	_	_
,	_	_
by	_	_
combining	_	_
the	_	_
inception	_	_
module	_	_
and	_	_
residual	_	_
connection	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
18	_	_
.	_	_

#257
This	_	_
makes	_	_
the	_	_
network	_	_
even	_	_
more	_	_
efficient	_	_
,	_	_
leading	_	_
to	_	_
much	_	_
lower	_	_
training	_	_
times	_	_
compared	_	_
to	_	_
a	_	_
similarly	_	_
complex	_	_
inception	_	_
network	_	_
.	_	_

#258
Both	_	_
of	_	_
these	_	_
network	_	_
architectures	_	_
are	_	_
hundreds	_	_
of	_	_
layers	_	_
deep	_	_
and	_	_
contain	_	_
a	_	_
wide	_	_
variety	_	_
of	_	_
layers	_	_
,	_	_
inception	_	_
modules	_	_
and	_	_
residual	_	_
blocks	_	_
,	_	_
for	_	_
which	_	_
reason	_	_
we	_	_
will	_	_
not	_	_
inspect	_	_
their	_	_
structures	_	_
in	_	_
more	_	_
detail	_	_
4	_	_
.	_	_

#259
The	_	_
second	_	_
place	_	_
in	_	_
the	_	_
2016	_	_
ILSVRC	_	_
classification	_	_
challenge	_	_
was	_	_
achieved	_	_
by	_	_
[	_	_
35	_	_
]	_	_
,	_	_
who	_	_
proposed	_	_
a	_	_
new	_	_
network	_	_
architecture	_	_
,	_	_
named	_	_
ResNeXt	_	_
.	_	_

#260
Similarly	_	_
to	_	_
Inception-ResNets	_	_
,	_	_
the	_	_
ResNeXt	_	_
architecture	_	_
is	_	_
combining	_	_
the	_	_
residual	_	_
building	_	_
blocks	_	_
and	_	_
connections	_	_
of	_	_
ResNets	_	_
with	_	_
the	_	_
parallelization	_	_
strategy	_	_
of	_	_
inception	_	_
architectures	_	_
.	_	_

#261
Unlike	_	_
InceptionResNets	_	_
,	_	_
the	_	_
different	_	_
convolution	_	_
paths	_	_
that	_	_
are	_	_
concatenated	_	_
in	_	_
ResNeXts	_	_
have	_	_
similar	_	_
hyperparameters	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
19	_	_
.	_	_

#262
Therefore	_	_
,	_	_
the	_	_
number	_	_
of	_	_
paths	_	_
is	_	_
variable	_	_
and	_	_
the	_	_
paths	_	_
do	_	_
not	_	_
have	_	_
to	_	_
be	_	_
adapted	_	_
for	_	_
a	_	_
specific	_	_
purpose	_	_
,	_	_
which	_	_
facilitates	_	_
the	_	_
design	_	_
of	_	_
new	_	_
ResNext	_	_
variants	_	_
.	_	_

#263
Furthermore	_	_
,	_	_
the	_	_
flexibility	_	_
of	_	_
the	_	_
design	_	_
enables	_	_
ResNext	_	_
to	_	_
introduce	_	_
a	_	_
new	_	_
hyperparameter	_	_
called	_	_
Cardinality	_	_
,	_	_
which	_	_
specifies	_	_
how	_	_
many	_	_
parallel	_	_
convolution	_	_
paths	_	_
each	_	_
block	_	_
contains	_	_
.	_	_

#264
According	_	_
to	_	_
4The	_	_
exact	_	_
network	_	_
architecture	_	_
is	_	_
explained	_	_
in	_	_
detail	_	_
in	_	_
the	_	_
original	_	_
paper	_	_
by	_	_
[	_	_
34	_	_
]	_	_
.	_	_

#265
Fig.	_	_
15	_	_
.	_	_

#266
The	_	_
GoogLeNet	_	_
,	_	_
containing	_	_
nine	_	_
inception	_	_
modules	_	_
,	_	_
five	_	_
pooling	_	_
layers	_	_
,	_	_
three	_	_
convolutional	_	_
layers	_	_
after	_	_
the	_	_
input	_	_
layer	_	_
,	_	_
as	_	_
well	_	_
as	_	_
a	_	_
fully-connected	_	_
layer	_	_
before	_	_
the	_	_
output	_	_
layer	_	_
Fig.	_	_
16	_	_
.	_	_

#267
A	_	_
residual	_	_
block	_	_
,	_	_
consisting	_	_
of	_	_
two	_	_
convolutional	_	_
layers	_	_
,	_	_
and	_	_
the	_	_
corresponding	_	_
residual	_	_
connection	_	_
.	_	_

#268
The	_	_
input	_	_
to	_	_
the	_	_
residual	_	_
block	_	_
(	_	_
x	_	_
)	_	_
is	_	_
forwarded	_	_
by	_	_
the	_	_
residual	_	_
connection	_	_
and	_	_
later	_	_
added	_	_
to	_	_
the	_	_
output	_	_
of	_	_
the	_	_
residual	_	_
block	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
)	_	_
to	_	_
obtain	_	_
the	_	_
final	_	_
output	_	_
(	_	_
f	_	_
(	_	_
x	_	_
)	_	_
+	_	_
x	_	_
)	_	_
[	_	_
35	_	_
]	_	_
,	_	_
increasing	_	_
the	_	_
cardinality	_	_
of	_	_
a	_	_
network	_	_
has	_	_
a	_	_
stronger	_	_
influence	_	_
on	_	_
the	_	_
performance	_	_
of	_	_
a	_	_
network	_	_
than	_	_
increasing	_	_
the	_	_
number	_	_
of	_	_
layers	_	_
or	_	_
the	_	_
number	_	_
of	_	_
neurons	_	_
per	_	_
layer	_	_
,	_	_
for	_	_
which	_	_
reason	_	_
their	_	_
ResNext	_	_
managed	_	_
to	_	_
outperform	_	_
all	_	_
previous	_	_
Inception-ResNet	_	_
architectures	_	_
while	_	_
having	_	_
a	_	_
much	_	_
simpler	_	_
design	_	_
,	_	_
as	_	_
well	_	_
as	_	_
a	_	_
lower	_	_
complexity	_	_
.	_	_

#269
The	_	_
best	_	_
performing	_	_
approaches	_	_
on	_	_
the	_	_
2016	_	_
ILSVRC	_	_
classification	_	_
,	_	_
localization	_	_
and	_	_
detection	_	_
challenges	_	_
all	_	_
used	_	_
ensembles	_	_
5	_	_
of	_	_
ResNet101	_	_
,	_	_
Inception-v4	_	_
and	_	_
Inception-ResNet-v1	_	_
networks	_	_
.	_	_

#270
G.	_	_
Densenet	_	_
,	_	_
DPN	_	_
and	_	_
MobileNets	_	_
(	_	_
2017	_	_
)	_	_
As	_	_
described	_	_
in	_	_
Sect.	_	_
IV-E	_	_
,	_	_
the	_	_
enhanced	_	_
information	_	_
flow	_	_
of	_	_
residual	_	_
connections	_	_
in	_	_
ResNets	_	_
and	_	_
its	_	_
modifications	_	_
is	_	_
5Network	_	_
ensembles	_	_
will	_	_
be	_	_
inspected	_	_
more	_	_
closely	_	_
in	_	_
the	_	_
next	_	_
section	_	_
.	_	_

#271
enabling	_	_
the	_	_
training	_	_
of	_	_
much	_	_
deeper	_	_
networks	_	_
.	_	_

#272
Instead	_	_
of	_	_
summing	_	_
up	_	_
the	_	_
output	_	_
of	_	_
a	_	_
residual	_	_
connection	_	_
with	_	_
the	_	_
output	_	_
of	_	_
the	_	_
corresponding	_	_
residual	_	_
block	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
16	_	_
,	_	_
Dense	_	_
Convolutional	_	_
Networks	_	_
(	_	_
DenseNets	_	_
)	_	_
[	_	_
36	_	_
]	_	_
combine	_	_
the	_	_
two	_	_
outputs	_	_
by	_	_
depthwise	_	_
filter	_	_
concatenation	_	_
,	_	_
as	_	_
performed	_	_
in	_	_
inception	_	_
modules	_	_
.	_	_

#273
Furthermore	_	_
,	_	_
DenseNets	_	_
are	_	_
adding	_	_
one	_	_
such	_	_
connection	_	_
from	_	_
each	_	_
layer	_	_
to	_	_
all	_	_
subsequent	_	_
ones	_	_
with	_	_
matching	_	_
input	_	_
sizes	_	_
.	_	_

#274
By	_	_
doing	_	_
so	_	_
,	_	_
the	_	_
learned	_	_
features	_	_
of	_	_
a	_	_
layer	_	_
can	_	_
be	_	_
reused	_	_
by	_	_
any	_	_
of	_	_
the	_	_
following	_	_
layers	_	_
.	_	_

#275
Therefore	_	_
,	_	_
later	_	_
layers	_	_
need	_	_
to	_	_
produce	_	_
much	_	_
fewer	_	_
feature	_	_
maps	_	_
,	_	_
resulting	_	_
in	_	_
less	_	_
complex	_	_
architectures	_	_
with	_	_
fewer	_	_
parameters	_	_
.	_	_

#276
Since	_	_
the	_	_
width	_	_
and	_	_
height	_	_
of	_	_
layers	_	_
in	_	_
CNNs	_	_
are	_	_
gradually	_	_
decreasing	_	_
,	_	_
connecting	_	_
all	_	_
compatible	_	_
layers	_	_
is	_	_
dividing	_	_
the	_	_
network	_	_
into	_	_
Dense	_	_
Blocks	_	_
.	_	_

#277
Between	_	_
these	_	_
blocks	_	_
,	_	_
pooling	_	_
layers	_	_
are	_	_
used	_	_
to	_	_
alter	_	_
the	_	_
sizes	_	_
accordingly	_	_
.	_	_

#278
These	_	_
layers	_	_
are	_	_
referred	_	_
to	_	_
as	_	_
Transition	_	_
Layers	_	_
.	_	_

#279
An	_	_
example	_	_
of	_	_
a	_	_
dense	_	_
block	_	_
with	_	_
previous	_	_
and	_	_
subsequent	_	_
transition	_	_
layers	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
20	_	_
.	_	_

#280
Due	_	_
to	_	_
the	_	_
high	_	_
layer	_	_
interconnectivity	_	_
,	_	_
DenseNets	_	_
are	_	_
easy	_	_
to	_	_
train	_	_
and	_	_
naturally	_	_
scale	_	_
well	_	_
with	_	_
increasing	_	_
depth	_	_
and	_	_
increasing	_	_
amount	_	_
of	_	_
parameters	_	_
.	_	_

#281
According	_	_
to	_	_
[	_	_
37	_	_
]	_	_
,	_	_
the	_	_
design	_	_
of	_	_
DenseNets	_	_
encourages	_	_
the	_	_
learning	_	_
of	_	_
new	_	_
features	_	_
,	_	_
while	_	_
ResNet	_	_
architectures	_	_
is	_	_
leading	_	_
to	_	_
increased	_	_
feature	_	_
reuse	_	_
.	_	_

#282
Since	_	_
both	_	_
architectures	_	_
have	_	_
advantages	_	_
over	_	_
each	_	_
other	_	_
,	_	_
Chen	_	_
et	_	_
al.	_	_
combined	_	_
the	_	_
two	_	_
architectures	_	_
into	_	_
a	_	_
Dual	_	_
Path	_	_
Network	_	_
(	_	_
DPN	_	_
)	_	_
,	_	_
with	_	_
which	_	_
they	_	_
won	_	_
first	_	_
place	_	_
in	_	_
the	_	_
2017	_	_
ILSVRC	_	_
localization	_	_
challenge	_	_
and	_	_
finished	_	_
top	_	_
three	_	_
in	_	_
both	_	_
classification	_	_
and	_	_
detection	_	_
.	_	_

#283
In	_	_
order	_	_
to	_	_
combine	_	_
the	_	_
networks	_	_
,	_	_
the	_	_
output	_	_
of	_	_
a	_	_
layer	_	_
is	_	_
split	_	_
and	_	_
one	_	_
part	_	_
is	_	_
combined	_	_
with	_	_
a	_	_
residual	_	_
connection	_	_
,	_	_
whereas	_	_
the	_	_
other	_	_
is	_	_
forwarded	_	_
to	_	_
all	_	_
subsequent	_	_
Fig.	_	_
17	_	_
.	_	_

#284
The	_	_
ResNet101	_	_
,	_	_
consisting	_	_
of	_	_
a	_	_
7×	_	_
7	_	_
convolutional	_	_
layer	_	_
,	_	_
a	_	_
2×	_	_
2	_	_
max	_	_
pooling	_	_
layer	_	_
,	_	_
33	_	_
residual	_	_
blocks	_	_
,	_	_
a	_	_
7×	_	_
7	_	_
average	_	_
pooling	_	_
layer	_	_
and	_	_
a	_	_
final	_	_
fully-connected	_	_
layer	_	_
before	_	_
the	_	_
output	_	_
layer	_	_
Fig.	_	_
18	_	_
.	_	_

#285
A	_	_
building	_	_
block	_	_
of	_	_
an	_	_
Inception-ResNet	_	_
.	_	_

#286
The	_	_
input	_	_
(	_	_
x	_	_
)	_	_
is	_	_
processed	_	_
by	_	_
an	_	_
inception	_	_
module	_	_
and	_	_
also	_	_
forwarded	_	_
by	_	_
a	_	_
residual	_	_
connection	_	_
Fig.	_	_
19	_	_
.	_	_

#287
A	_	_
residual	_	_
block	_	_
of	_	_
a	_	_
ResNeXt	_	_
.	_	_

#288
Within	_	_
the	_	_
block	_	_
,	_	_
multiple	_	_
convolutions	_	_
with	_	_
similar	_	_
hyperparameters	_	_
are	_	_
performed	_	_
in	_	_
parallel	_	_
and	_	_
merged	_	_
by	_	_
filter	_	_
concatenation	_	_
layers	_	_
,	_	_
as	_	_
performed	_	_
in	_	_
DenseNets	_	_
.	_	_

#289
This	_	_
approach	_	_
is	_	_
illustrated	_	_
in	_	_
Fig.	_	_
21	_	_
.	_	_

#290
All	_	_
of	_	_
the	_	_
previous	_	_
architectures	_	_
we	_	_
inspected	_	_
were	_	_
designed	_	_
to	_	_
achieve	_	_
the	_	_
highest	_	_
accuracies	_	_
possible	_	_
,	_	_
for	_	_
which	_	_
reason	_	_
recent	_	_
architectures	_	_
have	_	_
become	_	_
ever	_	_
more	_	_
complex	_	_
.	_	_

#291
While	_	_
a	_	_
high	_	_
complexity	_	_
is	_	_
not	_	_
problematic	_	_
for	_	_
challenges	_	_
like	_	_
the	_	_
Fig.	_	_
20	_	_
.	_	_

#292
A	_	_
dense	_	_
block	_	_
of	_	_
a	_	_
DenseNet	_	_
together	_	_
with	_	_
the	_	_
previous	_	_
and	_	_
subsequent	_	_
transition	_	_
layers	_	_
.	_	_

#293
Every	_	_
layer	_	_
is	_	_
directly	_	_
connected	_	_
to	_	_
every	_	_
subsequent	_	_
one	_	_
Fig.	_	_
21	_	_
.	_	_

#294
Four	_	_
convolutional	_	_
layers	_	_
in	_	_
a	_	_
block	_	_
of	_	_
a	_	_
DPN	_	_
.	_	_

#295
Each	_	_
layer	_	_
has	_	_
a	_	_
corresponding	_	_
residual	_	_
connection	_	_
and	_	_
is	_	_
also	_	_
forwarding	_	_
its	_	_
output	_	_
to	_	_
all	_	_
subsequent	_	_
layers	_	_
.	_	_

#296
Before	_	_
each	_	_
layer	_	_
,	_	_
the	_	_
two	_	_
streams	_	_
of	_	_
information	_	_
are	_	_
merged	_	_
by	_	_
performing	_	_
addition	_	_
ILSVRC	_	_
,	_	_
it	_	_
is	_	_
very	_	_
cumbersome	_	_
for	_	_
real-time	_	_
applications	_	_
with	_	_
restricted	_	_
hardware	_	_
,	_	_
such	_	_
as	_	_
mobile	_	_
applications	_	_
or	_	_
embedded	_	_
systems	_	_
.	_	_

#297
Therefore	_	_
,	_	_
a	_	_
new	_	_
DCNN	_	_
architecture	_	_
was	_	_
introduced	_	_
to	_	_
tackle	_	_
this	_	_
problem	_	_
.	_	_

#298
These	_	_
networks	_	_
,	_	_
called	_	_
MobileNets	_	_
[	_	_
38	_	_
]	_	_
,	_	_
are	_	_
built	_	_
to	_	_
be	_	_
as	_	_
time	_	_
efficient	_	_
as	_	_
possible	_	_
by	_	_
replacing	_	_
standard	_	_
convolutions	_	_
by	_	_
Depthwise	_	_
Separable	_	_
Convolutions	_	_
,	_	_
as	_	_
first	_	_
introduced	_	_
by	_	_
[	_	_
39	_	_
]	_	_
.	_	_

#299
As	_	_
we	_	_
have	_	_
seen	_	_
in	_	_
Sect.	_	_
II-C1	_	_
,	_	_
a	_	_
neuron	_	_
in	_	_
a	_	_
convolutional	_	_
layer	_	_
combines	_	_
the	_	_
outputs	_	_
of	_	_
a	_	_
square	_	_
sized	_	_
region	_	_
across	_	_
the	_	_
width	_	_
and	_	_
height	_	_
dimensions	_	_
and	_	_
across	_	_
the	_	_
whole	_	_
input	_	_
depth	_	_
.	_	_

#300
Depthwise	_	_
separable	_	_
convolutions	_	_
split	_	_
this	_	_
process	_	_
into	_	_
two	_	_
steps	_	_
:	_	_
A	_	_
Depthwise	_	_
Convolution	_	_
and	_	_
a	_	_
Pointwise	_	_
Convolution	_	_
.	_	_

#301
The	_	_
depthwise	_	_
convolution	_	_
acts	_	_
as	_	_
a	_	_
filter	_	_
by	_	_
only	_	_
considering	_	_
the	_	_
square	_	_
sized	_	_
regions	_	_
within	_	_
a	_	_
single	_	_
depth	_	_
slice	_	_
.	_	_

#302
The	_	_
point	_	_
wise	_	_
convolution	_	_
then	_	_
performs	_	_
a	_	_
1×	_	_
1	_	_
convolution	_	_
to	_	_
merge	_	_
the	_	_
information	_	_
across	_	_
the	_	_
whole	_	_
depth	_	_
.	_	_

#303
Additionally	_	_
,	_	_
two	_	_
new	_	_
hyperparameters	_	_
are	_	_
used	_	_
for	_	_
MobileNets	_	_
,	_	_
which	_	_
enable	_	_
creations	_	_
of	_	_
even	_	_
faster	_	_
architectures	_	_
by	_	_
trading	_	_
off	_	_
accuracy	_	_
and	_	_
execution	_	_
time	_	_
.	_	_

#304
The	_	_
first	_	_
hyperparameter	_	_
,	_	_
called	_	_
Width	_	_
Multiplier	_	_
,	_	_
is	_	_
a	_	_
value	_	_
between	_	_
zero	_	_
and	_	_
one	_	_
that	_	_
lowers	_	_
the	_	_
number	_	_
of	_	_
neurons	_	_
in	_	_
all	_	_
layers	_	_
by	_	_
the	_	_
given	_	_
factor	_	_
,	_	_
which	_	_
results	_	_
in	_	_
a	_	_
squared	_	_
reduction	_	_
of	_	_
execution	_	_
time	_	_
and	_	_
has	_	_
been	_	_
found	_	_
to	_	_
decrease	_	_
the	_	_
output	_	_
quality	_	_
less	_	_
severely	_	_
than	_	_
lowering	_	_
the	_	_
number	_	_
of	_	_
layers	_	_
in	_	_
the	_	_
network	_	_
[	_	_
38	_	_
]	_	_
.	_	_

#305
The	_	_
other	_	_
hyperparameter	_	_
,	_	_
Resolution	_	_
Multiplier	_	_
,	_	_
lowers	_	_
the	_	_
resolution	_	_
of	_	_
the	_	_
input	_	_
image	_	_
by	_	_
the	_	_
given	_	_
factor	_	_
,	_	_
which	_	_
also	_	_
results	_	_
in	_	_
a	_	_
squared	_	_
reduction	_	_
of	_	_
execution	_	_
time	_	_
.	_	_

#306
By	_	_
adjusting	_	_
these	_	_
parameters	_	_
it	_	_
is	_	_
,	_	_
therefore	_	_
,	_	_
possible	_	_
to	_	_
construct	_	_
MobileNets	_	_
that	_	_
exactly	_	_
match	_	_
the	_	_
execution	_	_
time	_	_
requirement	_	_
of	_	_
a	_	_
given	_	_
application	_	_
while	_	_
preserving	_	_
a	_	_
relatively	_	_
high	_	_
quality	_	_
of	_	_
results	_	_
.	_	_

#307
The	_	_
first	_	_
version	_	_
of	_	_
such	_	_
a	_	_
MobileNet	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
22	_	_
.	_	_

#308
H.	_	_
Improvement	_	_
over	_	_
time	_	_
After	_	_
having	_	_
inspected	_	_
the	_	_
best	_	_
architectures	_	_
of	_	_
each	_	_
year	_	_
,	_	_
it	_	_
would	_	_
be	_	_
interesting	_	_
to	_	_
see	_	_
,	_	_
how	_	_
effective	_	_
each	_	_
innovation	_	_
Fig.	_	_
22	_	_
.	_	_

#309
The	_	_
MobileNet-v1	_	_
,	_	_
consisting	_	_
of	_	_
a	_	_
3×	_	_
3	_	_
convolutional	_	_
layer	_	_
,	_	_
13	_	_
3×	_	_
3	_	_
depthwise	_	_
separable	_	_
convolutions	_	_
,	_	_
a	_	_
7×	_	_
7	_	_
average	_	_
pooling	_	_
layer	_	_
and	_	_
a	_	_
final	_	_
fully-connected	_	_
layer	_	_
before	_	_
the	_	_
output	_	_
layer	_	_
Fig.	_	_
23	_	_
.	_	_

#310
Results	_	_
of	_	_
the	_	_
best	_	_
performing	_	_
architectures	_	_
on	_	_
the	_	_
ILSVRC	_	_
classification	_	_
,	_	_
localization	_	_
and	_	_
detection	_	_
challenges	_	_
and	_	_
modification	_	_
was	_	_
and	_	_
how	_	_
fast	_	_
the	_	_
field	_	_
is	_	_
advancing	_	_
.	_	_

#311
For	_	_
this	_	_
reason	_	_
,	_	_
we	_	_
visualized	_	_
the	_	_
results	_	_
of	_	_
the	_	_
all	_	_
winning	_	_
architectures	_	_
on	_	_
the	_	_
classification	_	_
,	_	_
localization	_	_
and	_	_
detection	_	_
tasks	_	_
in	_	_
the	_	_
ILSVRC	_	_
in	_	_
Fig.	_	_
23	_	_
.	_	_

#312
Since	_	_
detection	_	_
accuracy	_	_
is	_	_
measured	_	_
in	_	_
Mean	_	_
Average	_	_
Precision	_	_
(	_	_
MAP	_	_
)	_	_
,	_	_
we	_	_
define	_	_
the	_	_
corresponding	_	_
error	_	_
as	_	_
error	_	_
=	_	_
1−MAP	_	_
.	_	_

#313
As	_	_
Fig.	_	_
23	_	_
is	_	_
showing	_	_
,	_	_
the	_	_
classification	_	_
error	_	_
has	_	_
been	_	_
decreased	_	_
by	_	_
a	_	_
large	_	_
quantity	_	_
due	_	_
to	_	_
DCNNs	_	_
.	_	_

#314
In	_	_
2011	_	_
,	_	_
when	_	_
the	_	_
winning	_	_
architecture	_	_
was	_	_
not	_	_
a	_	_
DCNN	_	_
yet	_	_
,	_	_
the	_	_
classification	_	_
error	_	_
amounted	_	_
to	_	_
26	_	_
percent	_	_
and	_	_
only	_	_
five	_	_
years	_	_
later	_	_
,	_	_
in	_	_
2016	_	_
,	_	_
it	_	_
was	_	_
possible	_	_
to	_	_
lower	_	_
the	_	_
error	_	_
to	_	_
three	_	_
percent	_	_
,	_	_
which	_	_
is	_	_
even	_	_
lower	_	_
than	_	_
the	_	_
human	_	_
error	_	_
rate	_	_
of	_	_
about	_	_
five	_	_
percent	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#315
The	_	_
localization	_	_
and	_	_
detection	_	_
results	_	_
have	_	_
been	_	_
steadily	_	_
improving	_	_
as	_	_
well	_	_
,	_	_
but	_	_
as	_	_
we	_	_
can	_	_
see	_	_
,	_	_
the	_	_
advent	_	_
of	_	_
residual	_	_
networks	_	_
in	_	_
2015	_	_
had	_	_
a	_	_
particularly	_	_
strong	_	_
impact	_	_
.	_	_

#316
V.	_	_
CONCLUSION	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
have	_	_
explored	_	_
what	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
are	_	_
,	_	_
why	_	_
they	_	_
work	_	_
as	_	_
well	_	_
as	_	_
they	_	_
do	_	_
for	_	_
object	_	_
recognition	_	_
tasks	_	_
and	_	_
how	_	_
current	_	_
state-of-the-art	_	_
architectures	_	_
are	_	_
composed	_	_
.	_	_

#317
As	_	_
suggested	_	_
in	_	_
III-A	_	_
,	_	_
most	_	_
of	_	_
the	_	_
DCNN	_	_
architectures	_	_
we	_	_
inspected	_	_
follow	_	_
a	_	_
clear	_	_
twopart	_	_
design	_	_
pattern	_	_
:	_	_
In	_	_
the	_	_
first	_	_
part	_	_
(	_	_
at	_	_
the	_	_
beginning	_	_
of	_	_
the	_	_
network	_	_
)	_	_
multiple	_	_
convolution	_	_
and	_	_
pooling	_	_
layers	_	_
are	_	_
stacked	_	_
on	_	_
top	_	_
of	_	_
each	_	_
other	_	_
to	_	_
produce	_	_
abstract	_	_
data	_	_
representations	_	_
and	_	_
in	_	_
the	_	_
second	_	_
part	_	_
(	_	_
at	_	_
the	_	_
end	_	_
of	_	_
the	_	_
network	_	_
)	_	_
additional	_	_
fully	_	_
connected	_	_
layers	_	_
are	_	_
used	_	_
to	_	_
forward	_	_
these	_	_
abstractions	_	_
to	_	_
the	_	_
output	_	_
layer	_	_
.	_	_

#318
Regarding	_	_
hyperparameters	_	_
,	_	_
a	_	_
good	_	_
choice	_	_
for	_	_
the	_	_
convolution	_	_
layer	_	_
seems	_	_
to	_	_
be	_	_
a	_	_
window	_	_
size	_	_
of	_	_
three	_	_
with	_	_
a	_	_
padding	_	_
of	_	_
one	_	_
and	_	_
a	_	_
stride	_	_
of	_	_
one	_	_
or	_	_
two	_	_
.	_	_

#319
For	_	_
pooling	_	_
layers	_	_
,	_	_
a	_	_
window	_	_
size	_	_
of	_	_
two	_	_
with	_	_
no	_	_
padding	_	_
and	_	_
a	_	_
stride	_	_
of	_	_
one	_	_
or	_	_
two	_	_
are	_	_
frequently	_	_
selected	_	_
.	_	_

#320
Finally	_	_
,	_	_
in	_	_
fully-connected	_	_
layers	_	_
,	_	_
the	_	_
amount	_	_
of	_	_
neurons	_	_
should	deontic	_
be	_	_
greater	_	_
or	_	_
equal	_	_
to	_	_
the	_	_
amount	_	_
of	_	_
neurons	_	_
in	_	_
the	_	_
following	_	_
layer	_	_
and	_	_
be	_	_
within	_	_
the	_	_
same	_	_
order	_	_
of	_	_
magnitude	_	_
.	_	_

#321
One	_	_
of	_	_
the	_	_
more	_	_
general	_	_
key	_	_
design	_	_
ideas	_	_
that	_	_
we	_	_
can	_	_
observe	_	_
in	_	_
recent	_	_
network	_	_
architectures	_	_
is	_	_
that	_	_
direct	_	_
connections	_	_
from	_	_
earlier	_	_
to	_	_
later	_	_
layers	_	_
seem	_	_
to	_	_
be	_	_
necessary	_	_
to	_	_
achieve	_	_
state-of-the-art	_	_
results	_	_
.	_	_

#322
As	_	_
explained	_	_
in	_	_
Section	_	_
IV-E	_	_
,	_	_
such	_	_
connections	_	_
are	_	_
crucial	_	_
for	_	_
networks	_	_
with	_	_
many	_	_
layers	_	_
,	_	_
as	_	_
the	_	_
information	_	_
on	_	_
the	_	_
prediction	_	_
quality	_	_
can	_	_
otherwise	_	_
not	_	_
be	_	_
reliably	_	_
transmitted	_	_
back	_	_
to	_	_
earlier	_	_
layers	_	_
.	_	_

#323
Another	_	_
key	_	_
design	_	_
idea	_	_
is	_	_
that	_	_
intra-layer	_	_
parallelism	_	_
,	_	_
as	_	_
seen	_	_
in	_	_
Inception	_	_
architectures	_	_
and	_	_
ResNext	_	_
,	_	_
is	_	_
desirable	_	_
in	_	_
large	_	_
networks	_	_
.	_	_

#324
As	_	_
mentioned	_	_
in	_	_
IV-F	_	_
,	_	_
such	_	_
parallel	_	_
convolution	_	_
paths	_	_
significantly	_	_
decrease	_	_
the	_	_
network	_	_
complexity	_	_
,	_	_
as	_	_
it	_	_
leads	_	_
to	_	_
less	_	_
layers	_	_
and	_	_
less	_	_
neurons	_	_
per	_	_
layer	_	_
being	_	_
required	_	_
to	_	_
achieve	_	_
similar	_	_
results	_	_
.	_	_

#325
Lastly	_	_
,	_	_
we	_	_
would	_	_
like	_	_
to	_	_
mention	_	_
some	_	_
selected	_	_
resources	_	_
for	_	_
further	_	_
research	_	_
.	_	_

#326
To	_	_
readers	_	_
that	_	_
are	_	_
interested	_	_
in	_	_
a	_	_
more	_	_
in-depth	_	_
introduction	_	_
to	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
,	_	_
we	_	_
highly	_	_
recommend	_	_
the	_	_
Stanford	_	_
University	_	_
course	_	_
CS231n6	_	_
.	_	_

#327
For	_	_
further	_	_
details	_	_
about	_	_
specific	_	_
parts	_	_
of	_	_
this	_	_
survey	_	_
,	_	_
we	_	_
strongly	_	_
recommend	_	_
to	_	_
consult	_	_
the	_	_
corresponding	_	_
original	_	_
papers	_	_
as	_	_
listed	_	_
in	_	_
the	_	_
References	_	_
section	_	_
.	_	_

#328
ACKNOWLEDGMENT	_	_
The	_	_
first	_	_
version	_	_
of	_	_
this	_	_
paper	_	_
was	_	_
created	_	_
during	_	_
the	_	_
seminar	_	_
Human-Robot-Interaction	_	_
,	_	_
held	_	_
in	_	_
a	_	_
joint	_	_
cooperation	_	_
by	_	_
the	_	_
Technical	_	_
University	_	_
of	_	_
Munich	_	_
and	_	_
Fortiss	_	_
GmbH	_	_
.	_	_

#329
Therefore	_	_
,	_	_
the	_	_
authors	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
both	_	_
corporations	_	_
and	_	_
the	_	_
supervisor	_	_
of	_	_
the	_	_
paper	_	_
,	_	_
Andrej	_	_
Pangercic	_	_
,	_	_
in	_	_
particular	_	_
.	_	_

#330
Furthermore	_	_
,	_	_
we	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
Oleksandr	_	_
Melkonyan	_	_
,	_	_
Yuriy	_	_
Arabskyy	_	_
and	_	_
Cristian	_	_
Plop	_	_
for	_	_
their	_	_
valuable	_	_
inputs	_	_
and	_	_
corrections	_	_
.	_	_