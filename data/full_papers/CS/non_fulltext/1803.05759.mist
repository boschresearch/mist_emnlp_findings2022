#0
Salient	_	_
Region	_	_
Segmentation	_	_
Sen	_	_
He∗	_	_
,	_	_
Nicolas	_	_
Pugeault†	_	_
Department	_	_
of	_	_
Computer	_	_
Science	_	_
University	_	_
of	_	_
Exeter	_	_
Harrison	_	_
Building	_	_
,	_	_
Streatham	_	_
Campus	_	_
,	_	_
Exeter	_	_
,	_	_
EX4	_	_
4QF	_	_
Email	_	_
:	_	_
∗sh752	_	_
@	_	_
exeter.ac.uk	_	_
,	_	_
†N.Pugeault	_	_
@	_	_
exeter.ac.uk	_	_
Abstract—Saliency	_	_
prediction	_	_
is	_	_
a	_	_
well	_	_
studied	_	_
problem	_	_
in	_	_
computer	_	_
vision	_	_
.	_	_

#1
Early	_	_
saliency	_	_
models	_	_
were	_	_
based	_	_
on	_	_
low-level	_	_
hand-crafted	_	_
feature	_	_
derived	_	_
from	_	_
insights	_	_
gained	_	_
in	_	_
neuroscience	_	_
and	_	_
psychophysics	_	_
.	_	_

#2
In	_	_
the	_	_
wake	_	_
of	_	_
deep	_	_
learning	_	_
breakthrough	_	_
,	_	_
a	_	_
new	_	_
cohort	_	_
of	_	_
models	_	_
were	_	_
proposed	_	_
based	_	_
on	_	_
neural	_	_
network	_	_
architectures	_	_
,	_	_
allowing	_	_
significantly	_	_
higher	_	_
gaze	_	_
prediction	_	_
than	_	_
previous	_	_
shallow	_	_
models	_	_
,	_	_
on	_	_
all	_	_
metrics	_	_
.	_	_

#3
However	_	_
,	_	_
most	_	_
models	_	_
treat	_	_
the	_	_
saliency	_	_
prediction	_	_
as	_	_
a	_	_
regression	_	_
problem	_	_
,	_	_
and	_	_
accurate	_	_
regression	_	_
of	_	_
high-dimensional	_	_
data	_	_
is	_	_
known	_	_
to	_	_
be	_	_
a	_	_
hard	_	_
problem	_	_
.	_	_

#4
Furthermore	_	_
,	_	_
it	_	_
is	_	_
unclear	_	_
that	_	_
intermediate	_	_
levels	_	_
of	_	_
saliency	_	_
(	_	_
ie	_	_
,	_	_
neither	_	_
very	_	_
high	_	_
,	_	_
nor	_	_
very	_	_
low	_	_
)	_	_
are	_	_
meaningful	_	_
:	_	_
Something	_	_
is	_	_
either	_	_
salient	_	_
,	_	_
or	_	_
it	_	_
is	_	_
not	_	_
.	_	_

#5
Drawing	_	_
from	_	_
those	_	_
two	_	_
observations	_	_
,	_	_
we	_	_
reformulate	_	_
the	_	_
saliency	_	_
prediction	_	_
problem	_	_
as	_	_
a	_	_
salient	_	_
region	_	_
segmentation	_	_
problem	_	_
.	_	_

#6
We	_	_
demonstrate	_	_
that	_	_
the	_	_
reformulation	_	_
allows	_	_
for	_	_
faster	_	_
convergence	_	_
than	_	_
the	_	_
classical	_	_
regression	_	_
problem	_	_
,	_	_
while	_	_
performance	_	_
is	_	_
comparable	_	_
to	_	_
stateoftheart	_	_
.	_	_

#7
We	_	_
also	_	_
visualise	_	_
the	_	_
general	_	_
features	_	_
learned	_	_
by	_	_
the	_	_
model	_	_
,	_	_
which	_	_
are	_	_
showed	_	_
to	_	_
be	_	_
consistent	_	_
with	_	_
insights	_	_
from	_	_
psychophysics	_	_
.	_	_

#8
I	_	_
.	_	_

#9
INTRODUCTION	_	_
The	_	_
human	_	_
visual	_	_
system	_	_
receives	_	_
about	_	_
108	_	_
to	_	_
109	_	_
bits	_	_
of	_	_
information	_	_
per	_	_
second	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#10
In	_	_
order	_	_
to	_	_
process	_	_
such	_	_
a	_	_
large	_	_
quantity	_	_
of	_	_
information	_	_
efficiently	_	_
,	_	_
the	_	_
visual	_	_
system	_	_
relies	_	_
on	_	_
dynamic	_	_
attention	_	_
:	_	_
selectively	_	_
focusing	_	_
cognitive	_	_
resources	_	_
on	_	_
parts	_	_
of	_	_
the	_	_
scene	_	_
.	_	_

#11
This	_	_
process	_	_
has	_	_
been	_	_
extensively	_	_
studied	_	_
by	_	_
psychologists	_	_
,	_	_
some	_	_
have	_	_
proposed	_	_
computational	_	_
models	_	_
to	_	_
mimic	_	_
this	_	_
clever	_	_
mechanism	_	_
and	_	_
predict	_	_
where	_	_
a	_	_
human	_	_
witness	_	_
would	_	_
look	_	_
in	_	_
a	_	_
given	_	_
.	_	_

#12
The	_	_
most	_	_
widespread	_	_
model	_	_
is	_	_
based	_	_
on	_	_
Triesman’s	_	_
feature	_	_
integration	_	_
theory	_	_
[	_	_
2	_	_
]	_	_
and	_	_
the	_	_
concept	_	_
of	_	_
centre-surround	_	_
difference	_	_
:	_	_
regions	_	_
which	_	_
have	_	_
features	_	_
different	_	_
from	_	_
their	_	_
surroundings	_	_
are	_	_
likely	_	_
to	_	_
attract	_	_
a	_	_
viewer’s	_	_
attention	_	_
.	_	_

#13
Such	_	_
regions	_	_
,	_	_
which	_	_
attract	_	_
a	_	_
viewer’s	_	_
gaze	_	_
,	_	_
are	_	_
called	_	_
salient	_	_
,	_	_
and	_	_
thus	_	_
computational	_	_
models	_	_
predicting	_	_
such	_	_
regions	_	_
are	_	_
called	_	_
saliency	_	_
models	_	_
.	_	_

#14
Features	_	_
commonly	_	_
used	_	_
to	_	_
explain	_	_
saliency	_	_
include	_	_
,	_	_
eg	_	_
,	_	_
colour	_	_
,	_	_
intensity	_	_
and	_	_
orientation	_	_
[	_	_
3	_	_
]	_	_
,	_	_
and	_	_
additional	_	_
features	_	_
(	_	_
eg	_	_
,	_	_
depth	_	_
,	_	_
contour	_	_
)	_	_
can	_	_
improve	_	_
the	_	_
saliency	_	_
models’	_	_
predictiveness	_	_
but	_	_
it	_	_
is	_	_
not	_	_
fully	_	_
understood	_	_
which	_	_
features	_	_
are	_	_
salient	_	_
.	_	_

#15
The	_	_
fast	_	_
development	_	_
and	_	_
success	_	_
of	_	_
deep	_	_
learning	_	_
approaches	_	_
in	_	_
computer	_	_
vision	_	_
,	_	_
together	_	_
with	_	_
the	_	_
availability	_	_
of	_	_
large	_	_
scale	_	_
datasets	_	_
in	_	_
visual	_	_
attention	_	_
has	_	_
allowed	_	_
to	_	_
try	_	_
and	_	_
learn	_	_
salient	_	_
features	_	_
from	_	_
scratch	_	_
by	_	_
trying	_	_
to	_	_
predict	_	_
human	_	_
viewers’	_	_
gaze	_	_
[	_	_
4	_	_
]	_	_
,	_	_
[	_	_
5	_	_
]	_	_
,	_	_
[	_	_
6	_	_
]	_	_
,	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#16
These	_	_
data-driven	_	_
approaches	_	_
have	_	_
demonstrated	_	_
the	_	_
potential	_	_
to	_	_
predict	_	_
viewers’	_	_
gaze	_	_
with	_	_
significantly	_	_
higher	_	_
accuracy	_	_
than	_	_
previous	_	_
,	_	_
hand-crafted	_	_
saliency	_	_
models	_	_
.	_	_

#17
One	_	_
challenge	_	_
when	_	_
using	_	_
a	_	_
data-driven	_	_
approach	_	_
to	_	_
learn	_	_
saliency	_	_
from	_	_
human	_	_
gaze	_	_
,	_	_
is	_	_
the	_	_
difficulty	_	_
of	_	_
regressing	_	_
a	_	_
(	_	_
a	_	_
)	_	_
Original	_	_
image	_	_
(	_	_
b	_	_
)	_	_
Saliency	_	_
Map	_	_
(	_	_
SM	_	_
)	_	_
(	_	_
c	_	_
)	_	_
Salient	_	_
Region	_	_
Map	_	_
(	_	_
SRM	_	_
)	_	_
Fig.	_	_
1	_	_
:	_	_
Example	_	_
of	_	_
original	_	_
image	_	_
,	_	_
saliency	_	_
map	_	_
and	_	_
salient	_	_
region	_	_
map	_	_
(	_	_
three	_	_
saliency	_	_
levels	_	_
)	_	_
.	_	_

#18
complete	_	_
,	_	_
high-dimensional	_	_
saliency	_	_
map	_	_
:	_	_
accurate	_	_
regression	_	_
of	_	_
high-dimensional	_	_
data	_	_
is	_	_
known	_	_
to	_	_
be	_	_
a	_	_
hard	_	_
problem	_	_
.	_	_

#19
More	_	_
importantly	_	_
,	_	_
although	_	_
it	_	_
is	_	_
clear	_	_
what	_	_
is	_	_
signified	_	_
by	_	_
a	_	_
high	_	_
or	_	_
low	_	_
saliency	_	_
,	_	_
it	_	_
is	_	_
less	_	_
clear	_	_
how	_	_
meaningful	_	_
is	_	_
the	_	_
accurate	_	_
saliency	_	_
scoring	_	_
of	_	_
intermediate	_	_
regions	_	_
(	_	_
can	_	_
something	_	_
be	_	_
‘somewhat	_	_
more	_	_
salient’	_	_
than	_	_
another	_	_
?	_	_
)	_	_
.	_	_

#20
Arguably	_	_
,	_	_
the	_	_
regression	_	_
problem	_	_
to	_	_
be	_	_
solved	_	_
is	_	_
much	_	_
harder	_	_
than	_	_
strictly	_	_
necessary	_	_
from	_	_
the	_	_
problem	_	_
definition	_	_
:	_	_
estimating	_	_
accurately	_	_
the	_	_
relative	_	_
saliency	_	_
level	_	_
of	_	_
all	_	_
pixels	_	_
is	_	_
intrinsically	_	_
ambiguous	_	_
,	_	_
whereas	_	_
we	_	_
are	_	_
only	_	_
really	_	_
interested	_	_
in	_	_
the	_	_
salient	_	_
regions	_	_
of	_	_
the	_	_
image	_	_
.	_	_

#21
In	_	_
this	_	_
work	_	_
,	_	_
instead	_	_
of	_	_
predicting	_	_
a	_	_
pixel-wise	_	_
saliency	_	_
value	_	_
,	_	_
we	_	_
predict	_	_
salient	_	_
regions	_	_
based	_	_
on	_	_
three	_	_
saliency	_	_
levels	_	_
(	_	_
high	_	_
,	_	_
medium	_	_
and	_	_
low	_	_
)	_	_
.	_	_

#22
This	_	_
simplification	_	_
of	_	_
the	_	_
problem	_	_
allows	_	_
us	_	_
to	_	_
adopt	_	_
an	_	_
image	_	_
segmentation	_	_
approach	_	_
based	_	_
on	_	_
an	_	_
encoder-decoder	_	_
network	_	_
such	_	_
that	_	_
the	_	_
network’s	_	_
output	_	_
is	_	_
of	_	_
the	_	_
same	_	_
size	_	_
as	_	_
its	_	_
input	_	_
,	_	_
avoiding	_	_
the	_	_
need	_	_
to	_	_
rescale	_	_
the	_	_
output	_	_
.	_	_

#23
We	_	_
demonstrate	_	_
that	_	_
although	_	_
our	_	_
approach	_	_
involves	_	_
a	_	_
major	_	_
reformulation	_	_
and	_	_
simplification	_	_
of	_	_
the	_	_
problem	_	_
,	_	_
we	_	_
can	_	_
predict	_	_
viewers’	_	_
gaze	_	_
with	_	_
an	_	_
accuracy	_	_
comparable	_	_
to	_	_
state-of-the-art	_	_
approaches	_	_
.	_	_

#24
II	_	_
.	_	_

#25
RELATED	_	_
WORK	_	_
Existing	_	_
saliency	_	_
prediction	_	_
methods	_	_
can	_	_
be	_	_
divided	_	_
into	_	_
two	_	_
groups	_	_
based	_	_
on	_	_
whether	_	_
the	_	_
features	_	_
used	_	_
in	_	_
saliency	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#26
9v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
5	_	_
M	_	_
ar	_	_
2	_	_
prediction	_	_
are	_	_
hand-crafted	_	_
or	_	_
learned	_	_
from	_	_
the	_	_
data	_	_
.	_	_

#27
We	_	_
will	_	_
discuss	_	_
those	_	_
two	_	_
categories	_	_
in	_	_
turn	_	_
.	_	_

#28
A	_	_
.	_	_

#29
Models	_	_
based	_	_
on	_	_
hand-crafted	_	_
features	_	_
Most	_	_
hand-crafted	_	_
feature-based	_	_
models	_	_
originate	_	_
from	_	_
the	_	_
Treisman’s	_	_
feature	_	_
integration	_	_
theory	_	_
[	_	_
2	_	_
]	_	_
,	_	_
and	_	_
make	_	_
use	_	_
of	_	_
hand-crafted	_	_
features	_	_
based	_	_
on	_	_
what	_	_
has	_	_
been	_	_
shown	_	_
to	_	_
attract	_	_
visual	_	_
attention	_	_
by	_	_
Psychologists	_	_
.	_	_

#30
Itti	_	_
&	_	_
Koch	_	_
[	_	_
8	_	_
]	_	_
,	_	_
[	_	_
3	_	_
]	_	_
were	_	_
the	_	_
first	_	_
to	_	_
propose	_	_
a	_	_
computational	_	_
model	_	_
for	_	_
saliency	_	_
prediction	_	_
based	_	_
on	_	_
the	_	_
feature	_	_
integration	_	_
theory	_	_
and	_	_
central	_	_
surround	_	_
difference	_	_
.	_	_

#31
Their	_	_
model	_	_
uses	_	_
a	_	_
topological	_	_
architecture	_	_
at	_	_
multiple	_	_
scale	_	_
to	_	_
get	_	_
a	_	_
conspicuity	_	_
map	_	_
for	_	_
each	_	_
feature	_	_
,	_	_
and	_	_
then	_	_
use	_	_
a	_	_
combination	_	_
scheme	_	_
to	_	_
combine	_	_
conspicuity	_	_
maps	_	_
for	_	_
all	_	_
the	_	_
features	_	_
used	_	_
in	_	_
saliency	_	_
prediction	_	_
to	_	_
get	_	_
the	_	_
final	_	_
saliency	_	_
map	_	_
.	_	_

#32
Their	_	_
model	_	_
uses	_	_
three	_	_
common	_	_
features	_	_
:	_	_
colour	_	_
,	_	_
intensity	_	_
and	_	_
orientation	_	_
.	_	_

#33
Later	_	_
variants	_	_
on	_	_
this	_	_
model	_	_
[	_	_
9	_	_
]	_	_
also	_	_
integrate	_	_
the	_	_
depth	_	_
feature	_	_
into	_	_
the	_	_
saliency	_	_
prediction	_	_
model	_	_
.	_	_

#34
However	_	_
,	_	_
many	_	_
other	_	_
features	_	_
could	capability-speculation	_
attract	_	_
our	_	_
attention	_	_
;	_	_
for	_	_
detailed	_	_
survey	_	_
of	_	_
hand-crafted	_	_
feature	_	_
based	_	_
saliency	_	_
prediction	_	_
model	_	_
,	_	_
we	_	_
refer	_	_
to	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#35
B.	_	_
Models	_	_
based	_	_
on	_	_
machine	_	_
learning	_	_
method	_	_

#36
Although	_	_
psychological	_	_
research	_	_
has	_	_
provided	_	_
elements	_	_
of	_	_
answer	_	_
,	_	_
what	_	_
visual	_	_
features	_	_
are	_	_
salient	_	_
is	_	_
still	_	_
an	_	_
open	_	_
question	_	_
,	_	_
hence	_	_
there	_	_
is	_	_
an	_	_
interest	_	_
to	_	_
use	_	_
machine	_	_
learning	_	_
to	_	_
learn	_	_
salient	_	_
features	_	_
from	_	_
data	_	_
.	_	_

#37
Kienzle	_	_
et	_	_
al.	_	_
[	_	_
10	_	_
]	_	_
first	_	_
tried	_	_
to	_	_
learn	_	_
features	_	_
from	_	_
eye	_	_
fixation	_	_
data	_	_
,	_	_
and	_	_
found	_	_
confirmation	_	_
that	_	_
visual	_	_
patterns	_	_
similar	_	_
to	_	_
central	_	_
surround	_	_
difference	_	_
attract	_	_
viewers’	_	_
attention	_	_
.	_	_

#38
Since	_	_
2014	_	_
,	_	_
the	_	_
development	_	_
of	_	_
deep	_	_
learning	_	_
approaches	_	_
and	_	_
their	_	_
successes	_	_
in	_	_
many	_	_
computer	_	_
vision	_	_
problems	_	_
,	_	_
along	_	_
with	_	_
the	_	_
availability	_	_
of	_	_
large	_	_
scale	_	_
datasets	_	_
in	_	_
visual	_	_
attention	_	_
,	_	_
lead	_	_
to	_	_
the	_	_
rise	_	_
of	_	_
deep	_	_
learning	_	_
approaches	_	_
in	_	_
saliency	_	_
prediction	_	_
.	_	_

#39
Vig	_	_
et	_	_
al.	_	_
[	_	_
11	_	_
]	_	_
were	_	_
the	_	_
first	_	_
to	_	_
use	_	_
a	_	_
deep	_	_
neural	_	_
network	_	_
for	_	_
saliency	_	_
prediction	_	_
,	_	_
although	_	_
many	_	_
other	_	_
approaches	_	_
followed	_	_
.	_	_

#40
They	_	_
use	_	_
an	_	_
optimisation	_	_
algorithm	_	_
to	_	_
search	_	_
the	_	_
best	_	_
features	_	_
in	_	_
a	_	_
deep	_	_
model	_	_
created	_	_
for	_	_
face	_	_
recognition	_	_
.	_	_

#41
After	_	_
that	_	_
,	_	_
most	_	_
deep	_	_
models	_	_
for	_	_
saliency	_	_
prediction	_	_
use	_	_
transfer	_	_
learning	_	_
method	_	_
and	_	_
fine-tuning	_	_
on	_	_
a	_	_
deep	_	_
network	_	_
pre-trained	_	_
for	_	_
image	_	_
classification	_	_
with	_	_
some	_	_
modification	_	_
,	_	_
removing	_	_
the	_	_
fully	_	_
connected	_	_
layer	_	_
and	_	_
building	_	_
a	_	_
regressor	_	_
above	_	_
the	_	_
convolutional	_	_
part	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#42
The	_	_
final	_	_
step	_	_
is	_	_
to	_	_
rescale	_	_
the	_	_
network’s	_	_
output	_	_
to	_	_
the	_	_
input	_	_
image’s	_	_
size	_	_
.	_	_

#43
The	_	_
main	_	_
difference	_	_
between	_	_
those	_	_
deep	_	_
models	_	_
is	_	_
the	_	_
loss	_	_
function	_	_
used	_	_
for	_	_
training	_	_
:	_	_
[	_	_
12	_	_
]	_	_
directly	_	_
use	_	_
Euclidean	_	_
loss	_	_
function	_	_
to	_	_
train	_	_
their	_	_
model	_	_
;	_	_
[	_	_
13	_	_
]	_	_
use	_	_
the	_	_
Kullback-Leibler	_	_
divergence	_	_
as	_	_
the	_	_
loss	_	_
function	_	_
,	_	_
as	_	_
it	_	_
is	_	_
one	_	_
of	_	_
the	_	_
evaluation	_	_
metric	_	_
for	_	_
saliency	_	_
model	_	_
;	_	_
[	_	_
6	_	_
]	_	_
compares	_	_
different	_	_
loss	_	_
functions	_	_
and	_	_
found	_	_
that	_	_
the	_	_
Bhattacharyya	_	_
distance	_	_
is	_	_
the	_	_
best	_	_
loss	_	_
function	_	_
;	_	_
and	_	_
[	_	_
5	_	_
]	_	_
use	_	_
the	_	_
maximum	_	_
likelihood	_	_
method	_	_
to	_	_
train	_	_
their	_	_
model	_	_
.	_	_

#44
The	_	_
approach	_	_
proposed	_	_
in	_	_
this	_	_
article	_	_
is	_	_
also	_	_
based	_	_
on	_	_
deep	_	_
learning	_	_
,	_	_
but	_	_
in	_	_
contrast	_	_
to	_	_
those	_	_
who	_	_
treat	_	_
saliency	_	_
prediction	_	_
as	_	_
a	_	_
regression	_	_
problem	_	_
,	_	_
we	_	_
reformulate	_	_
the	_	_
problem	_	_
as	_	_
a	_	_
segmentation	_	_
problem	_	_
,	_	_
and	_	_
we	_	_
use	_	_
the	_	_
encoder-decoder	_	_
architecture	_	_
to	_	_
achieve	_	_
pixel-wise	_	_
prediction	_	_
rather	_	_
than	_	_
the	_	_
coarse	_	_
prediction	_	_
provided	_	_
by	_	_
other	_	_
deep	_	_
networks	_	_
.	_	_

#45
III	_	_
.	_	_

#46
SALIENT	_	_
REGION	_	_
SEGMENTATION	_	_
Before	_	_
introducing	_	_
our	_	_
method	_	_
,	_	_
we	_	_
first	_	_
introduce	_	_
three	_	_
terms	_	_
that	_	_
will	_	_
be	_	_
used	_	_
in	_	_
the	_	_
later	_	_
parts	_	_
(	_	_
example	_	_
in	_	_
Figure	_	_
1	_	_
)	_	_
.	_	_

#47
Fixation	_	_
map	_	_
(	_	_
FM	_	_
)	_	_
:	_	_
A	_	_
fixation	_	_
map	_	_
is	_	_
a	_	_
binary	_	_
map	_	_
,	_	_
which	_	_
records	_	_
the	_	_
human	_	_
eye	_	_
fixation	_	_
locations	_	_
using	_	_
eye	_	_
tracker	_	_
when	_	_
generating	_	_
the	_	_
dataset	_	_
for	_	_
saliency	_	_
prediction	_	_
.	_	_

#48
Saliency	_	_
map	_	_
(	_	_
SM	_	_
)	_	_
:	_	_
A	_	_
saliency	_	_
map	_	_
is	_	_
derived	_	_
from	_	_
fixation	_	_
maps	_	_
by	_	_
convolving	_	_
a	_	_
Gaussian	_	_
filter	_	_
with	_	_
the	_	_
fixation	_	_
map	_	_
.	_	_

#49
It	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
for	_	_
all	_	_
of	_	_
the	_	_
saliency	_	_
prediction	_	_
model	_	_
at	_	_
the	_	_
moment	_	_
.	_	_

#50
Salient	_	_
region	_	_
map	_	_
(	_	_
SRM	_	_
)	_	_
:	_	_
A	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
derived	_	_
by	_	_
quantizing	_	_
a	_	_
saliency	_	_
map	_	_
into	_	_
several	_	_
saliency	_	_
levels	_	_
.	_	_

#51
It	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
in	_	_
our	_	_
method	_	_
.	_	_

#52
See	_	_
Figure	_	_
1	_	_
.	_	_

#53
All	_	_
existing	_	_
saliency	_	_
prediction	_	_
models	_	_
work	_	_
by	_	_
trying	_	_
to	_	_
predict	_	_
saliency	_	_
levels	_	_
at	_	_
every	_	_
pixel	_	_
.	_	_

#54
However	_	_
,	_	_
estimating	_	_
intermediate	_	_
saliency	_	_
values	_	_
accurately	_	_
increases	_	_
significantly	_	_
the	_	_
complexity	_	_
of	_	_
the	_	_
learning	_	_
problem	_	_
,	_	_
with	_	_
little	_	_
benefit	_	_
.	_	_

#55
Therefore	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
reformulate	_	_
the	_	_
problem	_	_
as	_	_
a	_	_
segmentation	_	_
problem	_	_
to	_	_
separate	_	_
regions	_	_
of	_	_
high	_	_
saliency	_	_
from	_	_
regions	_	_
of	_	_
low	_	_
saliency	_	_
,	_	_
rather	_	_
than	_	_
regressing	_	_
a	_	_
pixel-wise	_	_
saliency	_	_
map	_	_
.	_	_

#56
A.	_	_
Salient	_	_
Region	_	_
Thresholding	_	_
The	_	_
saliency	_	_
map	_	_
is	_	_
a	_	_
continuous	_	_
map	_	_
while	_	_
the	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
a	_	_
discrete	_	_
map	_	_
.	_	_

#57
The	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
derived	_	_
directly	_	_
from	_	_
the	_	_
saliency	_	_
map	_	_
by	_	_
assigning	_	_
each	_	_
pixel	_	_
a	_	_
saliency	_	_
level	_	_
according	_	_
to	_	_
their	_	_
saliency	_	_
value	_	_
.	_	_

#58
The	_	_
higher	_	_
the	_	_
saliency	_	_
value	_	_
of	_	_
a	_	_
pixel	_	_
,	_	_
the	_	_
higher	_	_
its	_	_
saliency	_	_
level	_	_
:	_	_
R	_	_
(	_	_
x	_	_
)	_	_
=	_	_
	_	_
0	_	_
if	_	_
0	_	_
6	_	_
S	_	_
(	_	_
x	_	_
)	_	_
<	_	_
255	_	_
K	_	_
×	_	_
1	_	_
K−1	_	_
if	_	_
255	_	_
K	_	_
×	_	_
1	_	_
6	_	_
S	_	_
(	_	_
x	_	_
)	_	_
<	_	_
255	_	_
K	_	_
×	_	_
2	_	_
...	_	_
...	_	_
255	_	_
if	_	_
frac255K×	_	_
(	_	_
K-1	_	_
)	_	_
6	_	_
S	_	_
(	_	_
X	_	_
)	_	_
<	_	_
255	_	_
K	_	_
×K	_	_
(	_	_
1	_	_
)	_	_
where	_	_
R	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
saliency	_	_
level	_	_
of	_	_
salient	_	_
region	_	_
map	_	_
at	_	_
pixel	_	_
x	_	_
,	_	_
S	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
saliency	_	_
value	_	_
of	_	_
saliency	_	_
map	_	_
at	_	_
pixel	_	_
x	_	_
,	_	_
and	_	_
K	_	_
is	_	_
the	_	_
number	_	_
of	_	_
saliency	_	_
levels	_	_
.	_	_

#59
The	_	_
resulting	_	_
salient	_	_
region	_	_
map	_	_
will	_	_
be	_	_
used	_	_
as	_	_
ground	_	_
truth	_	_
in	_	_
the	_	_
training	_	_
stage	_	_
.	_	_

#60
B.	_	_
Encoder-Decoder	_	_
Segmentation	_	_
The	_	_
classical	_	_
Convolutional	_	_
Neural	_	_
Network	_	_
is	_	_
an	_	_
encoder	_	_
plus	_	_
a	_	_
classifier	_	_
or	_	_
a	_	_
regressor	_	_
.	_	_

#61
The	_	_
size	_	_
of	_	_
such	_	_
a	_	_
network’s	_	_
output	_	_
is	_	_
very	_	_
small	_	_
compared	_	_
to	_	_
the	_	_
input	_	_
image	_	_
due	_	_
to	_	_
the	_	_
successive	_	_
pooling	_	_
layers	_	_
,	_	_
therefore	_	_
preventing	_	_
pixel-wise	_	_
classification	_	_
in	_	_
this	_	_
architecture	_	_
.	_	_

#62
Recently	_	_
,	_	_
the	_	_
encoder-decoder	_	_
architecture	_	_
has	_	_
achieved	_	_
great	_	_
performance	_	_
in	_	_
semantic	_	_
segmentation	_	_
task	_	_
;	_	_
and	_	_
therefore	_	_
this	_	_
is	_	_
the	_	_
architecture	_	_
we	_	_
adopt	_	_
for	_	_
salient	_	_
region	_	_
segmentation	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#63
There	_	_
are	_	_
mainly	_	_
two	_	_
different	_	_
encoder-decoder	_	_
architectures	_	_
:	_	_
the	_	_
FCN	_	_
(	_	_
Fully	_	_
Convolutional	_	_
Networks	_	_
)	_	_
[	_	_
14	_	_
]	_	_
and	_	_
SegNet	_	_
[	_	_
15	_	_
]	_	_
.	_	_

#64
The	_	_
main	_	_
difference	_	_
between	_	_
these	_	_
two	_	_
approaches	_	_
is	_	_
the	_	_
upsampling	_	_
process	_	_
used	_	_
in	_	_
the	_	_
decoder	_	_
.	_	_

#65
In	_	_
FCN	_	_
,	_	_
they	_	_
learn	_	_
an	_	_
upsampling	_	_
filter	_	_
to	_	_
do	_	_
deconvolution	_	_
against	_	_
the	_	_
corresponding	_	_
pooling	_	_
layer	_	_
such	_	_
that	_	_
the	_	_
output	_	_
size	_	_
is	_	_
equal	_	_
to	_	_
the	_	_
input	_	_
size	_	_
.	_	_

#66
In	_	_
contrast	_	_
,	_	_
SegNet	_	_
uses	_	_
indices	_	_
in	_	_
the	_	_
pooling	_	_
stage	_	_
,	_	_
and	_	_
do	_	_
(	_	_
a	_	_
)	_	_
input	_	_
(	_	_
b	_	_
)	_	_
network	_	_
(	_	_
c	_	_
)	_	_
output	_	_
Fig.	_	_
2	_	_
:	_	_
Model	_	_
architecture	_	_
:	_	_
this	_	_
is	_	_
a	_	_
fully	_	_
convolutional	_	_
network	_	_
without	_	_
fully	_	_
connected	_	_
layer	_	_
.	_	_

#67
The	_	_
last	_	_
red	_	_
layer	_	_
is	_	_
the	_	_
softmax	_	_
with	_	_
cross	_	_
entropy	_	_
loss	_	_
layer	_	_
in	_	_
the	_	_
training	_	_
stage	_	_
(	_	_
it	_	_
is	_	_
replaced	_	_
by	_	_
the	_	_
argmax	_	_
function	_	_
in	_	_
the	_	_
inference	_	_
stage	_	_
)	_	_
.	_	_

#68
unpooling	_	_
to	_	_
get	_	_
a	_	_
sparse	_	_
feature	_	_
map	_	_
at	_	_
first	_	_
in	_	_
the	_	_
decoder	_	_
(	_	_
this	_	_
method	_	_
is	_	_
also	_	_
used	_	_
in	_	_
CNN	_	_
feature	_	_
visualisation	_	_
of	_	_
[	_	_
16	_	_
]	_	_
)	_	_
,	_	_
then	_	_
learn	_	_
a	_	_
filter	_	_
to	_	_
get	_	_
a	_	_
dense	_	_
feature	_	_
map	_	_
.	_	_

#69
As	_	_
the	_	_
SegNet	_	_
architecture	_	_
has	_	_
a	_	_
higher	_	_
classification	_	_
accuracy	_	_
than	_	_
FCN	_	_
for	_	_
semantic	_	_
segmentation	_	_
,	_	_
we	_	_
adopt	_	_
this	_	_
architecture	_	_
for	_	_
salient	_	_
region	_	_
segmentation	_	_
.	_	_

#70
C.	_	_
Median	_	_
Frequency	_	_
Balancing	_	_
For	_	_
training	_	_
our	_	_
network	_	_
,	_	_
we	_	_
use	_	_
a	_	_
softmax	_	_
with	_	_
cross	_	_
entropy	_	_
loss	_	_
function	_	_
.	_	_

#71
However	_	_
,	_	_
the	_	_
dataset	_	_
is	_	_
highly	_	_
unbalanced	_	_
:	_	_
low	_	_
saliency	_	_
regions	_	_
occupies	_	_
a	_	_
much	_	_
larger	_	_
proportion	_	_
of	_	_
the	_	_
whole	_	_
map	_	_
than	_	_
high	_	_
saliency	_	_
regions	_	_
do	_	_
(	_	_
see	_	_
Figure	_	_
1	_	_
)	_	_
.	_	_

#72
Therefore	_	_
,	_	_
we	_	_
use	_	_
Eigen	_	_
and	_	_
Fergus’s	_	_
median	_	_
frequency	_	_
balancing	_	_
method	_	_
[	_	_
17	_	_
]	_	_
to	_	_
weight	_	_
the	_	_
loss	_	_
caused	_	_
by	_	_
each	_	_
class	_	_
:	_	_
E	_	_
(	_	_
x	_	_
)	_	_
=	_	_
K∑	_	_
i=1	_	_
Wi	_	_
(	_	_
gi	_	_
(	_	_
x	_	_
)	_	_
ln	_	_
pi	_	_
(	_	_
x	_	_
)	_	_
+	_	_
(	_	_
1−	_	_
gi	_	_
(	_	_
x	_	_
)	_	_
)	_	_
ln	_	_
(	_	_
1−	_	_
pi	_	_
(	_	_
x	_	_
)	_	_
)	_	_
)	_	_
(	_	_
2	_	_
)	_	_
where	_	_
E	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
loss	_	_
caused	_	_
by	_	_
pixel	_	_
x	_	_
during	_	_
training	_	_
,	_	_
K	_	_
is	_	_
the	_	_
number	_	_
of	_	_
saliency	_	_
levels	_	_
,	_	_
Wi	_	_
is	_	_
the	_	_
weight	_	_
of	_	_
ith	_	_
saliency	_	_
level	_	_
determined	_	_
by	_	_
median	_	_
frequency	_	_
balancing	_	_
method	_	_
,	_	_
pi	_	_
is	_	_
the	_	_
prediction	_	_
probability	_	_
that	_	_
the	_	_
saliency	_	_
level	_	_
of	_	_
pixel	_	_
x	_	_
is	_	_
i	_	_
,	_	_
and	_	_
gi	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
that	_	_
the	_	_
saliency	_	_
level	_	_
of	_	_
pixel	_	_
x	_	_
is	_	_
i	_	_
.	_	_

#73
The	_	_
whole	_	_
model	_	_
architecture	_	_
is	_	_
in	_	_
Figure	_	_
2	_	_
.	_	_

#74
D.	_	_
Region	_	_
Restriction	_	_
To	_	_
make	_	_
the	_	_
salient	_	_
region	_	_
more	_	_
selective	_	_
,	_	_
we	_	_
use	_	_
the	_	_
output	_	_
of	_	_
a	_	_
binary	_	_
(	_	_
salient/non-salient	_	_
)	_	_
model	_	_
to	_	_
restrict	_	_
the	_	_
regions	_	_
of	_	_
the	_	_
output	_	_
with	_	_
more	_	_
quantization	_	_
levels	_	_
,	_	_
such	_	_
that	_	_
the	_	_
pixels	_	_
that	_	_
are	_	_
non-salient	_	_
in	_	_
the	_	_
binary	_	_
model’s	_	_
output	_	_
are	_	_
inhibited	_	_
in	_	_
the	_	_
quantized	_	_
model’s	_	_
output	_	_
(	_	_
example	_	_
in	_	_
Figure	_	_
3	_	_
)	_	_
.	_	_

#75
E.	_	_
Training	_	_
The	_	_
Model	_	_
We	_	_
trained	_	_
two	_	_
models	_	_
,	_	_
one	_	_
model	_	_
with	_	_
three	_	_
saliency	_	_
levels	_	_
and	_	_
one	_	_
model	_	_
with	_	_
two	_	_
saliency	_	_
levels	_	_
.	_	_

#76
We	_	_
use	_	_
the	_	_
second	_	_
model’s	_	_
output	_	_
to	_	_
restrict	_	_
the	_	_
first	_	_
model’s	_	_
output	_	_
.	_	_

#77
The	_	_
parameters	_	_
of	_	_
the	_	_
encoder	_	_
part	_	_
are	_	_
initialized	_	_
from	_	_
the	_	_
VGG-16	_	_
network	_	_
[	_	_
18	_	_
]	_	_
.	_	_

#78
The	_	_
filters	_	_
in	_	_
the	_	_
decoder	_	_
are	_	_
initialized	_	_
by	_	_
MSRA	_	_
method	_	_
in	_	_
Caffe	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#79
During	_	_
the	_	_
training	_	_
stage	_	_
,	_	_
all	_	_
layers	_	_
are	_	_
learnt	_	_
with	_	_
an	_	_
initial	_	_
learning	_	_
rate	_	_
of	_	_
0.01	_	_
,	_	_
we	_	_
decrease	_	_
the	_	_
learning	_	_
rate	_	_
using	_	_
the	_	_
step	_	_
learning	_	_
policy	_	_
with	_	_
a	_	_
step	_	_
size	_	_
of	_	_
500	_	_
.	_	_

#80
All	_	_
training	_	_
is	_	_
based	_	_
on	_	_
the	_	_
SALICON	_	_
dataset	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#81
We	_	_
only	_	_
use	_	_
the	_	_
training	_	_
dataset	_	_
(	_	_
10K	_	_
images	_	_
)	_	_
in	_	_
SALICON	_	_
to	_	_
train	_	_
our	_	_
model	_	_
,	_	_
and	_	_
we	_	_
use	_	_
the	_	_
first	_	_
500	_	_
images	_	_
in	_	_
the	_	_
validation	_	_
dataset	_	_
of	_	_
SALICON	_	_
as	_	_
validation	_	_
.	_	_

#82
It	_	_
takes	_	_
almost	_	_
22	_	_
hours	_	_
to	_	_
train	_	_
each	_	_
model	_	_
on	_	_
a	_	_
K40	_	_
GPU	_	_
.	_	_

#83
IV	_	_
.	_	_

#84
EVALUATION	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
first	_	_
describe	_	_
the	_	_
datasets	_	_
and	_	_
the	_	_
evaluation	_	_
metrics	_	_
for	_	_
saliency	_	_
prediction	_	_
models	_	_
,	_	_
followed	_	_
by	_	_
a	_	_
comparison	_	_
and	_	_
discussion	_	_
of	_	_
the	_	_
quantitative	_	_
and	_	_
qualitative	_	_
aspects	_	_
of	_	_
the	_	_
result	_	_
.	_	_

#85
A.	_	_
Datasets	_	_
SALICON	_	_
[	_	_
20	_	_
]	_	_
:	_	_
This	_	_
is	_	_
the	_	_
largest	_	_
dataset	_	_
publicly	_	_
available	_	_
with	_	_
fixation	_	_
map	_	_
and	_	_
saliency	_	_
map	_	_
in	_	_
visual	_	_
attention	_	_
domain	_	_
.	_	_

#86
It	_	_
consists	_	_
of	_	_
20,000	_	_
images	_	_
taken	_	_
from	_	_
the	_	_
MSCOCO	_	_
dataset	_	_
,	_	_
10,000	_	_
in	_	_
the	_	_
training	_	_
dataset	_	_
,	_	_
5,000	_	_
in	_	_
the	_	_
validation	_	_
dataset	_	_
,	_	_
and	_	_
5,000	_	_
in	_	_
the	_	_
testing	_	_
dataset	_	_
.	_	_

#87
The	_	_
eye	_	_
fixation	_	_
data	_	_
is	_	_
recorded	_	_
by	_	_
mouse-click	_	_
instead	_	_
of	_	_
eye-tracking	_	_
system	_	_
.	_	_

#88
We	_	_
use	_	_
the	_	_
training	_	_
dataset	_	_
for	_	_
training	_	_
the	_	_
model	_	_
,	_	_
and	_	_
the	_	_
first	_	_
500	_	_
data	_	_
in	_	_
the	_	_
validation	_	_
dataset	_	_
for	_	_
validation	_	_
and	_	_
all	_	_
the	_	_
validation	_	_
data	_	_
for	_	_
testing	_	_
the	_	_
classification	_	_
accuracy	_	_
.	_	_

#89
MIT1003	_	_
[	_	_
21	_	_
]	_	_
:	_	_
This	_	_
dataset	_	_
contains	_	_
1,003	_	_
images	_	_
with	_	_
fixation	_	_
and	_	_
saliency	_	_
map	_	_
.	_	_

#90
The	_	_
fixation	_	_
map	_	_
comes	_	_
from	_	_
15	_	_
viewers	_	_
when	_	_
free	_	_
viewing	_	_
the	_	_
original	_	_
image	_	_
for	_	_
3	_	_
seconds	_	_
recorded	_	_
by	_	_
the	_	_
eye-tracking	_	_
system	_	_
.	_	_

#91
We	_	_
use	_	_
this	_	_
dataset	_	_
to	_	_
evaluate	_	_
the	_	_
loss	_	_
in	_	_
several	_	_
saliency	_	_
evaluation	_	_
metrics	_	_
compared	_	_
to	_	_
the	_	_
traditional	_	_
pixel-wise	_	_
saliency	_	_
value	_	_
prediction	_	_
methods	_	_
.	_	_

#92
MIT300	_	_
[	_	_
22	_	_
]	_	_
:	_	_
This	_	_
dataset	_	_
contains	_	_
300	_	_
images	_	_
and	_	_
it	_	_
is	_	_
the	_	_
MIT	_	_
saliency	_	_
benchmark	_	_
.	_	_

#93
The	_	_
fixation	_	_
map	_	_
(	_	_
not	_	_
publicly	_	_
available	_	_
)	_	_
of	_	_
this	_	_
dataset	_	_
recorded	_	_
the	_	_
fixation	_	_
locations	_	_
of	_	_
39	_	_
people	_	_
.	_	_

#94
We	_	_
use	_	_
this	_	_
dataset	_	_
to	_	_
compare	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
model	_	_
with	_	_
other	_	_
state-of-the-art	_	_
saliency	_	_
prediction	_	_
models	_	_
over	_	_
several	_	_
evaluation	_	_
metrics	_	_
.	_	_

#95
B.	_	_
Metrics	_	_
The	_	_
ground	_	_
truth	_	_
for	_	_
our	_	_
model	_	_
is	_	_
the	_	_
salient	_	_
region	_	_
map	_	_
,	_	_
not	_	_
the	_	_
saliency	_	_
map	_	_
.	_	_

#96
Therefore	_	_
,	_	_
we	_	_
evaluate	_	_
our	_	_
model	_	_
using	_	_
the	_	_
classification	_	_
accuracy	_	_
and	_	_
the	_	_
evaluation	_	_
metrics	_	_
based	_	_
on	_	_
the	_	_
fixation	_	_
map	_	_
.	_	_

#97
(	_	_
a	_	_
)	_	_
2	_	_
levels	_	_
(	_	_
b	_	_
)	_	_
3	_	_
levels	_	_
(	_	_
c	_	_
)	_	_
restriction	_	_
Fig.	_	_
3	_	_
:	_	_
Region	_	_
Restriction	_	_
:	_	_
the	_	_
output	_	_
of	_	_
two	_	_
saliency	_	_
levels	_	_
model	_	_
,	_	_
the	_	_
output	_	_
of	_	_
three	_	_
saliency	_	_
levels	_	_
model	_	_
and	_	_
the	_	_
output	_	_
after	_	_
region	_	_
restriction	_	_
.	_	_

#98
Class	_	_
Accuracy	_	_
all	_	_
class	_	_
84.21	_	_
%	_	_
saliency	_	_
level	_	_
1	_	_
86.6	_	_
%	_	_
saliency	_	_
level	_	_
2	_	_
68.11	_	_
%	_	_
saliency	_	_
level	_	_
3	_	_
65.11	_	_
%	_	_
TABLE	_	_
I	_	_
:	_	_
The	_	_
overall	_	_
classification	_	_
accuracy	_	_
(	_	_
all	_	_
class	_	_
in	_	_
the	_	_
table	_	_
)	_	_
,	_	_
saliency	_	_
level	_	_
1	_	_
,	_	_
2	_	_
and	_	_
3	_	_
classification	_	_
accuracy	_	_
for	_	_
3	_	_
saliency	_	_
levels	_	_
model	_	_
.	_	_

#99
Classification	_	_
accuracy	_	_
:	_	_
This	_	_
is	_	_
the	_	_
evaluation	_	_
used	_	_
in	_	_
semantic	_	_
segmentation	_	_
tasks	_	_
,	_	_
we	_	_
test	_	_
the	_	_
overall	_	_
classification	_	_
accuracy	_	_
(	_	_
the	_	_
accuracy	_	_
for	_	_
all	_	_
saliency	_	_
levels	_	_
)	_	_
,	_	_
per	_	_
class	_	_
accuracy	_	_
(	_	_
the	_	_
accuracy	_	_
for	_	_
each	_	_
saliency	_	_
level	_	_
)	_	_
.	_	_

#100
AUC-Judd	_	_
:	_	_
This	_	_
metric	_	_
is	_	_
proposed	_	_
by	_	_
Judd	_	_
in	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#101
The	_	_
model	_	_
prediction	_	_
is	_	_
treated	_	_
as	_	_
a	_	_
binary	_	_
classifier	_	_
to	_	_
separate	_	_
positive	_	_
from	_	_
negative	_	_
samples	_	_
at	_	_
various	_	_
thresholds	_	_
,	_	_
and	_	_
the	_	_
ROC	_	_
curve	_	_
is	_	_
calculated	_	_
from	_	_
true	_	_
positive	_	_
(	_	_
TP	_	_
)	_	_
and	_	_
false	_	_
positive	_	_
(	_	_
FP	_	_
)	_	_
rates	_	_
.	_	_

#102
The	_	_
final	_	_
score	_	_
is	_	_
the	_	_
area	_	_
under	_	_
the	_	_
ROC	_	_
curve	_	_
,	_	_
where	_	_
larger	_	_
AUC	_	_
signify	_	_
better	_	_
detection	_	_
.	_	_

#103
sAUC	_	_
:	_	_
This	_	_
metric	_	_
is	_	_
introduced	_	_
in	_	_
[	_	_
24	_	_
]	_	_
at	_	_
2008	_	_
.	_	_

#104
It	_	_
is	_	_
the	_	_
same	_	_
as	_	_
AUC-Borji	_	_
but	_	_
removed	_	_
the	_	_
central-bias	_	_
(	_	_
when	_	_
photographing	_	_
the	_	_
image	_	_
,	_	_
people	_	_
like	_	_
to	_	_
place	_	_
the	_	_
interesting	_	_
objects	_	_
in	_	_
the	_	_
central	_	_
part	_	_
of	_	_
the	_	_
image	_	_
)	_	_
.	_	_

#105
NSS	_	_
[	_	_
23	_	_
]	_	_
:	_	_
This	_	_
metric	_	_
is	_	_
the	_	_
normalised	_	_
scanpath	_	_
saliency	_	_
between	_	_
the	_	_
model’s	_	_
prediction	_	_
and	_	_
the	_	_
fixation	_	_
map	_	_
.	_	_

#106
It	_	_
is	_	_
measured	_	_
as	_	_
the	_	_
mean	_	_
value	_	_
of	_	_
the	_	_
normalised	_	_
model	_	_
prediction	_	_
at	_	_
fixation	_	_
locations	_	_
.	_	_

#107
C.	_	_
Evaluation	_	_
results	_	_
1	_	_
)	_	_
Classification	_	_
accuracy	_	_
:	_	_
Table	_	_
I	_	_
records	_	_
the	_	_
proposed	_	_
model’s	_	_
classification	_	_
accuracy	_	_
on	_	_
the	_	_
SALICON	_	_
dataset	_	_
,	_	_
overall	_	_
and	_	_
for	_	_
each	_	_
saliency	_	_
level	_	_
,	_	_
showing	_	_
that	_	_
classification	_	_
accuracy	_	_
is	_	_
lower	_	_
for	_	_
higher	_	_
saliency	_	_
levels	_	_
.	_	_

#108
In	_	_
contrast	_	_
to	_	_
classical	_	_
semantic	_	_
segmentation	_	_
,	_	_
an	_	_
object	_	_
can	_	_
belong	_	_
to	_	_
multiple	_	_
classes	_	_
in	_	_
salient	_	_
region	_	_
segmentation	_	_
:	_	_
the	_	_
high	_	_
saliency	_	_
level	_	_
region	_	_
is	_	_
generally	_	_
surrounded	_	_
by	_	_
a	_	_
low	_	_
saliency	_	_
level	_	_
region	_	_
.	_	_

#109
It	_	_
makes	_	_
it	_	_
difficult	_	_
to	_	_
distinguish	_	_
them	_	_
with	_	_
higher	_	_
accuracy	_	_
,	_	_
as	_	_
they	_	_
easily	_	_
overlap	_	_
(	_	_
in	_	_
particular	_	_
,	_	_
the	_	_
low	_	_
saliency	_	_
region	_	_
covers	_	_
the	_	_
high	_	_
saliency	_	_
region	_	_
)	_	_
.	_	_

#110
Ground	_	_
truth	_	_
AUC-Judd	_	_
AUC-shuffled	_	_
NSS	_	_
saliency	_	_
map	_	_
0.9700	_	_
0.8899	_	_
4.0543	_	_
salient	_	_
region	_	_
map	_	_
(	_	_
3	_	_
levels	_	_
)	_	_
0.7830	_	_
0.7507	_	_
3.7385	_	_
quantization	_	_
loss	_	_
19.28	_	_
%	_	_
15.64	_	_
%	_	_
7.79	_	_
%	_	_
TABLE	_	_
II	_	_
:	_	_
The	_	_
performance	_	_
decline	_	_
from	_	_
saliency	_	_
map	_	_
to	_	_
the	_	_
salient	_	_
region	_	_
map	_	_
,	_	_
this	_	_
is	_	_
evaluated	_	_
on	_	_
the	_	_
MIT1003	_	_
dataset	_	_
.	_	_

#111
Models	_	_
AUC-Judd	_	_
AUC-shuffled	_	_
NSS	_	_
SALGAN	_	_
0.773	_	_
2.589	_	_
PDP	_	_
0.880	_	_
0.783	_	_
2.419	_	_
ours	_	_
(	_	_
no	_	_
region	_	_
restriction	_	_
)	_	_
0.7485	_	_
0.6724	_	_
1.5923	_	_
ours	_	_
(	_	_
with	_	_
region	_	_
restriction	_	_
)	_	_
0.7008	_	_
0.6463	_	_
1.7950	_	_
loss	_	_
compared	_	_
to	_	_
state-of-the-art	_	_
20.36	_	_
%	_	_
12.63	_	_
%	_	_
30.67	_	_
%	_	_
TABLE	_	_
III	_	_
:	_	_
Comparison	_	_
with	_	_
the	_	_
state-of-the-art	_	_
models	_	_
,	_	_
all	_	_
results	_	_
are	_	_
tested	_	_
on	_	_
SALICON	_	_
validation	_	_
dataset	_	_
,	_	_
the	_	_
decline	_	_
is	_	_
computed	_	_
based	_	_
on	_	_
the	_	_
map	_	_
after	_	_
region	_	_
restriction	_	_
.	_	_

#112
2	_	_
)	_	_
Saliency	_	_
evaluation	_	_
metrics	_	_
:	_	_
Due	_	_
to	_	_
the	_	_
quantization	_	_
step	_	_
,	_	_
it	_	_
is	_	_
expected	_	_
that	_	_
even	_	_
a	_	_
true	_	_
salient	_	_
region	_	_
map	_	_
would	_	_
yield	_	_
a	_	_
somewhat	_	_
lower	_	_
gaze	_	_
prediction	_	_
performance	_	_
on	_	_
most	_	_
metrics	_	_
compared	_	_
to	_	_
the	_	_
original	_	_
(	_	_
continuous	_	_
)	_	_
saliency	_	_
map	_	_
.	_	_

#113
This	_	_
loss	_	_
is	_	_
measured	_	_
one	_	_
the	_	_
MIT1003	_	_
dataset	_	_
and	_	_
recorded	_	_
in	_	_
Table	_	_
II	_	_
.	_	_

#114
In	_	_
this	_	_
table	_	_
,	_	_
we	_	_
can	_	_
also	_	_
see	_	_
that	_	_
the	_	_
estimated	_	_
loss	_	_
is	_	_
larger	_	_
for	_	_
AUC	_	_
metric	_	_
than	_	_
for	_	_
NSS	_	_
,	_	_
supporting	_	_
our	_	_
hypothesis	_	_
that	_	_
quantized	_	_
regions	_	_
encode	_	_
most	_	_
saliency	_	_
information	_	_
.	_	_

#115
We	_	_
evaluated	_	_
our	_	_
model	_	_
on	_	_
the	_	_
SALICON	_	_
validation	_	_
dataset	_	_
(	_	_
Table	_	_
III	_	_
)	_	_
and	_	_
the	_	_
MIT300	_	_
official	_	_
test	_	_
set	_	_
(	_	_
Table	_	_
IV	_	_
)	_	_
.	_	_

#116
Models	_	_
AUC-Judd	_	_
AUC-shuffled	_	_
NSS	_	_
human	_	_
baseline	_	_
0.92	_	_
0.81	_	_
3.29	_	_
Deep	_	_
Gaze	_	_
ii	_	_
0.88	_	_
0.72	_	_
1.29	_	_
Deep	_	_
Gaze	_	_
i	_	_
0.84	_	_
0.66	_	_
1.22	_	_
SALGAN	_	_
0.86	_	_
0.72	_	_
2.04	_	_
PDP	_	_
0.85	_	_
0.73	_	_
2.05	_	_
ours	_	_
(	_	_
3	_	_
levels	_	_
)	_	_
0.76	_	_
0.68	_	_
1.32	_	_
loss	_	_
compared	_	_
to	_	_
state-of-the-art	_	_
13.64	_	_
%	_	_
6.85	_	_
%	_	_
35.61	_	_
%	_	_
TABLE	_	_
IV	_	_
:	_	_
Comparison	_	_
with	_	_
the	_	_
state-of-the-art	_	_
models	_	_
,	_	_
all	_	_
results	_	_
are	_	_
tested	_	_
on	_	_
the	_	_
MIT300	_	_
saliency	_	_
benchmark	_	_
based	_	_
on	_	_
the	_	_
map	_	_
without	_	_
region	_	_
restriction	_	_
[	_	_
25	_	_
]	_	_
.	_	_

#117
(	_	_
a	_	_
)	_	_
image	_	_
(	_	_
b	_	_
)	_	_
saliency	_	_
map	_	_
(	_	_
c	_	_
)	_	_
salient	_	_
region	_	_
(	_	_
d	_	_
)	_	_
prediction	_	_
Fig.	_	_
4	_	_
:	_	_
NSS	_	_
decline	_	_
explanation	_	_
,	_	_
the	_	_
red	_	_
spots	_	_
are	_	_
human	_	_
eye	_	_
fixations	_	_
recorded	_	_
when	_	_
generating	_	_
the	_	_
dataset	_	_
.	_	_

#118
These	_	_
results	_	_
show	_	_
that	_	_
although	_	_
our	_	_
proposed	_	_
model	_	_
does	_	_
not	_	_
quite	_	_
reach	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
latest	_	_
deep	_	_
architectures	_	_
such	_	_
as	_	_
SalGAN	_	_
and	_	_
PDP	_	_
,	_	_
it	_	_
nonetheless	_	_
achieves	_	_
high	_	_
performance	_	_
.	_	_

#119
Indeed	_	_
,	_	_
when	_	_
considering	_	_
the	_	_
official	_	_
MIT300	_	_
benchmark	_	_
(	_	_
Table	_	_
IV	_	_
)	_	_
,	_	_
it	_	_
is	_	_
interesting	_	_
to	_	_
note	_	_
that	_	_
,	_	_
despite	_	_
working	_	_
on	_	_
quantized	_	_
regions	_	_
,	_	_
our	_	_
model’s	_	_
NSS	_	_
score	_	_
clearly	_	_
outperforms	_	_
some	_	_
previous	_	_
methods	_	_
such	_	_
as	_	_
Deep	_	_
Gaze	_	_
i	_	_
,	_	_
Deep	_	_
Gaze	_	_
ii	_	_
.	_	_

#120
Moreover	_	_
,	_	_
the	_	_
results	_	_
on	_	_
the	_	_
SALICON	_	_
dataset	_	_
(	_	_
Table	_	_
III	_	_
)	_	_
show	_	_
that	_	_
the	_	_
application	_	_
of	_	_
the	_	_
region	_	_
restriction	_	_
mechanism	_	_
described	_	_
in	_	_
section	_	_
III-D	_	_
appears	_	_
to	_	_
simultaneously	_	_
reduce	_	_
the	_	_
AUC	_	_
scores	_	_
while	_	_
at	_	_
the	_	_
same	_	_
time	_	_
increasing	_	_
the	_	_
NSS	_	_
score	_	_
by	_	_
a	_	_
significant	_	_
margin	_	_
.	_	_

#121
This	_	_
result	_	_
outline	_	_
that	_	_
AUC	_	_
is	_	_
in	_	_
line	_	_
with	_	_
recent	_	_
arguments	_	_
by	_	_
[	_	_
26	_	_
]	_	_
and	_	_
the	_	_
NSS	_	_
score	_	_
is	_	_
a	_	_
better	_	_
evaluation	_	_
metric	_	_
for	_	_
saliency	_	_
datasets	_	_
based	_	_
on	_	_
fixation	_	_
patterns	_	_
.	_	_

#122
The	_	_
reason	_	_
for	_	_
the	_	_
large	_	_
NSS	_	_
loss	_	_
is	_	_
due	_	_
to	_	_
our	_	_
models	_	_
output	_	_
has	_	_
a	_	_
high	_	_
standard	_	_
deviation	_	_
,	_	_
which	_	_
lead	_	_
to	_	_
a	_	_
pixel	_	_
value	_	_
decrease	_	_
after	_	_
normalisation	_	_
when	_	_
compute	_	_
the	_	_
NSS	_	_
score	_	_
.	_	_

#123
See	_	_
Figure	_	_
4	_	_
.	_	_

#124
It	_	_
would	_	_
appear	_	_
at	_	_
first	_	_
glance	_	_
that	_	_
the	_	_
NSS	_	_
score	_	_
for	_	_
our	_	_
model	_	_
prediction	_	_
should	inference	_
be	_	_
higher	_	_
than	_	_
the	_	_
salient	_	_
region	_	_
map	_	_
,	_	_
as	_	_
it	_	_
covers	_	_
more	_	_
eye	_	_
fixation	_	_
locations	_	_
—	_	_
this	_	_
is	_	_
not	_	_
the	_	_
case	_	_
.	_	_

#125
The	_	_
NSS	_	_
score	_	_
for	_	_
the	_	_
saliency	_	_
map	_	_
is	_	_
4.2324	_	_
,	_	_
for	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
3.9484	_	_
,	_	_
and	_	_
for	_	_
our	_	_
prediction	_	_
is	_	_
3.1471	_	_
.	_	_

#126
As	_	_
the	_	_
standard	_	_
deviation	_	_
for	_	_
the	_	_
saliency	_	_
map	_	_
is	_	_
0.0329	_	_
,	_	_
for	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
0.0777	_	_
,	_	_
but	_	_
for	_	_
our	_	_
model	_	_
prediction	_	_
is	_	_
0.1923	_	_
,	_	_
after	_	_
normalisation	_	_
,	_	_
the	_	_
max	_	_
value	_	_
for	_	_
the	_	_
saliency	_	_
map	_	_
is	_	_
11.6710	_	_
,	_	_
for	_	_
salient	_	_
region	_	_
map	_	_
is	_	_
12.7703	_	_
,	_	_
but	_	_
for	_	_
our	_	_
model	_	_
prediction	_	_
is	_	_
4.9508	_	_
,	_	_
which	_	_
will	_	_
lead	_	_
to	_	_
a	_	_
large	_	_
decline	_	_
in	_	_
the	_	_
NSS	_	_
computation	_	_
.	_	_

#127
D.	_	_
Reformulation	_	_
Gain	_	_
One	_	_
important	_	_
advantage	_	_
of	_	_
reformulating	_	_
the	_	_
problem	_	_
as	_	_
semantic	_	_
segmentation	_	_
is	_	_
that	_	_
it	_	_
is	_	_
a	_	_
simpler	_	_
problem	_	_
to	_	_
learn	_	_
.	_	_

#128
To	_	_
demonstrate	_	_
this	_	_
we	_	_
compare	_	_
the	_	_
training	_	_
convergence	_	_
of	_	_
the	_	_
our	_	_
segmentation	_	_
model	_	_
with	_	_
a	_	_
regression	_	_
model	_	_
using	_	_
the	_	_
same	_	_
architecture	_	_
,	_	_
same	_	_
dataset	_	_
and	_	_
same	_	_
learning	_	_
strategy	_	_
,	_	_
but	_	_
treating	_	_
saliency	_	_
map	_	_
as	_	_
the	_	_
ground	_	_
truth	_	_
and	_	_
using	_	_
a	_	_
common	_	_
Euclidean	_	_
loss	_	_
.	_	_

#129
The	_	_
results	_	_
are	_	_
illustrated	_	_
in	_	_
Figure	_	_
5	_	_
.	_	_

#130
In	_	_
Figures	_	_
5a	_	_
and	_	_
5b	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
proposed	_	_
method	_	_
has	_	_
a	_	_
much	_	_
faster	_	_
convergence	_	_
speed	_	_
:	_	_
it	_	_
only	_	_
requires	_	_
2,000	_	_
iterations	_	_
to	_	_
converge	_	_
(	_	_
only	_	_
two	_	_
epochs	_	_
)	_	_
.	_	_

#131
In	_	_
contrast	_	_
,	_	_
the	_	_
regression-based	_	_
model	_	_
is	_	_
slow	_	_
to	_	_
converge	_	_
and	_	_
oscillates	_	_
easily	_	_
.	_	_

#132
And	_	_
in	_	_
Figures	_	_
5c	_	_
and	_	_
5d	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
using	_	_
the	_	_
deconvolution	_	_
method	_	_
in	_	_
the	_	_
decoder	_	_
performs	_	_
better	_	_
for	_	_
both	_	_
segmentation	_	_
and	_	_
regression	_	_
.	_	_

#133
(	_	_
a	_	_
)	_	_
segmentation	_	_
training	_	_
log	_	_
(	_	_
b	_	_
)	_	_
regression	_	_
training	_	_
log	_	_
(	_	_
c	_	_
)	_	_
unpooling	_	_
and	_	_
deconvolution	_	_
in	_	_
segmentation	_	_
(	_	_
d	_	_
)	_	_
unpooling	_	_
and	_	_
deconvolution	_	_
in	_	_
regression	_	_
Fig.	_	_
5	_	_
:	_	_
Comparison	_	_
of	_	_
segmentation	_	_
versus	_	_
regression	_	_
and	_	_
unpooling	_	_
versus	_	_
deconvolution	_	_
.	_	_

#134
E.	_	_
General	_	_
Features	_	_
Learned	_	_
by	_	_
The	_	_
Network	_	_
In	_	_
recent	_	_
years	_	_
,	_	_
several	_	_
model	_	_
for	_	_
saliency	_	_
prediction	_	_
were	_	_
proposed	_	_
using	_	_
deep	_	_
learning	_	_
method	_	_
,	_	_
but	_	_
none	_	_
of	_	_
them	_	_
visualised	_	_
what	_	_
is	_	_
learned	_	_
by	_	_
those	_	_
models	_	_
.	_	_

#135
Therefore	_	_
,	_	_
we	_	_
adopt	_	_
the	_	_
popular	_	_
deep	_	_
neural	_	_
network	_	_
visualisation	_	_
technique	_	_
in	_	_
[	_	_
16	_	_
]	_	_
to	_	_
analyse	_	_
our	_	_
model	_	_
.	_	_

#136
There	_	_
are	_	_
three	_	_
main	_	_
processes	_	_
in	_	_
the	_	_
visualisation	_	_
:	_	_
upsampling	_	_
,	_	_
deconvolution	_	_
,	_	_
and	_	_
non-linearity	_	_
(	_	_
usually	_	_
a	_	_
ReLU	_	_
function	_	_
)	_	_
—the	_	_
reverse	_	_
of	_	_
the	_	_
forward	_	_
pass	_	_
when	_	_
input	_	_
an	_	_
image	_	_
into	_	_
the	_	_
deep	_	_
neural	_	_
network	_	_
.	_	_

#137
However	_	_
,	_	_
in	_	_
this	_	_
paper	_	_
we	_	_
are	_	_
more	_	_
interested	_	_
in	_	_
visualising	_	_
to	_	_
what	_	_
patterns	_	_
the	_	_
deep	_	_
neurons	_	_
are	_	_
attuned	_	_
to	_	_
rather	_	_
than	_	_
features	_	_
for	_	_
individual	_	_
images	_	_
.	_	_

#138
This	_	_
required	_	_
a	_	_
modification	_	_
of	_	_
the	_	_
upsampling	_	_
process	_	_
.	_	_

#139
The	_	_
classic	_	_
upsampling	_	_
method	_	_
in	_	_
feature	_	_
visualisation	_	_
is	_	_
unpooling	_	_
,	_	_
using	_	_
the	_	_
pooling	_	_
indices	_	_
in	_	_
the	_	_
forward	_	_
pass	_	_
to	_	_
do	_	_
unpooling	_	_
.	_	_

#140
Because	_	_
pooling	_	_
indices	_	_
only	_	_
exist	_	_
when	_	_
processing	_	_
an	_	_
actual	_	_
image	_	_
through	_	_
the	_	_
network	_	_
,	_	_
these	_	_
indices	_	_
are	_	_
not	_	_
available	_	_
when	_	_
visualising	_	_
a	_	_
neuron’s	_	_
receptive	_	_
field	_	_
in	_	_
abstraction	_	_
from	_	_
any	_	_
input	_	_
.	_	_

#141
Hence	_	_
,	_	_
in	_	_
order	_	_
to	_	_
visualise	_	_
general	_	_
individual	_	_
neuron’s	_	_
receptive	_	_
fields	_	_
,	_	_
we	_	_
set	_	_
the	_	_
pooled	_	_
feature	_	_
map	_	_
as	_	_
a	_	_
sparse	_	_
matrix	_	_
(	_	_
with	_	_
only	_	_
one	_	_
non-zero	_	_
value	_	_
)	_	_
and	_	_
do	_	_
upsampling	_	_
by	_	_
repeating	_	_
this	_	_
sparse	_	_
matrix	_	_
.	_	_

#142
Here	_	_
,	_	_
we	_	_
show	_	_
the	_	_
visualisation	_	_
results	_	_
of	_	_
our	_	_
model	_	_
:	_	_
the	_	_
features	_	_
learned	_	_
by	_	_
the	_	_
last	_	_
layer	_	_
of	_	_
the	_	_
encoder	_	_
part	_	_
in	_	_
our	_	_
Fig.	_	_
6	_	_
:	_	_
Illustration	_	_
of	_	_
the	_	_
receptive	_	_
field	_	_
of	_	_
the	_	_
features	_	_
learnt	_	_
by	_	_
the	_	_
first	_	_
64	_	_
neurons	_	_
of	_	_
top	_	_
encoder	_	_
layer	_	_
in	_	_
our	_	_
architecture	_	_
.	_	_

#143
network	_	_
,	_	_
there	_	_
are	_	_
512	_	_
neurons	_	_
.	_	_

#144
Here	_	_
,	_	_
we	_	_
show	_	_
the	_	_
general	_	_
feature	_	_
for	_	_
the	_	_
first	_	_
64	_	_
neurons	_	_
.	_	_

#145
From	_	_
the	_	_
Figure	_	_
6	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
features	_	_
learned	_	_
by	_	_
our	_	_
network	_	_
are	_	_
very	_	_
similar	_	_
to	_	_
central-surround	_	_
patterns	_	_
,	_	_
which	_	_
is	_	_
consistent	_	_
with	_	_
the	_	_
research	_	_
in	_	_
Psychology	_	_
[	_	_
27	_	_
]	_	_
.	_	_

#146
V.	_	_
CONCLUSION	_	_
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
reformulate	_	_
the	_	_
saliency	_	_
prediction	_	_
problem	_	_
as	_	_
an	_	_
image	_	_
segmentation	_	_
problem	_	_
,	_	_
according	_	_
to	_	_
different	_	_
saliency	_	_
levels	_	_
rather	_	_
than	_	_
the	_	_
traditional	_	_
pixwise	_	_
saliency	_	_
value	_	_
prediction	_	_
.	_	_

#147
We	_	_
adopt	_	_
the	_	_
encoder-decoder	_	_
architecture	_	_
in	_	_
semantic	_	_
segmentation	_	_
to	_	_
do	_	_
salient	_	_
region	_	_
segmentation	_	_
.	_	_

#148
Our	_	_
results	_	_
show	_	_
that	_	_
even	_	_
if	_	_
we	_	_
treat	_	_
it	_	_
as	_	_
a	_	_
segmentation	_	_
problem	_	_
,	_	_
it	_	_
can	_	_
still	_	_
challenge	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
,	_	_
and	_	_
the	_	_
proposed	_	_
model	_	_
trains	_	_
faster	_	_
and	_	_
more	_	_
reliably	_	_
than	_	_
regression-based	_	_
models	_	_
.	_	_

#149
Finally	_	_
,	_	_
we	_	_
also	_	_
demonstrated	_	_
that	_	_
the	_	_
learnt	_	_
deep	_	_
features	_	_
are	_	_
consistent	_	_
with	_	_
the	_	_
centre-surround	_	_
hypothesis	_	_
.	_	_