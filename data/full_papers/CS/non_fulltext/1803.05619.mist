#0
Fast	_	_
End-to-End	_	_
Trainable	_	_
Guided	_	_
Filter	_	_
Huikai	_	_
Wu	_	_
,	_	_
Student	_	_
Member	_	_
,	_	_
IEEE	_	_
,	_	_
Shuai	_	_
Zheng	_	_
,	_	_
Junge	_	_
Zhang	_	_
,	_	_
Member	_	_
,	_	_
IEEE	_	_
,	_	_
and	_	_
Kaiqi	_	_
Huang	_	_
,	_	_
Senior	_	_
Member	_	_
,	_	_
IEEE	_	_
Abstract‚ÄîDense	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
has	_	_
been	_	_
advanced	_	_
by	_	_
harnessing	_	_
the	_	_
capabilities	_	_
of	_	_
Fully	_	_
Convolutional	_	_
Networks	_	_
(	_	_
FCNs	_	_
)	_	_
.	_	_

#1
One	_	_
central	_	_
issue	_	_
of	_	_
FCNs	_	_
is	_	_
the	_	_
limited	_	_
capacity	_	_
to	_	_
handle	_	_
joint	_	_
upsampling	_	_
.	_	_

#2
To	_	_
address	_	_
the	_	_
problem	_	_
,	_	_
we	_	_
present	_	_
a	_	_
novel	_	_
building	_	_
block	_	_
for	_	_
FCNs	_	_
,	_	_
namely	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
which	_	_
is	_	_
designed	_	_
for	_	_
efficiently	_	_
generating	_	_
a	_	_
high-resolution	_	_
output	_	_
given	_	_
the	_	_
corresponding	_	_
low-resolution	_	_
one	_	_
and	_	_
a	_	_
high-resolution	_	_
guidance	_	_
map	_	_
.	_	_

#3
Such	_	_
a	_	_
layer	_	_
contains	_	_
learnable	_	_
parameters	_	_
,	_	_
which	_	_
can	_	_
be	_	_
integrated	_	_
with	_	_
FCNs	_	_
and	_	_
jointly	_	_
optimized	_	_
through	_	_
end-to-end	_	_
training	_	_
.	_	_

#4
To	_	_
further	_	_
take	_	_
advantage	_	_
of	_	_
end-to-end	_	_
training	_	_
,	_	_
we	_	_
plug	_	_
in	_	_
a	_	_
trainable	_	_
transformation	_	_
function	_	_
for	_	_
generating	_	_
the	_	_
task-specific	_	_
guidance	_	_
map	_	_
.	_	_

#5
Based	_	_
on	_	_
the	_	_
proposed	_	_
layer	_	_
,	_	_
we	_	_
present	_	_
a	_	_
general	_	_
framework	_	_
for	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
,	_	_
named	_	_
deep	_	_
guided	_	_
filtering	_	_
network	_	_
(	_	_
DGF	_	_
)	_	_
.	_	_

#6
The	_	_
proposed	_	_
network	_	_
is	_	_
evaluated	_	_
on	_	_
five	_	_
image	_	_
processing	_	_
tasks	_	_
.	_	_

#7
Experiments	_	_
on	_	_
MIT-Adobe	_	_
FiveK	_	_
Dataset	_	_
demonstrate	_	_
that	_	_
DGF	_	_
runs	_	_
10-100	_	_
times	_	_
faster	_	_
and	_	_
achieves	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
.	_	_

#8
We	_	_
also	_	_
show	_	_
that	_	_
DGF	_	_
helps	_	_
to	_	_
improve	_	_
the	_	_
performance	_	_
of	_	_
multiple	_	_
computer	_	_
vision	_	_
tasks	_	_
.	_	_

#9
Index	_	_
Terms‚ÄîJoint	_	_
Upsampling	_	_
,	_	_
Guided	_	_
Filtering	_	_
,	_	_
Pixel-wise	_	_
Image	_	_
Prediction	_	_
,	_	_
Model	_	_
Acceleration	_	_
,	_	_
Fully	_	_
Convolutional	_	_
Networks	_	_
I	_	_
.	_	_

#10
INTRODUCTION	_	_
DENSE	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
is	_	_
a	_	_
fundamental	_	_
image	_	_
processing	_	_
and	_	_
computer	_	_
vision	_	_
problem	_	_
and	_	_
has	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
applications	_	_
.	_	_

#11
In	_	_
image	_	_
processing	_	_
,	_	_
dense	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
enables	_	_
smoothing	_	_
an	_	_
image	_	_
while	_	_
preserving	_	_
the	_	_
edges	_	_
[	_	_
7	_	_
]	_	_
‚Äì	_	_
[	_	_
9	_	_
]	_	_
,	_	_
enhancing	_	_
the	_	_
details	_	_
of	_	_
an	_	_
image	_	_
[	_	_
3	_	_
]	_	_
,	_	_
[	_	_
10	_	_
]	_	_
,	_	_
transferring	_	_
the	_	_
style	_	_
from	_	_
a	_	_
reference	_	_
image	_	_
[	_	_
11	_	_
]	_	_
,	_	_
[	_	_
12	_	_
]	_	_
,	_	_
dehazing	_	_
the	_	_
photos	_	_
[	_	_
4	_	_
]	_	_
,	_	_
[	_	_
13	_	_
]	_	_
‚Äì	_	_
[	_	_
15	_	_
]	_	_
,	_	_
and	_	_
retouching	_	_
the	_	_
images	_	_
for	_	_
global	_	_
tonal	_	_
adjustment	_	_
[	_	_
2	_	_
]	_	_
.	_	_

#12
In	_	_
computer	_	_
vision	_	_
,	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
not	_	_
only	_	_
addresses	_	_
the	_	_
problem	_	_
of	_	_
segmenting	_	_
an	_	_
image	_	_
into	_	_
semantic	_	_
parts	_	_
[	_	_
16	_	_
]	_	_
‚Äì	_	_
[	_	_
18	_	_
]	_	_
,	_	_
but	_	_
also	_	_
helps	_	_
to	_	_
estimate	_	_
depth	_	_
from	_	_
a	_	_
single	_	_
image	_	_
[	_	_
19	_	_
]	_	_
,	_	_
and	_	_
detect	_	_
the	_	_
most	_	_
salient	_	_
object	_	_
in	_	_
an	_	_
image	_	_
[	_	_
20	_	_
]	_	_
,	_	_
[	_	_
21	_	_
]	_	_
.	_	_

#13
H.	_	_
Wu	_	_
and	_	_
J.	_	_
Zhang	_	_
are	_	_
with	_	_
Institute	_	_
of	_	_
Automation	_	_
,	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
,	_	_
Beijing	_	_
100190	_	_
,	_	_
China	_	_
,	_	_
and	_	_
also	_	_
with	_	_
the	_	_
University	_	_
of	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
,	_	_
Beijing	_	_
100049	_	_
,	_	_
China	_	_
.	_	_

#14
E-mail	_	_
:	_	_
huikai.wu	_	_
@	_	_
nlpr.ia.ac.cn	_	_
,	_	_
jgzhang	_	_
@	_	_
nlpr.ia.ac.cn	_	_
S.	_	_
Zheng	_	_
is	_	_
with	_	_
eBay	_	_
Research	_	_
.	_	_

#15
The	_	_
work	_	_
was	_	_
conducted	_	_
while	_	_
the	_	_
author	_	_
was	_	_
at	_	_
the	_	_
University	_	_
of	_	_
Oxford	_	_
.	_	_

#16
E-mail	_	_
:	_	_
shuzheng	_	_
@	_	_
ebay.com	_	_
K.	_	_
Huang	_	_
is	_	_
with	_	_
Institute	_	_
of	_	_
Automation	_	_
,	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
,	_	_
Beijing	_	_
100190	_	_
,	_	_
China	_	_
,	_	_
and	_	_
also	_	_
with	_	_
the	_	_
University	_	_
of	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
,	_	_
Beijing	_	_
100049	_	_
,	_	_
China	_	_
,	_	_
and	_	_
the	_	_
CAS	_	_
Center	_	_
for	_	_
Excellence	_	_
in	_	_
Brain	_	_
Science	_	_
and	_	_
Intelligence	_	_
Technology	_	_
,	_	_
100190	_	_
.	_	_

#17
E-mail	_	_
:	_	_
kqhuang	_	_
@	_	_
nlpr.ia.ac.cn	_	_
A	_	_
preliminary	_	_
version	_	_
of	_	_
this	_	_
work	_	_
[	_	_
1	_	_
]	_	_
appeared	_	_
in	_	_
IEEE	_	_
Conference	_	_
on	_	_
Computer	_	_
Vision	_	_
and	_	_
Pattern	_	_
Recognition	_	_
,	_	_
2018	_	_
.	_	_

#18
The	_	_
code	_	_
is	_	_
available	_	_
at	_	_
https	_	_
:	_	_
//github.com/wuhuikai/DeepGuidedFilter	_	_
.	_	_

#19
This	_	_
work	_	_
is	_	_
funded	_	_
by	_	_
the	_	_
National	_	_
Key	_	_
Research	_	_
and	_	_
Development	_	_
Program	_	_
of	_	_
China	_	_
(	_	_
Grant	_	_
2016YFB1001004	_	_
and	_	_
Grant	_	_
2016YFB1001005	_	_
)	_	_
,	_	_
the	_	_
National	_	_
Natural	_	_
Science	_	_
Foundation	_	_
of	_	_
China	_	_
(	_	_
Grant	_	_
61673375	_	_
,	_	_
Grant	_	_
61721004	_	_
and	_	_
Grant	_	_
61403383	_	_
)	_	_
and	_	_
the	_	_
Projects	_	_
of	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
(	_	_
Grant	_	_
QYZDB-SSW-JSC006	_	_
and	_	_
Grant	_	_
173211KYS-B20160008	_	_
)	_	_
.	_	_

#20
The	_	_
authors	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
Patrick	_	_
PeÃÅrez	_	_
and	_	_
Philip	_	_
Torr	_	_
for	_	_
their	_	_
helpful	_	_
suggestions	_	_
.	_	_

#21
Fig.	_	_
1	_	_
.	_	_

#22
Example	_	_
Results	_	_
of	_	_
Deep	_	_
Guided	_	_
Filtering	_	_
Network	_	_
.	_	_

#23
The	_	_
top	_	_
row	_	_
shows	_	_
the	_	_
input	_	_
images	_	_
,	_	_
and	_	_
the	_	_
bottom	_	_
row	_	_
presents	_	_
the	_	_
corresponding	_	_
outputs	_	_
.	_	_

#24
From	_	_
left	_	_
to	_	_
right	_	_
:	_	_
image	_	_
retouching	_	_
[	_	_
2	_	_
]	_	_
,	_	_
multi-scale	_	_
detail	_	_
manipulation	_	_
[	_	_
3	_	_
]	_	_
,	_	_
non-local	_	_
dehazing	_	_
[	_	_
4	_	_
]	_	_
,	_	_
saliency	_	_
detection	_	_
[	_	_
5	_	_
]	_	_
,	_	_
and	_	_
depth	_	_
estimation	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#25
Best	_	_
viewed	_	_
in	_	_
color	_	_
.	_	_

#26
Recent	_	_
methods	_	_
[	_	_
22	_	_
]	_	_
‚Äì	_	_
[	_	_
24	_	_
]	_	_
usually	_	_
employ	_	_
Fully	_	_
Convolutional	_	_
Networks	_	_
(	_	_
FCNs	_	_
)	_	_
for	_	_
these	_	_
applications	_	_
,	_	_
achieving	_	_
state-of-the-art	_	_
performance	_	_
.	_	_

#27
However	_	_
,	_	_
FCNs	_	_
usually	_	_
have	_	_
a	_	_
huge	_	_
computational	_	_
complexity	_	_
and	_	_
memory	_	_
usage	_	_
on	_	_
high-resolution	_	_
input	_	_
images	_	_
,	_	_
which	_	_
limits	_	_
the	_	_
deployment	_	_
of	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
algorithms	_	_
in	_	_
real-world	_	_
applications	_	_
.	_	_

#28
To	_	_
accelerate	_	_
FCNs	_	_
,	_	_
we	_	_
present	_	_
a	_	_
general	_	_
framework	_	_
by	_	_
following	_	_
a	_	_
coarse-to-fine	_	_
fashion	_	_
,	_	_
which	_	_
firstly	_	_
downsamples	_	_
the	_	_
input	_	_
image	_	_
,	_	_
executes	_	_
the	_	_
algorithm	_	_
at	_	_
low	_	_
resolution	_	_
,	_	_
and	_	_
then	_	_
upsamples	_	_
the	_	_
result	_	_
back	_	_
to	_	_
the	_	_
original	_	_
resolution	_	_
.	_	_

#29
The	_	_
main	_	_
challenge	_	_
is	_	_
restoring	_	_
the	_	_
low-resolution	_	_
output	_	_
to	_	_
the	_	_
original	_	_
resolution	_	_
with	_	_
rich	_	_
details	_	_
and	_	_
sharp	_	_
edges	_	_
.	_	_

#30
This	_	_
challenge	_	_
can	_	_
be	_	_
formulated	_	_
as	_	_
joint	_	_
upsampling	_	_
,	_	_
which	_	_
aims	_	_
at	_	_
generating	_	_
a	_	_
high-resolution	_	_
output	_	_
given	_	_
the	_	_
corresponding	_	_
low-resolution	_	_
one	_	_
and	_	_
a	_	_
high-resolution	_	_
guidance	_	_
map	_	_
.	_	_

#31
However	_	_
,	_	_
existing	_	_
building	_	_
blocks	_	_
of	_	_
FCNs	_	_
have	_	_
limited	_	_
capability	_	_
to	_	_
handle	_	_
such	_	_
a	_	_
problem	_	_
.	_	_

#32
To	_	_
enhance	_	_
the	_	_
ability	_	_
of	_	_
FCNs	_	_
for	_	_
joint	_	_
upsampling	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
reformulate	_	_
the	_	_
widely	_	_
used	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
into	_	_
a	_	_
fully	_	_
differentiable	_	_
building	_	_
block	_	_
,	_	_
which	_	_
can	_	_
be	_	_
(	_	_
1	_	_
)	_	_
jointly	_	_
trained	_	_
with	_	_
FCNs	_	_
,	_	_
(	_	_
2	_	_
)	_	_
adapted	_	_
for	_	_
different	_	_
tasks	_	_
by	_	_
learnable	_	_
parameters	_	_
,	_	_
and	_	_
(	_	_
3	_	_
)	_	_
directly	_	_
supervised	_	_
by	_	_
high-resolution	_	_
ground	_	_
truth	_	_
.	_	_

#33
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
building	_	_
block	_	_
for	_	_
FCNs	_	_
named	_	_
guided	_	_
filtering	_	_
layer	_	_
.	_	_

#34
Concretely	_	_
,	_	_
the	_	_
original	_	_
guided	_	_
filter	_	_
is	_	_
expressed	_	_
as	_	_
a	_	_
computational	_	_
graph	_	_
consisting	_	_
of	_	_
dilated	_	_
convolutions	_	_
and	_	_
pointwise	_	_
convolutions	_	_
with	_	_
learnable	_	_
parameters	_	_
,	_	_
which	_	_
can	_	_
adaptively	_	_
evolve	_	_
for	_	_
different	_	_
tasks	_	_
.	_	_

#35
A	_	_
trainable	_	_
transformation	_	_
function	_	_
is	_	_
introduced	_	_
into	_	_
the	_	_
proposed	_	_
layer	_	_
,	_	_
which	_	_
can	_	_
generate	_	_
a	_	_
task-specific	_	_
guidance	_	_
map	_	_
.	_	_

#36
As	_	_
a	_	_
result	_	_
,	_	_
all	_	_
the	_	_
parameters	_	_
of	_	_
a	_	_
guided	_	_
filtering	_	_
layer	_	_
can	_	_
be	_	_
learned	_	_
in	_	_
a	_	_
data-driven	_	_
manner	_	_
through	_	_
endtoar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#37
9v	_	_
2	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
5	_	_
Se	_	_
p	_	_
end	_	_
training	_	_
.	_	_

#38
Moreover	_	_
,	_	_
such	_	_
a	_	_
layer	_	_
can	_	_
be	_	_
easily	_	_
integrated	_	_
with	_	_
a	_	_
pre-defined	_	_
FCN	_	_
without	_	_
extra	_	_
efforts	_	_
.	_	_

#39
By	_	_
equipping	_	_
FCNs	_	_
with	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
we	_	_
present	_	_
a	_	_
general	_	_
framework	_	_
for	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
tasks	_	_
named	_	_
Deep	_	_
Guided	_	_
Filtering	_	_
Network	_	_
(	_	_
DGF	_	_
)	_	_
,	_	_
which	_	_
can	_	_
largely	_	_
reduce	_	_
the	_	_
computational	_	_
complexity	_	_
and	_	_
memory	_	_
usage	_	_
.	_	_

#40
The	_	_
proposed	_	_
framework	_	_
can	_	_
be	_	_
widely	_	_
employed	_	_
for	_	_
many	_	_
image	_	_
processing	_	_
and	_	_
computer	_	_
vision	_	_
tasks	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig	_	_
1	_	_
.	_	_

#41
Experiments	_	_
show	_	_
that	_	_
DGF	_	_
achieves	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
in	_	_
quality	_	_
,	_	_
speed	_	_
,	_	_
and	_	_
memory	_	_
usage	_	_
.	_	_

#42
In	_	_
summary	_	_
,	_	_
the	_	_
main	_	_
contribution	_	_
of	_	_
this	_	_
paper	_	_
is	_	_
that	_	_
(	_	_
1	_	_
)	_	_
we	_	_
develop	_	_
an	_	_
end-to-end	_	_
trainable	_	_
guided	_	_
filtering	_	_
layer	_	_
with	_	_
learnable	_	_
parameters	_	_
and	_	_
a	_	_
trainable	_	_
guidance	_	_
map	_	_
,	_	_
which	_	_
enhances	_	_
the	_	_
ability	_	_
of	_	_
FCNs	_	_
for	_	_
joint	_	_
upsampling	_	_
;	_	_
(	_	_
2	_	_
)	_	_
by	_	_
combining	_	_
with	_	_
FCNs	_	_
,	_	_
the	_	_
proposed	_	_
layer	_	_
significantly	_	_
improves	_	_
the	_	_
state-of-the-art	_	_
results	_	_
in	_	_
multiple	_	_
image	_	_
processing	_	_
tasks	_	_
,	_	_
and	_	_
runs	_	_
10-100√ó	_	_
faster	_	_
than	_	_
the	_	_
alternatives	_	_
;	_	_
and	_	_
(	_	_
3	_	_
)	_	_
additional	_	_
experiments	_	_
show	_	_
that	_	_
our	_	_
approach	_	_
generalizes	_	_
well	_	_
to	_	_
many	_	_
computer	_	_
vision	_	_
tasks	_	_
and	_	_
achieves	_	_
significant	_	_
improvements	_	_
over	_	_
baseline	_	_
methods	_	_
.	_	_

#43
An	_	_
early	_	_
version	_	_
of	_	_
this	_	_
paper	_	_
[	_	_
1	_	_
]	_	_
appeared	_	_
in	_	_
IEEE	_	_
Conference	_	_
on	_	_
Computer	_	_
Vision	_	_
and	_	_
Pattern	_	_
Recognition	_	_
(	_	_
2018	_	_
)	_	_
,	_	_
to	_	_
which	_	_
we	_	_
have	_	_
made	_	_
substantial	_	_
extensions	_	_
.	_	_

#44
The	_	_
improvements	_	_
are	_	_
shown	_	_
below	_	_
:	_	_
(	_	_
1	_	_
)	_	_
[	_	_
1	_	_
]	_	_
formulate	_	_
the	_	_
original	_	_
guided	_	_
filter	_	_
into	_	_
a	_	_
series	_	_
of	_	_
spatially	_	_
varying	_	_
linear	_	_
transformation	_	_
matrices	_	_
without	_	_
any	_	_
learnable	_	_
parameters	_	_
.	_	_

#45
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
reformulate	_	_
the	_	_
original	_	_
guided	_	_
filter	_	_
into	_	_
a	_	_
block	_	_
of	_	_
dilated	_	_
convolutions	_	_
and	_	_
pointwise	_	_
convolutions	_	_
with	_	_
learnable	_	_
parameters	_	_
.	_	_

#46
Such	_	_
a	_	_
formulation	_	_
enables	_	_
guided	_	_
filtering	_	_
layer	_	_
to	_	_
fit	_	_
a	_	_
specific	_	_
task	_	_
through	_	_
end-to-end	_	_
training	_	_
.	_	_

#47
(	_	_
2	_	_
)	_	_
Based	_	_
on	_	_
the	_	_
improved	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
we	_	_
further	_	_
boosted	_	_
the	_	_
performance	_	_
of	_	_
DGF	_	_
in	_	_
five	_	_
image	_	_
processing	_	_
tasks	_	_
.	_	_

#48
(	_	_
3	_	_
)	_	_
We	_	_
conduct	_	_
a	_	_
systematic	_	_
ablation	_	_
study	_	_
on	_	_
five	_	_
image	_	_
processing	_	_
tasks	_	_
to	_	_
analysis	_	_
the	_	_
influence	_	_
of	_	_
each	_	_
hyper-parameter	_	_
in	_	_
DGF	_	_
.	_	_

#49
(	_	_
4	_	_
)	_	_
We	_	_
demonstrate	_	_
the	_	_
upper	_	_
bound	_	_
of	_	_
the	_	_
proposed	_	_
layer‚Äôs	_	_
ability	_	_
in	_	_
joint	_	_
upsampling	_	_
through	_	_
a	_	_
comprehensive	_	_
experiment	_	_
.	_	_

#50
(	_	_
5	_	_
)	_	_
Both	_	_
the	_	_
training	_	_
code	_	_
and	_	_
testing	_	_
code	_	_
are	_	_
released	_	_
for	_	_
reproducing	_	_
the	_	_
experimental	_	_
results	_	_
in	_	_
this	_	_
paper	_	_
and	_	_
supporting	_	_
further	_	_
research	_	_
as	_	_
well	_	_
as	_	_
other	_	_
applications	_	_
.	_	_

#51
II	_	_
.	_	_

#52
RELATED	_	_
WORK	_	_
A	_	_
.	_	_

#53
Joint	_	_
Upsampling	_	_
The	_	_
most	_	_
related	_	_
works	_	_
to	_	_
our	_	_
method	_	_
are	_	_
along	_	_
the	_	_
direction	_	_
of	_	_
joint	_	_
upsampling	_	_
.	_	_

#54
Many	_	_
algorithms	_	_
have	_	_
been	_	_
developed	_	_
to	_	_
tackle	_	_
this	_	_
problem	_	_
.	_	_

#55
Joint	_	_
bilateral	_	_
upsampling	_	_
[	_	_
26	_	_
]	_	_
applies	_	_
a	_	_
bilateral	_	_
filter	_	_
[	_	_
27	_	_
]	_	_
to	_	_
the	_	_
high-resolution	_	_
guidance	_	_
map	_	_
,	_	_
resulting	_	_
in	_	_
a	_	_
piecewise-smoothing	_	_
high-resolution	_	_
output	_	_
.	_	_

#56
The	_	_
underlying	_	_
bilateral	_	_
filter	_	_
usually	_	_
requires	_	_
a	_	_
large	_	_
amount	_	_
of	_	_
computation	_	_
resources	_	_
.	_	_

#57
Thus	_	_
,	_	_
many	_	_
methods	_	_
[	_	_
28	_	_
]	_	_
‚Äì	_	_
[	_	_
30	_	_
]	_	_
are	_	_
presented	_	_
to	_	_
reduce	_	_
the	_	_
computation	_	_
complexity	_	_
.	_	_

#58
Build	_	_
on	_	_
joint	_	_
bilateral	_	_
upsampling	_	_
,	_	_
Barron	_	_
et	_	_
al.	_	_
[	_	_
31	_	_
]	_	_
present	_	_
a	_	_
new	_	_
form	_	_
of	_	_
bilateral-space	_	_
optimization	_	_
that	_	_
efficiently	_	_
solves	_	_
a	_	_
regularized	_	_
least-squares	_	_
optimization	_	_
problem	_	_
to	_	_
produce	_	_
an	_	_
output	_	_
that	_	_
is	_	_
bilateral-smooth	_	_
and	_	_
close	_	_
to	_	_
the	_	_
input	_	_
.	_	_

#59
Gharbi	_	_
et	_	_
al.	_	_
[	_	_
32	_	_
]	_	_
first	_	_
compute	_	_
a	_	_
description	_	_
of	_	_
the	_	_
transformation	_	_
from	_	_
a	_	_
highly	_	_
compressed	_	_
input	_	_
to	_	_
output	_	_
.	_	_

#60
Then	_	_
a	_	_
high-fidelity	_	_
approximation	_	_
of	_	_
the	_	_
output	_	_
can	_	_
be	_	_
constructed	_	_
by	_	_
applying	_	_
the	_	_
recipe	_	_
to	_	_
the	_	_
high-quality	_	_
input	_	_
.	_	_

#61
Similarly	_	_
,	_	_
bilateral	_	_
guided	_	_
upsampling	_	_
[	_	_
33	_	_
]	_	_
fits	_	_
an	_	_
image	_	_
operator	_	_
with	_	_
a	_	_
grid	_	_
of	_	_
local	_	_
affine	_	_
models	_	_
on	_	_
the	_	_
low-resolution	_	_
input/output	_	_
pair	_	_
firstly	_	_
.	_	_

#62
The	_	_
high-resolution	_	_
output	_	_
is	_	_
then	_	_
generated	_	_
by	_	_
applying	_	_
the	_	_
local	_	_
affine	_	_
model	_	_
to	_	_
the	_	_
high-resolution	_	_
input	_	_
image	_	_
.	_	_

#63
This	_	_
method	_	_
serves	_	_
as	_	_
a	_	_
post-processing	_	_
operation	_	_
,	_	_
while	_	_
our	_	_
approach	_	_
can	_	_
be	_	_
jointly	_	_
trained	_	_
with	_	_
the	_	_
entire	_	_
FCN	_	_
.	_	_

#64
Deep	_	_
bilateral	_	_
learning	_	_
[	_	_
22	_	_
]	_	_
integrate	_	_
bilateral	_	_
filter	_	_
with	_	_
FCNs	_	_
,	_	_
which	_	_
can	_	_
be	_	_
jointly	_	_
learned	_	_
through	_	_
end-to-end	_	_
training	_	_
.	_	_

#65
However	_	_
,	_	_
this	_	_
method	_	_
requires	_	_
producing	_	_
affine	_	_
coefficients	_	_
before	_	_
obtaining	_	_
outputs	_	_
,	_	_
which	_	_
lacks	_	_
direct	_	_
supervision	_	_
from	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#66
For	_	_
computer	_	_
vision	_	_
tasks	_	_
,	_	_
the	_	_
number	_	_
of	_	_
affine	_	_
coefficients	_	_
is	_	_
usually	_	_
very	_	_
large	_	_
,	_	_
which	_	_
becomes	_	_
the	_	_
bottleneck	_	_
of	_	_
performance	_	_
and	_	_
speed	_	_
.	_	_

#67
Besides	_	_
bilateral	_	_
filter	_	_
,	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
is	_	_
also	_	_
widely	_	_
used	_	_
in	_	_
joint	_	_
upsampling	_	_
,	_	_
which	_	_
derived	_	_
from	_	_
a	_	_
local	_	_
linear	_	_
model	_	_
and	_	_
computes	_	_
the	_	_
filtering	_	_
output	_	_
by	_	_
considering	_	_
the	_	_
content	_	_
of	_	_
a	_	_
guidance	_	_
image	_	_
.	_	_

#68
Compared	_	_
to	_	_
it	_	_
,	_	_
our	_	_
method	_	_
is	_	_
formulated	_	_
as	_	_
a	_	_
fully	_	_
differentiable	_	_
building	_	_
block	_	_
with	_	_
learnable	_	_
parameters	_	_
,	_	_
which	_	_
can	_	_
be	_	_
jointly	_	_
trained	_	_
with	_	_
FCNs	_	_
and	_	_
adaptively	_	_
adjusted	_	_
according	_	_
to	_	_
a	_	_
specific	_	_
task	_	_
.	_	_

#69
Similarly	_	_
,	_	_
Yuan	_	_
et	_	_
al.	_	_
[	_	_
34	_	_
]	_	_
employ	_	_
a	_	_
locally-affine	_	_
model	_	_
to	_	_
relate	_	_
patches	_	_
from	_	_
low-resolution	_	_
RAW	_	_
images	_	_
to	_	_
high-resolution	_	_
JPEG	_	_
images	_	_
.	_	_

#70
The	_	_
above	_	_
methods	_	_
are	_	_
based	_	_
on	_	_
edge-preserving	_	_
local	_	_
filters	_	_
.	_	_

#71
Differently	_	_
,	_	_
other	_	_
methods	_	_
[	_	_
35	_	_
]	_	_
‚Äì	_	_
[	_	_
37	_	_
]	_	_
produce	_	_
high-resolution	_	_
outputs	_	_
by	_	_
optimizing	_	_
manually	_	_
designed	_	_
objective	_	_
functions	_	_
involving	_	_
all	_	_
or	_	_
many	_	_
pixels	_	_
.	_	_

#72
The	_	_
objective	_	_
functions	_	_
typically	_	_
consist	_	_
of	_	_
data	_	_
terms	_	_
and	_	_
regularization	_	_
terms	_	_
like	_	_
total	_	_
variation	_	_
(	_	_
TV	_	_
)	_	_
[	_	_
35	_	_
]	_	_
,	_	_
weighted	_	_
least	_	_
squares	_	_
(	_	_
WLS	_	_
)	_	_
[	_	_
36	_	_
]	_	_
,	_	_
and	_	_
scale	_	_
map	_	_
scheme	_	_
[	_	_
37	_	_
]	_	_
.	_	_

#73
Following	_	_
these	_	_
methods	_	_
,	_	_
Shen	_	_
et	_	_
al.	_	_
[	_	_
38	_	_
]	_	_
propose	_	_
mutual-structure	_	_
to	_	_
reserve	_	_
the	_	_
structural	_	_
information	_	_
that	_	_
is	_	_
contained	_	_
in	_	_
both	_	_
images	_	_
.	_	_

#74
Similarly	_	_
,	_	_
Ham	_	_
et	_	_
al.	_	_
[	_	_
39	_	_
]	_	_
formulate	_	_
the	_	_
issue	_	_
as	_	_
a	_	_
non-convex	_	_
optimization	_	_
problem	_	_
,	_	_
which	_	_
is	_	_
solved	_	_
by	_	_
the	_	_
majorization-minimization	_	_
algorithm	_	_
.	_	_

#75
Compared	_	_
to	_	_
our	_	_
method	_	_
,	_	_
the	_	_
main	_	_
drawbacks	_	_
of	_	_
these	_	_
methods	_	_
are	_	_
(	_	_
1	_	_
)	_	_
they	_	_
rely	_	_
on	_	_
hand-designed	_	_
objective	_	_
functions	_	_
,	_	_
and	_	_
(	_	_
2	_	_
)	_	_
they	_	_
are	_	_
usually	_	_
time-consuming	_	_
.	_	_

#76
B	_	_
.	_	_

#77
Deep	_	_
Learning	_	_
based	_	_
Image	_	_
Filter	_	_
Recently	_	_
,	_	_
deep	_	_
learning	_	_
based	_	_
methods	_	_
are	_	_
proposed	_	_
in	_	_
image	_	_
processing	_	_
tasks	_	_
,	_	_
which	_	_
largely	_	_
advanced	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
.	_	_

#78
Such	_	_
tasks	_	_
include	_	_
image	_	_
denoising	_	_
[	_	_
40	_	_
]	_	_
,	_	_
image	_	_
demosaicking	_	_
[	_	_
41	_	_
]	_	_
,	_	_
image	_	_
deblurring	_	_
[	_	_
42	_	_
]	_	_
,	_	_
image	_	_
matting	_	_
[	_	_
43	_	_
]	_	_
,	_	_
rain	_	_
drop	_	_
removal	_	_
[	_	_
44	_	_
]	_	_
,	_	_
image	_	_
dehazing	_	_
[	_	_
45	_	_
]	_	_
,	_	_
and	_	_
image	_	_
colorization	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#79
The	_	_
above	_	_
methods	_	_
mainly	_	_
focus	_	_
on	_	_
solving	_	_
one	_	_
specific	_	_
image	_	_
processing	_	_
task	_	_
.	_	_

#80
Differently	_	_
,	_	_
some	_	_
other	_	_
works	_	_
[	_	_
47	_	_
]	_	_
‚Äì	_	_
[	_	_
49	_	_
]	_	_
aim	_	_
at	_	_
approximating	_	_
a	_	_
general	_	_
class	_	_
of	_	_
operators	_	_
.	_	_

#81
Xu	_	_
et	_	_
al.	_	_
[	_	_
47	_	_
]	_	_
employ	_	_
deep	_	_
neural	_	_
networks	_	_
to	_	_
approximate	_	_
a	_	_
variety	_	_
of	_	_
edge-preserving	_	_
filters	_	_
with	_	_
a	_	_
gradient-domain	_	_
training	_	_
procedure	_	_
,	_	_
while	_	_
Liu	_	_
et	_	_
al.	_	_
[	_	_
48	_	_
]	_	_
combine	_	_
a	_	_
convolutional	_	_
network	_	_
and	_	_
a	_	_
set	_	_
of	_	_
recurrent	_	_
networks	_	_
to	_	_
approximate	_	_
various	_	_
image	_	_
filters	_	_
.	_	_

#82
Xu	_	_
et	_	_
al.	_	_
[	_	_
47	_	_
]	_	_
and	_	_
Liu	_	_
et	_	_
al.	_	_
[	_	_
48	_	_
]	_	_
deploy	_	_
neural	_	_
networks	_	_
to	_	_
generate	_	_
high-resolution	_	_
output	_	_
directly	_	_
,	_	_
accelerating	_	_
the	_	_
operation	_	_
by	_	_
dedicatedly	_	_
designed	_	_
network	_	_
architectures	_	_
.	_	_

#83
Similarly	_	_
,	_	_
Chen	_	_
et	_	_
al.	_	_
[	_	_
23	_	_
]	_	_
propose	_	_
context	_	_
aggregation	_	_
networks	_	_
to	_	_
ùêº	_	_
"	_	_
ùëÇ	_	_
"	_	_
ùêº	_	_
$	_	_
Mean	_	_
Filter	_	_
ùëì	_	_
&	_	_
?	_	_
ÃÖ	_	_
?	_	_
"	_	_

#84
ùëÇ	_	_
)	_	_
"	_	_
Local	_	_
Linear	_	_
Model	_	_
Bilinear	_	_
Upsample	_	_
ùëì‚Üë	_	_
Œ£	_	_
,	_	_
Œ£	_	_
,	_	_
.	_	_

#85
ùê¥	_	_
"	_	_
,	_	_
ùëè	_	_
"	_	_
ùê¥	_	_
$	_	_
ùëè	_	_
$	_	_
Linear	_	_
LayerùëÇ	_	_
$	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
ùêÖ	_	_
(	_	_
ùêà	_	_
)	_	_
ùêÖ	_	_
(	_	_
ùêà	_	_
)	_	_
ùê∫	_	_
$	_	_
ùê∫	_	_
"	_	_
ùúïùêø	_	_
ùúïùëÇ	_	_
$	_	_
‚ÅÑ	_	_
ùúïùêø	_	_
ùúïùê∫	_	_
$	_	_
‚ÅÑ	_	_
ùúïùêø	_	_
ùúïùê¥	_	_
"	_	_
‚ÅÑ	_	_
ùúïùêø	_	_
ùúïùëè	_	_
"	_	_
‚ÅÑ	_	_
ùúïùêø	_	_
ùúïùëÇ	_	_
"	_	_
‚ÅÑ	_	_
‚Ä¶‚Ä¶	_	_
ùëü	_	_
ùúñ	_	_
Fig.	_	_
2	_	_
.	_	_

#86
Computation	_	_
Graph	_	_
of	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
.	_	_

#87
Guided	_	_
filtering	_	_
layer	_	_
takes	_	_
low-resolution	_	_
image	_	_
Il	_	_
,	_	_
high-resolution	_	_
image	_	_
Ih	_	_
and	_	_
low-resolution	_	_
output	_	_
Ol	_	_
as	_	_
inputs	_	_
,	_	_
generating	_	_
the	_	_
high-resolution	_	_
output	_	_
Oh	_	_
.	_	_

#88
Compared	_	_
to	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
,	_	_
the	_	_
proposed	_	_
layer	_	_
is	_	_
reformulated	_	_
into	_	_
a	_	_
fully	_	_
differentiable	_	_
block	_	_
and	_	_
employs	_	_
F	_	_
(	_	_
I	_	_
)	_	_
to	_	_
generate	_	_
task-specific	_	_
guidance	_	_
map	_	_
.	_	_

#89
accelerate	_	_
a	_	_
wide	_	_
variety	_	_
of	_	_
image	_	_
processing	_	_
operators	_	_
,	_	_
which	_	_
performs	_	_
superior	_	_
to	_	_
the	_	_
prior	_	_
works	_	_
[	_	_
33	_	_
]	_	_
,	_	_
[	_	_
47	_	_
]	_	_
,	_	_
[	_	_
48	_	_
]	_	_
,	_	_
[	_	_
50	_	_
]	_	_
,	_	_
[	_	_
51	_	_
]	_	_
,	_	_
achieving	_	_
the	_	_
best	_	_
results	_	_
regarding	_	_
speed	_	_
and	_	_
accuracy	_	_
.	_	_

#90
Our	_	_
approach	_	_
is	_	_
complementary	_	_
to	_	_
this	_	_
method	_	_
,	_	_
which	_	_
can	_	_
deliver	_	_
comparable	_	_
or	_	_
better	_	_
results	_	_
and	_	_
runs	_	_
10-100√ó	_	_
faster	_	_
.	_	_

#91
Compared	_	_
to	_	_
all	_	_
the	_	_
related	_	_
works	_	_
,	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
can	_	_
be	_	_
end-to-end	_	_
trained	_	_
with	_	_
the	_	_
entire	_	_
network	_	_
and	_	_
generalize	_	_
well	_	_
across	_	_
different	_	_
tasks	_	_
ranging	_	_
from	_	_
image	_	_
processing	_	_
to	_	_
computer	_	_
vision	_	_
,	_	_
while	_	_
achieving	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
in	_	_
both	_	_
quality	_	_
and	_	_
speed	_	_
.	_	_

#92
III	_	_
.	_	_

#93
GUIDED	_	_
FILTERING	_	_
LAYER	_	_
A	_	_
.	_	_

#94
Problem	_	_
Formulation	_	_
Given	_	_
a	_	_
high-resolution	_	_
image	_	_
Ih	_	_
and	_	_
the	_	_
corresponding	_	_
low-resolution	_	_
output	_	_
Ol	_	_
,	_	_
joint	_	_
upsampling	_	_
aims	_	_
at	_	_
generating	_	_
a	_	_
high-resolution	_	_
output	_	_
Oh	_	_
that	_	_
is	_	_
visually	_	_
similar	_	_
to	_	_
Ol	_	_
and	_	_
preserves	_	_
the	_	_
edges	_	_
and	_	_
details	_	_
from	_	_
Ih	_	_
.	_	_

#95
In	_	_
the	_	_
literature	_	_
of	_	_
joint	_	_
upsampling	_	_
,	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
is	_	_
one	_	_
of	_	_
the	_	_
most	_	_
widely	_	_
used	_	_
algorithms	_	_
that	_	_
has	_	_
shown	_	_
better	_	_
performance	_	_
regarding	_	_
the	_	_
trade-off	_	_
between	_	_
speed	_	_
and	_	_
accuracy	_	_
.	_	_

#96
B	_	_
.	_	_

#97
Guided	_	_
Filter	_	_
Revisited	_	_
To	_	_
address	_	_
joint	_	_
upsampling	_	_
,	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
takes	_	_
a	_	_
low-resolution	_	_
image	_	_
Il	_	_
,	_	_
the	_	_
corresponding	_	_
high-resolution	_	_
image	_	_
Ih	_	_
,	_	_
and	_	_
a	_	_
low-resolution	_	_
output	_	_
Ol	_	_
as	_	_
inputs	_	_
,	_	_
producing	_	_
the	_	_
high-resolution	_	_
output	_	_
Oh	_	_
.	_	_

#98
Concretely	_	_
,	_	_
Al	_	_
and	_	_
bl	_	_
are	_	_
firstly	_	_
obtained	_	_
by	_	_
minimizing	_	_
a	_	_
reconstruction	_	_
error	_	_
between	_	_
OÃÇl	_	_
and	_	_
Ol	_	_
,	_	_
where	_	_
OÃÇl	_	_
subjects	_	_
to	_	_
a	_	_
local	_	_
linear	_	_
model	_	_
:	_	_
OÃÇil	_	_
=	_	_
Akl	_	_
I	_	_
i	_	_
l	_	_
+	_	_
bkl	_	_
,	_	_
‚àÄi	_	_
‚àà	_	_
œâk	_	_
.	_	_

#99
(	_	_
1	_	_
)	_	_
œâk	_	_
is	_	_
the	_	_
k-th	_	_
local	_	_
square	_	_
window	_	_
on	_	_
Il	_	_
,	_	_
and	_	_
Iil	_	_
is	_	_
the	_	_
i-th	_	_
pixel	_	_
inside	_	_
œâk	_	_
.	_	_

#100
Ah	_	_
and	_	_
bh	_	_
are	_	_
then	_	_
produced	_	_
by	_	_
upsampling	_	_
Al	_	_
and	_	_
bl	_	_
.	_	_

#101
The	_	_
high-resolution	_	_
output	_	_
Oh	_	_
is	_	_
finally	_	_
generated	_	_
by	_	_
a	_	_
linear	_	_
transformation	_	_
model	_	_
:	_	_
Oh	_	_
=	_	_
Ah	_	_
‚àó	_	_
Ih	_	_
+	_	_
bh	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
‚àó	_	_
is	_	_
element-wise	_	_
multiplication	_	_
.	_	_

#102
Algorithm	_	_
1	_	_
:	_	_
Gradients	_	_
for	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
Input	_	_
:	_	_
Low-resolution	_	_
image	_	_
Il	_	_
High-resolution	_	_
image	_	_
Ih	_	_
Low-resolution	_	_
output	_	_
Ol	_	_
Derivative	_	_
for	_	_
high-resolution	_	_
output	_	_
‚àÇOh	_	_
Output	_	_
:	_	_
Gradients	_	_
for	_	_
all	_	_
the	_	_
inputs	_	_
1	_	_
‚àÇbl	_	_
=	_	_
‚àÇOh	_	_
¬∑	_	_
‚àáblf‚Üë	_	_
‚àÇAl	_	_
=	_	_
‚àÇOh	_	_
‚àóGh	_	_
¬∑	_	_
‚àáAl	_	_
f‚Üë	_	_
‚àí	_	_
‚àÇbl	_	_
‚àó	_	_
GÃÑl	_	_
2	_	_
‚àÇŒ£GlOl	_	_
=	_	_
‚àÇAl/	_	_
(	_	_
Œ£Gl	_	_
+	_	_
Œµ	_	_
)	_	_
‚àÇŒ£Gl	_	_
=	_	_
‚àí‚àÇAl	_	_
‚àó	_	_
Œ£GlOl	_	_
/	_	_
(	_	_
Œ£Gl	_	_
+	_	_
Œµ	_	_
)	_	_
2	_	_
3	_	_
‚àÇOÃÑl	_	_
=	_	_
‚àÇbl	_	_
‚àí	_	_
‚àÇŒ£GlOl	_	_
‚àó	_	_
GÃÑl	_	_
‚àÇOl	_	_
=	_	_
‚àÇŒ£GlOl	_	_
¬∑	_	_
‚àáGl‚àóOl	_	_
f¬µ	_	_
‚àóGl	_	_
+	_	_
‚àÇOÃÑl	_	_
¬∑	_	_
‚àáOl	_	_
f¬µ	_	_
4	_	_
‚àÇGÃÑl	_	_
=	_	_
‚àí‚àÇbl	_	_
‚àóAl	_	_
‚àí	_	_
‚àÇŒ£GlOl	_	_
‚àó	_	_
OÃÑl	_	_
‚àí	_	_
2‚àÇŒ£Gl	_	_
‚àó	_	_
GÃÑl	_	_
5	_	_
‚àÇGl	_	_
=	_	_
‚àÇŒ£GlOl	_	_
¬∑	_	_
‚àáGl‚àóOl	_	_
f¬µ	_	_
‚àóOl	_	_
+	_	_
2‚àÇŒ£Gl	_	_
¬∑	_	_
‚àáGl‚àóGl	_	_
f¬µ	_	_
‚àóGl	_	_
+	_	_
‚àÇGÃÑl	_	_
¬∑	_	_
‚àáGl	_	_
f¬µ	_	_
6	_	_
‚àÇIl	_	_
=	_	_
‚àÇGl	_	_
¬∑	_	_
‚àáIlF	_	_
‚àÇIh	_	_
=	_	_
‚àÇOh	_	_
‚àóAh	_	_
¬∑	_	_
‚àáIhF	_	_
C.	_	_
Fully	_	_
Differentiable	_	_
Guided	_	_
Filter	_	_
The	_	_
original	_	_
guided	_	_
filter	_	_
can	_	_
only	_	_
be	_	_
employed	_	_
as	_	_
a	_	_
post-processing	_	_
operation	_	_
,	_	_
which	_	_
is	_	_
not	_	_
differentiable	_	_
and	_	_
can	_	_
not	_	_
be	_	_
end-to-end	_	_
trained	_	_
with	_	_
FCNs	_	_
.	_	_

#103
To	_	_
enhance	_	_
the	_	_
ability	_	_
of	_	_
FCNs	_	_
for	_	_
joint	_	_
upsampling	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
building	_	_
block	_	_
by	_	_
reformulating	_	_
guided	_	_
filter	_	_
into	_	_
a	_	_
fully	_	_
differentiable	_	_
layer	_	_
.	_	_

#104
Such	_	_
a	_	_
layer	_	_
,	_	_
named	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
can	_	_
be	_	_
jointly	_	_
trained	_	_
with	_	_
FCNs	_	_
from	_	_
scratch	_	_
,	_	_
and	_	_
directly	_	_
supervised	_	_
by	_	_
high-resolution	_	_
targets	_	_
.	_	_

#105
The	_	_
computation	_	_
graph	_	_
of	_	_
guided	_	_
filtering	_	_
layer	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
.	_	_

#106
Al	_	_
and	_	_
bl	_	_
are	_	_
obtained	_	_
by	_	_
employing	_	_
mean	_	_
filter	_	_
f¬µ	_	_
and	_	_
local	_	_
linear	_	_
model	_	_
to	_	_
Il	_	_
and	_	_
Ol	_	_
,	_	_
where	_	_
f¬µ	_	_
is	_	_
implemented	_	_
as	_	_
a	_	_
box	_	_
filter	_	_
to	_	_
reduce	_	_
the	_	_
computation	_	_
complexity	_	_
.	_	_

#107
Ah	_	_
and	_	_
bh	_	_
are	_	_
then	_	_
generated	_	_
by	_	_
bilinear	_	_
upsampling	_	_
f‚Üë	_	_
.	_	_

#108
Oh	_	_
is	_	_
finally	_	_
produced	_	_
by	_	_
a	_	_
linear	_	_
layer	_	_
taking	_	_
Ah	_	_
,	_	_
bh	_	_
and	_	_
Ih	_	_
as	_	_
inputs	_	_
.	_	_

#109
r	_	_
is	_	_
the	_	_
radius	_	_
of	_	_
f¬µ	_	_
and	_	_
Œµ	_	_
is	_	_
the	_	_
regularization	_	_
term	_	_
,	_	_
which	_	_
are	_	_
set	_	_
to	_	_
be	_	_
1	_	_
and	_	_
1e-8	_	_
by	_	_
default	_	_
.	_	_

#110
The	_	_
equations	_	_
for	_	_
propagating	_	_
the	_	_
gradients	_	_
through	_	_
guided	_	_
filtering	_	_
layer	_	_
are	_	_
shown	_	_
in	_	_
Algorithm	_	_
1	_	_
.	_	_

#111
By	_	_
formulating	_	_
each	_	_
operator	_	_
into	_	_
a	_	_
differentiable	_	_
function	_	_
,	_	_
the	_	_
gradient	_	_
of	_	_
Oh	_	_
backpropagates	_	_
to	_	_
Ol	_	_
,	_	_
Il	_	_
,	_	_
and	_	_
Ih	_	_
through	_	_
the	_	_
computation	_	_
graph	_	_
,	_	_
which	_	_
enables	_	_
both	_	_
the	_	_
joint	_	_
training	_	_
of	_	_
FCNs	_	_
and	_	_
guided	_	_
filtering	_	_
layer	_	_
with	_	_
direct	_	_
guidance	_	_
from	_	_
the	_	_
high-resolution	_	_
targets	_	_
.	_	_

#112
As	_	_
a	_	_
result	_	_
,	_	_
FCNs	_	_
can	_	_
learn	_	_
to	_	_
generate	_	_
a	_	_
more	_	_
suitable	_	_
Ol	_	_
for	_	_
guided	_	_
filtering	_	_
layer	_	_
to	_	_
restore	_	_
Oh	_	_
.	_	_

#113
D.	_	_
Learn	_	_
to	_	_
Generate	_	_
Task-Specific	_	_
Guidance	_	_
Map	_	_
In	_	_
Section	_	_
III-C	_	_
,	_	_
Ih	_	_
,	_	_
Il	_	_
and	_	_
Oh	_	_
,	_	_
Ol	_	_
are	_	_
assumed	_	_
to	_	_
have	_	_
the	_	_
same	_	_
number	_	_
of	_	_
channels	_	_
.	_	_

#114
When	_	_
the	_	_
channel	_	_
sizes	_	_
are	_	_
different	_	_
,	_	_
a	_	_
transformation	_	_
function	_	_
is	_	_
required	_	_
to	_	_
transform	_	_
Ih	_	_
and	_	_
Il	_	_
into	_	_
a	_	_
guidance	_	_
map	_	_
with	_	_
the	_	_
same	_	_
number	_	_
of	_	_
channels	_	_
as	_	_
Oh	_	_
and	_	_
Ol	_	_
.	_	_

#115
Even	_	_
when	_	_
the	_	_
channel	_	_
sizes	_	_
are	_	_
the	_	_
same	_	_
,	_	_
a	_	_
guidance	_	_
map	_	_
better	_	_
than	_	_
Ih	_	_
and	_	_
Il	_	_
is	_	_
necessary	_	_
for	_	_
higher	_	_
performance	_	_
.	_	_

#116
Existing	_	_
methods	_	_
usually	_	_
manually	_	_
design	_	_
the	_	_
transformation	_	_
function	_	_
for	_	_
different	_	_
tasks	_	_
,	_	_
requiring	_	_
lots	_	_
of	_	_
efforts	_	_
and	_	_
attempts	_	_
.	_	_

#117
On	_	_
the	_	_
contrary	_	_
,	_	_
since	_	_
the	_	_
proposed	_	_
ùêº	_	_
"	_	_
ùêº	_	_
#	_	_
‚Ä¶	_	_
ùê∂	_	_
#	_	_
(	_	_
ùêº	_	_
#	_	_
)	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
ùê∫ùêπ	_	_
(	_	_
ùêº	_	_
#	_	_
,	_	_
ùêº	_	_
"	_	_
,	_	_
ùëÇ	_	_
#	_	_
)	_	_
ùëÇ	_	_
#	_	_
ùëÇ	_	_
"	_	_
Fig.	_	_
3	_	_
.	_	_

#118
Framework	_	_
Overview	_	_
of	_	_
Deep	_	_
Guided	_	_
Filtering	_	_
Network	_	_
.	_	_

#119
Given	_	_
the	_	_
input	_	_
image	_	_
Ih	_	_
,	_	_
we	_	_
first	_	_
downsample	_	_
it	_	_
to	_	_
obtain	_	_
Il	_	_
.	_	_

#120
The	_	_
corresponding	_	_
low-resolution	_	_
output	_	_
Ol	_	_
is	_	_
then	_	_
generated	_	_
by	_	_
the	_	_
FCN	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
.	_	_

#121
Finally	_	_
,	_	_
Il	_	_
,	_	_
Ih	_	_
and	_	_
Ol	_	_
are	_	_
fed	_	_
into	_	_
guided	_	_
filtering	_	_
layer	_	_
GF	_	_
(	_	_
Il	_	_
,	_	_
Ih	_	_
,	_	_
Ol	_	_
)	_	_
to	_	_
generate	_	_
the	_	_
high-resolution	_	_
output	_	_
Oh	_	_
.	_	_

#122
!	_	_
"	_	_

#123
#	_	_
"	_	_
!	_	_
$	_	_
Dilated	_	_
Conv	_	_
&	_	_
ÃÖ	_	_
"	_	_
'	_	_
#	_	_
"	_	_
Pointwise	_	_
Convolution	_	_
Block	_	_
Bilinear	_	_
Upsample	_	_
(	_	_
"	_	_
)	_	_
"	_	_
(	_	_
$	_	_
)	_	_
$	_	_
Linear	_	_
Layer	_	_
#	_	_
$	_	_
Convolutional	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
*	_	_
(	_	_
,	_	_
)	_	_
*	_	_
(	_	_
,	_	_
)	_	_
&	_	_
$	_	_
&	_	_
"	_	_
Dilated	_	_
Conv	_	_
Fig.	_	_
4	_	_
.	_	_

#124
Computation	_	_
Graph	_	_
of	_	_
Convolutional	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
.	_	_

#125
Dilated	_	_
convolutions	_	_
and	_	_
a	_	_
pointwise	_	_
convolution	_	_
block	_	_
are	_	_
introduced	_	_
to	_	_
replace	_	_
mean	_	_
filter	_	_
and	_	_
local	_	_
linear	_	_
model	_	_
.	_	_

#126
With	_	_
learnable	_	_
parameters	_	_
,	_	_
such	_	_
a	_	_
layer	_	_
can	_	_
adaptively	_	_
fit	_	_
a	_	_
specific	_	_
task	_	_
through	_	_
end-to-end	_	_
training	_	_
.	_	_

#127
guided	_	_
filtering	_	_
layer	_	_
is	_	_
fully	_	_
differentiable	_	_
,	_	_
we	_	_
can	_	_
automatically	_	_
learn	_	_
a	_	_
transformation	_	_
function	_	_
to	_	_
generate	_	_
more	_	_
suitable	_	_
,	_	_
task-specific	_	_
guidance	_	_
maps	_	_
by	_	_
end-to-end	_	_
training	_	_
.	_	_

#128
As	_	_
shown	_	_
in	_	_
Figure	_	_
2	_	_
,	_	_
the	_	_
transformation	_	_
function	_	_
F	_	_
(	_	_
I	_	_
)	_	_
transforms	_	_
Ih	_	_
and	_	_
Il	_	_
into	_	_
task-specific	_	_
guidance	_	_
maps	_	_
Gh	_	_
and	_	_
Gl	_	_
.	_	_

#129
F	_	_
(	_	_
I	_	_
)	_	_
is	_	_
a	_	_
FCN	_	_
block	_	_
composed	_	_
of	_	_
two	_	_
convolution	_	_
layers	_	_
,	_	_
between	_	_
which	_	_
are	_	_
an	_	_
adaptive	_	_
normalization	_	_
layer	_	_
[	_	_
23	_	_
]	_	_
and	_	_
a	_	_
leaky	_	_
ReLU	_	_
layer	_	_
.	_	_

#130
The	_	_
kernel	_	_
size	_	_
of	_	_
both	_	_
convolution	_	_
layers	_	_
is	_	_
set	_	_
to	_	_
be	_	_
1√ó1	_	_
,	_	_
and	_	_
the	_	_
channel	_	_
size	_	_
of	_	_
the	_	_
first	_	_
convolution	_	_
layer	_	_
is	_	_
set	_	_
to	_	_
be	_	_
16	_	_
by	_	_
default	_	_
.	_	_

#131
E.	_	_
Convolutional	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
Except	_	_
F	_	_
(	_	_
I	_	_
)	_	_
,	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
is	_	_
a	_	_
parameter-free	_	_
block	_	_
,	_	_
which	_	_
behaves	_	_
in	_	_
the	_	_
same	_	_
manner	_	_
for	_	_
all	_	_
different	_	_
tasks	_	_
.	_	_

#132
However	_	_
,	_	_
due	_	_
to	_	_
the	_	_
huge	_	_
differences	_	_
between	_	_
tasks	_	_
,	_	_
a	_	_
single	_	_
guided	_	_
filtering	_	_
layer	_	_
without	_	_
learnable	_	_
parameters	_	_
can	_	_
not	_	_
perform	_	_
well	_	_
in	_	_
all	_	_
kinds	_	_
of	_	_
scenarios	_	_
.	_	_

#133
To	_	_
solve	_	_
the	_	_
problem	_	_
,	_	_
we	_	_
introduce	_	_
learnable	_	_
parameters	_	_
into	_	_
guided	_	_
filtering	_	_
layer	_	_
by	_	_
replacing	_	_
the	_	_
non-parametric	_	_
operations	_	_
into	_	_
convolution	_	_
layers	_	_
.	_	_

#134
As	_	_
a	_	_
result	_	_
,	_	_
the	_	_
improved	_	_
layer	_	_
,	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
becomes	_	_
more	_	_
powerful	_	_
for	_	_
processing	_	_
various	_	_
applications	_	_
,	_	_
which	_	_
can	_	_
adaptively	_	_
fit	_	_
a	_	_
specific	_	_
task	_	_
through	_	_
end-to-end	_	_
training	_	_
.	_	_

#135
The	_	_
architecture	_	_
of	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
4	_	_
.	_	_

#136
Compared	_	_
to	_	_
that	_	_
in	_	_
Figure	_	_
2	_	_
,	_	_
dilated	_	_
convolutions	_	_
are	_	_
introduced	_	_
to	_	_
replace	_	_
mean	_	_
filter	_	_
f¬µ	_	_
,	_	_
and	_	_
a	_	_
convolution	_	_
block	_	_
composed	_	_
of	_	_
pointwise	_	_
convolutions	_	_
takes	_	_
the	_	_
place	_	_
of	_	_
a	_	_
local	_	_
linear	_	_
model	_	_
.	_	_

#137
As	_	_
for	_	_
the	_	_
hyper-parameters	_	_
in	_	_
Section	_	_
III-C	_	_
,	_	_
Œµ	_	_
is	_	_
removed	_	_
,	_	_
and	_	_
r	_	_
represents	_	_
the	_	_
dilation	_	_
rates	_	_
in	_	_
the	_	_
dilated	_	_
convolutions	_	_
.	_	_

#138
IV	_	_
.	_	_

#139
DEEP	_	_
GUIDED	_	_
FILTERING	_	_
NETWORK	_	_
Based	_	_
on	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
we	_	_
present	_	_
a	_	_
general	_	_
framework	_	_
for	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
tasks	_	_
,	_	_
named	_	_
Deep	_	_
Guided	_	_
Filtering	_	_
Network	_	_
(	_	_
DGF	_	_
)	_	_
.	_	_

#140
By	_	_
integrating	_	_
the	_	_
proposed	_	_
layer	_	_
with	_	_
FCNs	_	_
following	_	_
a	_	_
coarse-to-fine	_	_
manner	_	_
,	_	_
DGF	_	_
can	_	_
generate	_	_
high-resolution	_	_
,	_	_
edge-preserving	_	_
outputs	_	_
with	_	_
a	_	_
much	_	_
lower	_	_
computational	_	_
cost	_	_
and	_	_
memory	_	_
usage	_	_
.	_	_

#141
The	_	_
architecture	_	_
of	_	_
DGF	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
3	_	_
.	_	_

#142
First	_	_
,	_	_
we	_	_
downsample	_	_
the	_	_
original	_	_
input	_	_
image	_	_
Ih	_	_
to	_	_
obtain	_	_
the	_	_
low-resolution	_	_
input	_	_
Il	_	_
.	_	_

#143
Then	_	_
,	_	_
a	_	_
FCN	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
is	_	_
applied	_	_
to	_	_
Il	_	_
,	_	_
generating	_	_
the	_	_
corresponding	_	_
low-resolution	_	_
output	_	_
Ol	_	_
.	_	_

#144
Finally	_	_
,	_	_
the	_	_
high-resolution	_	_
output	_	_
Oh	_	_
is	_	_
generated	_	_
by	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
taking	_	_
Il	_	_
,	_	_
Ih	_	_
and	_	_
Ol	_	_
as	_	_
inputs	_	_
.	_	_

#145
The	_	_
entire	_	_
network	_	_
is	_	_
end-to-end	_	_
trainable	_	_
,	_	_
which	_	_
could	_	_
<	_	_
capability-feasibility-uncertainty	_	_
>	_	_
be	_	_
learned	_	_
from	_	_
scratch	_	_
.	_	_

#146
A.	_	_
Fully	_	_
Convolutional	_	_
Network	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_

#147
DGF	_	_
is	_	_
a	_	_
general	_	_
framework	_	_
for	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
tasks	_	_
,	_	_
which	_	_
can	_	_
remarkably	_	_
reduce	_	_
the	_	_
computational	_	_
complexity	_	_
and	_	_
memory	_	_
usage	_	_
of	_	_
the	_	_
underlying	_	_
algorithms	_	_
.	_	_

#148
Concretely	_	_
,	_	_
given	_	_
a	_	_
specific	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
task	_	_
,	_	_
an	_	_
FCN	_	_
C	_	_
(	_	_
I	_	_
)	_	_
can	_	_
be	_	_
designed	_	_
to	_	_
achieve	_	_
excellent	_	_
performance	_	_
without	_	_
considering	_	_
the	_	_
speed	_	_
and	_	_
memory	_	_
cost	_	_
.	_	_

#149
In	_	_
order	_	_
to	_	_
obtain	_	_
significant	_	_
optimization	_	_
on	_	_
speed	_	_
and	_	_
memory	_	_
,	_	_
we	_	_
can	_	_
simply	_	_
drop	_	_
C	_	_
(	_	_
I	_	_
)	_	_
into	_	_
the	_	_
proposed	_	_
framework	_	_
DGF	_	_
to	_	_
serve	_	_
as	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
without	_	_
any	_	_
other	_	_
modifications	_	_
.	_	_

#150
Since	_	_
C	_	_
(	_	_
I	_	_
)	_	_
processes	_	_
the	_	_
input	_	_
images	_	_
in	_	_
low	_	_
resolution	_	_
rather	_	_
than	_	_
the	_	_
original	_	_
resolution	_	_
,	_	_
the	_	_
speed	_	_
,	_	_
and	_	_
memory	_	_
usage	_	_
can	_	_
be	_	_
largely	_	_
improved	_	_
.	_	_

#151
Moreover	_	_
,	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
system	_	_
is	_	_
also	_	_
comparable	_	_
to	_	_
the	_	_
previous	_	_
state-of-the-art	_	_
one	_	_
,	_	_
thanks	_	_
to	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
.	_	_

#152
This	_	_
is	_	_
due	_	_
to	_	_
that	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
significantly	_	_
enhances	_	_
the	_	_
capabilities	_	_
of	_	_
FCNs	_	_
in	_	_
the	_	_
task	_	_
of	_	_
joint	_	_
upsampling	_	_
.	_	_

#153
B	_	_
.	_	_

#154
Guided	_	_
Filtering	_	_
Layer	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
there	_	_
are	_	_
four	_	_
variants	_	_
of	_	_
DGF	_	_
in	_	_
total	_	_
according	_	_
to	_	_
different	_	_
configurations	_	_
of	_	_
the	_	_
guided	_	_
filtering	_	_
layer	_	_
.	_	_

#155
1	_	_
)	_	_
DGFs	_	_
:	_	_
The	_	_
original	_	_
guided	_	_
filter	_	_
[	_	_
25	_	_
]	_	_
is	_	_
employed	_	_
as	_	_
post-processing	_	_
operation	_	_
without	_	_
any	_	_
training	_	_
.	_	_

#156
Cl	_	_
(	_	_
Il	_	_
)	_	_
is	_	_
trained	_	_
with	_	_
low-resolution	_	_
input/output	_	_
pairs	_	_
before	_	_
inserted	_	_
into	_	_
DGFs	_	_
.	_	_

#157
2	_	_
)	_	_
DGFb	_	_
:	_	_
Guided	_	_
filtering	_	_
layer	_	_
in	_	_
Figure	_	_
2	_	_
is	_	_
employed	_	_
in	_	_
DGFb	_	_
.	_	_

#158
F	_	_
(	_	_
I	_	_
)	_	_
is	_	_
an	_	_
identity	_	_
function	_	_
when	_	_
the	_	_
inputs	_	_
and	_	_
outputs	_	_
have	_	_
the	_	_
same	_	_
number	_	_
of	_	_
channels	_	_
.	_	_

#159
When	_	_
the	_	_
channel	_	_
sizes	_	_
are	_	_
different	_	_
,	_	_
F	_	_
(	_	_
I	_	_
)	_	_
transforms	_	_
the	_	_
inputs	_	_
into	_	_
a	_	_
grey	_	_
image	_	_
by	_	_
averaging	_	_
along	_	_
the	_	_
channel	_	_
axis	_	_
.	_	_

#160
Cl	_	_
(	_	_
Il	_	_
)	_	_
and	_	_
guided	_	_
filtering	_	_
layer	_	_
are	_	_
jointly	_	_
trained	_	_
from	_	_
scratch	_	_
under	_	_
the	_	_
supervision	_	_
directly	_	_
from	_	_
the	_	_
high-resolution	_	_
targets	_	_
.	_	_

#161
3	_	_
)	_	_
DGFcb	_	_
:	_	_
Compared	_	_
to	_	_
DGFb	_	_
,	_	_
the	_	_
guided	_	_
filtering	_	_
layer	_	_
is	_	_
replaced	_	_
by	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
in	_	_
Figure	_	_
4	_	_
.	_	_

#162
4	_	_
)	_	_
DGFc	_	_
:	_	_
Compared	_	_
to	_	_
DGFcb	_	_
,	_	_
F	_	_
(	_	_
I	_	_
)	_	_
proposed	_	_
in	_	_
Section	_	_
III-D	_	_
is	_	_
introduced	_	_
,	_	_
which	_	_
can	_	_
learn	_	_
to	_	_
generate	_	_
task-oriented	_	_
guidance	_	_
maps	_	_
without	_	_
manually	_	_
design	_	_
.	_	_

#163
As	_	_
a	_	_
result	_	_
,	_	_
DGFc	_	_
is	_	_
not	_	_
only	_	_
end-to-end	_	_
trainable	_	_
but	_	_
can	_	_
also	_	_
fit	_	_
different	_	_
tasks	_	_
better	_	_
by	_	_
adjusting	_	_
the	_	_
trainable	_	_
convolution	_	_
weights	_	_
and	_	_
the	_	_
learnable	_	_
F	_	_
(	_	_
I	_	_
)	_	_
.	_	_

#164
C.	_	_
Objective	_	_
Function	_	_
DGF	_	_
is	_	_
trained	_	_
end-to-end	_	_
under	_	_
the	_	_
supervision	_	_
directly	_	_
from	_	_
the	_	_
high-resolution	_	_
targets	_	_
.	_	_

#165
Concretely	_	_
,	_	_
given	_	_
the	_	_
high-resolution	_	_
output	_	_
Oh	_	_
and	_	_
the	_	_
corresponding	_	_
target	_	_
Th	_	_
,	_	_
the	_	_
objective	_	_
function	_	_
is	_	_
defined	_	_
as	_	_
L	_	_
(	_	_
Oh	_	_
,	_	_
Th	_	_
)	_	_
.	_	_

#166
The	_	_
concrete	_	_
formulation	_	_
varies	_	_
with	_	_
different	_	_
tasks	_	_
.	_	_

#167
Usually	_	_
,	_	_
the	_	_
objective	_	_
function	_	_
for	_	_
training	_	_
C	_	_
(	_	_
I	_	_
)	_	_
can	_	_
be	_	_
directly	_	_
employed	_	_
to	_	_
train	_	_
DGF	_	_
without	_	_
any	_	_
adjustment	_	_
.	_	_

#168
V.	_	_
EXPERIMENTS	_	_
:	_	_
IMAGE	_	_
PROCESSING	_	_
TASKS	_	_
To	_	_
show	_	_
the	_	_
effectiveness	_	_
of	_	_
our	_	_
method	_	_
,	_	_
we	_	_
employ	_	_
DGF	_	_
to	_	_
clone	_	_
five	_	_
widely-used	_	_
image	_	_
processing	_	_
operators	_	_
.	_	_

#169
Concretely	_	_
,	_	_
the	_	_
ground	_	_
truth	_	_
images	_	_
are	_	_
first	_	_
generated	_	_
by	_	_
applying	_	_
L0	_	_
smoothing	_	_
operator	_	_
[	_	_
7	_	_
]	_	_
,	_	_
detail	_	_
manipulation	_	_
operator	_	_
[	_	_
3	_	_
]	_	_
,	_	_
style	_	_
transfer	_	_
operator	_	_
[	_	_
11	_	_
]	_	_
,	_	_
non-local	_	_
dehazing	_	_
operator	_	_
[	_	_
4	_	_
]	_	_
,	_	_
and	_	_
image	_	_
retouching	_	_
operator	_	_
[	_	_
2	_	_
]	_	_
to	_	_
the	_	_
input	_	_
images	_	_
.	_	_

#170
Then	_	_
,	_	_
the	_	_
input/ground-truth	_	_
pairs	_	_
are	_	_
used	_	_
to	_	_
train	_	_
DGF	_	_
in	_	_
a	_	_
supervised	_	_
way	_	_
to	_	_
clone	_	_
the	_	_
corresponding	_	_
image	_	_
processing	_	_
operator	_	_
.	_	_

#171
TABLE	_	_
I	_	_
THE	_	_
ARCHITECTURE	_	_
OF	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
AND	_	_
F	_	_
(	_	_
I	_	_
)	_	_
FOR	_	_
CLONING	_	_
IMAGE	_	_
PROCESSING	_	_
OPERATORS	_	_
.	_	_

#172
THE	_	_
NEGATIVE	_	_
SLOPE	_	_
OF	_	_
LEAKY	_	_
RELU	_	_
IS	_	_
SET	_	_
TO	_	_
0.2	_	_
.	_	_

#173
Cl	_	_
(	_	_
Il	_	_
)	_	_
F	_	_
(	_	_
I	_	_
)	_	_
Layer	_	_
1	_	_
2	_	_
3	_	_
4	_	_
5	_	_
6	_	_
7	_	_
8	_	_
1	_	_
2	_	_
Kernel	_	_
3	_	_
3	_	_
3	_	_
3	_	_
3	_	_
3	_	_
3	_	_
1	_	_
3	_	_
1	_	_
#	_	_
Channels	_	_
24	_	_
24	_	_
24	_	_
24	_	_
24	_	_
24	_	_
24	_	_
3	_	_
16	_	_
3	_	_
Dilation	_	_
1	_	_
1	_	_
2	_	_
4	_	_
8	_	_
16	_	_
1	_	_
1	_	_
1	_	_
1	_	_
Bias	_	_
7	_	_
7	_	_
7	_	_
7	_	_
7	_	_
7	_	_
7	_	_
X	_	_
7	_	_
X	_	_
AdaptNorm	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
7	_	_
X	_	_
7	_	_
Leaky	_	_
ReLU	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
X	_	_
7	_	_
X	_	_
7	_	_
A	_	_
.	_	_

#174
Details	_	_
of	_	_
Five	_	_
Image	_	_
Processing	_	_
Operators	_	_
1	_	_
)	_	_
L0	_	_
Smoothing	_	_
:	_	_
L0	_	_
smoothing	_	_
[	_	_
7	_	_
]	_	_
is	_	_
effective	_	_
for	_	_
sharpening	_	_
major	_	_
edges	_	_
while	_	_
eliminating	_	_
minor	_	_
edges	_	_
by	_	_
the	_	_
use	_	_
of	_	_
L0	_	_
gradient	_	_
minimization	_	_
.	_	_

#175
To	_	_
generate	_	_
the	_	_
ground	_	_
truth	_	_
images	_	_
,	_	_
we	_	_
use	_	_
the	_	_
official	_	_
implementation	_	_
with	_	_
the	_	_
default	_	_
parameters1	_	_
.	_	_

#176
2	_	_
)	_	_
Detail	_	_
Manipulation	_	_
:	_	_
Multi-scale	_	_
detail	_	_
manipulation	_	_
[	_	_
3	_	_
]	_	_
enhances	_	_
an	_	_
image	_	_
by	_	_
boosting	_	_
features	_	_
at	_	_
multiple	_	_
scales	_	_
.	_	_

#177
Concretely	_	_
,	_	_
a	_	_
three-level	_	_
decomposition	_	_
(	_	_
coarse	_	_
base	_	_
level	_	_
b	_	_
and	_	_
two	_	_
detail	_	_
levels	_	_
d1	_	_
,	_	_
d2	_	_
)	_	_
of	_	_
the	_	_
CIELAB	_	_
lightness	_	_
channel	_	_
is	_	_
first	_	_
constructed	_	_
given	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#178
The	_	_
resulting	_	_
image	_	_
is	_	_
then	_	_
obtained	_	_
by	_	_
a	_	_
non-linear	_	_
combination	_	_
of	_	_
b	_	_
,	_	_
d1	_	_
and	_	_
d2	_	_
.	_	_

#179
To	_	_
generate	_	_
the	_	_
ground	_	_
truth	_	_
images	_	_
,	_	_
we	_	_
first	_	_
generate	_	_
coarsescale	_	_
,	_	_
medium-scale	_	_
,	_	_
and	_	_
fine-scale	_	_
images	_	_
with	_	_
the	_	_
official	_	_
implementation	_	_
and	_	_
the	_	_
default	_	_
parameters2	_	_
.	_	_

#180
The	_	_
final	_	_
output	_	_
is	_	_
then	_	_
yielded	_	_
by	_	_
averaging	_	_
the	_	_
three	_	_
images	_	_
.	_	_

#181
3	_	_
)	_	_
Style	_	_
Transfer	_	_
:	_	_
Photographic	_	_
style	_	_
transfer	_	_
[	_	_
11	_	_
]	_	_
aims	_	_
at	_	_
transferring	_	_
the	_	_
photographic	_	_
style	_	_
of	_	_
a	_	_
reference	_	_
image	_	_
to	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#182
To	_	_
generate	_	_
the	_	_
ground	_	_
truth	_	_
images	_	_
,	_	_
we	_	_
employ	_	_
the	_	_
official	_	_
implementation	_	_
with	_	_
the	_	_
default	_	_
setting	_	_
and	_	_
the	_	_
default	_	_
reference	_	_
image3	_	_
.	_	_

#183
The	_	_
generated	_	_
outputs	_	_
are	_	_
grey	_	_
images	_	_
,	_	_
which	_	_
are	_	_
transformed	_	_
into	_	_
RGB	_	_
images	_	_
as	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#184
4	_	_
)	_	_
Non-local	_	_
Dehazing	_	_
:	_	_
Non-local	_	_
dehazing	_	_
[	_	_
4	_	_
]	_	_
employs	_	_
a	_	_
non-local	_	_
prior	_	_
to	_	_
remove	_	_
the	_	_
effects	_	_
of	_	_
atmospheric	_	_
absorption	_	_
and	_	_
scattering	_	_
in	_	_
the	_	_
input	_	_
image	_	_
.	_	_

#185
We	_	_
use	_	_
the	_	_
official	_	_
implementation	_	_
with	_	_
default	_	_
parameters	_	_
to	_	_
generate	_	_
ground	_	_
truth	_	_
images4	_	_
.	_	_

#186
5	_	_
)	_	_
Image	_	_
Retouching	_	_
:	_	_
Image	_	_
retouching	_	_
aims	_	_
at	_	_
automatically	_	_
improving	_	_
the	_	_
aesthetic	_	_
quality	_	_
of	_	_
the	_	_
input	_	_
image	_	_
by	_	_
global	_	_
tonal	_	_
adjustment	_	_
.	_	_

#187
Human	_	_
experts	_	_
are	_	_
employed	_	_
to	_	_
generate	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#188
B	_	_
.	_	_

#189
Details	_	_
of	_	_
DGF	_	_
We	_	_
employ	_	_
Context	_	_
Aggregation	_	_
Network	_	_
(	_	_
CAN	_	_
)	_	_
[	_	_
23	_	_
]	_	_
as	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
for	_	_
all	_	_
the	_	_
five	_	_
image	_	_
processing	_	_
operators	_	_
.	_	_

#190
The	_	_
detailed	_	_
architectures	_	_
of	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
and	_	_
F	_	_
(	_	_
I	_	_
)	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
I.	_	_
AdaptNorm	_	_
represents	_	_
adaptive	_	_
normalization	_	_
proposed	_	_
by	_	_
Chen	_	_
et	_	_
al.	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#191
Leaky	_	_
ReLU	_	_
is	_	_
employed	_	_
as	_	_
the	_	_
nonlinearity	_	_
,	_	_
of	_	_
1http	_	_
:	_	_
//www.cse.cuhk.edu.hk/‚àºleojia/projects/L0smoothing	_	_
2http	_	_
:	_	_
//www.cs.huji.ac.il/‚àºdanix/epd	_	_
3http	_	_
:	_	_
//www.di.ens.fr/‚àºaubry/code/matlab	_	_
fast	_	_
llf	_	_
and	_	_
style	_	_
transfer.zip	_	_
4https	_	_
:	_	_
//github.com/danaberman/non-local-dehazing	_	_
512¬≤	_	_
1024¬≤	_	_
1536¬≤	_	_
2048¬≤	_	_
2560¬≤	_	_
3072¬≤	_	_
M	_	_
em	_	_
or	_	_
y	_	_
U	_	_
sa	_	_
ge	_	_
(	_	_
G	_	_
)	_	_
Image	_	_
Resolution	_	_
512¬≤	_	_
1024¬≤	_	_
1536¬≤	_	_
2048¬≤	_	_
2560¬≤	_	_
3072¬≤	_	_
R	_	_
un	_	_
ni	_	_
ng	_	_
Ti	_	_
m	_	_
e	_	_
(	_	_
m	_	_
s	_	_
)	_	_
Image	_	_
Resolution	_	_
(	_	_
a	_	_
)	_	_
Speed	_	_
(	_	_
b	_	_
)	_	_
GPU	_	_
Memory	_	_
DGFbc	_	_
DGFb	_	_
DGF	_	_
&	_	_
DBL	_	_
CAN	_	_
Fig.	_	_
5	_	_
.	_	_

#192
Speed	_	_
and	_	_
Memory	_	_
Usage	_	_
Comparison	_	_
on	_	_
GPU	_	_
Devices	_	_
.	_	_

#193
which	_	_
the	_	_
negative	_	_
slope	_	_
is	_	_
set	_	_
to	_	_
0.2	_	_
.	_	_

#194
As	_	_
for	_	_
the	_	_
objective	_	_
function	_	_
,	_	_
we	_	_
use	_	_
L2	_	_
loss	_	_
by	_	_
following	_	_
the	_	_
convention	_	_
of	_	_
previous	_	_
works	_	_
[	_	_
22	_	_
]	_	_
,	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#195
C.	_	_
Experimental	_	_
Setup	_	_
Our	_	_
experiments	_	_
are	_	_
taken	_	_
on	_	_
MIT-Adobe	_	_
FiveK	_	_
Dataset	_	_
[	_	_
2	_	_
]	_	_
,	_	_
which	_	_
contains	_	_
2,500/2,500	_	_
high-resolution	_	_
photographs	_	_
as	_	_
the	_	_
training/testing	_	_
images	_	_
.	_	_

#196
In	_	_
the	_	_
dataset	_	_
,	_	_
each	_	_
photograph	_	_
contains	_	_
five	_	_
annotations	_	_
from	_	_
five	_	_
experts	_	_
,	_	_
which	_	_
can	_	_
be	_	_
used	_	_
as	_	_
the	_	_
ground	_	_
truth	_	_
for	_	_
image	_	_
retouching	_	_
.	_	_

#197
Instead	_	_
of	_	_
all	_	_
five	_	_
annotations	_	_
,	_	_
we	_	_
employ	_	_
the	_	_
annotation	_	_
from	_	_
expert	_	_
A	_	_
as	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#198
As	_	_
for	_	_
the	_	_
other	_	_
four	_	_
image	_	_
processing	_	_
operators	_	_
,	_	_
the	_	_
ground	_	_
truth	_	_
images	_	_
are	_	_
generated	_	_
following	_	_
the	_	_
instructions	_	_
in	_	_
Section	_	_
V-A	_	_
.	_	_

#199
As	_	_
for	_	_
training	_	_
,	_	_
we	_	_
first	_	_
train	_	_
the	_	_
network	_	_
for	_	_
150	_	_
epochs	_	_
,	_	_
with	_	_
the	_	_
input/target	_	_
images	_	_
resized	_	_
to	_	_
512s5	_	_
.	_	_

#200
To	_	_
improve	_	_
the	_	_
generalization	_	_
ability	_	_
,	_	_
we	_	_
further	_	_
train	_	_
the	_	_
network	_	_
for	_	_
30	_	_
epochs	_	_
,	_	_
with	_	_
training	_	_
data	_	_
randomly	_	_
resized	_	_
to	_	_
a	_	_
specific	_	_
resolution	_	_
between	_	_
512s	_	_
and	_	_
1672s	_	_
.	_	_

#201
As	_	_
for	_	_
Il	_	_
,	_	_
the	_	_
spatial	_	_
resolution	_	_
is	_	_
64s	_	_
regardless	_	_
of	_	_
the	_	_
resolution	_	_
of	_	_
Ih	_	_
.	_	_

#202
Adam	_	_
is	_	_
employed	_	_
as	_	_
the	_	_
optimizer	_	_
,	_	_
with	_	_
learning	_	_
rate	_	_
set	_	_
to	_	_
0.0001	_	_
and	_	_
batch	_	_
size	_	_
set	_	_
to	_	_
1	_	_
.	_	_

#203
Our	_	_
primary	_	_
baseline	_	_
is	_	_
Deep	_	_
Bilateral	_	_
Learning	_	_
(	_	_
DBL	_	_
)	_	_
[	_	_
22	_	_
]	_	_
,	_	_
which	_	_
shares	_	_
a	_	_
similar	_	_
architecture	_	_
to	_	_
ours	_	_
and	_	_
achieves	_	_
a	_	_
good	_	_
trade-off	_	_
between	_	_
quality	_	_
and	_	_
speed	_	_
.	_	_

#204
Another	_	_
strong	_	_
baseline	_	_
is	_	_
CAN	_	_
[	_	_
23	_	_
]	_	_
,	_	_
which	_	_
achieves	_	_
state-of-the-art	_	_
performance	_	_
while	_	_
runs	_	_
reasonably	_	_
fast	_	_
.	_	_

#205
To	_	_
ensure	_	_
a	_	_
fair	_	_
comparison	_	_
,	_	_
we	_	_
train	_	_
the	_	_
models	_	_
using	_	_
the	_	_
official	_	_
implementations	_	_
and	_	_
training	_	_
procedures	_	_
for	_	_
both	_	_
methods	_	_
.	_	_

#206
D.	_	_
Experimental	_	_
Results	_	_
1	_	_
)	_	_
Running	_	_
Time	_	_
and	_	_
Memory	_	_
Usage	_	_
:	_	_
The	_	_
running	_	_
time	_	_
and	_	_
memory	_	_
usage	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
5	_	_
,	_	_
which	_	_
are	_	_
measured	_	_
on	_	_
a	_	_
workstation	_	_
with	_	_
Intel	_	_
E5-2650	_	_
2.20GHz	_	_
CPU	_	_
and	_	_
Nvidia	_	_
Titan	_	_
X	_	_
(	_	_
Pascal	_	_
)	_	_
GPU	_	_
.	_	_

#207
On	_	_
GPU	_	_
devices	_	_
,	_	_
both	_	_
DGFb	_	_
and	_	_
DGFc	_	_
b	_	_
take	_	_
less	_	_
than	_	_
10ms	_	_
to	_	_
process	_	_
an	_	_
image	_	_
with	_	_
resolution	_	_
ranging	_	_
from	_	_
5122	_	_
to	_	_
5xs	_	_
means	_	_
the	_	_
short	_	_
side	_	_
of	_	_
an	_	_
image	_	_
is	_	_
resized	_	_
to	_	_
x	_	_
without	_	_
changing	_	_
the	_	_
aspect	_	_
ratio	_	_
.	_	_

#208
30722	_	_
.	_	_

#209
DGFc	_	_
is	_	_
slightly	_	_
slower	_	_
because	_	_
of	_	_
the	_	_
usage	_	_
of	_	_
F	_	_
(	_	_
I	_	_
)	_	_
,	_	_
but	_	_
it	_	_
still	_	_
runs	_	_
in	_	_
real-time	_	_
on	_	_
images	_	_
with	_	_
resolution	_	_
30722	_	_
.	_	_

#210
All	_	_
the	_	_
three	_	_
variants	_	_
of	_	_
our	_	_
method	_	_
run	_	_
much	_	_
faster	_	_
than	_	_
CAN	_	_
and	_	_
DBL	_	_
among	_	_
all	_	_
resolutions	_	_
.	_	_

#211
Specifically	_	_
,	_	_
DGFb	_	_
,	_	_
DGFc	_	_
b	_	_
,	_	_
and	_	_
DGFc	_	_
take	_	_
6ms	_	_
,	_	_
6ms	_	_
,	_	_
and	_	_
21ms	_	_
respectively	_	_
for	_	_
an	_	_
image	_	_
in	_	_
20482	_	_
.	_	_

#212
CAN	_	_
takes	_	_
160ms	_	_
for	_	_
an	_	_
image	_	_
in	_	_
20482	_	_
,	_	_
which	_	_
are	_	_
more	_	_
than	_	_
25√ó	_	_
,	_	_
25√ó	_	_
,	_	_
and	_	_
7√ó	_	_
slower	_	_
than	_	_
our	_	_
method	_	_
.	_	_

#213
DBL	_	_
takes	_	_
51ms	_	_
in	_	_
the	_	_
same	_	_
setting	_	_
,	_	_
which	_	_
is	_	_
slightly	_	_
faster	_	_
than	_	_
CAN	_	_
but	_	_
more	_	_
than	_	_
8√ó	_	_
slower	_	_
than	_	_
DGFb	_	_
and	_	_
DGFc	_	_
b	_	_
.	_	_

#214
The	_	_
advantage	_	_
of	_	_
our	_	_
method	_	_
in	_	_
speed	_	_
is	_	_
even	_	_
more	_	_
significant	_	_
as	_	_
the	_	_
resolution	_	_
grows	_	_
.	_	_

#215
For	_	_
Ih	_	_
with	_	_
h	_	_
√ó	_	_
w	_	_
√ó	_	_
nI	_	_
and	_	_
Oh	_	_
with	_	_
h	_	_
√ó	_	_
w	_	_
√ó	_	_
nO	_	_
,	_	_
the	_	_
theoretical	_	_
computational	_	_
complexities	_	_
of	_	_
DGFb	_	_
,	_	_
DGFc	_	_
b	_	_
,	_	_
DGFc	_	_
,	_	_
and	_	_
DBL	_	_
are	_	_
O	_	_
(	_	_
nO√óh√ów	_	_
)	_	_
,	_	_
O	_	_
(	_	_
nO√óh√ów	_	_
)	_	_
,	_	_
O	_	_
(	_	_
(	_	_
nI	_	_
+nO	_	_
)	_	_
√ó	_	_
h√ó	_	_
w	_	_
)	_	_
and	_	_
O	_	_
(	_	_
nI	_	_
√ó	_	_
nO	_	_
√ó	_	_
h√ó	_	_
w	_	_
)	_	_
respectively	_	_
.	_	_

#216
As	_	_
for	_	_
memory	_	_
usage	_	_
,	_	_
our	_	_
method	_	_
takes	_	_
less	_	_
GPU	_	_
memory	_	_
space	_	_
than	_	_
both	_	_
baseline	_	_
methods	_	_
.	_	_

#217
CAN	_	_
is	_	_
the	_	_
most	_	_
memory	_	_
inefficient	_	_
method	_	_
that	_	_
takes	_	_
nearly	_	_
10G	_	_
GPU	_	_
memory	_	_
to	_	_
process	_	_
an	_	_
image	_	_
with	_	_
resolution	_	_
20482	_	_
.	_	_

#218
DGFc	_	_
takes	_	_
a	_	_
similar	_	_
amount	_	_
of	_	_
memory	_	_
space	_	_
to	_	_
that	_	_
of	_	_
DBL	_	_
but	_	_
grows	_	_
slower	_	_
as	_	_
the	_	_
resolution	_	_
increases	_	_
.	_	_

#219
DGFb	_	_
and	_	_
DGFc	_	_
b	_	_
are	_	_
the	_	_
most	_	_
memory	_	_
efficient	_	_
methods	_	_
,	_	_
which	_	_
take	_	_
less	_	_
than	_	_
1G	_	_
memory	_	_
even	_	_
on	_	_
images	_	_
with	_	_
resolution	_	_
30722	_	_
.	_	_

#220
2	_	_
)	_	_
Quantitative	_	_
and	_	_
Qualitative	_	_
Comparison	_	_
:	_	_
The	_	_
performance	_	_
of	_	_
each	_	_
method	_	_
is	_	_
evaluated	_	_
on	_	_
the	_	_
test	_	_
set	_	_
of	_	_
MIT-Adobe	_	_
FiveK	_	_
dataset	_	_
with	_	_
input/target	_	_
images	_	_
resized	_	_
to	_	_
1024s	_	_
.	_	_

#221
MSE	_	_
,	_	_
PSNR	_	_
,	_	_
and	_	_
SSIM	_	_
serve	_	_
as	_	_
the	_	_
evaluation	_	_
metrics	_	_
.	_	_

#222
As	_	_
shown	_	_
in	_	_
Table	_	_
II	_	_
,	_	_
our	_	_
method	_	_
achieves	_	_
the	_	_
state-of-the-art	_	_
performance	_	_
in	_	_
style	_	_
transfer	_	_
,	_	_
non-local	_	_
dehazing	_	_
,	_	_
and	_	_
image	_	_
retouching	_	_
;	_	_
while	_	_
obtaining	_	_
comparable	_	_
results	_	_
in	_	_
L0	_	_
smoothing	_	_
and	_	_
multi-scale	_	_
detail	_	_
manipulation	_	_
.	_	_

#223
Concretely	_	_
,	_	_
DGFc	_	_
achieves	_	_
26.17	_	_
dB	_	_
in	_	_
PSNR	_	_
for	_	_
style	_	_
transfer	_	_
,	_	_
which	_	_
improves	_	_
over	_	_
CAN	_	_
and	_	_
DBL	_	_
by	_	_
4.86	_	_
dB	_	_
and	_	_
2.85	_	_
dB	_	_
respectively	_	_
.	_	_

#224
Compared	_	_
to	_	_
DBL	_	_
,	_	_
our	_	_
method	_	_
outperforms	_	_
it	_	_
across	_	_
all	_	_
the	_	_
five	_	_
tasks	_	_
in	_	_
all	_	_
three	_	_
metrics	_	_
by	_	_
a	_	_
large	_	_
margin	_	_
.	_	_

#225
The	_	_
qualitative	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
86	_	_
.	_	_

#226
3	_	_
)	_	_
The	_	_
Role	_	_
of	_	_
Guided	_	_
Filtering	_	_
Layer	_	_
:	_	_
To	_	_
show	_	_
the	_	_
effect	_	_
of	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
and	_	_
F	_	_
(	_	_
I	_	_
)	_	_
,	_	_
we	_	_
replace	_	_
Ol	_	_
6More	_	_
qualitative	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
http	_	_
:	_	_
//wuhuikai.me/	_	_
DeepGuidedFilterProject/	_	_
#	_	_
visual	_	_
.	_	_

#227
TABLE	_	_
II	_	_
QUANTITATIVE	_	_
COMPARISON	_	_
ON	_	_
IMAGE	_	_
PROCESSING	_	_
TASKS	_	_
.	_	_

#228
THE	_	_
1ST	_	_
,	_	_
2ND	_	_
AND	_	_
3RD	_	_
METHODS	_	_
ARE	_	_
HIGHLIGHTED	_	_
AS	_	_
RED	_	_
,	_	_
GREEN	_	_
,	_	_
AND	_	_
BLUE	_	_
.	_	_

#229
Method	_	_
L0	_	_
Smoothing	_	_
[	_	_
7	_	_
]	_	_
Detail	_	_
Manipulation	_	_
[	_	_
3	_	_
]	_	_
Style	_	_
Transfer	_	_
[	_	_
11	_	_
]	_	_
Non-local	_	_
Dehazing	_	_
[	_	_
4	_	_
]	_	_
Image	_	_
Retouching	_	_
[	_	_
2	_	_
]	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
Input	_	_
73	_	_
29.61	_	_
0.796	_	_
443	_	_
22.12	_	_
0.789	_	_
3534	_	_
13.28	_	_
0.521	_	_
2081	_	_
16.95	_	_
0.684	_	_
1507	_	_
18.44	_	_
0.727	_	_
CAN	_	_
[	_	_
23	_	_
]	_	_
27	_	_
35.05	_	_
0.970	_	_
9	_	_
38.97	_	_
0.986	_	_
519	_	_
21.31	_	_
0.870	_	_
355	_	_
24.47	_	_
0.862	_	_
964	_	_
20.43	_	_
0.744	_	_
DBL	_	_
[	_	_
22	_	_
]	_	_
39	_	_
32.35	_	_
0.896	_	_
75	_	_
29.84	_	_
0.924	_	_
354	_	_
23.32	_	_
0.834	_	_
502	_	_
23.27	_	_
0.852	_	_
1056	_	_
20.21	_	_
0.748	_	_
DJF	_	_
[	_	_
52	_	_
]	_	_
90	_	_
29.40	_	_
0.937	_	_
100	_	_
28.99	_	_
0.927	_	_
383	_	_
22.73	_	_
0.856	_	_
649	_	_
21.04	_	_
0.724	_	_
1216	_	_
18.89	_	_
0.702	_	_
DGFs	_	_
35	_	_
32.93	_	_
0.912	_	_
92	_	_
29.12	_	_
0.905	_	_
333	_	_
23.22	_	_
0.735	_	_
351	_	_
24.53	_	_
0.871	_	_
872	_	_
20.81	_	_
0.757	_	_
DGFb	_	_
33	_	_
33.20	_	_
0.911	_	_
77	_	_
29.95	_	_
0.905	_	_
318	_	_
23.42	_	_
0.738	_	_
323	_	_
25.53	_	_
0.892	_	_
875	_	_
20.94	_	_
0.762	_	_
DGFc	_	_
b	_	_
32	_	_
33.39	_	_
0.917	_	_
69	_	_
30.48	_	_
0.916	_	_
305	_	_
23.61	_	_
0.751	_	_
293	_	_
25.76	_	_
0.896	_	_
855	_	_
21.04	_	_
0.760	_	_
DGFc	_	_
30	_	_
33.69	_	_
0.923	_	_
48	_	_
32.10	_	_
0.940	_	_
181	_	_
26.17	_	_
0.880	_	_
296	_	_
25.85	_	_
0.902	_	_
855	_	_
21.09	_	_
0.767	_	_
TABLE	_	_
III	_	_
UPPER	_	_
BOUND	_	_
FOR	_	_
OUR	_	_
METHOD	_	_
ON	_	_
IMAGE	_	_
PROCESSING	_	_
TASKS	_	_
.	_	_

#230
Method	_	_
L0	_	_
Smoothing	_	_
[	_	_
7	_	_
]	_	_
Detail	_	_
Manipulation	_	_
[	_	_
3	_	_
]	_	_
Style	_	_
Transfer	_	_
[	_	_
11	_	_
]	_	_
Non-local	_	_
Dehazing	_	_
[	_	_
4	_	_
]	_	_
Image	_	_
Retouching	_	_
[	_	_
2	_	_
]	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
MSE	_	_
PSNR	_	_
SSIM	_	_
DGFb	_	_
27	_	_
34.52	_	_
0.923	_	_
93	_	_
29.23	_	_
0.904	_	_
261	_	_
24.28	_	_
0.752	_	_
49	_	_
33.00	_	_
0.956	_	_
100	_	_
30.82	_	_
0.859	_	_
DGFc	_	_
b	_	_
25	_	_
34.93	_	_
0.925	_	_
71	_	_
30.35	_	_
0.917	_	_
245	_	_
24.55	_	_
0.761	_	_
34	_	_
34.35	_	_
0.964	_	_
94	_	_
30.97	_	_
0.857	_	_
DGFc	_	_
23	_	_
35.27	_	_
0.930	_	_
42	_	_
32.66	_	_
0.947	_	_
109	_	_
28.34	_	_
0.899	_	_
28	_	_
35.19	_	_
0.968	_	_
90	_	_
31.19	_	_
0.860	_	_
with	_	_
low-resolution	_	_
ground	_	_
truth	_	_
to	_	_
generate	_	_
Oh	_	_
.	_	_

#231
The	_	_
obtained	_	_
result	_	_
represents	_	_
the	_	_
performance	_	_
upper	_	_
bound	_	_
of	_	_
each	_	_
DGF	_	_
variant	_	_
.	_	_

#232
As	_	_
shown	_	_
in	_	_
Table	_	_
III	_	_
,	_	_
by	_	_
reformulating	_	_
guided	_	_
filtering	_	_
layer	_	_
into	_	_
learnable	_	_
convolution	_	_
layers	_	_
,	_	_
DGFc	_	_
b	_	_
outperforms	_	_
DGFb	_	_
in	_	_
all	_	_
five	_	_
tasks	_	_
.	_	_

#233
By	_	_
further	_	_
introducing	_	_
F	_	_
(	_	_
I	_	_
)	_	_
into	_	_
the	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
DGFc	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
.	_	_

#234
Similar	_	_
results	_	_
can	_	_
be	_	_
observed	_	_
in	_	_
Table	_	_
II	_	_
.	_	_

#235
By	_	_
jointly	_	_
end-to-end	_	_
training	_	_
,	_	_
DGFb	_	_
achieves	_	_
better	_	_
performance	_	_
on	_	_
most	_	_
tasks	_	_
than	_	_
DGFs	_	_
.	_	_

#236
Concretely	_	_
,	_	_
DGFb	_	_
improves	_	_
1	_	_
dB	_	_
and	_	_
0.83	_	_
dB	_	_
(	_	_
PSNR	_	_
)	_	_
for	_	_
non-local	_	_
dehazing	_	_
and	_	_
detail	_	_
manipulation	_	_
.	_	_

#237
By	_	_
reformulating	_	_
into	_	_
convolutional	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
the	_	_
performance	_	_
is	_	_
further	_	_
improved	_	_
by	_	_
comparing	_	_
DGFb	_	_
and	_	_
DGFc	_	_
b	_	_
.	_	_

#238
By	_	_
adding	_	_
learnable	_	_
F	_	_
(	_	_
I	_	_
)	_	_
,	_	_
we	_	_
gain	_	_
significant	_	_
improvements	_	_
in	_	_
several	_	_
tasks	_	_
,	_	_
especially	_	_
in	_	_
tasks	_	_
that	_	_
are	_	_
resolution-dependent	_	_
.	_	_

#239
Table	_	_
II	_	_
shows	_	_
that	_	_
DGFc	_	_
increases	_	_
PSNR	_	_
by	_	_
2.56	_	_
dB	_	_
and	_	_
1.62	_	_
dB	_	_
compared	_	_
to	_	_
DGFc	_	_
b	_	_
for	_	_
style	_	_
transfer	_	_
and	_	_
detail	_	_
manipulation	_	_
.	_	_

#240
DJF	_	_
[	_	_
52	_	_
]	_	_
is	_	_
the	_	_
state-of-the-art	_	_
method	_	_
for	_	_
joint	_	_
upsampling	_	_
.	_	_

#241
To	_	_
verify	_	_
the	_	_
effectiveness	_	_
of	_	_
our	_	_
method	_	_
,	_	_
we	_	_
replace	_	_
guided	_	_
filtering	_	_
layer	_	_
in	_	_
DGF	_	_
with	_	_
DJF	_	_
.	_	_

#242
Results	_	_
in	_	_
Table	_	_
II	_	_
show	_	_
that	_	_
our	_	_
method	_	_
outperforms	_	_
DJF	_	_
in	_	_
all	_	_
tasks	_	_
.	_	_

#243
Besides	_	_
,	_	_
our	_	_
method	_	_
also	_	_
runs	_	_
much	_	_
faster	_	_
than	_	_
DJF	_	_
,	_	_
which	_	_
takes	_	_
9√ó	_	_
less	_	_
time	_	_
than	_	_
DJF	_	_
on	_	_
images	_	_
with	_	_
resolution	_	_
10242	_	_
(	_	_
5ms	_	_
v.s	_	_
.	_	_

#244
46ms	_	_
)	_	_
.	_	_

#245
4	_	_
)	_	_
Cross	_	_
Resolution	_	_
Generalization	_	_
:	_	_
In	_	_
the	_	_
main	_	_
experiment	_	_
,	_	_
our	_	_
method	_	_
is	_	_
evaluated	_	_
on	_	_
1024s	_	_
images	_	_
.	_	_

#246
To	_	_
show	_	_
the	_	_
generalization	_	_
ability	_	_
of	_	_
DGF	_	_
for	_	_
processing	_	_
images	_	_
in	_	_
different	_	_
resolutions	_	_
,	_	_
the	_	_
pre-trained	_	_
DGF	_	_
is	_	_
directed	_	_
employed	_	_
on	_	_
images	_	_
in	_	_
512s	_	_
,	_	_
1024s	_	_
,	_	_
1536s	_	_
,	_	_
and	_	_
2048s	_	_
without	_	_
finetuning	_	_
.	_	_

#247
As	_	_
shown	_	_
in	_	_
Figure	_	_
6	_	_
,	_	_
our	_	_
method	_	_
performs	_	_
equally	_	_
well	_	_
across	_	_
different	_	_
resolutions	_	_
on	_	_
all	_	_
tasks	_	_
except	_	_
style	_	_
transfer	_	_
.	_	_

#248
The	_	_
reason	_	_
is	_	_
that	_	_
style	_	_
transfer	_	_
is	_	_
highly	_	_
resolution-dependent	_	_
.	_	_

#249
Concretely	_	_
,	_	_
given	_	_
a	_	_
reference	_	_
image	_	_
with	_	_
a	_	_
fixed	_	_
resolution	_	_
,	_	_
the	_	_
styles	_	_
of	_	_
the	_	_
outputs	_	_
are	_	_
different	_	_
for	_	_
input	_	_
images	_	_
with	_	_
different	_	_
resolutions	_	_
.	_	_

#250
5	_	_
)	_	_
Ablation	_	_
Study	_	_
:	_	_
A	_	_
series	_	_
of	_	_
experiments	_	_
are	_	_
taken	_	_
in	_	_
this	_	_
section	_	_
to	_	_
validate	_	_
the	_	_
effect	_	_
of	_	_
each	_	_
hyper-parameter	_	_
in	_	_
the	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
.	_	_

#251
512¬≤	_	_
1024¬≤	_	_
1536¬≤	_	_
2048¬≤	_	_
PS	_	_
N	_	_
R	_	_
Image	_	_
Resolution	_	_
T-1	_	_
T-2	_	_
T-3	_	_
T-4	_	_
T-5	_	_
Fig.	_	_
6	_	_
.	_	_

#252
Cross	_	_
Resolution	_	_
Generalization	_	_
.	_	_

#253
T-x	_	_
represents	_	_
the	_	_
x-th	_	_
image	_	_
processing	_	_
task	_	_
in	_	_
Table	_	_
II	_	_
.	_	_

#254
TABLE	_	_
IV	_	_
SPEED	_	_
AND	_	_
MEMORY	_	_
USAGE	_	_
IN	_	_
DIFFERENT	_	_
RESOLUTIONS	_	_
OF	_	_
Il	_	_
.	_	_

#255
Resolution	_	_
32	_	_
64	_	_
96	_	_
128	_	_
256	_	_
512	_	_
Running	_	_
Time	_	_
(	_	_
ms	_	_
)	_	_
6	_	_
6	_	_
6	_	_
7	_	_
7	_	_
19	_	_
Memory	_	_
Usage	_	_
(	_	_
G	_	_
)	_	_
0.67	_	_
0.68	_	_
0.70	_	_
0.73	_	_
0.87	_	_
1.41	_	_
The	_	_
role	_	_
of	_	_
radius	_	_
r	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
7a	_	_
.	_	_

#256
The	_	_
performance	_	_
drops	_	_
quickly	_	_
as	_	_
r	_	_
grows	_	_
,	_	_
and	_	_
the	_	_
default	_	_
setting	_	_
(	_	_
r	_	_
=	_	_
1	_	_
)	_	_
obtains	_	_
the	_	_
best	_	_
PSNR	_	_
score	_	_
.	_	_

#257
The	_	_
effect	_	_
of	_	_
the	_	_
resolution	_	_
of	_	_
Il	_	_
is	_	_
shown	_	_
in	_	_
Figure	_	_
7b	_	_
.	_	_

#258
For	_	_
L0	_	_
smoothing	_	_
,	_	_
multi-scale	_	_
detail	_	_
manipulation	_	_
,	_	_
and	_	_
non-local	_	_
dehazing	_	_
,	_	_
the	_	_
performance	_	_
grows	_	_
as	_	_
the	_	_
resolution	_	_
of	_	_
Il	_	_
increases	_	_
.	_	_

#259
For	_	_
style	_	_
transfer	_	_
and	_	_
image	_	_
retouching	_	_
,	_	_
higher	_	_
resolution	_	_
is	_	_
not	_	_
always	_	_
better	_	_
.	_	_

#260
The	_	_
corresponding	_	_
running	_	_
time	_	_
and	_	_
memory	_	_
usage	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
IV	_	_
.	_	_

#261
When	_	_
the	_	_
resolution	_	_
of	_	_
Il	_	_
is	_	_
128	_	_
or	_	_
256	_	_
,	_	_
our	_	_
method	_	_
can	_	_
not	_	_
only	_	_
achieve	_	_
an	_	_
excellent	_	_
performance	_	_
but	_	_
also	_	_
run	_	_
very	_	_
fast	_	_
.	_	_

#262
The	_	_
function	_	_
of	_	_
F	_	_
(	_	_
I	_	_
)	_	_
is	_	_
also	_	_
explored	_	_
by	_	_
varying	_	_
the	_	_
dilation	_	_
rate	_	_
.	_	_

#263
Figure	_	_
7c	_	_
shows	_	_
that	_	_
increasing	_	_
the	_	_
dilation	_	_
rate	_	_
can	_	_
improve	_	_
the	_	_
performance	_	_
to	_	_
a	_	_
degree	_	_
.	_	_

#264
T-1	_	_
T-2	_	_
T-3	_	_
T-4	_	_
T-5	_	_
1	_	_
2	_	_
3	_	_
4	_	_
5	_	_
(	_	_
a	_	_
)	_	_
Radius	_	_
r	_	_
T-1	_	_
T-2	_	_
T-3	_	_
T-4	_	_
T-5	_	_
32	_	_
64	_	_
96	_	_
128	_	_
256	_	_
512	_	_
(	_	_
b	_	_
)	_	_
Resolution	_	_
of	_	_
Il	_	_
T-1	_	_
T-2	_	_
T-3	_	_
T-4	_	_
T-5	_	_
0	_	_
1	_	_
2	_	_
3	_	_
4	_	_
(	_	_
c	_	_
)	_	_
Dilation	_	_
Rate	_	_
of	_	_
F	_	_
(	_	_
I	_	_
)	_	_
Fig.	_	_
7	_	_
.	_	_

#265
Ablation	_	_
Study	_	_
.	_	_

#266
The	_	_
performance	_	_
(	_	_
PSNR	_	_
)	_	_
with	_	_
different	_	_
(	_	_
a	_	_
)	_	_
radius	_	_
r	_	_
of	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
(	_	_
b	_	_
)	_	_
resolution	_	_
of	_	_
Il	_	_
,	_	_
and	_	_
(	_	_
c	_	_
)	_	_
dilation	_	_
rate	_	_
of	_	_
F	_	_
(	_	_
I	_	_
)	_	_
.	_	_

#267
T-x	_	_
represents	_	_
the	_	_
x-th	_	_
image	_	_
processing	_	_
task	_	_
in	_	_
Table	_	_
II	_	_
.	_	_

#268
Best	_	_
viewed	_	_
in	_	_
color	_	_
.	_	_

#269
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
c	_	_
)	_	_
CAN	_	_
(	_	_
d	_	_
)	_	_
DBL	_	_
(	_	_
e	_	_
)	_	_
GT	_	_
Non-local	_	_
Dehazing	_	_
[	_	_
4	_	_
]	_	_
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
c	_	_
)	_	_
CAN	_	_
(	_	_
d	_	_
)	_	_
DBL	_	_
(	_	_
e	_	_
)	_	_
GT	_	_
Style	_	_
Transfer	_	_
[	_	_
11	_	_
]	_	_
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
c	_	_
)	_	_
CAN	_	_
(	_	_
d	_	_
)	_	_
DBL	_	_
(	_	_
e	_	_
)	_	_
GT	_	_
Multi-scale	_	_
Detail	_	_
Manipulation	_	_
[	_	_
3	_	_
]	_	_
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
c	_	_
)	_	_
CAN	_	_
(	_	_
d	_	_
)	_	_
DBL	_	_
(	_	_
e	_	_
)	_	_
GT	_	_
L0	_	_
Smoothing	_	_
[	_	_
7	_	_
]	_	_
Fig.	_	_
8	_	_
.	_	_

#270
Qualitative	_	_
Results	_	_
in	_	_
Image	_	_
Processing	_	_
.	_	_

#271
Our	_	_
method	_	_
DGFc	_	_
is	_	_
more	_	_
visually	_	_
appealing	_	_
than	_	_
other	_	_
approaches	_	_
.	_	_

#272
Best	_	_
viewed	_	_
in	_	_
color	_	_
.	_	_

#273
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ground	_	_
Truth	_	_
(	_	_
c	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
d	_	_
)	_	_
Baseline	_	_
Saliency	_	_
Object	_	_
Detection	_	_
[	_	_
20	_	_
]	_	_
(	_	_
a	_	_
)	_	_
Input	_	_
(	_	_
b	_	_
)	_	_
Ground	_	_
Truth	_	_
(	_	_
c	_	_
)	_	_
Ours	_	_
(	_	_
DGF	_	_
)	_	_
(	_	_
d	_	_
)	_	_
Baseline	_	_
Semantic	_	_
Segmentation	_	_
[	_	_
16	_	_
]	_	_
Fig.	_	_
9	_	_
.	_	_

#274
Qualitative	_	_
Results	_	_
in	_	_
Computer	_	_
Vision	_	_
.	_	_

#275
Best	_	_
viewed	_	_
in	_	_
color	_	_
.	_	_

#276
VI	_	_
.	_	_

#277
EXPERIMENTS	_	_
:	_	_
COMPUTER	_	_
VISION	_	_
TASKS	_	_
The	_	_
proposed	_	_
guided	_	_
filtering	_	_
layer	_	_
can	_	_
dramatically	_	_
advance	_	_
the	_	_
performance	_	_
of	_	_
multiple	_	_
image	_	_
processing	_	_
tasks	_	_
in	_	_
accuracy	_	_
,	_	_
speed	_	_
,	_	_
and	_	_
memory	_	_
usage	_	_
.	_	_

#278
Moreover	_	_
,	_	_
our	_	_
method	_	_
can	_	_
also	_	_
be	_	_
employed	_	_
to	_	_
replace	_	_
the	_	_
time-consuming	_	_
conditional	_	_
random	_	_
field	_	_
(	_	_
CRF	_	_
)	_	_
in	_	_
many	_	_
computer	_	_
vision	_	_
applications	_	_
.	_	_

#279
To	_	_
evaluate	_	_
the	_	_
effectiveness	_	_
of	_	_
our	_	_
method	_	_
,	_	_
we	_	_
take	_	_
an	_	_
experiment	_	_
on	_	_
three	_	_
computer	_	_
vision	_	_
tasks	_	_
ranging	_	_
from	_	_
low-level	_	_
vision	_	_
to	_	_
high	_	_
level-vision	_	_
,	_	_
namely	_	_
depth	_	_
estimation	_	_
[	_	_
19	_	_
]	_	_
,	_	_
saliency	_	_
object	_	_
detection	_	_
[	_	_
20	_	_
]	_	_
,	_	_
and	_	_
semantic	_	_
segmentation	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#280
A	_	_
.	_	_

#281
Details	_	_
of	_	_
Three	_	_
Computer	_	_
Vision	_	_
Tasks	_	_
1	_	_
)	_	_
Depth	_	_
Estimation	_	_
:	_	_
Depth	_	_
estimation	_	_
is	_	_
proposed	_	_
by	_	_
Saxena	_	_
et	_	_
al.	_	_
[	_	_
19	_	_
]	_	_
,	_	_
which	_	_
aims	_	_
at	_	_
predicting	_	_
the	_	_
depth	_	_
at	_	_
each	_	_
pixel	_	_
of	_	_
an	_	_
image	_	_
with	_	_
monocular	_	_
cues	_	_
.	_	_

#282
For	_	_
this	_	_
task	_	_
,	_	_
KITTI	_	_
[	_	_
53	_	_
]	_	_
is	_	_
the	_	_
most	_	_
widely	_	_
used	_	_
dataset	_	_
,	_	_
which	_	_
contains	_	_
42,382	_	_
rectified	_	_
stereo	_	_
pairs	_	_
from	_	_
61	_	_
scenes	_	_
.	_	_

#283
In	_	_
this	_	_
paper	_	_
,	_	_
29,000/1,159	_	_
images	_	_
from	_	_
the	_	_
official	_	_
training	_	_
set	_	_
are	_	_
used	_	_
for	_	_
training	_	_
and	_	_
evaluation	_	_
,	_	_
which	_	_
covers	_	_
33	_	_
scenes	_	_
.	_	_

#284
The	_	_
remaining	_	_
28	_	_
scenes	_	_
of	_	_
the	_	_
official	_	_
training	_	_
set	_	_
contain	_	_
200	_	_
high-quality	_	_
disparity	_	_
images	_	_
,	_	_
which	_	_
are	_	_
used	_	_
for	_	_
testing	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#285
2	_	_
)	_	_
Saliency	_	_
Object	_	_
Detection	_	_
:	_	_
Saliency	_	_
object	_	_
detection	_	_
is	_	_
used	_	_
to	_	_
detect	_	_
the	_	_
most	_	_
salient	_	_
object	_	_
in	_	_
an	_	_
image	_	_
,	_	_
which	_	_
is	_	_
formulated	_	_
as	_	_
an	_	_
image	_	_
segmentation	_	_
problem	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#286
MSRAB	_	_
[	_	_
54	_	_
]	_	_
and	_	_
the	_	_
official	_	_
training/validation/test	_	_
split	_	_
[	_	_
54	_	_
]	_	_
is	_	_
used	_	_
in	_	_
our	_	_
experiment	_	_
.	_	_

#287
3	_	_
)	_	_
Semantic	_	_
Segmentation	_	_
:	_	_
Semantic	_	_
segmentation	_	_
aims	_	_
at	_	_
assigning	_	_
each	_	_
pixel	_	_
of	_	_
an	_	_
image	_	_
to	_	_
one	_	_
of	_	_
the	_	_
pre-defined	_	_
labels	_	_
[	_	_
16	_	_
]	_	_
.	_	_

#288
To	_	_
evaluate	_	_
our	_	_
method	_	_
,	_	_
PASCAL	_	_
VOC	_	_
2012	_	_
benchmark	_	_
[	_	_
55	_	_
]	_	_
is	_	_
used	_	_
in	_	_
this	_	_
paper	_	_
,	_	_
which	_	_
involves	_	_
20	_	_
foreground	_	_
object	_	_
classes	_	_
and	_	_
one	_	_
background	_	_
class	_	_
.	_	_

#289
The	_	_
original	_	_
dataset	_	_
contains	_	_
1,464	_	_
,	_	_
1,449	_	_
,	_	_
and	_	_
1,456	_	_
pixel-wise	_	_
labeled	_	_
images	_	_
for	_	_
training	_	_
,	_	_
validation	_	_
,	_	_
and	_	_
testing	_	_
,	_	_
respectively	_	_
.	_	_

#290
The	_	_
training	_	_
set	_	_
is	_	_
further	_	_
augmented	_	_
by	_	_
extra	_	_
annotations	_	_
[	_	_
56	_	_
]	_	_
,	_	_
resulting	_	_
in	_	_
10,582	_	_
images	_	_
.	_	_

#291
We	_	_
use	_	_
the	_	_
10,582	_	_
augmented	_	_
images	_	_
for	_	_
training	_	_
and	_	_
the	_	_
1,449	_	_
validation	_	_
images	_	_
for	_	_
testing	_	_
.	_	_

#292
B	_	_
.	_	_

#293
Details	_	_
of	_	_
DGF	_	_
When	_	_
applying	_	_
DGF	_	_
to	_	_
computer	_	_
vision	_	_
tasks	_	_
,	_	_
the	_	_
high-resolution	_	_
input	_	_
image	_	_
is	_	_
directly	_	_
processed	_	_
by	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
without	_	_
downsampling	_	_
,	_	_
generating	_	_
the	_	_
low-resolution	_	_
output	_	_
Ol	_	_
.	_	_

#294
As	_	_
for	_	_
the	_	_
architecture	_	_
of	_	_
Cl	_	_
(	_	_
Il	_	_
)	_	_
,	_	_
MonoDepth7	_	_
[	_	_
6	_	_
]	_	_
,	_	_
DSS8	_	_
[	_	_
5	_	_
]	_	_
,	_	_
DeepLab-V29	_	_
[	_	_
24	_	_
]	_	_
are	_	_
employed	_	_
for	_	_
depth	_	_
estimation	_	_
,	_	_
saliency	_	_
detection	_	_
,	_	_
and	_	_
semantic	_	_
segmentation	_	_
respectively	_	_
.	_	_

#295
The	_	_
corresponding	_	_
training	_	_
and	_	_
testing	_	_
procedures	_	_
and	_	_
loss	_	_
functions	_	_
are	_	_
also	_	_
used	_	_
to	_	_
train	_	_
our	_	_
network	_	_
.	_	_

#296
As	_	_
for	_	_
the	_	_
hyper-parameters	_	_
of	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
r	_	_
and	_	_
Œµ	_	_
are	_	_
determined	_	_
by	_	_
grid	_	_
search	_	_
on	_	_
the	_	_
validation	_	_
set	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Table	_	_
V.	_	_
Notably	_	_
,	_	_
a	_	_
second	_	_
guided	_	_
filtering	_	_
layer	_	_
is	_	_
applied	_	_
in	_	_
the	_	_
saliency	_	_
detection	_	_
task	_	_
to	_	_
achieve	_	_
better	_	_
performance	_	_
.	_	_

#297
C.	_	_
Main	_	_
Results	_	_
The	_	_
performances	_	_
of	_	_
our	_	_
method	_	_
and	_	_
baseline	_	_
methods	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
VI	_	_
.	_	_

#298
For	_	_
depth	_	_
estimation	_	_
,	_	_
DGFs	_	_
obtain	_	_
0.177	_	_
improvements	_	_
in	_	_
rms	_	_
over	_	_
the	_	_
baseline	_	_
.	_	_

#299
By	_	_
end-to-end	_	_
training	_	_
and	_	_
adding	_	_
the	_	_
learnable	_	_
guidance	_	_
map	_	_
,	_	_
we	_	_
achieve	_	_
the	_	_
best	_	_
performance	_	_
(	_	_
5.887	_	_
)	_	_
in	_	_
rms	_	_
.	_	_

#300
Similar	_	_
results	_	_
are	_	_
obtained	_	_
in	_	_
saliency	_	_
detection	_	_
and	_	_
semantic	_	_
segmentation	_	_
.	_	_

#301
FŒ≤	_	_
increases	_	_
from	_	_
90.61	_	_
%	_	_
to	_	_
91.29	_	_
%	_	_
by	_	_
applying	_	_
the	_	_
guided	_	_
filtering	_	_
layer	_	_
to	_	_
saliency	_	_
detection	_	_
.	_	_

#302
By	_	_
replacing	_	_
DGFs	_	_
with	_	_
DGF	_	_
,	_	_
FŒ≤	_	_
further	_	_
improves	_	_
to	_	_
91.75	_	_
%	_	_
.	_	_

#303
For	_	_
segmentation	_	_
,	_	_
DGF	_	_
obtains	_	_
73.58	_	_
%	_	_
in	_	_
mean	_	_
IOU	_	_
,	_	_
which	_	_
has	_	_
an	_	_
improvement	_	_
of	_	_
1.79	_	_
%	_	_
compared	_	_
to	_	_
the	_	_
baseline	_	_
method	_	_
.	_	_

#304
We	_	_
also	_	_
compare	_	_
our	_	_
method	_	_
with	_	_
DenseCRF	_	_
[	_	_
57	_	_
]	_	_
,	_	_
which	_	_
is	_	_
commonly	_	_
used	_	_
in	_	_
saliency	_	_
detection	_	_
and	_	_
semantic	_	_
segmentation	_	_
.	_	_

#305
Experiments	_	_
show	_	_
that	_	_
our	_	_
method	_	_
is	_	_
comparable	_	_
to	_	_
DenseCRF	_	_
in	_	_
saliency	_	_
detection	_	_
,	_	_
and	_	_
obtains	_	_
better	_	_
performance	_	_
in	_	_
semantic	_	_
segmentation	_	_
.	_	_

#306
Besides	_	_
,	_	_
the	_	_
proposed	_	_
7https	_	_
:	_	_
//github.com/mrharicot/monodepth	_	_
8https	_	_
:	_	_
//github.com/wuhuikai/DeepGuidedFilter	_	_
9https	_	_
:	_	_
//github.com/isht7/pytorch-deeplab-resnet	_	_
TABLE	_	_
V	_	_
HYPER-PARAMETERS	_	_
OF	_	_
GUIDED	_	_
FILTERING	_	_
LAYER	_	_
.	_	_

#307
1st	_	_
guided	_	_
filtering	_	_
layer	_	_
2nd	_	_
guided	_	_
filtering	_	_
layer	_	_
Hyper-Params	_	_
r	_	_
Œµ	_	_
r	_	_
Œµ	_	_
Depth	_	_
4	_	_
1e-2	_	_
-	_	_
Saliency	_	_
8	_	_
1e-2	_	_
8	_	_
1e-2	_	_
Segmentation	_	_
4	_	_
1e-2	_	_
-	_	_
TABLE	_	_
VI	_	_
QUANTITATIVE	_	_
COMPARISON	_	_
ON	_	_
COMPUTER	_	_
VISION	_	_
TASKS	_	_
.	_	_

#308
Method	_	_
Depth	_	_
Estimation	_	_
Saliency	_	_
Detection	_	_
Segmentation	_	_
rms	_	_
log10	_	_
FŒ≤	_	_
Mean	_	_
IOU	_	_
Baseline	_	_
6.081	_	_
0.216	_	_
90.61	_	_
%	_	_
71.79	_	_
%	_	_
DenseCRF	_	_
-	_	_
-	_	_
91.87	_	_
%	_	_
72.69	_	_
%	_	_
DGFs	_	_
5.904	_	_
0.211	_	_
91.29	_	_
%	_	_
71.72	_	_
%	_	_
DGF	_	_
5.887	_	_
0.209	_	_
91.75	_	_
%	_	_
73.58	_	_
%	_	_
layer	_	_
performs	_	_
at	_	_
least	_	_
10√ó	_	_
faster	_	_
than	_	_
DenseCRF	_	_
.	_	_

#309
Averagely	_	_
,	_	_
our	_	_
approach	_	_
takes	_	_
34ms	_	_
to	_	_
process	_	_
a	_	_
5122	_	_
image	_	_
,	_	_
while	_	_
DenseCRF	_	_
takes	_	_
432ms	_	_
.	_	_

#310
Figure	_	_
9	_	_
shows	_	_
the	_	_
visual	_	_
results	_	_
of	_	_
our	_	_
method	_	_
and	_	_
baselines	_	_
.	_	_

#311
The	_	_
results	_	_
obtained	_	_
by	_	_
our	_	_
approach	_	_
are	_	_
better	_	_
in	_	_
preserving	_	_
edges	_	_
and	_	_
details10	_	_
.	_	_

#312
VII	_	_
.	_	_

#313
CONCLUSION	_	_
We	_	_
present	_	_
a	_	_
novel	_	_
building	_	_
block	_	_
for	_	_
FCN	_	_
,	_	_
namely	_	_
guided	_	_
filtering	_	_
layer	_	_
,	_	_
which	_	_
aims	_	_
at	_	_
enhancing	_	_
the	_	_
ability	_	_
of	_	_
FCNs	_	_
for	_	_
joint	_	_
upsampling	_	_
.	_	_

#314
By	_	_
formulating	_	_
the	_	_
guided	_	_
filter	_	_
into	_	_
a	_	_
fully	_	_
differentiable	_	_
module	_	_
with	_	_
learnable	_	_
convolutional	_	_
kernels	_	_
,	_	_
FCN-based	_	_
pixel-wise	_	_
image	_	_
prediction	_	_
approaches	_	_
can	_	_
benefit	_	_
from	_	_
end-to-end	_	_
training	_	_
and	_	_
generate	_	_
high-quality	_	_
results	_	_
.	_	_

#315
We	_	_
further	_	_
extend	_	_
the	_	_
proposed	_	_
layer	_	_
with	_	_
a	_	_
learnable	_	_
transformation	_	_
function	_	_
,	_	_
which	_	_
makes	_	_
it	_	_
generalize	_	_
well	_	_
to	_	_
different	_	_
tasks	_	_
by	_	_
producing	_	_
task-specific	_	_
guidance	_	_
maps	_	_
.	_	_

#316
We	_	_
integrate	_	_
the	_	_
guided	_	_
filtering	_	_
layer	_	_
with	_	_
FCNs	_	_
and	_	_
evaluate	_	_
it	_	_
on	_	_
five	_	_
image	_	_
processing	_	_
tasks	_	_
and	_	_
three	_	_
computer	_	_
vision	_	_
tasks	_	_
.	_	_

#317
Experiments	_	_
show	_	_
that	_	_
the	_	_
proposed	_	_
layer	_	_
could	capability	_
achieve	_	_
state-of-the-art	_	_
performance	_	_
while	_	_
taking	_	_
10-100√ó	_	_
less	_	_
computational	_	_
cost	_	_
.	_	_

#318
We	_	_
also	_	_
conduct	_	_
a	_	_
comprehensive	_	_
ablation	_	_
study	_	_
,	_	_
which	_	_
demonstrates	_	_
the	_	_
contribution	_	_
of	_	_
each	_	_
component	_	_
as	_	_
well	_	_
as	_	_
the	_	_
hyper-parameters	_	_
.	_	_

#319
ACKNOWLEDGEMENT	_	_

#320
This	_	_
work	_	_
is	_	_
funded	_	_
by	_	_
the	_	_
National	_	_
Key	_	_
Research	_	_
and	_	_
Development	_	_
Program	_	_
of	_	_
China	_	_
(	_	_
Grant	_	_
2016YFB1001004	_	_
and	_	_
Grant	_	_
2016YFB1001005	_	_
)	_	_
,	_	_
the	_	_
National	_	_
Natural	_	_
Science	_	_
Foundation	_	_
of	_	_
China	_	_
(	_	_
Grant	_	_
61673375	_	_
,	_	_
Grant	_	_
61721004	_	_
and	_	_
Grant	_	_
61403383	_	_
)	_	_
and	_	_
the	_	_
Projects	_	_
of	_	_
Chinese	_	_
Academy	_	_
of	_	_
Sciences	_	_
(	_	_
Grant	_	_
QYZDB-SSW-JSC006	_	_
and	_	_
Grant	_	_
173211KYSB20160008	_	_
)	_	_
.	_	_

#321
The	_	_
authors	_	_
would	_	_
like	_	_
to	_	_
thank	_	_
Patrick	_	_
PeÃÅrez	_	_
and	_	_
Philip	_	_
Torr	_	_
for	_	_
their	_	_
helpful	_	_
suggestions	_	_
.	_	_

#322
10More	_	_
qualitative	_	_
results	_	_
are	_	_
shown	_	_
in	_	_
http	_	_
:	_	_
//wuhuikai.me/	_	_
DeepGuidedFilterProject/	_	_
#	_	_
visual	_	_
.	_	_