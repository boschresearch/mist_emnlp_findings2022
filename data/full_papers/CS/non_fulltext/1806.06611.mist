#0
On	_	_
Multi-resident	_	_
Activity	_	_
Recognition	_	_
in	_	_
Ambient	_	_
Smart-Homes	_	_
Son	_	_
N.	_	_
Tran	_	_
,	_	_
Qing	_	_
Zhang	_	_
,	_	_
and	_	_
Mohan	_	_
Karunanithi	_	_
The	_	_
Australian	_	_
E-Health	_	_
Research	_	_
Centre	_	_
,	_	_
CSIRO	_	_
,	_	_
Brisbane	_	_
,	_	_
QLD	_	_
4026	_	_
,	_	_
Australia	_	_
{	_	_
son.tran	_	_
,	_	_
qing.zhang	_	_
,	_	_
mohan.karunanithi	_	_
}	_	_
@	_	_
csiro.au	_	_
Abstract	_	_
.	_	_

#1
Increasing	_	_
attention	_	_
to	_	_
the	_	_
research	_	_
on	_	_
activity	_	_
monitoring	_	_
in	_	_
smart	_	_
homes	_	_
has	_	_
motivated	_	_
the	_	_
employment	_	_
of	_	_
ambient	_	_
intelligence	_	_
to	_	_
reduce	_	_
the	_	_
deployment	_	_
cost	_	_
and	_	_
solve	_	_
the	_	_
privacy	_	_
issue	_	_
.	_	_

#2
Several	_	_
approaches	_	_
have	_	_
been	_	_
proposed	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
,	_	_
however	_	_
,	_	_
there	_	_
still	_	_
lacks	_	_
a	_	_
comprehensive	_	_
benchmark	_	_
for	_	_
future	_	_
research	_	_
and	_	_
practical	_	_
selection	_	_
of	_	_
models	_	_
.	_	_

#3
In	_	_
this	_	_
paper	_	_
we	_	_
study	_	_
different	_	_
methods	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
and	_	_
evaluate	_	_
them	_	_
on	_	_
same	_	_
sets	_	_
of	_	_
data	_	_
.	_	_

#4
The	_	_
experimental	_	_
results	_	_
show	_	_
that	_	_
recurrent	_	_
neural	_	_
network	_	_
with	_	_
gated	_	_
recurrent	_	_
units	_	_
is	_	_
better	_	_
than	_	_
other	_	_
models	_	_
and	_	_
also	_	_
considerably	_	_
efficient	_	_
,	_	_
and	_	_
that	_	_
using	_	_
combined	_	_
activities	_	_
as	_	_
single	_	_
labels	_	_
is	_	_
more	_	_
effective	_	_
than	_	_
represent	_	_
them	_	_
as	_	_
separate	_	_
labels	_	_
.	_	_

#5
Keywords	_	_
:	_	_
Multiresident	_	_
activity	_	_
,	_	_
recognition	_	_
,	_	_
pervasive	_	_
computing	_	_
,	_	_
smart	_	_
homes	_	_

#6
1	_	_
Introduction	_	_

#7
In	_	_
intelligent	_	_
environments	_	_
such	_	_
as	_	_
smart	_	_
homes	_	_
activity	_	_
recognition	_	_
plays	_	_
an	_	_
important	_	_
role	_	_
,	_	_
especially	_	_
when	_	_
applying	_	_
to	_	_
health	_	_
monitoring	_	_
and	_	_
assistance	_	_
[	_	_
10	_	_
]	_	_
.	_	_

#8
Many	_	_
efforts	_	_
has	_	_
been	_	_
made	_	_
to	_	_
model	_	_
the	_	_
activities	_	_
of	_	_
residents	_	_
in	_	_
order	_	_
to	_	_
facilitate	_	_
reasoning	_	_
of	_	_
their	_	_
behaviour	_	_
.	_	_

#9
The	_	_
success	_	_
of	_	_
such	_	_
models	_	_
would	_	_
result	_	_
in	_	_
reducing	_	_
cost	_	_
of	_	_
traditional	_	_
health	_	_
care	_	_
,	_	_
a	_	_
smarter	_	_
and	_	_
safer	_	_
home	_	_
for	_	_
eldercare	_	_
,	_	_
and	_	_
better	_	_
assistance	_	_
for	_	_
patients	_	_
.	_	_

#10
Classifying	_	_
human	_	_
activities	_	_
has	_	_
been	_	_
studied	_	_
intensively	_	_
within	_	_
computer	_	_
vision	_	_
domain	_	_
[	_	_
17	_	_
]	_	_
.	_	_

#11
This	_	_
,	_	_
however	_	_
,	_	_
may	_	_
raise	_	_
an	_	_
issue	_	_
on	_	_
privacy	_	_
of	_	_
residents	_	_
due	_	_
to	_	_
the	_	_
use	_	_
of	_	_
unwelcome	_	_
devices	_	_
,	_	_
i.e.	_	_
cameras	_	_
.	_	_

#12
Alternatively	_	_
,	_	_
many	_	_
other	_	_
approaches	_	_
rely	_	_
on	_	_
wearable	_	_
sensors	_	_
[	_	_
16	_	_
]	_	_
,	_	_
which	_	_
seems	_	_
less	_	_
intrusive	_	_
but	_	_
require	_	_
users	_	_
to	_	_
wear	_	_
an	_	_
electronic	_	_
device	_	_
everywhere	_	_
and	_	_
everytime	_	_
.	_	_

#13
Recent	_	_
attention	_	_
is	_	_
aiming	_	_
at	_	_
intelligent	_	_
environments	_	_
where	_	_
residents	_	_
can	_	_
live	_	_
their	_	_
own	_	_
way	_	_
,	_	_
without	_	_
being	_	_
disturbed	_	_
by	_	_
the	_	_
presence	_	_
of	_	_
a	_	_
device	_	_
on	_	_
their	_	_
bodies	_	_
.	_	_

#14
This	_	_
is	_	_
an	_	_
important	_	_
research	_	_
topic	_	_
that	_	_
would	_	_
shape	_	_
the	_	_
future	_	_
of	_	_
smart	_	_
homes	_	_
.	_	_

#15
With	_	_
the	_	_
advance	_	_
in	_	_
pervasive	_	_
sensing	_	_
technologies	_	_
one	_	_
can	_	_
install	_	_
a	_	_
set	_	_
of	_	_
non-intrusive	_	_
sensors	_	_
in	_	_
the	_	_
environment	_	_
with	_	_
respect	_	_
to	_	_
residents’	_	_
privacy	_	_
[	_	_
25,20	_	_
]	_	_
.	_	_

#16
However	_	_
,	_	_
in	_	_
contrast	_	_
to	_	_
the	_	_
development	_	_
of	_	_
ambient	_	_
hardware	_	_
,	_	_
the	_	_
reality	_	_
of	_	_
intelligent	_	_
algorithms	_	_
for	_	_
such	_	_
modern	_	_
smart	_	_
homes	_	_
is	_	_
still	_	_
challenging	_	_
.	_	_

#17
Activity	_	_
recognition	_	_
in	_	_
ambient	_	_
environment	_	_
has	_	_
been	_	_
studied	_	_
for	_	_
years	_	_
,	_	_
most	_	_
of	_	_
that	_	_
focuses	_	_
on	_	_
single	_	_
resident	_	_
,	_	_
aiming	_	_
to	_	_
support	_	_
independent	_	_
living	_	_
[	_	_
23	_	_
]	_	_
.	_	_

#18
ar	_	_
X	_	_
iv	_	_
:1	_	_
6	_	_
.	_	_

#19
1v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
8	_	_
Ju	_	_
n	_	_
2	_	_
Tran	_	_
et	_	_
al.	_	_
However	_	_
,	_	_
in	_	_
practice	_	_
this	_	_
is	_	_
not	_	_
always	_	_
the	_	_
case	_	_
since	_	_
modern	_	_
smart	_	_
environments	_	_
should	_	_
be	_	_
able	_	_
to	_	_
support	_	_
multiple	_	_
occupants	_	_
.	_	_

#20
As	_	_
a	_	_
result	_	_
,	_	_
there	_	_
is	_	_
a	_	_
growing	_	_
desire	_	_
for	_	_
a	_	_
model	_	_
that	_	_
is	_	_
capable	_	_
of	_	_
capturing	_	_
the	_	_
complexity	_	_
nature	_	_
of	_	_
both	_	_
independent	_	_
and	_	_
joint	_	_
activities	_	_
.	_	_

#21
This	_	_
is	_	_
a	_	_
challenging	_	_
task	_	_
because	_	_
different	_	_
from	_	_
the	_	_
case	_	_
of	_	_
single	_	_
resident	_	_
where	_	_
the	_	_
sensors’	_	_
states	_	_
reflect	_	_
directly	_	_
the	_	_
activity	_	_
of	_	_
a	_	_
specific	_	_
person	_	_
,	_	_
in	_	_
multi-resident	_	_
case	_	_
that	_	_
information	_	_
,	_	_
as	_	_
known	_	_
as	_	_
data	_	_
association	_	_
is	_	_
not	_	_
commonly	_	_
known	_	_
.	_	_

#22
In	_	_
recent	_	_
work	_	_
,	_	_
temporal	_	_
approaches	_	_
have	_	_
been	_	_
widely	_	_
employed	_	_
to	_	_
model	_	_
activities	_	_
in	_	_
smart	_	_
homes	_	_
(	_	_
see	_	_
the	_	_
survey	_	_
[	_	_
2	_	_
]	_	_
)	_	_
.	_	_

#23
However	_	_
,	_	_
there	_	_
still	_	_
lacks	_	_
a	_	_
comprehensive	_	_
study	_	_
on	_	_
how	_	_
different	_	_
sequence	_	_
models	_	_
perform	_	_
in	_	_
this	_	_
application	_	_
domain	_	_
.	_	_

#24
In	_	_
this	_	_
paper	_	_
we	_	_
investigate	_	_
the	_	_
use	_	_
of	_	_
hidden	_	_
Markov	_	_
models	_	_
(	_	_
HMMs	_	_
)	_	_
,	_	_
conditional	_	_
random	_	_
fields	_	_
(	_	_
CRFs	_	_
)	_	_
,	_	_
and	_	_
recurrent	_	_
neural	_	_
networks	_	_
(	_	_
RNNs	_	_
)	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
in	_	_
ambient	_	_
smart	_	_
homes	_	_
.	_	_

#25
The	_	_
study	_	_
also	_	_
focuses	_	_
on	_	_
two	_	_
different	_	_
methods	_	_
of	_	_
encoding	_	_
activities	_	_
of	_	_
multiple	_	_
residents	_	_
:	_	_
combined	_	_
labels	_	_
and	_	_
separate	_	_
labels	_	_
.	_	_

#26
We	_	_
expect	_	_
that	_	_
the	_	_
work	_	_
would	_	_
serve	_	_
as	_	_
a	_	_
benchmark	_	_
and	_	_
guideline	_	_
for	_	_
those	_	_
who	_	_
seek	_	_
for	_	_
a	_	_
solution	_	_
in	_	_
this	_	_
domain	_	_
.	_	_

#27
In	_	_
general	_	_
,	_	_
we	_	_
model	_	_
the	_	_
activity	_	_
sequences	_	_
of	_	_
multiple	_	_
residents	_	_
as	_	_
an	_	_
input	_	_
of	_	_
a	_	_
function	_	_
with	_	_
the	_	_
input	_	_
is	_	_
the	_	_
states	_	_
of	_	_
a	_	_
smart	_	_
home	_	_
.	_	_

#28
Such	_	_
states	_	_
in	_	_
this	_	_
case	_	_
are	_	_
defined	_	_
as	_	_
the	_	_
values	_	_
of	_	_
the	_	_
sensors	_	_
in	_	_
the	_	_
smart	_	_
home	_	_
.	_	_

#29
The	_	_
output	_	_
can	_	_
be	_	_
divided	_	_
into	_	_
different	_	_
types	_	_
of	_	_
representation	_	_
.	_	_

#30
First	_	_
,	_	_
we	_	_
consider	_	_
the	_	_
output	_	_
as	_	_
a	_	_
single	_	_
variable	_	_
which	_	_
is	_	_
the	_	_
combined	_	_
activities	_	_
of	_	_
all	_	_
residents	_	_
.	_	_

#31
With	_	_
this	_	_
we	_	_
can	_	_
apply	_	_
sequence	_	_
models	_	_
such	_	_
as	_	_
HMMs	_	_
,	_	_
CRFs	_	_
,	_	_
and	_	_
RNNs	_	_
for	_	_
classification	_	_
task	_	_
straightforward	_	_
.	_	_

#32
The	_	_
second	_	_
type	_	_
of	_	_
output	_	_
is	_	_
encoded	_	_
by	_	_
separating	_	_
it	_	_
into	_	_
multiple	_	_
variables	_	_
,	_	_
with	_	_
each	_	_
presenting	_	_
the	_	_
activities	_	_
of	_	_
a	_	_
resident	_	_
,	_	_
while	_	_
sharing	_	_
the	_	_
same	_	_
input	_	_
.	_	_

#33
In	_	_
order	_	_
to	_	_
deal	_	_
with	_	_
multiple	_	_
residents	_	_
we	_	_
need	_	_
more	_	_
complex	_	_
models	_	_
to	_	_
capture	_	_
the	_	_
interactive	_	_
and	_	_
collaborative	_	_
behaviours	_	_
.	_	_

#34
For	_	_
HMMs	_	_
,	_	_
we	_	_
employ	_	_
the	_	_
factorial	_	_
variant	_	_
[	_	_
11	_	_
]	_	_
add	_	_
more	_	_
cross	_	_
dependencies	_	_
.	_	_

#35
In	_	_
the	_	_
case	_	_
CRFs	_	_
,	_	_
cross	_	_
dependencies	_	_
would	_	_
be	_	_
computationally	_	_
expensive	_	_
especially	_	_
for	_	_
the	_	_
data	_	_
of	_	_
very	_	_
long	_	_
sequences	_	_
,	_	_
so	_	_
that	_	_
factorial	_	_
CRFs	_	_
[	_	_
22	_	_
]	_	_
are	_	_
used	_	_
.	_	_

#36
For	_	_
RNNs	_	_
,	_	_
we	_	_
share	_	_
the	_	_
hidden	_	_
layer	_	_
for	_	_
all	_	_
different	_	_
label	_	_
variables	_	_
.	_	_

#37
In	_	_
the	_	_
experiments	_	_
,	_	_
we	_	_
conduct	_	_
evaluations	_	_
of	_	_
the	_	_
models	_	_
on	_	_
three	_	_
smart	_	_
homes	_	_
from	_	_
two	_	_
benchmark	_	_
datasets	_	_
.	_	_

#38
The	_	_
results	_	_
show	_	_
that	_	_
recurrent	_	_
neural	_	_
network	_	_
with	_	_
gated	_	_
recurrent	_	_
units	_	_
is	_	_
better	_	_
than	_	_
other	_	_
models	_	_
and	_	_
also	_	_
considerably	_	_
efficient	_	_
.	_	_

#39
We	_	_
also	_	_
found	_	_
that	_	_
using	_	_
combined	_	_
labels	_	_
is	_	_
more	_	_
effective	_	_
than	_	_
separate-labels	_	_
.	_	_

#40
The	_	_
paper	_	_
is	_	_
organised	_	_
as	_	_
follows	_	_
.	_	_

#41
In	_	_
the	_	_
next	_	_
section	_	_
we	_	_
review	_	_
the	_	_
related	_	_
literature	_	_
of	_	_
our	_	_
work	_	_
.	_	_

#42
Section	_	_
3	_	_
describes	_	_
the	_	_
multi-activity	_	_
modelling	_	_
framework	_	_
and	_	_
the	_	_
models	_	_
studied	_	_
in	_	_
this	_	_
papers	_	_
.	_	_

#43
In	_	_
section	_	_
4	_	_
we	_	_
perform	_	_
experiments	_	_
and	_	_
analyse	_	_
the	_	_
results	_	_
.	_	_

#44
Finally	_	_
,	_	_
in	_	_
Section	_	_
5	_	_
we	_	_
conclude	_	_
the	_	_
work	_	_
.	_	_

#45
2	_	_
Related	_	_
Work	_	_

#46
Research	_	_
on	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
has	_	_
been	_	_
emerging	_	_
recently	_	_
due	_	_
to	_	_
the	_	_
increasing	_	_
demand	_	_
for	_	_
health	_	_
monitoring	_	_
in	_	_
ambient	_	_
intelligent	_	_
environments	_	_
.	_	_

#47
The	_	_
task	_	_
can	_	_
be	_	_
done	_	_
by	_	_
employing	_	_
sequence	_	_
models	_	_
to	_	_
perform	_	_
prediction	_	_
on	_	_
the	_	_
activity	_	_
events	_	_
over	_	_
time	_	_
.	_	_

#48
Hidden	_	_
Markov	_	_
Models	_	_
[	_	_
19	_	_
]	_	_
is	_	_
a	_	_
popular	_	_
On	_	_
Multi-resident	_	_
Activity	_	_
Recognition	_	_
in	_	_
Ambient	_	_
Smart-Homes	_	_
3	_	_
statistical	_	_
model	_	_
for	_	_
sequential	_	_
data	_	_
.	_	_

#49
It	_	_
is	_	_
characterised	_	_
by	_	_
the	_	_
dependency	_	_
of	_	_
an	_	_
observation	_	_
variable	_	_
on	_	_
a	_	_
hidden	_	_
variable	_	_
at	_	_
each	_	_
time	_	_
step	_	_
,	_	_
and	_	_
the	_	_
dependency	_	_
of	_	_
the	_	_
hidden	_	_
variable	_	_
itself	_	_
on	_	_
its	_	_
previous	_	_
state	_	_
.	_	_

#50
HMMs	_	_
can	_	_
be	_	_
employed	_	_
for	_	_
activity	_	_
recognition	_	_
easily	_	_
.	_	_

#51
In	_	_
particular	_	_
,	_	_
one	_	_
can	_	_
define	_	_
the	_	_
observation	_	_
as	_	_
the	_	_
sensors	_	_
state	_	_
,	_	_
i.e.	_	_
video	_	_
frame	_	_
,	_	_
wearable	_	_
or/and	_	_
ambient	_	_
sensors’	_	_
values	_	_
,	_	_
and	_	_
the	_	_
hidden	_	_
variable	_	_
as	_	_
the	_	_
activity	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#52
In	_	_
multi-resident	_	_
smart	_	_
homes	_	_
,	_	_
HMMs	_	_
have	_	_
been	_	_
studied	_	_
intensively	_	_
,	_	_
as	_	_
being	_	_
showed	_	_
in	_	_
previous	_	_
works	_	_
[	_	_
1,4,20,7	_	_
]	_	_
.	_	_

#53
A	_	_
straightforward	_	_
approach	_	_
is	_	_
to	_	_
use	_	_
a	_	_
single	_	_
HMM	_	_
for	_	_
combined	_	_
activities	_	_
,	_	_
i.e.	_	_
treating	_	_
the	_	_
activities	_	_
of	_	_
all	_	_
residents	_	_
as	_	_
a	_	_
random	_	_
variable	_	_
.	_	_

#54
For	_	_
example	_	_
,	_	_
the	_	_
activities	_	_
can	_	_
be	_	_
combined	_	_
as	_	_
joint	_	_
labels	_	_
so	_	_
that	_	_
they	_	_
can	_	_
be	_	_
represented	_	_
by	_	_
a	_	_
single	_	_
hidden	_	_
variable	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#55
Another	_	_
method	_	_
to	_	_
model	_	_
the	_	_
activities	_	_
of	_	_
multiple	_	_
residents	_	_
is	_	_
to	_	_
create	_	_
multiple	_	_
HMMs	_	_
,	_	_
one	_	_
for	_	_
each	_	_
resident	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#56
Such	_	_
model	_	_
,	_	_
as	_	_
known	_	_
as	_	_
parallel	_	_
HMM	_	_
,	_	_
has	_	_
been	_	_
evaluated	_	_
in	_	_
the	_	_
case	_	_
that	_	_
data	_	_
association	_	_
is	_	_
provided	_	_
.	_	_

#57
This	_	_
means	_	_
that	_	_
the	_	_
observation	_	_
has	_	_
been	_	_
separated	_	_
for	_	_
each	_	_
resident	_	_
and	_	_
only	_	_
represents	_	_
the	_	_
sensors	_	_
which	_	_
are	_	_
associating	_	_
to	_	_
that	_	_
resident	_	_
.	_	_

#58
The	_	_
disadvantage	_	_
of	_	_
this	_	_
model	_	_
is	_	_
the	_	_
hidden	_	_
variables	_	_
of	_	_
all	_	_
HMMs	_	_
are	_	_
independent	_	_
from	_	_
each	_	_
others	_	_
.	_	_

#59
In	_	_
multi-resident	_	_
environments	_	_
,	_	_
however	_	_
,	_	_
there	_	_
always	_	_
exist	_	_
correlation	_	_
and	_	_
interaction	_	_
between	_	_
the	_	_
residents	_	_
.	_	_

#60
This	_	_
issue	_	_
is	_	_
addressed	_	_
by	_	_
adding	_	_
the	_	_
crossed	_	_
dependencies	_	_
to	_	_
the	_	_
hidden	_	_
variables	_	_
in	_	_
all	_	_
HMMs	_	_
.	_	_

#61
By	_	_
coupling	_	_
such	_	_
HMMs	_	_
one	_	_
can	_	_
assume	_	_
that	_	_
the	_	_
activity	_	_
of	_	_
a	_	_
resident	_	_
is	_	_
dependent	_	_
not	_	_
only	_	_
on	_	_
his	_	_
previous	_	_
activity	_	_
but	_	_
also	_	_
on	_	_
the	_	_
previous	_	_
activities	_	_
of	_	_
other	_	_
residents	_	_
.	_	_

#62
There	_	_
was	_	_
a	_	_
proposal	_	_
of	_	_
coupled	_	_
HMM	_	_
and	_	_
factorial	_	_
HMM	_	_
in	_	_
computer	_	_
vision	_	_
domain	_	_
[	_	_
3	_	_
]	_	_
,	_	_
but	_	_
only	_	_
coupled	_	_
HMM	_	_
was	_	_
employed	_	_
for	_	_
sensor	_	_
data	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#63
Besides	_	_
HMMs	_	_
,	_	_
CRFs	_	_
[	_	_
9,13	_	_
]	_	_
and	_	_
incremental	_	_
decision	_	_
trees	_	_
(	_	_
IDT	_	_
)	_	_
[	_	_
18	_	_
]	_	_
also	_	_
have	_	_
been	_	_
used	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
.	_	_

#64
From	_	_
learning	_	_
perspective	_	_
,	_	_
the	_	_
problem	_	_
of	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
multi-tasks	_	_
learning	_	_
on	_	_
sequence	_	_
data	_	_
.	_	_

#65
However	_	_
,	_	_
most	_	_
of	_	_
the	_	_
work	_	_
we	_	_
found	_	_
in	_	_
literature	_	_
focus	_	_
on	_	_
modelling	_	_
different	_	_
tasks	_	_
from	_	_
different	_	_
data	_	_
sourses	_	_
by	_	_
taking	_	_
the	_	_
advantage	_	_
of	_	_
recurrent	_	_
neural	_	_
networks	_	_
in	_	_
learning	_	_
more	_	_
generalised	_	_
representation	_	_
from	_	_
larger	_	_
amount	_	_
of	_	_
data	_	_
combined	_	_
.	_	_

#66
Different	_	_
from	_	_
that	_	_
,	_	_
in	_	_
this	_	_
work	_	_
we	_	_
do	_	_
not	_	_
have	_	_
such	_	_
augmentation	_	_
since	_	_
there	_	_
exist	_	_
only	_	_
one	_	_
dataset	_	_
for	_	_
activities	_	_
of	_	_
multiple	_	_
residents	_	_
.	_	_

#67
3	_	_
Multi-Resident	_	_
Activity	_	_
Modelling	_	_

#68
Let	_	_
us	_	_
denote	_	_
am	_	_
,	_	_
t	_	_
and	_	_
ot	_	_
as	_	_
the	_	_
activity	_	_
of	_	_
resident	_	_
m	_	_
and	_	_
the	_	_
sensors’	_	_
state	_	_
at	_	_
time	_	_
t	_	_
respectively	_	_
.	_	_

#69
For	_	_
ease	_	_
of	_	_
presentation	_	_
we	_	_
denote	_	_
at	_	_
=	_	_
{	_	_
a1	_	_
,	_	_
t	_	_
,	_	_
a2	_	_
,	_	_
t	_	_
,	_	_
..	_	_
,	_	_
aM	_	_
,	_	_
t	_	_
}	_	_
as	_	_
the	_	_
activities	_	_
of	_	_
all	_	_
M	_	_
residents	_	_
at	_	_
time	_	_
t.	_	_
We	_	_
use	_	_
t1	_	_
:	_	_
t2	_	_
to	_	_
denote	_	_
a	_	_
sequence	_	_
of	_	_
events/states	_	_
from	_	_
time	_	_
t1	_	_
to	_	_
t2	_	_
.	_	_

#70
For	_	_
example	_	_
,	_	_
at1	_	_
:	_	_
t2	_	_
=	_	_
{	_	_
at1	_	_
,	_	_
..	_	_
,	_	_
at2	_	_
}	_	_
is	_	_
the	_	_
sequence	_	_
of	_	_
activities	_	_
performed	_	_
by	_	_
all	_	_
residents	_	_
from	_	_
time	_	_
t1	_	_
to	_	_
t2	_	_
.	_	_

#71
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
evaluate	_	_
two	_	_
ways	_	_
of	_	_
modelling	_	_
the	_	_
activities	_	_
of	_	_
multiple	_	_
residents	_	_
.	_	_

#72
First	_	_
,	_	_
we	_	_
combine	_	_
the	_	_
activities	_	_
such	_	_
that	_	_
the	_	_
activities	_	_
of	_	_
all	_	_
resident	_	_
at	_	_
a	_	_
time	_	_
step	_	_
is	_	_
represented	_	_
by	_	_
a	_	_
single	_	_
variable	_	_
.	_	_

#73
For	_	_
that	_	_
we	_	_
need	_	_
to	_	_
predict	_	_
a1	_	_
:	_	_
T	_	_
given	_	_
the	_	_
states	_	_
of	_	_
sensors	_	_
o1	_	_
:	_	_
T	_	_
.	_	_

#74
Second	_	_
,	_	_
we	_	_
model	_	_
each	_	_
resident’s	_	_
activity	_	_
as	_	_
a	_	_
separate	_	_
variable	_	_
.	_	_

#75
4	_	_
Tran	_	_
et	_	_
al.	_	_
3.1	_	_
HMM-based	_	_
Approaches	_	_

#76
A	_	_
HMM	_	_
[	_	_
19	_	_
]	_	_
consists	_	_
of	_	_
a	_	_
single	_	_
hidden	_	_
and	_	_
an	_	_
observation	_	_
variable	_	_
which	_	_
assumes	_	_
a	_	_
Markov	_	_
process	_	_
.	_	_

#77
In	_	_
the	_	_
case	_	_
of	_	_
combined	_	_
labels	_	_
we	_	_
can	_	_
use	_	_
a	_	_
single	_	_
HMM	_	_
to	_	_
model	_	_
the	_	_
activities	_	_
as	_	_
a	_	_
joint	_	_
distribution	_	_
as	_	_
:	_	_
p	_	_
(	_	_
a1	_	_
:	_	_
T	_	_
,	_	_
o1	_	_
:	_	_
T	_	_
)	_	_
=	_	_
p	_	_
(	_	_
o1|a1	_	_
)	_	_
p	_	_
(	_	_
a1	_	_
)	_	_
T∏	_	_
t=2	_	_
p	_	_
(	_	_
ot|at	_	_
)	_	_
p	_	_
(	_	_
at|at−1	_	_
)	_	_
(	_	_
1	_	_
)	_	_
Inference	_	_
of	_	_
activities	_	_
given	_	_
a	_	_
sequence	_	_
of	_	_
sensors’	_	_
states	_	_
can	_	_
be	_	_
done	_	_
efficiently	_	_
using	_	_
dynamic	_	_
programming	_	_
,	_	_
i.e.	_	_
Viterbi	_	_
algorithm	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#78
For	_	_
the	_	_
separate	_	_
labels	_	_
,	_	_
different	_	_
HMMs	_	_
have	_	_
been	_	_
used	_	_
such	_	_
as	_	_
parallel	_	_
HMMs	_	_
,	_	_
coupled	_	_
HMMs	_	_
[	_	_
24,21	_	_
]	_	_
.	_	_

#79
In	_	_
this	_	_
paper	_	_
we	_	_
use	_	_
factorial	_	_
HMM	_	_
with	_	_
cross	_	_
dependency	_	_
shown	_	_
in	_	_
Figure	_	_
1d	_	_
,	_	_
as	_	_
this	_	_
variant	_	_
achieves	_	_
better	_	_
performance	_	_
than	_	_
the	_	_
other	_	_
HMMs	_	_
[	_	_
21	_	_
]	_	_
.	_	_

#80
Factorial	_	_
HMM	_	_
[	_	_
11	_	_
]	_	_
,	_	_
is	_	_
a	_	_
HMM	_	_
with	_	_
multiple	_	_
hidden	_	_
variables	_	_
.	_	_

#81
In	_	_
order	_	_
to	_	_
represent	_	_
the	_	_
relations	_	_
between	_	_
activities	_	_
among	_	_
residents	_	_
,	_	_
we	_	_
add	_	_
cross	_	_
connections	_	_
from	_	_
all	_	_
hidden	_	_
variables	_	_
at	_	_
time	_	_
t	_	_
−	_	_
1	_	_
to	_	_
each	_	_
hidden	_	_
variable	_	_
at	_	_
time	_	_
t.	_	_
This	_	_
results	_	_
in	_	_
a	_	_
factorial	_	_
HMM	_	_
model	_	_
with	_	_
cross	_	_
dependency	_	_
as	_	_
we	_	_
introduce	_	_
here	_	_
in	_	_
the	_	_
paper	_	_
.	_	_

#82
The	_	_
joint	_	_
distribution	_	_
of	_	_
this	_	_
HMM	_	_
is	_	_
:	_	_
p	_	_
(	_	_
a1	_	_
:	_	_
T	_	_
,	_	_
o1	_	_
:	_	_
T	_	_
)	_	_
=	_	_
p	_	_
(	_	_
o1|a1	_	_
)	_	_
∏	_	_
m	_	_
p	_	_
(	_	_
am,1	_	_
)	_	_
T∏	_	_
t=2	_	_
(	_	_
p	_	_
(	_	_
ot|at	_	_
)	_	_
∏	_	_
m	_	_
p	_	_
(	_	_
am	_	_
,	_	_
t|at−1	_	_
)	_	_
)	_	_
(	_	_
2	_	_
)	_	_
Similar	_	_
to	_	_
a	_	_
normal	_	_
HMM	_	_
,	_	_
inference	_	_
of	_	_
activities	_	_
can	_	_
easily	_	_
done	_	_
by	_	_
dynamic	_	_
programming	_	_
.	_	_

#83
Here	_	_
,	_	_
only	_	_
the	_	_
transition	_	_
and	_	_
the	_	_
prior	_	_
probabilities	_	_
are	_	_
changed	_	_
in	_	_
comparison	_	_
to	_	_
the	_	_
HMM	_	_
above	_	_
.	_	_

#84
Therefore	_	_
,	_	_
we	_	_
can	_	_
apply	_	_
the	_	_
Viterbi	_	_
algorithm	_	_
by	_	_
replacing	_	_
p	_	_
(	_	_
at|at−1	_	_
)	_	_
with	_	_
∏	_	_
m	_	_
p	_	_
(	_	_
am	_	_
,	_	_
t|at−1	_	_
)	_	_
and	_	_
p	_	_
(	_	_
a1	_	_
)	_	_
with	_	_
∏	_	_
m	_	_
p	_	_
(	_	_
am,1	_	_
)	_	_
.	_	_

#85
(	_	_
a	_	_
)	_	_
hmm	_	_
(	_	_
b	_	_
)	_	_
crf	_	_
(	_	_
c	_	_
)	_	_
rnn	_	_
(	_	_
d	_	_
)	_	_
fhmm	_	_
(	_	_
e	_	_
)	_	_
fcrf	_	_
(	_	_
f	_	_
)	_	_
mrnn	_	_
Fig.	_	_
1	_	_
:	_	_
Sequence	_	_
models	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_
.	_	_

#86
hmm	_	_
:	_	_
hidden	_	_
Markov	_	_
model	_	_
;	_	_
crf	_	_
:	_	_
conditional	_	_
random	_	_
field	_	_
;	_	_
rnn	_	_
:	_	_
recurrent	_	_
neural	_	_
networks	_	_
;	_	_
fhmm	_	_
:	_	_
factorial	_	_
hidden	_	_
markov	_	_
model	_	_
(	_	_
with	_	_
cross	_	_
dependencies	_	_
)	_	_
;	_	_
fcrf	_	_
:	_	_
factorial	_	_
conditional	_	_
random	_	_
field	_	_
;	_	_
mrnn	_	_
:	_	_
multi-labels	_	_
recurrent	_	_
neural	_	_
networks	_	_
.	_	_

#87
The	_	_
top	_	_
row	_	_
depicts	_	_
the	_	_
models	_	_
for	_	_
combined	_	_
labels	_	_
and	_	_
the	_	_
bottom	_	_
row	_	_
depicts	_	_
the	_	_
models	_	_
for	_	_
separate	_	_
labels	_	_
.	_	_

#88
On	_	_
Multi-resident	_	_
Activity	_	_
Recognition	_	_
in	_	_
Ambient	_	_
Smart-Homes	_	_
5	_	_

#89
3.2	_	_
CRF-based	_	_
Approaches	_	_

#90
Conditional	_	_
random	_	_
field	_	_
is	_	_
a	_	_
probabilistic	_	_
graphical	_	_
model	_	_
which	_	_
can	_	_
be	_	_
used	_	_
for	_	_
modelling	_	_
sequences	_	_
,	_	_
similar	_	_
as	_	_
HMMs	_	_
.	_	_

#91
The	_	_
difference	_	_
here	_	_
is	_	_
that	_	_
a	_	_
CRF	_	_
is	_	_
a	_	_
discriminative	_	_
model	_	_
representing	_	_
a	_	_
conditional	_	_
distribution	_	_
:	_	_
p	_	_
(	_	_
a1	_	_
:	_	_
T	_	_
|o1	_	_
:	_	_
T	_	_
)	_	_
=	_	_
Z	_	_
(	_	_
o1	_	_
:	_	_
T	_	_
)	_	_
∏	_	_
t	_	_
Ψ	_	_
(	_	_
at	_	_
,	_	_
at−1	_	_
,	_	_
ot	_	_
)	_	_
(	_	_
3	_	_
)	_	_
where	_	_
Ψ	_	_
(	_	_
at	_	_
,	_	_
at−1	_	_
,	_	_
ot	_	_
)	_	_
=	_	_
exp	_	_
(	_	_
∑	_	_
k	_	_
θkfk	_	_
(	_	_
ot	_	_
,	_	_
at	_	_
,	_	_
at−1	_	_
)	_	_
)	_	_
with	_	_
fk	_	_
are	_	_
the	_	_
feature	_	_
functions	_	_
,	_	_
θk	_	_
are	_	_
parameters	_	_
of	_	_
the	_	_
features	_	_
,	_	_
and	_	_
Z	_	_
(	_	_
o1	_	_
:	_	_
T	_	_
)	_	_
is	_	_
the	_	_
partition	_	_
function	_	_
:	_	_
Z	_	_
(	_	_
o1	_	_
:	_	_
T	_	_
)	_	_
=	_	_
∑	_	_
a′1	_	_
:	_	_
T	_	_
∏	_	_
t	_	_
Ψ	_	_
(	_	_
ot	_	_
,	_	_
a′	_	_
t	_	_
,	_	_
a′	_	_
t−1	_	_
)	_	_
(	_	_
4	_	_
)	_	_
This	_	_
is	_	_
a	_	_
CRF	_	_
for	_	_
combined	_	_
labels	_	_
,	_	_
for	_	_
the	_	_
case	_	_
of	_	_
separate	_	_
labels	_	_
,	_	_
we	_	_
split	_	_
the	_	_
hidden	_	_
unit	_	_
to	_	_
have	_	_
a	_	_
variant	_	_
as	_	_
known	_	_
as	_	_
factorial	_	_
CRF	_	_
[	_	_
22	_	_
]	_	_
.	_	_

#92
In	_	_
Figure	_	_
1e	_	_
we	_	_
illustrate	_	_
the	_	_
graphical	_	_
presentation	_	_
of	_	_
this	_	_
model	_	_
.	_	_

#93
3.3	_	_
RNN-based	_	_
Approaches	_	_

#94
A	_	_
recurrent	_	_
neural	_	_
network	_	_
is	_	_
constructed	_	_
by	_	_
rolling	_	_
a	_	_
feed-forward	_	_
neural	_	_
network	_	_
over	_	_
time	_	_
where	_	_
the	_	_
hidden	_	_
layer	_	_
is	_	_
connected	_	_
to	_	_
itself	_	_
by	_	_
a	_	_
recurrent	_	_
weights	_	_
.	_	_

#95
As	_	_
shown	_	_
in	_	_
Figure	_	_
1c	_	_
,	_	_
we	_	_
can	_	_
use	_	_
the	_	_
output	_	_
layer	_	_
to	_	_
represent	_	_
the	_	_
combine	_	_
activities	_	_
of	_	_
multiple	_	_
residents	_	_
.	_	_

#96
For	_	_
example	_	_
,	_	_
at	_	_
time	_	_
t	_	_
the	_	_
probability	_	_
of	_	_
a	_	_
joint	_	_
activity	_	_
at	_	_
is	_	_
:	_	_
p	_	_
(	_	_
at|o1	_	_
:	_	_
t	_	_
)	_	_
=	_	_
softmax	_	_
(	_	_
htU	_	_
+	_	_
b	_	_
)	_	_
(	_	_
5	_	_
)	_	_
where	_	_
U	_	_
is	_	_
the	_	_
weight	_	_
matrix	_	_
connecting	_	_
the	_	_
hidden	_	_
layer	_	_
and	_	_
the	_	_
output	_	_
layer	_	_
;	_	_
b	_	_
is	_	_
the	_	_
biases	_	_
of	_	_
the	_	_
output	_	_
units	_	_
.	_	_

#97
We	_	_
will	_	_
show	_	_
how	_	_
hidden	_	_
state	_	_
ht	_	_
is	_	_
computed	_	_
later	_	_
in	_	_
this	_	_
section	_	_
.	_	_

#98
For	_	_
the	_	_
other	_	_
case	_	_
where	_	_
activities	_	_
of	_	_
residents	_	_
are	_	_
modelled	_	_
separately	_	_
we	_	_
can	_	_
split	_	_
the	_	_
output	_	_
layer	_	_
into	_	_
multiple	_	_
layers	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Figure	_	_
1f	_	_
.	_	_

#99
Let	_	_
us	_	_
suppose	_	_
that	_	_
there	_	_
are	_	_
M	_	_
residents	_	_
,	_	_
the	_	_
probability	_	_
of	_	_
a	_	_
resident	_	_
m	_	_
performs	_	_
an	_	_
activity	_	_
am	_	_
,	_	_
t	_	_
at	_	_
time	_	_
t	_	_
is	_	_
:	_	_
p	_	_
(	_	_
am	_	_
,	_	_
t	_	_
)	_	_
=	_	_
softmax	_	_
(	_	_
htUm	_	_
+bm	_	_
)	_	_
.	_	_

#100
Here	_	_
each	_	_
output	_	_
layer	_	_
is	_	_
connected	_	_
with	_	_
a	_	_
shared	_	_
hidden	_	_
layer	_	_
by	_	_
a	_	_
weight	_	_
matrix	_	_
Um	_	_
.	_	_

#101
The	_	_
hidden	_	_
state	_	_
in	_	_
both	_	_
cases	_	_
(	_	_
combined	_	_
labels	_	_
and	_	_
separate	_	_
labels	_	_
)	_	_
is	_	_
computed	_	_
as	_	_
ht	_	_
=	_	_
tanh	_	_
(	_	_
otW	_	_
+	_	_
ht−1V	_	_
+	_	_
c	_	_
)	_	_
.	_	_

#102
This	_	_
is	_	_
the	_	_
simplest	_	_
form	_	_
of	_	_
hidden	_	_
unit	_	_
which	_	_
is	_	_
said	_	_
not	_	_
very	_	_
useful	_	_
to	_	_
capture	_	_
long-term	_	_
information	_	_
and	_	_
suffer	_	_
the	_	_
problem	_	_
of	_	_
vanishing/exploding	_	_
gradient	_	_
[	_	_
12	_	_
]	_	_
.	_	_

#103
This	_	_
is	_	_
also	_	_
shown	_	_
than	_	_
such	_	_
problems	_	_
can	_	_
be	_	_
ameliorated	_	_
by	_	_
using	_	_
complex	_	_
gates	_	_
in	_	_
the	_	_
hidden	_	_
units	_	_
.	_	_

#104
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
will	_	_
empirically	_	_
investigate	_	_
three	_	_
type	_	_
of	_	_
hidden	_	_
units	_	_
for	_	_
RNNs	_	_
,	_	_
including	_	_
the	_	_
original	_	_
“tanh”	_	_
and	_	_
two	_	_
others	_	_
with	_	_
more	_	_
complex	_	_
gates	_	_
as	_	_
known	_	_
as	_	_
long-short	_	_
term	_	_
memory	_	_
(	_	_
LSTM	_	_
)	_	_
[	_	_
12	_	_
]	_	_
and	_	_
gated	_	_
recurrent	_	_
units	_	_
(	_	_
GRU	_	_
)	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#105
6	_	_
Tran	_	_
et	_	_
al.	_	_
4	_	_
Experiments	_	_

#106
In	_	_
the	_	_
experiments	_	_
we	_	_
evaluate	_	_
the	_	_
effectiveness	_	_
of	_	_
all	_	_
the	_	_
models	_	_
above	_	_
on	_	_
three	_	_
smart	_	_
homes	_	_
data	_	_
in	_	_
two	_	_
benchmark	_	_
datasets	_	_
.	_	_

#107
The	_	_
CASAS	_	_
data1	_	_
was	_	_
collected	_	_
in	_	_
the	_	_
WSU	_	_
smart	_	_
department	_	_
Testbed	_	_
with	_	_
two	_	_
residents	_	_
where	_	_
each	_	_
resident	_	_
performing	_	_
15	_	_
unique	_	_
activities	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#108
The	_	_
data	_	_
is	_	_
collected	_	_
in	_	_
26	_	_
days	_	_
in	_	_
a	_	_
smart	_	_
home	_	_
equipped	_	_
with	_	_
37	_	_
ambient	_	_
sensors	_	_
.	_	_

#109
The	_	_
ARAS	_	_
data2	_	_
[	_	_
1	_	_
]	_	_
is	_	_
collected	_	_
in	_	_
two	_	_
different	_	_
houses	_	_
,	_	_
denoted	_	_
as	_	_
House	_	_
A	_	_
and	_	_
House	_	_
B	_	_
,	_	_
in	_	_
30	_	_
days	_	_
.	_	_

#110
In	_	_
these	_	_
environments	_	_
,	_	_
there	_	_
are	_	_
20	_	_
sensors	_	_
for	_	_
two	_	_
residents	_	_
in	_	_
each	_	_
house	_	_
where	_	_
each	_	_
resident	_	_
is	_	_
asked	_	_
to	_	_
perform	_	_
27	_	_
different	_	_
activities	_	_
.	_	_

#111
4.1	_	_
Evaluation	_	_

#112
We	_	_
denote	_	_
â1	_	_
:	_	_
T	_	_
with	_	_
âi	_	_
=	_	_
{	_	_
â1	_	_
,	_	_
t	_	_
,	_	_
â2	_	_
,	_	_
t	_	_
,	_	_
...	_	_
,	_	_
âM	_	_
,	_	_
t	_	_
}	_	_
as	_	_
the	_	_
predicted	_	_
activities	_	_
of	_	_
all	_	_
residents	_	_
in	_	_
the	_	_
house	_	_
for	_	_
each	_	_
instance	_	_
in	_	_
the	_	_
test	_	_
set	_	_
Dtest	_	_
.	_	_

#113
We	_	_
also	_	_
denote	_	_
a	_	_
ground	_	_
truth	_	_
is	_	_
a1	_	_
:	_	_
T	_	_
.	_	_

#114
The	_	_
performance	_	_
of	_	_
a	_	_
model	_	_
is	_	_
measured	_	_
by	_	_
the	_	_
accuracy	_	_
of	_	_
each	_	_
resident’s	_	_
activities	_	_
and	_	_
the	_	_
accuracy	_	_
of	_	_
all	_	_
residents’	_	_
activities	_	_
.	_	_

#115
The	_	_
former	_	_
is	_	_
computed	_	_
as	_	_
:	_	_
accuracym	_	_
=	_	_
|Dtest|	_	_
∑	_	_
am,1	_	_
:	_	_
T∈Dtest	_	_
T	_	_
∑	_	_
t	_	_
(	_	_
am	_	_
,	_	_
t	_	_
==	_	_
âm	_	_
,	_	_
t	_	_
)	_	_
(	_	_
6	_	_
)	_	_
where	_	_
am	_	_
,	_	_
t	_	_
and	_	_
âm	_	_
,	_	_
t	_	_
are	_	_
ground	_	_
truth	_	_
and	_	_
predicted	_	_
activity	_	_
of	_	_
resident	_	_
m	_	_
at	_	_
time	_	_
t	_	_
in	_	_
each	_	_
instance	_	_
of	_	_
the	_	_
test	_	_
set	_	_
Dtest	_	_
.	_	_

#116
Similarly	_	_
,	_	_
the	_	_
accuracy	_	_
for	_	_
activities	_	_
of	_	_
all	_	_
residents	_	_
is	_	_
:	_	_
accuracyall	_	_
=	_	_
|Dtest|	_	_
∑	_	_
a1	_	_
:	_	_
T∈Dtest	_	_
T	_	_
∑	_	_
t	_	_
(	_	_
at	_	_
==	_	_
ât	_	_
)	_	_
(	_	_
7	_	_
)	_	_

#117
4.2	_	_
Results	_	_

#118
We	_	_
partition	_	_
the	_	_
CASAS	_	_
data	_	_
into	_	_
24	_	_
days	_	_
for	_	_
training	_	_
,	_	_
1	_	_
day	_	_
for	_	_
validation	_	_
and	_	_
1	_	_
day	_	_
for	_	_
testing	_	_
.	_	_

#119
The	_	_
ARASA	_	_
and	_	_
ARASB	_	_
are	_	_
the	_	_
data	_	_
from	_	_
ARAS	_	_
House	_	_
A	_	_
and	_	_
ARAS	_	_
House	_	_
B	_	_
each	_	_
consist	_	_
of	_	_
7	_	_
days	_	_
for	_	_
training	_	_
,	_	_
2	_	_
days	_	_
for	_	_
validation	_	_
and	_	_
2	_	_
days	_	_
for	_	_
testing	_	_
.	_	_

#120
The	_	_
models	_	_
are	_	_
selected	_	_
as	_	_
follows	_	_
.	_	_

#121
For	_	_
HMM	_	_
and	_	_
fHMM	_	_
we	_	_
selected	_	_
the	_	_
best	_	_
models	_	_
based	_	_
on	_	_
the	_	_
Laplacian	_	_
smoothing	_	_
factor	_	_
.	_	_

#122
The	_	_
smoothing	_	_
factor	_	_
is	_	_
chosen	_	_
from	_	_
10−6	_	_
to	_	_
10−2	_	_
in	_	_
log-space	_	_
.	_	_

#123
For	_	_
the	_	_
CRFs	_	_
,	_	_
we	_	_
do	_	_
not	_	_
use	_	_
any	_	_
hyper-parameters	_	_
and	_	_
set	_	_
the	_	_
maximum	_	_
iteration	_	_
is	_	_
1000	_	_
.	_	_

#124
We	_	_
use	_	_
MALLET	_	_
to	_	_
implement	_	_
fCRFs	_	_
and	_	_
set	_	_
the	_	_
penalty	_	_
hyper-parameters	_	_
to	_	_
zeros	_	_
.	_	_

#125
For	_	_
the	_	_
recurrent	_	_
neural	_	_
networks	_	_
,	_	_
we	_	_
perform	_	_
model	_	_
selection	_	_
by	_	_
using	_	_
grid-like	_	_
search	_	_
on	_	_
number	_	_
of	_	_
hidden	_	_
units	_	_
within	_	_
{	_	_
10	_	_
,	_	_
50	_	_
,	_	_
100	_	_
,	_	_
500	_	_
,	_	_
1000	_	_
}	_	_
,	_	_
and	_	_
learning	_	_
rate	_	_
from	_	_
0.0001	_	_
to	_	_
1	_	_
in	_	_
log-space	_	_
.	_	_

#126
If	_	_
the	_	_
optima	_	_
is	_	_
not	_	_
apparent	_	_
we	_	_
expand	_	_
the	_	_
search	_	_
.	_	_

#127
1	_	_
http	_	_
:	_	_
//ailab.eecs.wsu.edu/casas/	_	_
2	_	_
http	_	_
:	_	_
//www.cmpe.boun.edu.tr/aras/	_	_
On	_	_
Multi-resident	_	_
Activity	_	_
Recognition	_	_
in	_	_
Ambient	_	_
Smart-Homes	_	_
7	_	_
PPPPPPPModel	_	_
Data	_	_
CASAS	_	_
ARAS	_	_
House-A	_	_
ARAS	_	_
House-B	_	_
Average	_	_
R1	_	_
R2	_	_
All	_	_
R1	_	_
R2	_	_
All	_	_
R1	_	_
R2	_	_
All	_	_
RNNtanh	_	_
66.62	_	_
64.88	_	_
58.08	_	_
67.02	_	_
73.09	_	_
53.07	_	_
81.09	_	_
78.16	_	_
76.15	_	_
68.68	_	_
mRNNtanh	_	_
69.14	_	_
60.61	_	_
47.94	_	_
68.12	_	_
74.74	_	_
53.26	_	_
91.93	_	_
78.99	_	_
76.86	_	_
69.06	_	_
RNNgru	_	_
92.26	_	_
87.68	_	_
83.66	_	_
69.79	_	_
74.20	_	_
56.23	_	_
81.69	_	_
78.95	_	_
76.83	_	_
77.92	_	_
mRNNgru	_	_
90.89	_	_
83.97	_	_
77.91	_	_
70.03	_	_
75.37	_	_
56.65	_	_
82.04	_	_
78.90	_	_
76.83	_	_
76.95	_	_
RNNlstm	_	_
89.40	_	_
87.45	_	_
82.14	_	_
69.94	_	_
73.59	_	_
56.44	_	_
82.15	_	_
77.84	_	_
76.72	_	_
77.29	_	_
mRNNlstm	_	_
69.05	_	_
84.30	_	_
77.49	_	_
69.97	_	_
75.39	_	_
56.25	_	_
81.66	_	_
78.92	_	_
76.60	_	_
74.40	_	_
HMM	_	_
65.24	_	_
65.82	_	_
56.58	_	_
43.95	_	_
54.93	_	_
19.13	_	_
79.29	_	_
76.98	_	_
75.07	_	_
59.67	_	_
fHMM	_	_
73.55	_	_
67.44	_	_
55.43	_	_
44.19	_	_
54.58	_	_
19.13	_	_
79.17	_	_
77.10	_	_
75.07	_	_
60.63	_	_
CRF	_	_
76.40	_	_
66.07	_	_
64.32	_	_
70.73	_	_
78.17	_	_
61.72	_	_
88.36	_	_
89.27	_	_
76.23	_	_
74.58	_	_
fCRF	_	_
58.21	_	_
56.76	_	_
45.84	_	_
69.50	_	_
69.50	_	_
55.95	_	_
76.01	_	_
76.01	_	_
74.44	_	_
64.69	_	_
Table	_	_
1	_	_
:	_	_
Prediction	_	_
accuracy	_	_
for	_	_
all	_	_
models	_	_
on	_	_
three	_	_
datasets	_	_
.	_	_

#128
R1	_	_
,	_	_
R2	_	_
,	_	_
All	_	_
are	_	_
accuracy	_	_
of	_	_
predicted	_	_
activities	_	_
of	_	_
resident	_	_
1	_	_
,	_	_
resident	_	_
2	_	_
and	_	_
the	_	_
joint	_	_
activities	_	_
The	_	_
RNNs	_	_
are	_	_
trained	_	_
using	_	_
stochastic	_	_
gradient	_	_
descent	_	_
with	_	_
early	_	_
stopping	_	_
.	_	_

#129
Due	_	_
to	_	_
the	_	_
need	_	_
for	_	_
initialisation	_	_
of	_	_
the	_	_
RNN	_	_
models	_	_
we	_	_
repeat	_	_
each	_	_
experiment	_	_
on	_	_
recurrent	_	_
neural	_	_
networks	_	_
50	_	_
times	_	_
and	_	_
report	_	_
the	_	_
average	_	_
results	_	_
.	_	_

#130
We	_	_
denote	_	_
RNN	_	_
and	_	_
mRNN	_	_
are	_	_
recurrent	_	_
neural	_	_
networks	_	_
for	_	_
combined	_	_
labels	_	_
and	_	_
separate	_	_
labels	_	_
respectively	_	_
.	_	_

#131
We	_	_
also	_	_
use	_	_
subcripts	_	_
tanh	_	_
,	_	_
gru	_	_
,	_	_
lstm	_	_
to	_	_
denote	_	_
the	_	_
type	_	_
of	_	_
hidden	_	_
units	_	_
in	_	_
RNNs	_	_
.	_	_

#132
Table	_	_
1	_	_
shows	_	_
the	_	_
results	_	_
of	_	_
all	_	_
models	_	_
on	_	_
three	_	_
datasets	_	_
.	_	_

#133
In	_	_
CASAS	_	_
data	_	_
,	_	_
RNNgru	_	_
outperforms	_	_
other	_	_
models	_	_
.	_	_

#134
In	_	_
ARASA	_	_
,	_	_
HMM	_	_
based	_	_
models	_	_
have	_	_
very	_	_
low	_	_
performance	_	_
.	_	_

#135
It	_	_
seems	_	_
that	_	_
the	_	_
simplicity	_	_
of	_	_
HMM	_	_
can	_	_
not	_	_
capture	_	_
the	_	_
complexity	_	_
of	_	_
this	_	_
data	_	_
as	_	_
we	_	_
learn	_	_
that	_	_
the	_	_
number	_	_
of	_	_
observed	_	_
sensors’	_	_
values	_	_
in	_	_
ARASA	_	_
is	_	_
10	_	_
times	_	_
more	_	_
than	_	_
CASAS	_	_
and	_	_
3	_	_
times	_	_
more	_	_
than	_	_
ARASB	_	_
.	_	_

#136
In	_	_
this	_	_
dataset	_	_
,	_	_
CRF	_	_
has	_	_
the	_	_
highest	_	_
accuracy	_	_
.	_	_

#137
In	_	_
ARASB	_	_
most	_	_
of	_	_
the	_	_
models	_	_
have	_	_
similar	_	_
performance	_	_
with	_	_
fCRF	_	_
achieves	_	_
the	_	_
lowest	_	_
accuracy	_	_
of	_	_
74.44	_	_
%	_	_
and	_	_
mRNNtanh	_	_
achieves	_	_
the	_	_
highest	_	_
accuracy	_	_
of	_	_
76.86	_	_
%	_	_
.	_	_

#138
We	_	_
observe	_	_
that	_	_
RNNs	_	_
with	_	_
complex	_	_
gates	_	_
(	_	_
GRU	_	_
and	_	_
LSTM	_	_
)	_	_
seem	_	_
to	_	_
overfit	_	_
the	_	_
training	_	_
and	_	_
validation	_	_
sets	_	_
since	_	_
they	_	_
have	_	_
much	_	_
more	_	_
paprameters	_	_
than	_	_
RNN	_	_
with	_	_
tanh	_	_
units	_	_
.	_	_

#139
In	_	_
the	_	_
“Average”	_	_
column	_	_
are	_	_
the	_	_
mean	_	_
accuracy	_	_
of	_	_
individual	_	_
activities	_	_
and	_	_
the	_	_
joint	_	_
activities	_	_
in	_	_
all	_	_
datasets	_	_
.	_	_

#140
We	_	_
can	_	_
see	_	_
that	_	_
among	_	_
all	_	_
models	_	_
,	_	_
RNNgru	_	_
achieves	_	_
the	_	_
best	_	_
performance	_	_
overall	_	_
.	_	_

#141
This	_	_
is	_	_
because	_	_
,	_	_
the	_	_
small	_	_
size	_	_
of	_	_
the	_	_
experimental	_	_
data	_	_
makes	_	_
the	_	_
compactness	_	_
of	_	_
RNNgru	_	_
advantagous	_	_
over	_	_
RNNlstm	_	_
which	_	_
has	_	_
larger	_	_
mber	_	_
of	_	_
parameters	_	_
.	_	_

#142
4.3	_	_
Combined	_	_
labels	_	_
versus	_	_
Separate	_	_
labels	_	_

#143
We	_	_
now	_	_
analyse	_	_
the	_	_
effectiveness	_	_
of	_	_
combined	_	_
labels	_	_
v.s	_	_
.	_	_

#144
separate	_	_
labels	_	_
.	_	_

#145
We	_	_
compute	_	_
the	_	_
average	_	_
accuracy	_	_
of	_	_
all	_	_
models	_	_
that	_	_
use	_	_
combined	_	_
labels	_	_
approach	_	_
on	_	_
each	_	_
data	_	_
and	_	_
compare	_	_
it	_	_
with	_	_
the	_	_
separate	_	_
labels	_	_
approach	_	_
.	_	_

#146
The	_	_
results	_	_
are	_	_
demonstrated	_	_
in	_	_
Figure	_	_
2	_	_
.	_	_

#147
It	_	_
is	_	_
consistent	_	_
that	_	_
models	_	_
with	_	_
combined	_	_
labels	_	_
have	_	_
better	_	_
prediction	_	_
accuracy	_	_
than	_	_
models	_	_
with	_	_
separate	_	_
labels	_	_
.	_	_

#148
An	_	_
interesting	_	_
finding	_	_
from	_	_
the	_	_
results	_	_
here	_	_
is	_	_
that	_	_
the	_	_
combined	_	_
label	_	_
approaches	_	_
not	_	_
only	_	_
have	_	_
higher	_	_
accuracy	_	_
for	_	_
joint	_	_
activities	_	_
but	_	_
also	_	_
outperform	_	_
the	_	_
separate	_	_
label	_	_
approach	_	_
in	_	_
predicting	_	_
individual	_	_
activities	_	_
.	_	_

#149
8	_	_
Tran	_	_
et	_	_
al.	_	_
Fig.	_	_
2	_	_
:	_	_
Combined	_	_
labels	_	_
v.s	_	_
separate	_	_
labels	_	_
for	_	_
multi-resident	_	_
activity	_	_
recognition	_	_

#150
4.4	_	_
Efficiency	_	_

#151
Finally	_	_
,	_	_
we	_	_
analyse	_	_
the	_	_
efficiency	_	_
of	_	_
the	_	_
models	_	_
in	_	_
this	_	_
application	_	_
.	_	_

#152
Theoretically	_	_
,	_	_
the	_	_
combined	_	_
labels	_	_
may	_	_
need	_	_
more	_	_
parameters	_	_
than	_	_
the	_	_
separate	_	_
labels	_	_
as	_	_
the	_	_
former	_	_
require	_	_
variables	_	_
to	_	_
represent	_	_
K1×K2	_	_
...	_	_
KM	_	_
activities	_	_
while	_	_
the	_	_
latter	_	_
use	_	_
M	_	_
variables	_	_
each	_	_
represent	_	_
Km	_	_
activity	_	_
values	_	_
.	_	_

#153
However	_	_
,	_	_
this	_	_
might	speculation	_
be	_	_
different	_	_
in	_	_
practice	_	_
where	_	_
smaller	_	_
combined	_	_
label	_	_
model	_	_
may	_	_
have	_	_
better	_	_
results	_	_
than	_	_
bigger	_	_
separate-label	_	_
models	_	_
.	_	_

#154
It	_	_
would	_	_
depends	_	_
on	_	_
how	_	_
models	_	_
are	_	_
selected	_	_
by	_	_
validation	_	_
sets	_	_
.	_	_

#155
In	_	_
Table	_	_
2	_	_
we	_	_
report	_	_
the	_	_
running	_	_
time	_	_
of	_	_
the	_	_
models	_	_
which	_	_
give	_	_
the	_	_
best	_	_
results	_	_
in	_	_
section	_	_
4.2	_	_
.	_	_

#156
The	_	_
most	_	_
effective	_	_
model	_	_
is	_	_
HMM	_	_
which	_	_
only	_	_
needs	_	_
less	_	_
than	_	_
one	_	_
minute	_	_
to	_	_
complete	_	_
the	_	_
training	_	_
and	_	_
prediction	_	_
for	_	_
any	_	_
of	_	_
the	_	_
three	_	_
datasets	_	_
.	_	_

#157
fHMM	_	_
is	_	_
slightly	_	_
less	_	_
efficient	_	_
than	_	_
HMM	_	_
.	_	_

#158
However	_	_
,	_	_
it	_	_
should	_	_
be	_	_
noted	_	_
that	_	_
both	_	_
HMM	_	_
and	_	_
fHMM	_	_
are	_	_
the	_	_
least	_	_
effective	_	_
models	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#159
fCRF	_	_
is	_	_
the	_	_
slowest	_	_
model	_	_
among	_	_
the	_	_
others	_	_
.	_	_

#160
Although	_	_
the	_	_
Java	_	_
implementation	_	_
of	_	_
MALLET	_	_
might	speculation	_
be	_	_
the	_	_
reason	_	_
of	_	_
this	_	_
inefficiency	_	_
the	_	_
prediction	_	_
performance	_	_
of	_	_
fCRF	_	_
is	_	_
only	_	_
better	_	_
than	_	_
HMM	_	_
and	_	_
fHMM	_	_
.	_	_

#161
So	_	_
the	_	_
use	_	_
of	_	_
this	_	_
model	_	_
in	_	_
practice	_	_
should	_	_
be	_	_
questioned	_	_
,	_	_
especially	_	_
when	_	_
its	_	_
results	_	_
are	_	_
even	_	_
lower	_	_
than	_	_
CRF	_	_
.	_	_

#162
Overall	_	_
,	_	_
among	_	_
all	_	_
models	_	_
,	_	_
RNNgru	_	_
would	_	_
be	_	_
the	_	_
best	_	_
choice	_	_
since	_	_
it	_	_
has	_	_
the	_	_
best	_	_
performance	_	_
while	_	_
being	_	_
considerably	_	_
efficient	_	_
.	_	_

#163
It	_	_
is	_	_
not	_	_
as	_	_
fast	_	_
as	_	_
RNNtanh	_	_
but	_	_
it	_	_
is	_	_
quicker	_	_
than	_	_
the	_	_
other	_	_
RNN	_	_
based	_	_
models	_	_
in	_	_
most	_	_
cases	_	_
.	_	_

#164
Especially	_	_
,	_	_
it	_	_
outperform	_	_
HMM	_	_
based	_	_
models	_	_
and	_	_
fCRF	_	_
models	_	_
with	_	_
large	_	_
margin	_	_
.	_	_

#165
CRF	_	_
is	_	_
also	_	_
a	_	_
good	_	_
choice	_	_
since	_	_
its	_	_
best	_	_
models	_	_
are	_	_
even	_	_
faster	_	_
than	_	_
RNNgru	_	_
in	_	_
CASAS	_	_
and	_	_
ARASB	_	_
.	_	_

#166
However	_	_
,	_	_
in	_	_
these	_	_
two	_	_
datasets	_	_
it	_	_
has	_	_
lower	_	_
accuracy	_	_
than	_	_
RNNgru	_	_
while	_	_
in	_	_
ARASA	_	_
it	_	_
is	_	_
much	_	_
slower	_	_
than	_	_
RNNgru	_	_
.	_	_

#167
5	_	_
Conclusions	_	_

#168
We	_	_
have	_	_
presented	_	_
a	_	_
benchmark	_	_
study	_	_
on	_	_
activity	_	_
recognition	_	_
for	_	_
multi-resident	_	_
smart	_	_
homes	_	_
with	_	_
ambient	_	_
sensors	_	_
.	_	_

#169
We	_	_
empirically	_	_
show	_	_
that	_	_
recurrent	_	_
neural	_	_
network	_	_
with	_	_
gated	_	_
recurrent	_	_
units	_	_
is	_	_
better	_	_
than	_	_
other	_	_
models	_	_
and	_	_
also	_	_
considerably	_	_
efficient	_	_
.	_	_

#170
We	_	_
also	_	_
show	_	_
that	_	_
using	_	_
combined	_	_
activities	_	_
as	_	_
single	_	_
labels	_	_
is	_	_
more	_	_
effective	_	_
than	_	_
represent	_	_
them	_	_
as	_	_
separate	_	_
labels	_	_
.	_	_

#171
On	_	_
Multi-resident	_	_
Activity	_	_
Recognition	_	_
in	_	_
Ambient	_	_
Smart-Homes	_	_
9	_	_
PPPPPPPModel	_	_
Data	_	_
CASAS	_	_
ARAS	_	_
House-A	_	_
ARAS	_	_
House-B	_	_
RNN	_	_
96.23	_	_
sec	_	_
3064.50	_	_
sec	_	_
3547.74	_	_
sec	_	_
mRNN	_	_
406.31	_	_
sec	_	_
3.39	_	_
hrs	_	_
3.44	_	_
hrs	_	_
GRU	_	_
203.58	_	_
sec	_	_
1.31	_	_
hrs	_	_
1.43	_	_
hrs	_	_
mGRU	_	_
545.39	_	_
sec	_	_
1.47	_	_
hrs	_	_
1.38	_	_
hrs	_	_
LSTM	_	_
183.64	_	_
sec	_	_
7.87	_	_
hrs	_	_
8.61	_	_
hrs	_	_
mLSTM	_	_
638.35	_	_
sec	_	_
6.54	_	_
hrs	_	_
1.71	_	_
hrs	_	_
HMM	_	_
0.17	_	_
sec	_	_
95.23	_	_
sec	_	_
50.74	_	_
sec	_	_
fHMM	_	_
0.23	_	_
sec	_	_
97.52	_	_
sec	_	_
50.77	_	_
sec	_	_
CRF	_	_
64.32	_	_
sec	_	_
∼8	_	_
hrs	_	_
289.17	_	_
sec	_	_
fCRF	_	_
3.6	_	_
hrs	_	_
∼419	_	_
hrs	_	_
∼220	_	_
hrs	_	_
Table	_	_
2	_	_
:	_	_
(	_	_
Average	_	_
)	_	_
Computational	_	_
time	_	_
for	_	_
best	_	_
models	_	_
in	_	_
section	_	_
4.2	_	_
to	_	_
train	_	_
and	_	_
predict	_	_
activities	_	_
in	_	_
three	_	_
datasets	_	_
.	_	_

#172
We	_	_
denote	_	_
the	_	_
time	_	_
in	_	_
hours	_	_
(	_	_
hrs	_	_
)	_	_
if	_	_
it	_	_
is	_	_
more	_	_
than	_	_
3600	_	_
seconds	_	_
otherwise	_	_
we	_	_
denote	_	_
it	_	_
in	_	_
seconds	_	_
(	_	_
sec	_	_
)	_	_
.	_	_