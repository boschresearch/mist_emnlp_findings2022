#0
Improving	_	_
Electron	_	_
Micrograph	_	_
Signal-to-Noise	_	_
with	_	_
an	_	_
Atrous	_	_
Convolutional	_	_
Encoder-Decoder	_	_
Jeffrey	_	_
M.	_	_
Ede	_	_
j.m.ede	_	_
@	_	_
warwick.ac.uk	_	_
Abstract	_	_
:	_	_
We	_	_
present	_	_
an	_	_
atrous	_	_
convolutional	_	_
encoder-decoder	_	_
trained	_	_
to	_	_
denoise	_	_
512×512	_	_
crops	_	_
from	_	_
electron	_	_
micrographs	_	_
.	_	_

#1
It	_	_
consists	_	_
of	_	_
a	_	_
modified	_	_
Xception	_	_
backbone	_	_
,	_	_
atrous	_	_
convoltional	_	_
spatial	_	_
pyramid	_	_
pooling	_	_
module	_	_
and	_	_
a	_	_
multi-stage	_	_
decoder	_	_
.	_	_

#2
Our	_	_
neural	_	_
network	_	_
was	_	_
trained	_	_
end-to-end	_	_
to	_	_
remove	_	_
Poisson	_	_
noise	_	_
applied	_	_
to	_	_
low-dose	_	_
(	_	_
300	_	_
counts	_	_
ppx	_	_
)	_	_
micrographs	_	_
created	_	_
from	_	_
a	_	_
new	_	_
dataset	_	_
of	_	_
17267	_	_
2048×2048	_	_
high-dose	_	_
(	_	_
>	_	_
2500	_	_
counts	_	_
ppx	_	_
)	_	_
micrographs	_	_
and	_	_
then	_	_
fine-tuned	_	_
for	_	_
ordinary	_	_
doses	_	_
(	_	_
200-2500	_	_
counts	_	_
ppx	_	_
)	_	_
.	_	_

#3
Its	_	_
performance	_	_
is	_	_
benchmarked	_	_
against	_	_
bilateral	_	_
,	_	_
non-local	_	_
means	_	_
,	_	_
total	_	_
variation	_	_
,	_	_
wavelet	_	_
,	_	_
Wiener	_	_
and	_	_
other	_	_
restoration	_	_
methods	_	_
with	_	_
their	_	_
default	_	_
parameters	_	_
.	_	_

#4
Our	_	_
network	_	_
outperforms	_	_
their	_	_
best	_	_
mean	_	_
squared	_	_
error	_	_
and	_	_
structural	_	_
similarity	_	_
index	_	_
performances	_	_
by	_	_
24.6	_	_
%	_	_
and	_	_
9.6	_	_
%	_	_
for	_	_
low	_	_
doses	_	_
and	_	_
by	_	_
43.7	_	_
%	_	_
and	_	_
5.5	_	_
%	_	_
for	_	_
ordinary	_	_
doses	_	_
.	_	_

#5
In	_	_
both	_	_
cases	_	_
,	_	_
our	_	_
network’s	_	_
mean	_	_
squared	_	_
error	_	_
has	_	_
the	_	_
lowest	_	_
variance	_	_
.	_	_

#6
Source	_	_
code	_	_
and	_	_
links	_	_
to	_	_
our	_	_
new	_	_
high-quality	_	_
dataset	_	_
and	_	_
trained	_	_
network	_	_
have	_	_
been	_	_
made	_	_
publicly	_	_
available	_	_
at	_	_
https	_	_
:	_	_
//github.com/Jeffrey-Ede/	_	_
Electron-Micrograph-Denoiser	_	_
.	_	_

#7
Keywords	_	_
:	_	_
deep	_	_
learning	_	_
,	_	_
denoising	_	_
,	_	_
electron	_	_
microscopy	_	_
,	_	_
low	_	_
dose	_	_

#8
1	_	_
Introduction	_	_

#9
Every	_	_
imaging	_	_
mode	_	_
in	_	_
electron	_	_
microscopy	_	_
is	_	_
limited	_	_
by	_	_
and	_	_
has	_	_
been	_	_
shaped	_	_
by	_	_
noise	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#10
Increasingly	_	_
,	_	_
ever	_	_
more	_	_
sophisticated	_	_
and	_	_
expensive	_	_
hardware	_	_
and	_	_
software	_	_
based	_	_
methods	_	_
are	_	_
being	_	_
developed	_	_
to	_	_
increase	_	_
resolution	_	_
,	_	_
including	_	_
aberration	_	_
correctors	_	_
[	_	_
2	_	_
,	_	_
3	_	_
]	_	_
,	_	_
advanced	_	_
cold	_	_
field	_	_
emission	_	_
guns	_	_
[	_	_
4	_	_
,	_	_
5	_	_
]	_	_
,	_	_
holography	_	_
[	_	_
6	_	_
,	_	_
7	_	_
]	_	_
and	_	_
others	_	_
[	_	_
8–10	_	_
]	_	_
.	_	_

#11
However	_	_
,	_	_
these	_	_
developments	_	_
are	_	_
all	_	_
fundamentally	_	_
limited	_	_
by	_	_
the	_	_
signal-to-noise	_	_
ratios	_	_
in	_	_
the	_	_
micrographs	_	_
they	_	_
are	_	_
being	_	_
applied	_	_
to	_	_
.	_	_

#12
Low-dose	_	_
applications	_	_
such	_	_
as	_	_
single-particle	_	_
cryogenic	_	_
microscopy	_	_
[	_	_
11	_	_
]	_	_
and	_	_
real-time	_	_
tomography	_	_
[	_	_
9	_	_
]	_	_
are	_	_
also	_	_
complicated	_	_
,	_	_
made	_	_
preventively	_	_
difficult	_	_
or	_	_
limited	_	_
by	_	_
noise	_	_
.	_	_

#13
Moving	_	_
towards	_	_
higher	_	_
resolution	_	_
,	_	_
a	_	_
large	_	_
number	_	_
of	_	_
general	_	_
[	_	_
12	_	_
]	_	_
and	_	_
electron	_	_
microscopy-specific	_	_
[	_	_
1	_	_
,	_	_
13	_	_
]	_	_
denoising	_	_
algorithms	_	_
have	_	_
been	_	_
developed	_	_
.	_	_

#14
However	_	_
,	_	_
most	_	_
of	_	_
these	_	_
algorithms	_	_
rely	_	_
on	_	_
laboriously	_	_
hand-crafted	_	_
filters	_	_
and	_	_
are	_	_
rarely	_	_
;	_	_
if	_	_
ever	_	_
,	_	_
truly	_	_
optimized	_	_
for	_	_
their	_	_
target	_	_
domains	_	_
e.g.	_	_
[	_	_
14	_	_
]	_	_
.	_	_

#15
Neural	_	_
networks	_	_
are	_	_
universal	_	_
approximators	_	_
[	_	_
15	_	_
]	_	_
that	_	_
overcome	_	_
these	_	_
difficulties	_	_
[	_	_
16	_	_
]	_	_
through	_	_
representation	_	_
learning	_	_
[	_	_
17	_	_
]	_	_
.	_	_

#16
As	_	_
a	_	_
result	_	_
,	_	_
networks	_	_
are	_	_
increasingly	_	_
being	_	_
applied	_	_
to	_	_
noise	_	_
removal	_	_
[	_	_
18–21	_	_
]	_	_
and	_	_
other	_	_
applications	_	_
in	_	_
electron	_	_
microscopy	_	_
[	_	_
22–25	_	_
]	_	_
and	_	_
other	_	_
areas	_	_
of	_	_
science	_	_
.	_	_

#17
The	_	_
recent	_	_
success	_	_
of	_	_
large	_	_
neural	_	_
networks	_	_
in	_	_
computer	_	_
vision	_	_
is	_	_
attributed	_	_
to	_	_
the	_	_
advent	_	_
of	_	_
graphical	_	_
processing	_	_
unit	_	_
(	_	_
GPU	_	_
)	_	_
acceleration	_	_
[	_	_
26	_	_
,	_	_
27	_	_
]	_	_
.	_	_

#18
In	_	_
particular	_	_
,	_	_
the	_	_
GPU	_	_
acceleration	_	_
of	_	_
large	_	_
convolutional	_	_
neural	_	_
networks	_	_
[	_	_
28	_	_
,	_	_
29	_	_
]	_	_
(	_	_
CNNs	_	_
)	_	_
in	_	_
distributed	_	_
settings	_	_
[	_	_
30	_	_
,	_	_
31	_	_
]	_	_
.	_	_

#19
Large	_	_
networks	_	_
that	_	_
surpass	_	_
human	_	_
performance	_	_
in	_	_
image	_	_
classification	_	_
[	_	_
32	_	_
,	_	_
33	_	_
]	_	_
,	_	_
computer	_	_
games	_	_
[	_	_
34–36	_	_
]	_	_
,	_	_
speech	_	_
recognition	_	_
[	_	_
37	_	_
,	_	_
38	_	_
]	_	_
,	_	_
relational	_	_
reasoning	_	_
[	_	_
39	_	_
]	_	_
and	_	_
in	_	_
many	_	_
other	_	_
applications	_	_
[	_	_
23	_	_
,	_	_
40–43	_	_
]	_	_
have	_	_
all	_	_
been	_	_
enabled	_	_
by	_	_
GPUs	_	_
.	_	_

#20
At	_	_
the	_	_
time	_	_
of	_	_
writing	_	_
,	_	_
there	_	_
are	_	_
no	_	_
large	_	_
neural	_	_
networks	_	_
for	_	_
electron	_	_
micrograph	_	_
denoising	_	_
.	_	_

#21
Instead	_	_
,	_	_
most	_	_
denoising	_	_
networks	_	_
act	_	_
on	_	_
many	_	_
small	_	_
overlapping	_	_
crops	_	_
e.g.	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#22
This	_	_
makes	_	_
them	_	_
computationally	_	_
inefficient	_	_
and	_	_
unable	_	_
to	_	_
utilize	_	_
all	_	_
the	_	_
information	_	_
available	_	_
.	_	_

#23
Some	_	_
large	_	_
denoising	_	_
networks	_	_
have	_	_
been	_	_
trained	_	_
as	_	_
part	_	_
of	_	_
generative	_	_
adversarial	_	_
networks	_	_
[	_	_
44	_	_
]	_	_
and	_	_
try	_	_
to	_	_
generate	_	_
micrographs	_	_
resembling	_	_
high-quality	_	_
training	_	_
data	_	_
as	_	_
closely	_	_
as	_	_
possible	_	_
.	_	_

#24
This	_	_
can	_	_
avoid	_	_
the	_	_
blurring	_	_
effect	_	_
of	_	_
most	_	_
filters	_	_
;	_	_
however	_	_
,	_	_
this	_	_
is	_	_
achieved	_	_
by	_	_
generating	_	_
features	_	_
that	_	_
might	options	_
be	_	_
in	_	_
high-quality	_	_
micrographs	_	_
.	_	_

#25
This	_	_
means	_	_
that	_	_
they	_	_
are	_	_
prone	_	_
to	_	_
producing	_	_
artifacts	_	_
;	_	_
something	_	_
that	_	_
is	_	_
often	_	_
undesirable	_	_
in	_	_
scientific	_	_
applications	_	_
.	_	_

#26
This	_	_
paper	_	_
presents	_	_
a	_	_
large	_	_
CNN	_	_
for	_	_
electron	_	_
micrograph	_	_
denoising	_	_
.	_	_

#27
Network	_	_
architecture	_	_
is	_	_
shown	_	_
in	_	_
section	_	_
2	_	_
.	_	_

#28
The	_	_
collation	_	_
of	_	_
a	_	_
new	_	_
high-quality	_	_
electron	_	_
micrograph	_	_
training	_	_
dataset	_	_
,	_	_
training	_	_
hyperparameters	_	_
and	_	_
learning	_	_
protocols	_	_
are	_	_
described	_	_
in	_	_
section	_	_
3	_	_
.	_	_

#29
Performance	_	_
comparison	_	_
with	_	_
other	_	_
methods’	_	_
,	_	_
error	_	_
characterization	_	_
and	_	_
example	_	_
usage	_	_
are	_	_
in	_	_
section	_	_
4	_	_
.	_	_

#30
Finally	_	_
,	_	_
architecture	_	_
and	_	_
hyperparameter	_	_
tuning	_	_
experiments	_	_
are	_	_
presented	_	_
in	_	_
section	_	_
5.	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
7	_	_
.	_	_

#31
4v	_	_
2	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
2	_	_
N	_	_
ov	_	_
2	_	_
Figure	_	_
1	_	_
:	_	_
Architecture	_	_
of	_	_
our	_	_
deep	_	_
convolutional	_	_
encoder-decoder	_	_
for	_	_
electron	_	_
micrograph	_	_
denoising	_	_
.	_	_

#32
The	_	_
entry	_	_
and	_	_
middle	_	_
flows	_	_
develop	_	_
high-level	_	_
features	_	_
that	_	_
are	_	_
sampled	_	_
at	_	_
multiple	_	_
scales	_	_
by	_	_
the	_	_
atrous	_	_
spatial	_	_
pyramid	_	_
pooling	_	_
module	_	_
.	_	_

#33
This	_	_
produces	_	_
rich	_	_
semantic	_	_
information	_	_
that	_	_
is	_	_
concatenated	_	_
with	_	_
low-level	_	_
entry	_	_
flow	_	_
features	_	_
and	_	_
resolved	_	_
into	_	_
denoised	_	_
micrographs	_	_
by	_	_
the	_	_
decoder	_	_
.	_	_

#34
Figure	_	_
2	_	_
:	_	_
Start	_	_
of	_	_
an	_	_
unmodified	_	_
Xception	_	_
entry	_	_
flow	_	_
[	_	_
45	_	_
]	_	_
.	_	_

#35
2	_	_
Architecture	_	_

#36
Our	_	_
highest	_	_
performing	_	_
denoising	_	_
network	_	_
was	_	_
trained	_	_
for	_	_
512×512	_	_
inputs	_	_
and	_	_
is	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#37
1	_	_
.	_	_

#38
It	_	_
consists	_	_
of	_	_
modified	_	_
Xception	_	_
[	_	_
45	_	_
]	_	_
entry	_	_
and	_	_
middle	_	_
flows	_	_
for	_	_
feature	_	_
extraction	_	_
,	_	_
an	_	_
atrous	_	_
spatial	_	_
pyramid	_	_
pooling	_	_
(	_	_
ASPP	_	_
)	_	_
module	_	_
[	_	_
46	_	_
,	_	_
47	_	_
]	_	_
that	_	_
samples	_	_
rich	_	_
high-level	_	_
semantics	_	_
at	_	_
multiple	_	_
scales	_	_
and	_	_
a	_	_
multi-stage	_	_
decoder	_	_
that	_	_
combines	_	_
low-level	_	_
entry	_	_
flow	_	_
features	_	_
with	_	_
ASPP	_	_
semantics	_	_
to	_	_
resolve	_	_
them	_	_
.	_	_

#39
The	_	_
architecture	_	_
is	_	_
inspired	_	_
by	_	_
Google’s	_	_
DeepLab3	_	_
[	_	_
46	_	_
]	_	_
,	_	_
DeepLab3+	_	_
[	_	_
47	_	_
]	_	_
and	_	_
other	_	_
encoder-decoder	_	_
architectures	_	_
[	_	_
18	_	_
,	_	_
20	_	_
,	_	_
48	_	_
]	_	_
.	_	_

#40
A	_	_
guide	_	_
for	_	_
readers	_	_
unfamiliar	_	_
with	_	_
convolution	_	_
arithmetic	_	_
for	_	_
deep	_	_
learning	_	_
is	_	_
[	_	_
49	_	_
]	_	_
.	_	_

#41
Convolutions	_	_
all	_	_
use	_	_
3×3	_	_
or	_	_
1×1	_	_
kernels	_	_
and	_	_
are	_	_
followed	_	_
by	_	_
batch	_	_
normalization	_	_
[	_	_
50	_	_
]	_	_
before	_	_
ReLU6	_	_
[	_	_
51	_	_
]	_	_
activation	_	_
.	_	_

#42
Similar	_	_
to	_	_
DeepLab3+	_	_
,	_	_
original	_	_
Xception	_	_
max	_	_
pooling	_	_
layers	_	_
have	_	_
been	_	_
replaced	_	_
with	_	_
strided	_	_
depthwise	_	_
separable	_	_
convolutions	_	_
,	_	_
enabling	_	_
the	_	_
network	_	_
to	_	_
learn	_	_
its	_	_
own	_	_
downsampling	_	_
.	_	_

#43
Extra	_	_
batch	_	_
normalization	_	_
is	_	_
added	_	_
between	_	_
the	_	_
depthwise	_	_
and	_	_
pointwise	_	_
convolutions	_	_
of	_	_
every	_	_
depthwise	_	_
separable	_	_
convolution	_	_
like	_	_
in	_	_
MobileNet	_	_
[	_	_
52	_	_
]	_	_
.	_	_

#44
The	_	_
following	_	_
subsections	_	_
correspond	_	_
to	_	_
the	_	_
subsections	_	_
of	_	_
fig	_	_
.	_	_

#45
1	_	_
.	_	_

#46
Training	_	_
details	_	_
follow	_	_
in	_	_
section	_	_
3	_	_
.	_	_

#47
2.1	_	_
Entry	_	_
Flow	_	_

#48
Other	_	_
than	_	_
the	_	_
modification	_	_
of	_	_
max	_	_
pooling	_	_
layers	_	_
to	_	_
strided	_	_
depthwise	_	_
separable	_	_
convolutions	_	_
,	_	_
most	_	_
entry	_	_
flow	_	_
convolutions	_	_
are	_	_
similar	_	_
to	_	_
Xception’s	_	_
[	_	_
45	_	_
]	_	_
.	_	_

#49
Common	_	_
convolutions	_	_
have	_	_
the	_	_
same	_	_
number	_	_
of	_	_
features	_	_
and	_	_
are	_	_
arranged	_	_
into	_	_
Xception-style	_	_
residual	_	_
[	_	_
53	_	_
]	_	_
blocks	_	_
to	_	_
reduce	_	_
semantic	_	_
decimation	_	_
during	_	_
downsampling	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#50
The	_	_
main	_	_
change	_	_
is	_	_
the	_	_
replacement	_	_
of	_	_
the	_	_
first	_	_
two	_	_
Xception	_	_
convolutions	_	_
with	_	_
an	_	_
residual	_	_
convolutional	_	_
downsampling	_	_
block	_	_
.	_	_

#51
For	_	_
reference	_	_
,	_	_
the	_	_
start	_	_
of	_	_
the	_	_
original	_	_
Xception	_	_
entry	_	_
flow	_	_
is	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#52
2	_	_
.	_	_

#53
This	_	_
change	_	_
was	_	_
made	_	_
so	_	_
that	_	_
the	_	_
lowest-level	_	_
features	_	_
concatenated	_	_
in	_	_
the	_	_
decoder	_	_
would	_	_
be	_	_
deeper	_	_
and	_	_
have	_	_
a	_	_
larger	_	_
feature	_	_
space	_	_
.	_	_

#54
An	_	_
alternative	_	_
solution	_	_
is	_	_
to	_	_
apply	_	_
additional	_	_
pre-concatenation	_	_
1×1	_	_
convolutions	_	_
to	_	_
change	_	_
the	_	_
feature	_	_
space	_	_
size	_	_
.	_	_

#55
This	_	_
was	_	_
the	_	_
approach	_	_
taken	_	_
in	_	_
DeepLab3+	_	_
[	_	_
47	_	_
]	_	_
to	_	_
prevent	_	_
high-level	_	_
ASPP	_	_
semantics	_	_
being	_	_
overwhelmed	_	_
by	_	_
low-level	_	_
features	_	_
in	_	_
the	_	_
decoder	_	_
.	_	_

#56
2.2	_	_
Middle	_	_
Flow	_	_

#57
Skip-3	_	_
residual	_	_
blocks	_	_
are	_	_
repeated	_	_
12	_	_
times	_	_
to	_	_
develop	_	_
high-level	_	_
semantics	_	_
to	_	_
flow	_	_
into	_	_
the	_	_
ASPP	_	_
module	_	_
.	_	_

#58
This	_	_
is	_	_
more	_	_
than	_	_
the	_	_
8	_	_
skip-3	_	_
blocks	_	_
used	_	_
in	_	_
Xception	_	_
[	_	_
45	_	_
]	_	_
as	_	_
the	_	_
inflowing	_	_
tensors	_	_
are	_	_
larger	_	_
;	_	_
32×32×728	_	_
rather	_	_
than	_	_
19×19×728	_	_
.	_	_

#59
Nevertheless	_	_
,	_	_
this	_	_
is	_	_
fewer	_	_
than	_	_
the	_	_
16	_	_
skip-3	_	_
residual	_	_
blocks	_	_
used	_	_
in	_	_
DeepLab3+	_	_
for	_	_
similar	_	_
tensors	_	_
[	_	_
47	_	_
]	_	_
.	_	_

#60
Middle	_	_
flow	_	_
tensors	_	_
are	_	_
much	_	_
smaller	_	_
than	_	_
those	_	_
in	_	_
other	_	_
parts	_	_
of	_	_
the	_	_
network	_	_
so	_	_
this	_	_
decision	_	_
was	_	_
a	_	_
compromise	_	_
between	_	_
expressibility	_	_
and	_	_
training	_	_
time	_	_
.	_	_

#61
2.3	_	_
Atrous	_	_
Spatial	_	_
Pyramid	_	_
Pooling	_	_

#62
This	_	_
is	_	_
the	_	_
ASPP	_	_
module	_	_
Google	_	_
developed	_	_
for	_	_
semantic	_	_
image	_	_
segmentation	_	_
[	_	_
46	_	_
,	_	_
47	_	_
]	_	_
without	_	_
a	_	_
pre-pooling	_	_
1×1	_	_
convolution	_	_
.	_	_

#63
It	_	_
is	_	_
being	_	_
used	_	_
rather	_	_
than	_	_
a	_	_
fully	_	_
connected	_	_
layer	_	_
;	_	_
like	_	_
the	_	_
one	_	_
in	_	_
[	_	_
18	_	_
]	_	_
’s	_	_
denoiser	_	_
,	_	_
to	_	_
sample	_	_
semantics	_	_
at	_	_
multiple	_	_
scales	_	_
as	_	_
it	_	_
requires	_	_
fewer	_	_
parameters	_	_
and	_	_
has	_	_
almost	_	_
identical	_	_
performance	_	_
.	_	_

#64
The	_	_
atrous	_	_
rates	_	_
of	_	_
6	_	_
,	_	_
12	_	_
and	_	_
18	_	_
are	_	_
the	_	_
same	_	_
as	_	_
those	_	_
used	_	_
in	_	_
the	_	_
original	_	_
ASPP	_	_
module	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#65
Importantly	_	_
,	_	_
a	_	_
bottleneck	_	_
1×1	_	_
convolution	_	_
is	_	_
used	_	_
to	_	_
reduce	_	_
the	_	_
number	_	_
of	_	_
output	_	_
features	_	_
from	_	_
3640	_	_
to	_	_
256	_	_
,	_	_
forcing	_	_
the	_	_
network	_	_
to	_	_
work	_	_
harder	_	_
to	_	_
develop	_	_
high-level	_	_
semantics	_	_
.	_	_

#66
2.4	_	_
Decoder	_	_

#67
Rich	_	_
ASPP	_	_
semantics	_	_
are	_	_
bilinearly	_	_
upsampled	_	_
from	_	_
32×32×256	_	_
to	_	_
128×128×256	_	_
feature	_	_
maps	_	_
and	_	_
concatenated	_	_
with	_	_
low-level	_	_
entry	_	_
flow	_	_
features	_	_
to	_	_
resolve	_	_
them	_	_
in	_	_
the	_	_
following	_	_
convolutions	_	_
.	_	_

#68
This	_	_
is	_	_
similar	_	_
to	_	_
the	_	_
approach	_	_
used	_	_
in	_	_
other	_	_
encoder-decoder	_	_
architectures	_	_
[	_	_
18	_	_
,	_	_
20	_	_
,	_	_
46–48	_	_
]	_	_
.	_	_

#69
There	_	_
are	_	_
two	_	_
residual	_	_
concatenations	_	_
with	_	_
low-level	_	_
features	_	_
from	_	_
the	_	_
entry	_	_
flow	_	_
;	_	_
rather	_	_
than	_	_
one	_	_
,	_	_
so	_	_
that	_	_
semantics	_	_
can	_	_
be	_	_
resolved	_	_
from	_	_
the	_	_
first	_	_
concatenation	_	_
before	_	_
being	_	_
resolved	_	_
into	_	_
fine	_	_
spatial	_	_
information	_	_
after	_	_
the	_	_
second	_	_
.	_	_

#70
To	_	_
prevent	_	_
rich	_	_
semantic	_	_
information	_	_
from	_	_
being	_	_
overwhelmed	_	_
by	_	_
low-level	_	_
features	_	_
,	_	_
feature	_	_
sizes	_	_
are	_	_
chosen	_	_
so	_	_
that	_	_
,	_	_
disregarding	_	_
spatial	_	_
dimensions	_	_
,	_	_
the	_	_
same	_	_
number	_	_
of	_	_
low-level	_	_
and	_	_
high-level	_	_
features	_	_
enter	_	_
the	_	_
decoder	_	_
.	_	_

#71
The	_	_
optimal	_	_
ratio	_	_
is	_	_
unknown	_	_
;	_	_
however	_	_
,	_	_
this	_	_
paper	_	_
and	_	_
Google’s	_	_
work	_	_
on	_	_
semantic	_	_
segmentation	_	_
[	_	_
46	_	_
,	_	_
47	_	_
]	_	_
establish	_	_
that	_	_
a	_	_
1:1	_	_
ratio	_	_
works	_	_
for	_	_
multiple	_	_
domains	_	_
.	_	_

#72
The	_	_
resolved	_	_
features	_	_
are	_	_
transpositionaly	_	_
convoluted	_	_
back	_	_
to	_	_
the	_	_
size	_	_
of	_	_
the	_	_
original	_	_
image	_	_
;	_	_
rather	_	_
than	_	_
resolved	_	_
at	_	_
that	_	_
scale	_	_
,	_	_
to	_	_
introduce	_	_
another	_	_
bottleneck	_	_
.	_	_

#73
This	_	_
forces	_	_
the	_	_
network	_	_
Figure	_	_
3	_	_
:	_	_
Mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
losses	_	_
of	_	_
our	_	_
neural	_	_
network	_	_
during	_	_
training	_	_
on	_	_
low	_	_
dose	_	_
(	_	_
300	_	_
counts	_	_
ppx	_	_
)	_	_
and	_	_
fine-tuning	_	_
for	_	_
ordinary	_	_
doses	_	_
(	_	_
200-2500	_	_
counts	_	_
ppx	_	_
)	_	_
.	_	_

#74
Learning	_	_
rates	_	_
(	_	_
LRs	_	_
)	_	_
and	_	_
the	_	_
freezing	_	_
of	_	_
batch	_	_
normalization	_	_
are	_	_
annotated	_	_
.	_	_

#75
Validation	_	_
losses	_	_
were	_	_
calculated	_	_
using	_	_
1	_	_
validation	_	_
example	_	_
after	_	_
every	_	_
5	_	_
training	_	_
batches	_	_
.	_	_

#76
to	_	_
work	_	_
harder	_	_
to	_	_
develop	_	_
meaningful	_	_
features	_	_
to	_	_
resolve	_	_
recovered	_	_
micrographs	_	_
from	_	_
in	_	_
the	_	_
final	_	_
convolutions	_	_
.	_	_

#77
3	_	_
Training	_	_

#78
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
discuss	_	_
training	_	_
with	_	_
the	_	_
TensorFlow	_	_
[	_	_
31	_	_
]	_	_
deep	_	_
learning	_	_
framework	_	_
.	_	_

#79
Training	_	_
was	_	_
performed	_	_
using	_	_
ADAM	_	_
[	_	_
54	_	_
]	_	_
optimized	_	_
synchronous	_	_
stochastic	_	_
gradient	_	_
decent	_	_
[	_	_
30	_	_
]	_	_
with	_	_
1	_	_
replica	_	_
network	_	_
on	_	_
each	_	_
of	_	_
2	_	_
Nvidia	_	_
GTX	_	_
1080	_	_
Ti	_	_
GPUs	_	_
.	_	_

#80
3.1	_	_
Data	_	_
Pipeline	_	_

#81
A	_	_
new	_	_
dataset	_	_
of	_	_
17267	_	_
2048×2048	_	_
high-quality	_	_
electron	_	_
micrographs	_	_
saved	_	_
to	_	_
University	_	_
of	_	_
Warwick	_	_
data	_	_
servers	_	_
over	_	_
several	_	_
years	_	_
was	_	_
collated	_	_
for	_	_
training	_	_
.	_	_

#82
Here	_	_
,	_	_
high-quality	_	_
refers	_	_
to	_	_
2048×2048	_	_
micrographs	_	_
with	_	_
mean	_	_
counts	_	_
per	_	_
pixel	_	_
above	_	_
2500	_	_
.	_	_

#83
The	_	_
dataset	_	_
was	_	_
collated	_	_
from	_	_
individual	_	_
micrographs	_	_
made	_	_
by	_	_
dozens	_	_
of	_	_
scientists	_	_
working	_	_
on	_	_
hundreds	_	_
of	_	_
projects	_	_
and	_	_
therefore	_	_
has	_	_
a	_	_
diverse	_	_
constitution	_	_
.	_	_

#84
It	_	_
has	_	_
been	_	_
made	_	_
publicly	_	_
available	_	_
as	_	_
a	_	_
set	_	_
of	_	_
TIFFs	_	_
[	_	_
55	_	_
]	_	_
via	_	_
https	_	_
:	_	_
//github.com/JeffreyEde/ElectronMicrographDenoiser	_	_
.	_	_

#85
The	_	_
dataset	_	_
was	_	_
split	_	_
into	_	_
11350	_	_
training	_	_
,	_	_
2431	_	_
validation	_	_
and	_	_
3486	_	_
test	_	_
micrographs	_	_
.	_	_

#86
For	_	_
training	_	_
,	_	_
each	_	_
micrograph	_	_
was	_	_
downsized	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
2	_	_
using	_	_
area	_	_
interpolation	_	_
to	_	_
1024×1024	_	_
.	_	_

#87
This	_	_
increased	_	_
mean	_	_
counts	_	_
per	_	_
pixels	_	_
above	_	_
10000	_	_
,	_	_
corresponding	_	_
to	_	_
signal-to-noise	_	_
ratios	_	_
above	_	_
100:1	_	_
.	_	_

#88
Next	_	_
,	_	_
512×512	_	_
crops	_	_
were	_	_
taken	_	_
at	_	_
random	_	_
positions	_	_
and	_	_
subject	_	_
to	_	_
a	_	_
random	_	_
combination	_	_
of	_	_
flips	_	_
and	_	_
90◦	_	_
rotations	_	_
to	_	_
augment	_	_
the	_	_
dataset	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
8	_	_
.	_	_

#89
Each	_	_
crop	_	_
was	_	_
then	_	_
linearly	_	_
transformed	_	_
to	_	_
have	_	_
values	_	_
between	_	_
0.0	_	_
and	_	_
1.0	_	_
.	_	_

#90
To	_	_
train	_	_
the	_	_
network	_	_
for	_	_
low	_	_
doses	_	_
,	_	_
Poisson	_	_
noise	_	_
was	_	_
applied	_	_
to	_	_
each	_	_
crop	_	_
after	_	_
scaling	_	_
by	_	_
a	_	_
number	_	_
sampled	_	_
from	_	_
an	_	_
exponential	_	_
distribution	_	_
with	_	_
probability	_	_
density	_	_
function	_	_
(	_	_
PDF	_	_
)	_	_
f	_	_
(	_	_
x	_	_
,	_	_
β	_	_
)	_	_
=	_	_
β	_	_
exp	_	_
(	_	_
−x	_	_
β	_	_
)	_	_
.	_	_

#91
(	_	_
1	_	_
)	_	_
We	_	_
chose	_	_
β	_	_
=	_	_
75.0	_	_
and	_	_
offset	_	_
the	_	_
numbers	_	_
sampled	_	_
by	_	_
25.0	_	_
before	_	_
scaling	_	_
crops	_	_
with	_	_
them	_	_
.	_	_

#92
These	_	_
numbers	_	_
and	_	_
distribution	_	_
are	_	_
arbitrary	_	_
and	_	_
were	_	_
chosen	_	_
to	_	_
expose	_	_
the	_	_
network	_	_
to	_	_
a	_	_
continuous	_	_
range	_	_
of	_	_
noise	_	_
levels	_	_
where	_	_
most	_	_
are	_	_
very	_	_
noisy	_	_
.	_	_

#93
After	_	_
noise	_	_
application	_	_
,	_	_
ground	_	_
truth	_	_
crops	_	_
were	_	_
scaled	_	_
to	_	_
have	_	_
the	_	_
same	_	_
means	_	_
as	_	_
their	_	_
noisy	_	_
counterparts	_	_
.	_	_

#94
After	_	_
being	_	_
trained	_	_
for	_	_
low-dose	_	_
applications	_	_
,	_	_
the	_	_
network	_	_
was	_	_
fine-tuned	_	_
for	_	_
ordinary	_	_
doses	_	_
by	_	_
training	_	_
it	_	_
on	_	_
crops	_	_
scaled	_	_
by	_	_
numbers	_	_
uniformly	_	_
distributed	_	_
between	_	_
200	_	_
and	_	_
2500	_	_
.	_	_

#95
3.2	_	_
Learning	_	_
Policy	_	_

#96
In	_	_
this	_	_
subsection	_	_
,	_	_
we	_	_
discuss	_	_
our	_	_
training	_	_
hyperparameters	_	_
and	_	_
learning	_	_
protocol	_	_
for	_	_
the	_	_
learning	_	_
curve	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#97
3	_	_
.	_	_

#98
Loss	_	_
metric	_	_
:	_	_
Our	_	_
network	_	_
was	_	_
trained	_	_
to	_	_
minimize	_	_
the	_	_
Huberised	_	_
[	_	_
56	_	_
]	_	_
scaled	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
between	_	_
denoised	_	_
and	_	_
high-quality	_	_
crops	_	_
:	_	_
L	_	_
=	_	_
{	_	_
1000MSE	_	_
,	_	_
1000MSE	_	_
<	_	_
1.0	_	_
(	_	_
1000MSE	_	_
)	_	_
2	_	_
,	_	_
1000MSE	_	_
≥	_	_
1.0	_	_
(	_	_
2	_	_
)	_	_
The	_	_
loss	_	_
is	_	_
Huberized	_	_
to	_	_
prevent	_	_
the	_	_
network	_	_
from	_	_
being	_	_
too	_	_
disturbed	_	_
by	_	_
batches	_	_
with	_	_
especially	_	_
noisy	_	_
crops	_	_
.	_	_

#99
To	_	_
surpass	_	_
our	_	_
low-dose	_	_
performance	_	_
benchmarks	_	_
,	_	_
our	_	_
network	_	_
had	_	_
to	_	_
achieve	_	_
a	_	_
MSE	_	_
lower	_	_
than	_	_
7.5	_	_
×	_	_
10−4	_	_
,	_	_
as	_	_
tabulated	_	_
in	_	_
table	_	_
1	_	_
.	_	_

#100
Consequently	_	_
,	_	_
MSEs	_	_
were	_	_
scaled	_	_
by	_	_
1000	_	_
to	_	_
limit	_	_
trainable	_	_
parameter	_	_
perturbations	_	_
by	_	_
MSEs	_	_
larger	_	_
than	_	_
1.0	_	_
×	_	_
10−3	_	_
.	_	_

#101
More	_	_
subtly	_	_
,	_	_
this	_	_
also	_	_
increased	_	_
our	_	_
network’s	_	_
effective	_	_
learning	_	_
rate	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
1000	_	_
.	_	_

#102
Our	_	_
network	_	_
was	_	_
trained	_	_
without	_	_
clipping	_	_
its	_	_
outputs	_	_
between	_	_
0.0	_	_
and	_	_
1.0	_	_
.	_	_

#103
Clipping	_	_
is	_	_
only	_	_
applied	_	_
optionally	_	_
during	_	_
inference	_	_
.	_	_

#104
Nevertheless	_	_
,	_	_
as	_	_
clipping	_	_
is	_	_
desirable	_	_
in	_	_
most	_	_
applications	_	_
,	_	_
all	_	_
performance	_	_
statistics	_	_
;	_	_
including	_	_
losses	_	_
during	_	_
training	_	_
,	_	_
are	_	_
reported	_	_
for	_	_
clipped	_	_
outputs	_	_
.	_	_

#105
Batch	_	_
normalization	_	_
:	_	_
Batch	_	_
normalization	_	_
layers	_	_
from	_	_
[	_	_
50	_	_
]	_	_
were	_	_
trained	_	_
with	_	_
a	_	_
decay	_	_
rate	_	_
of	_	_
0.999	_	_
for	_	_
134108	_	_
batches	_	_
.	_	_

#106
Batch	_	_
normalization	_	_
was	_	_
then	_	_
frozen	_	_
for	_	_
the	_	_
rest	_	_
of	_	_
training	_	_
.	_	_

#107
Batch	_	_
normalization	_	_
significantly	_	_
reduced	_	_
grid-like	_	_
increases	_	_
in	_	_
error	_	_
in	_	_
our	_	_
output	_	_
images	_	_
.	_	_

#108
As	_	_
a	_	_
result	_	_
,	_	_
it	_	_
was	_	_
not	_	_
frozen	_	_
until	_	_
the	_	_
instability	_	_
introduced	_	_
by	_	_
varying	_	_
batch	_	_
normalization	_	_
parameters	_	_
noticeably	_	_
limited	_	_
convergence	_	_
.	_	_

#109
Optimizer	_	_
:	_	_
ADAM	_	_
[	_	_
54	_	_
]	_	_
optimization	_	_
was	_	_
used	_	_
with	_	_
a	_	_
stepped	_	_
learning	_	_
rate	_	_
.	_	_

#110
For	_	_
the	_	_
low	_	_
dose	_	_
version	_	_
of	_	_
the	_	_
network	_	_
,	_	_
we	_	_
used	_	_
a	_	_
learning	_	_
rate	_	_
of	_	_
1.0×10−3	_	_
for	_	_
134108	_	_
batches	_	_
,	_	_
2.5×10−4	_	_
for	_	_
another	_	_
17713	_	_
batches	_	_
and	_	_
then	_	_
1.0×10−4	_	_
for	_	_
46690	_	_
batches	_	_
.	_	_

#111
The	_	_
network	_	_
was	_	_
then	_	_
fine-tuned	_	_
for	_	_
ordinary	_	_
doses	_	_
using	_	_
a	_	_
learning	_	_
rate	_	_
of	_	_
2.5×10−4	_	_
for	_	_
16773	_	_
batches	_	_
,	_	_
then	_	_
1.0×10−4	_	_
for	_	_
17562	_	_
batches	_	_
.	_	_

#112
The	_	_
unusual	_	_
numbers	_	_
of	_	_
batches	_	_
are	_	_
a	_	_
result	_	_
of	_	_
learning	_	_
rates	_	_
being	_	_
adjusted	_	_
after	_	_
saving	_	_
variables	_	_
;	_	_
something	_	_
that	_	_
happened	_	_
at	_	_
wall	_	_
clock	_	_
times	_	_
.	_	_

#113
We	_	_
found	_	_
the	_	_
recommended	_	_
[	_	_
31	_	_
,	_	_
54	_	_
]	_	_
ADAM	_	_
decay	_	_
rate	_	_
for	_	_
the	_	_
1st	_	_
moment	_	_
of	_	_
the	_	_
momentum	_	_
,	_	_
β1	_	_
=	_	_
0.9	_	_
,	_	_
to	_	_
be	_	_
too	_	_
high	_	_
and	_	_
chose	_	_
β1	_	_
=	_	_
0.5	_	_
instead	_	_
.	_	_

#114
This	_	_
lower	_	_
β1	_	_
made	_	_
training	_	_
more	_	_
resistant	_	_
to	_	_
varying	_	_
noise	_	_
levels	_	_
in	_	_
batches	_	_
.	_	_

#115
Our	_	_
experiments	_	_
with	_	_
β1	_	_
are	_	_
discussed	_	_
in	_	_
section	_	_
5	_	_
.	_	_

#116
Regularization	_	_
:	_	_
L2	_	_
regularization	_	_
[	_	_
57	_	_
]	_	_
was	_	_
applied	_	_
by	_	_
adding	_	_
5×10−5	_	_
times	_	_
the	_	_
quadrature	_	_
sum	_	_
of	_	_
all	_	_
trainable	_	_
variables	_	_
to	_	_
the	_	_
loss	_	_
function	_	_
.	_	_

#117
This	_	_
prevented	_	_
trainable	_	_
parameters	_	_
growing	_	_
unbounded	_	_
,	_	_
decreasing	_	_
their	_	_
ability	_	_
to	_	_
learn	_	_
in	_	_
proportion	_	_
[	_	_
58	_	_
]	_	_
.	_	_

#118
Importantly	_	_
,	_	_
this	_	_
ensures	_	_
that	_	_
our	_	_
network	_	_
will	_	_
continue	_	_
to	_	_
learn	_	_
effectively	_	_
if	_	_
it	_	_
is	_	_
fine-tuned	_	_
or	_	_
given	_	_
additional	_	_
training	_	_
.	_	_

#119
We	_	_
did	_	_
not	_	_
perform	_	_
an	_	_
extensive	_	_
search	_	_
for	_	_
our	_	_
regularization	_	_
rate	_	_
and	_	_
think	_	_
that	_	_
5×10−5	_	_
may	_	_
be	_	_
too	_	_
high	_	_
.	_	_

#120
Activation	_	_
:	_	_
All	_	_
neurons	_	_
are	_	_
ReLU6	_	_
[	_	_
51	_	_
]	_	_
activated	_	_
.	_	_

#121
Other	_	_
activations	_	_
are	_	_
discussed	_	_
in	_	_
section	_	_
5	_	_
.	_	_

#122
Initialization	_	_
:	_	_
All	_	_
weights	_	_
were	_	_
Xavier	_	_
[	_	_
59	_	_
]	_	_
initialized	_	_
.	_	_

#123
Biases	_	_
were	_	_
zero	_	_
initialized	_	_
.	_	_

#124
4	_	_
Performance	_	_

#125
In	_	_
this	_	_
section	_	_
,	_	_
our	_	_
network’s	_	_
MSE	_	_
and	_	_
structural	_	_
similarity	_	_
index	_	_
(	_	_
SSIM	_	_
)	_	_
performance	_	_
is	_	_
benchmarked	_	_
other	_	_
methods’	_	_
and	_	_
its	_	_
mean	_	_
error	_	_
per	_	_
pixel	_	_
is	_	_
mapped	_	_
.	_	_

#126
We	_	_
also	_	_
present	_	_
example	_	_
applications	_	_
of	_	_
our	_	_
network	_	_
to	_	_
noisy	_	_
electron	_	_
microscopy	_	_
and	_	_
scanning	_	_
transmission	_	_
electron	_	_
microscopy	_	_
(	_	_
STEM	_	_
)	_	_
images	_	_
.	_	_

#127
4.1	_	_
Benchmarking	_	_

#128
To	_	_
benchmark	_	_
our	_	_
network’s	_	_
performance	_	_
,	_	_
we	_	_
applied	_	_
it	_	_
and	_	_
9	_	_
popular	_	_
denoising	_	_
methods	_	_
to	_	_
20000	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
applied	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
test	_	_
set	_	_
micrographs	_	_
using	_	_
the	_	_
method	_	_
in	_	_
section	_	_
3.1	_	_
.	_	_

#129
This	_	_
benchmarking	_	_
is	_	_
done	_	_
for	_	_
the	_	_
low-dose	_	_
version	_	_
of	_	_
our	_	_
network	_	_
and	_	_
the	_	_
version	_	_
fine-tuned	_	_
for	_	_
ordinary	_	_
doses	_	_
.	_	_

#130
Implementation	_	_
details	_	_
for	_	_
each	_	_
denoising	_	_
methods	_	_
follow	_	_
.	_	_

#131
Unfiltered	_	_
:	_	_
For	_	_
reference	_	_
,	_	_
MSE	_	_
and	_	_
SSIM	_	_
statistics	_	_
were	_	_
collected	_	_
for	_	_
noisy	_	_
crops	_	_
without	_	_
any	_	_
denoising	_	_
method	_	_
being	_	_
applied	_	_
to	_	_
recover	_	_
them	_	_
.	_	_

#132
Gaussian	_	_
filter	_	_
:	_	_
OpenCV	_	_
[	_	_
60	_	_
]	_	_
default	_	_
implementation	_	_
of	_	_
Gaussian	_	_
blurring	_	_
for	_	_
a	_	_
3×3	_	_
kernel	_	_
.	_	_

#133
Bilateral	_	_
filter	_	_
:	_	_
OpenCV	_	_
[	_	_
60	_	_
]	_	_
implemented	_	_
bilateral	_	_
filtering	_	_
[	_	_
61	_	_
]	_	_
.	_	_

#134
We	_	_
used	_	_
a	_	_
default	_	_
9	_	_
pixel	_	_
neighborhoods	_	_
with	_	_
radiometric	_	_
and	_	_
spatial	_	_
scales	_	_
of	_	_
75	_	_
.	_	_

#135
These	_	_
scales	_	_
are	_	_
a	_	_
compromise	_	_
between	_	_
small	_	_
scales	_	_
;	_	_
less	_	_
than	_	_
10	_	_
,	_	_
that	_	_
have	_	_
little	_	_
effect	_	_
and	_	_
large	_	_
scales	_	_
;	_	_
more	_	_
than	_	_
150	_	_
,	_	_
that	_	_
cartoonize	_	_
images	_	_
.	_	_

#136
Median	_	_
filter	_	_
:	_	_
OpenCV	_	_
[	_	_
60	_	_
]	_	_
implemented	_	_
3×3	_	_
median	_	_
filter	_	_
.	_	_

#137
Wiener	_	_
:	_	_
Scipy	_	_
[	_	_
62	_	_
]	_	_
implementation	_	_
of	_	_
Wiener	_	_
filtering	_	_
.	_	_

#138
Wavelet	_	_
denoising	_	_
:	_	_
Scikit-image	_	_
[	_	_
63	_	_
]	_	_
implementation	_	_
of	_	_
BayesShrink	_	_
adaptive	_	_
wavelet	_	_
soft-thresholding	_	_
[	_	_
64	_	_
]	_	_
.	_	_

#139
The	_	_
noise	_	_
standard	_	_
deviation	_	_
used	_	_
to	_	_
compute	_	_
wavelet	_	_
detail	_	_
coefficient	_	_
thresholds	_	_
was	_	_
estimated	_	_
from	_	_
the	_	_
data	_	_
using	_	_
the	_	_
method	_	_
in	_	_
[	_	_
65	_	_
]	_	_
.	_	_

#140
Chambolle	_	_
total	_	_
variation	_	_
denoising	_	_
:	_	_
Scikit-image	_	_
[	_	_
63	_	_
]	_	_
implementation	_	_
of	_	_
Chambolle’s	_	_
iterative	_	_
total-variation	_	_
denoising	_	_
algorithm	_	_
[	_	_
66	_	_
]	_	_
.	_	_

#141
We	_	_
used	_	_
the	_	_
scikit-image	_	_
default	_	_
denoising	_	_
weight	_	_
of	_	_
0.1	_	_
and	_	_
ran	_	_
the	_	_
algorithm	_	_
until	_	_
either	_	_
the	_	_
fractional	_	_
change	_	_
in	_	_
the	_	_
algorithm’s	_	_
cost	_	_
function	_	_
was	_	_
less	_	_
than	_	_
2.0×	_	_
10−4	_	_
or	_	_
it	_	_
reached	_	_
200	_	_
iterations	_	_
.	_	_

#142
Bregman	_	_
total	_	_
variation	_	_
denoising	_	_
:	_	_
Scikit-image	_	_
[	_	_
63	_	_
]	_	_
implementation	_	_
of	_	_
split	_	_
Bregman	_	_
anisotropic	_	_
total	_	_
variation	_	_
denoising	_	_
[	_	_
67	_	_
,	_	_
68	_	_
]	_	_
.	_	_

#143
We	_	_
used	_	_
a	_	_
denoising	_	_
weight	_	_
of	_	_
0.1	_	_
and	_	_
ran	_	_
the	_	_
algorithm	_	_
until	_	_
either	_	_
the	_	_
fractional	_	_
change	_	_
in	_	_
the	_	_
algorithm’s	_	_
cost	_	_
function	_	_
was	_	_
less	_	_
than	_	_
2.0	_	_
×	_	_
10−4	_	_
or	_	_
it	_	_
reached	_	_
200	_	_
iterations	_	_
Non-local	_	_
means	_	_
:	_	_
Scikit-image	_	_
[	_	_
63	_	_
]	_	_
implementation	_	_
of	_	_
texture-preserving	_	_
non-local	_	_
means	_	_
denoising	_	_
[	_	_
69	_	_
]	_	_
.	_	_

#144
MSE	_	_
and	_	_
SSIM	_	_
performances	_	_
were	_	_
divided	_	_
into	_	_
200	_	_
equispaced	_	_
bins	_	_
in	_	_
[	_	_
0.0	_	_
,	_	_
1.2	_	_
]	_	_
×	_	_
10−3	_	_
and	_	_
[	_	_
0.0	_	_
,	_	_
1.0	_	_
]	_	_
,	_	_
respectively	_	_
,	_	_
for	_	_
both	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
.	_	_

#145
Performance	_	_
PDFs	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#146
4	_	_
are	_	_
Gaussian	_	_
kernel	_	_
density	_	_
estimated	_	_
[	_	_
70	_	_
,	_	_
71	_	_
]	_	_
(	_	_
KDE	_	_
)	_	_
from	_	_
these	_	_
.	_	_

#147
KDE	_	_
bandwidths	_	_
were	_	_
found	_	_
using	_	_
Scott’s	_	_
Rule	_	_
[	_	_
72	_	_
]	_	_
.	_	_

#148
To	_	_
ease	_	_
comparison	_	_
,	_	_
PDFs	_	_
for	_	_
each	_	_
set	_	_
of	_	_
performance	_	_
statistics	_	_
are	_	_
scaled	_	_
so	_	_
the	_	_
highest	_	_
value	_	_
in	_	_
each	_	_
set	_	_
is	_	_
1.0	_	_
.	_	_

#149
The	_	_
scale	_	_
factors	_	_
are	_	_
6.07×10−4	_	_
for	_	_
low	_	_
dose	_	_
MSE	_	_
,	_	_
2.99×10−1	_	_
for	_	_
low	_	_
dose	_	_
SSIM	_	_
,	_	_
3.03×10−4	_	_
for	_	_
ordinary	_	_
dose	_	_
MSE	_	_
and	_	_
9.17×10−2	_	_
for	_	_
ordinary	_	_
dose	_	_
SSIM	_	_
.	_	_

#150
Performance	_	_
statistics	_	_
are	_	_
also	_	_
summarized	_	_
in	_	_
table	_	_
1	_	_
.	_	_

#151
This	_	_
shows	_	_
that	_	_
our	_	_
network	_	_
outperforms	_	_
the	_	_
mean	_	_
MSE	_	_
and	_	_
SSIM	_	_
performance	_	_
of	_	_
every	_	_
other	_	_
method	_	_
for	_	_
both	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
.	_	_

#152
Its	_	_
MSE	_	_
variance	_	_
is	_	_
also	_	_
lower	_	_
for	_	_
both	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
.	_	_

#153
Figure	_	_
4	_	_
:	_	_
Gaussian	_	_
kernel	_	_
density	_	_
estimated	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
and	_	_
structural	_	_
similarity	_	_
index	_	_
(	_	_
SSIM	_	_
)	_	_
probability	_	_
density	_	_
functions	_	_
(	_	_
PDFs	_	_
)	_	_
for	_	_
denoising	_	_
methods	_	_
applied	_	_
to	_	_
20000	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
.	_	_

#154
To	_	_
ease	_	_
comparison	_	_
,	_	_
the	_	_
highest	_	_
values	_	_
in	_	_
each	_	_
MSE	_	_
and	_	_
SSIM	_	_
PDF	_	_
set	_	_
has	_	_
been	_	_
scaled	_	_
to	_	_
1.0	_	_
.	_	_

#155
Only	_	_
the	_	_
starts	_	_
of	_	_
MSE	_	_
PDFs	_	_
are	_	_
shown	_	_
.	_	_

#156
Low	_	_
Dose	_	_
,	_	_
300	_	_
counts	_	_
ppx	_	_
Ordinary	_	_
Dose	_	_
,	_	_
200-2500	_	_
counts	_	_
ppx	_	_
MSE	_	_
(	_	_
×10−3	_	_
)	_	_
SSIM	_	_
MSE	_	_
(	_	_
×10−3	_	_
)	_	_
SSIM	_	_
Method	_	_
Mean	_	_
Std	_	_
Dev	_	_
Mean	_	_
Std	_	_
Dev	_	_
Mean	_	_
Std	_	_
Dev	_	_
Mean	_	_
Std	_	_
Dev	_	_
Unfiltered	_	_
4.357	_	_
2.558	_	_
0.454	_	_
0.208	_	_
0.508	_	_
0.682	_	_
0.850	_	_
0.123	_	_
Gaussian	_	_
0.816	_	_
0.452	_	_
0.685	_	_
0.159	_	_
0.344	_	_
0.334	_	_
0.878	_	_
0.087	_	_
Bilateral	_	_
1.025	_	_
1.152	_	_
0.574	_	_
0.261	_	_
1.243	_	_
1.392	_	_
0.600	_	_
0.271	_	_
Median	_	_
1.083	_	_
0.618	_	_
0.618	_	_
0.171	_	_
0.507	_	_
0.512	_	_
0.821	_	_
0.126	_	_
Wiener	_	_
1.068	_	_
0.546	_	_
0.681	_	_
0.137	_	_
0.402	_	_
0.389	_	_
0.870	_	_
0.085	_	_
Wavelet	_	_
0.832	_	_
0.58	_	_
0.657	_	_
0.186	_	_
0.357	_	_
0.312	_	_
0.875	_	_
0.085	_	_
Chambolle	_	_
TV	_	_
0.746	_	_
0.725	_	_
0.686	_	_
0.192	_	_
0.901	_	_
0.909	_	_
0.674	_	_
0.217	_	_
Bregman	_	_
TV	_	_
1.109	_	_
1.031	_	_
0.544	_	_
0.268	_	_
4.074	_	_
3.025	_	_
0.348	_	_
0.312	_	_
NL	_	_
means	_	_
2.924	_	_
2.338	_	_
0.357	_	_
0.315	_	_
1.403	_	_
1.266	_	_
0.545	_	_
0.281	_	_
Neural	_	_
network	_	_
0.562	_	_
0.449	_	_
0.752	_	_
0.147	_	_
0.201	_	_
0.169	_	_
0.926	_	_
0.057	_	_
Table	_	_
1	_	_
:	_	_
Means	_	_
and	_	_
standard	_	_
deviations	_	_
of	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
and	_	_
structural	_	_
similarity	_	_
index	_	_
(	_	_
SSIM	_	_
)	_	_
test	_	_
set	_	_
performances	_	_
of	_	_
denoising	_	_
methods	_	_
for	_	_
20000	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
.	_	_

#157
Noise	_	_
removal	_	_
methods	_	_
were	_	_
implemented	_	_
with	_	_
default	_	_
parameters	_	_
,	_	_
as	_	_
described	_	_
in	_	_
the	_	_
main	_	_
text	_	_
.	_	_

#158
TV	_	_
-	_	_
total	_	_
variation	_	_
.	_	_

#159
NL	_	_
-	_	_
non-local	_	_
.	_	_

#160
Figure	_	_
5	_	_
:	_	_
Mean	_	_
absolute	_	_
errors	_	_
of	_	_
our	_	_
low	_	_
and	_	_
ordinary	_	_
dose	_	_
networks’	_	_
512×512	_	_
outputs	_	_
for	_	_
20000	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
.	_	_

#161
Contrast	_	_
limited	_	_
adaptive	_	_
histogram	_	_
equalization	_	_
[	_	_
73	_	_
]	_	_
has	_	_
been	_	_
used	_	_
to	_	_
massively	_	_
increase	_	_
contrast	_	_
,	_	_
revealing	_	_
grid-like	_	_
error	_	_
variation	_	_
.	_	_

#162
Subplots	_	_
show	_	_
the	_	_
top-left	_	_
16×16	_	_
pixels’	_	_
mean	_	_
absolute	_	_
errors	_	_
unadjusted	_	_
.	_	_

#163
Variations	_	_
are	_	_
small	_	_
and	_	_
errors	_	_
are	_	_
close	_	_
to	_	_
the	_	_
minimum	_	_
everywhere	_	_
,	_	_
except	_	_
at	_	_
the	_	_
edges	_	_
where	_	_
they	_	_
are	_	_
higher	_	_
.	_	_

#164
Low	_	_
dose	_	_
errors	_	_
are	_	_
in	_	_
[	_	_
0.0169	_	_
,	_	_
0.0320	_	_
]	_	_
;	_	_
ordinary	_	_
dose	_	_
errors	_	_
are	_	_
in	_	_
[	_	_
0.0098	_	_
,	_	_
0.0272	_	_
]	_	_
.	_	_

#165
4.2	_	_
Network	_	_
Error	_	_

#166
Mean	_	_
absolute	_	_
errors	_	_
for	_	_
each	_	_
pixel	_	_
of	_	_
our	_	_
network’s	_	_
output	_	_
are	_	_
shown	_	_
for	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
in	_	_
fig	_	_
.	_	_

#167
5	_	_
.	_	_

#168
The	_	_
errors	_	_
are	_	_
uniform	_	_
almost	_	_
everywhere	_	_
,	_	_
except	_	_
at	_	_
the	_	_
edges	_	_
where	_	_
they	_	_
are	_	_
higher	_	_
.	_	_

#169
For	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
,	_	_
the	_	_
mean	_	_
absolute	_	_
errors	_	_
per	_	_
pixel	_	_
are	_	_
0.0177	_	_
and	_	_
0.0102	_	_
,	_	_
respectively	_	_
.	_	_

#170
Small	_	_
,	_	_
grid-like	_	_
variations	_	_
in	_	_
absolute	_	_
error	_	_
are	_	_
revealed	_	_
by	_	_
contrasted	_	_
limited	_	_
adaptive	_	_
histogram	_	_
equalization	_	_
[	_	_
73	_	_
]	_	_
in	_	_
fig	_	_
.	_	_

#171
5	_	_
.	_	_

#172
These	_	_
variations	_	_
are	_	_
common	_	_
in	_	_
deep	_	_
learning	_	_
and	_	_
are	_	_
often	_	_
associated	_	_
with	_	_
transpositional	_	_
convolutions	_	_
.	_	_

#173
Consequently	_	_
,	_	_
some	_	_
authors	_	_
[	_	_
74	_	_
]	_	_
have	_	_
recommended	_	_
their	_	_
replacement	_	_
with	_	_
bilinear	_	_
upsampling	_	_
then	_	_
convolution	_	_
.	_	_

#174
We	_	_
tried	_	_
this	_	_
;	_	_
however	_	_
,	_	_
it	_	_
only	_	_
made	_	_
the	_	_
errors	_	_
less	_	_
grid-like	_	_
.	_	_

#175
Instead	_	_
,	_	_
we	_	_
found	_	_
batch	_	_
normalization	_	_
to	_	_
be	_	_
the	_	_
best	_	_
way	_	_
to	_	_
reduce	_	_
structured	_	_
error	_	_
variation	_	_
.	_	_

#176
This	_	_
is	_	_
demonstrated	_	_
by	_	_
errors	_	_
being	_	_
more	_	_
grid-like	_	_
for	_	_
the	_	_
ordinary	_	_
dose	_	_
version	_	_
of	_	_
our	_	_
network	_	_
,	_	_
which	_	_
was	_	_
trained	_	_
for	_	_
longer	_	_
after	_	_
batch	_	_
normalization	_	_
was	_	_
frozen	_	_
.	_	_

#177
Consequently	_	_
,	_	_
batch	_	_
normalization	_	_
was	_	_
not	_	_
frozen	_	_
until	_	_
the	_	_
instability	_	_
introduced	_	_
by	_	_
its	_	_
trainable	_	_
variables	_	_
noticeably	_	_
limited	_	_
convergence	_	_
.	_	_

#178
4.3	_	_
Example	_	_
Usage	_	_

#179
We	_	_
provide	_	_
several	_	_
example	_	_
applications	_	_
of	_	_
our	_	_
low-dose	_	_
network	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
high-quality	_	_
electron	_	_
micrographs	_	_
that	_	_
we	_	_
applied	_	_
noise	_	_
to	_	_
in	_	_
fig	_	_
6	_	_
and	_	_
fig	_	_
.	_	_

#180
7	_	_
.	_	_

#181
Several	_	_
example	_	_
applications	_	_
of	_	_
our	_	_
low-dose	_	_
network	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
scanning	_	_
transmission	_	_
electron	_	_
micrography	_	_
(	_	_
STEM	_	_
)	_	_
images	_	_
are	_	_
also	_	_
presented	_	_
in	_	_
fig	_	_
.	_	_

#182
8	_	_
.	_	_

#183
Our	_	_
network	_	_
has	_	_
not	_	_
been	_	_
trained	_	_
for	_	_
STEM	_	_
so	_	_
this	_	_
demonstrates	_	_
its	_	_
ability	_	_
to	_	_
generalize	_	_
to	_	_
other	_	_
domains	_	_
.	_	_

#184
Our	_	_
neural	_	_
network	_	_
is	_	_
designed	_	_
to	_	_
be	_	_
simple	_	_
and	_	_
easy	_	_
to	_	_
use	_	_
.	_	_

#185
An	_	_
example	_	_
of	_	_
our	_	_
network	_	_
being	_	_
loaded	_	_
once	_	_
and	_	_
used	_	_
for	_	_
inference	_	_
multiple	_	_
times	_	_
in	_	_
python	_	_
is	_	_
>	_	_
>	_	_
>	_	_
from	_	_
d	_	_
e	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
i	_	_
m	_	_
p	_	_
o	_	_
r	_	_
t	_	_
D	_	_
e	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
>	_	_
>	_	_
>	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
e	_	_
m	_	_
o	_	_
v	_	_
e	_	_
r	_	_
=	_	_
D	_	_
e	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
(	_	_
)	_	_
>	_	_
>	_	_
>	_	_
r	_	_
e	_	_
s	_	_
t	_	_
o	_	_
r	_	_
e	_	_
d	_	_
i	_	_
m	_	_
g	_	_
1	_	_
=	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
e	_	_
m	_	_
o	_	_
v	_	_
e	_	_
r	_	_
.	_	_

#186
d	_	_
e	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
(	_	_
img1	_	_
)	_	_
>	_	_
>	_	_
>	_	_
r	_	_
e	_	_
s	_	_
t	_	_
o	_	_
r	_	_
e	_	_
d	_	_
i	_	_
m	_	_
g	_	_
2	_	_
=	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
r	_	_
e	_	_
m	_	_
o	_	_
v	_	_
e	_	_
r	_	_
.	_	_

#187
d	_	_
e	_	_
n	_	_
o	_	_
i	_	_
s	_	_
e	_	_
(	_	_
img2	_	_
)	_	_
Under	_	_
the	_	_
hood	_	_
,	_	_
our	_	_
program	_	_
divides	_	_
images	_	_
larger	_	_
than	_	_
512×512	_	_
into	_	_
slightly	_	_
overlapping	_	_
512×512	_	_
crops	_	_
that	_	_
can	_	_
be	_	_
processed	_	_
by	_	_
our	_	_
network	_	_
.	_	_

#188
This	_	_
gives	_	_
higher	_	_
accuracy	_	_
than	_	_
using	_	_
non-overlapping	_	_
crops	_	_
as	_	_
our	_	_
network	_	_
has	_	_
much	_	_
higher	_	_
errors	_	_
for	_	_
the	_	_
couple	_	_
of	_	_
pixels	_	_
near	_	_
the	_	_
edges	_	_
of	_	_
its	_	_
outputs	_	_
.	_	_

#189
Reflection	_	_
padding	_	_
is	_	_
applied	_	_
to	_	_
images	_	_
before	_	_
cropping	_	_
to	_	_
reduce	_	_
errors	_	_
at	_	_
their	_	_
edges	_	_
after	_	_
restoration	_	_
.	_	_

#190
Users	_	_
can	_	_
customize	_	_
the	_	_
amount	_	_
overlap	_	_
,	_	_
padding	_	_
and	_	_
many	_	_
Noisy	_	_
Restored	_	_
Ground	_	_
Truth	_	_
Figure	_	_
6	_	_
:	_	_
Example	_	_
applications	_	_
of	_	_
the	_	_
noise-removal	_	_
network	_	_
to	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
applied	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
high-quality	_	_
micrographs	_	_
.	_	_

#191
Enlarged	_	_
64×64	_	_
regions	_	_
from	_	_
the	_	_
top	_	_
left	_	_
of	_	_
each	_	_
crop	_	_
are	_	_
shown	_	_
to	_	_
ease	_	_
comparison	_	_
.	_	_

#192
Figure	_	_
7	_	_
:	_	_
More	_	_
example	_	_
applications	_	_
of	_	_
the	_	_
noise-removal	_	_
network	_	_
to	_	_
instances	_	_
of	_	_
Poisson	_	_
noise	_	_
applied	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
high-quality	_	_
micrographs	_	_
.	_	_

#193
Enlarged	_	_
64×64	_	_
regions	_	_
from	_	_
the	_	_
top	_	_
left	_	_
of	_	_
each	_	_
crop	_	_
are	_	_
shown	_	_
to	_	_
ease	_	_
comparison	_	_
.	_	_

#194
Original	_	_
Denoised	_	_
Original	_	_
Denoised	_	_
Figure	_	_
8	_	_
:	_	_
Example	_	_
applications	_	_
of	_	_
our	_	_
neural	_	_
network	_	_
to	_	_
512×512	_	_
crops	_	_
from	_	_
scanning	_	_
transmission	_	_
electron	_	_
micrographs	_	_
.	_	_

#195
Enlarged	_	_
64×64	_	_
regions	_	_
from	_	_
the	_	_
top	_	_
left	_	_
of	_	_
each	_	_
crop	_	_
are	_	_
shown	_	_
to	_	_
ease	_	_
comparison	_	_
.	_	_

#196
Our	_	_
network	_	_
has	_	_
not	_	_
been	_	_
trained	_	_
for	_	_
this	_	_
domain	_	_
.	_	_

#197
0	_	_
5	_	_
10	_	_
15	_	_
20	_	_
25	_	_
30	_	_
Batches	_	_
×103	_	_
2.5	_	_
2.0	_	_
1.5	_	_
1.0	_	_
Lo	_	_
g	_	_
1	_	_
0	_	_
(	_	_
M	_	_
SE	_	_
)	_	_
1	_	_
=	_	_
0.2	_	_
1	_	_
=	_	_
0.9	_	_
1	_	_
=	_	_
0.2	_	_
,	_	_
Batch	_	_
Renorm	_	_
1	_	_
=	_	_
0.9	_	_
,	_	_
Batch	_	_
Renorm	_	_
1	_	_
=	_	_
0.2	_	_
,	_	_
ReLU6	_	_
Figure	_	_
9	_	_
:	_	_
Batch	_	_
size	_	_
2	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
learning	_	_
curves	_	_
for	_	_
various	_	_
training	_	_
hyperparameters	_	_
and	_	_
activation	_	_
functions	_	_
.	_	_

#198
Training	_	_
is	_	_
most	_	_
stable	_	_
for	_	_
ADAM	_	_
[	_	_
54	_	_
]	_	_
β1	_	_
=	_	_
0.2	_	_
without	_	_
batch	_	_
renormalization	_	_
[	_	_
75	_	_
]	_	_
and	_	_
with	_	_
ReLU6	_	_
[	_	_
51	_	_
]	_	_
;	_	_
rather	_	_
than	_	_
ReLU	_	_
[	_	_
76	_	_
]	_	_
,	_	_
activation	_	_
.	_	_

#199
other	_	_
options	_	_
or	_	_
use	_	_
default	_	_
values	_	_
we	_	_
have	_	_
chosen	_	_
.	_	_

#200
We	_	_
speed-tested	_	_
our	_	_
network	_	_
by	_	_
applying	_	_
it	_	_
to	_	_
20000	_	_
512×512	_	_
images	_	_
with	_	_
1	_	_
external	_	_
GTX	_	_
1080	_	_
Ti	_	_
GPU	_	_
and	_	_
1	_	_
thread	_	_
of	_	_
a	_	_
i7-6700	_	_
processor	_	_
.	_	_

#201
It	_	_
has	_	_
a	_	_
mean	_	_
batch	_	_
size	_	_
1	_	_
(	_	_
worst	_	_
case	_	_
)	_	_
inference	_	_
time	_	_
of	_	_
77.0	_	_
ms	_	_
.	_	_

#202
It	_	_
also	_	_
takes	_	_
a	_	_
few	_	_
seconds	_	_
to	_	_
load	_	_
before	_	_
it	_	_
is	_	_
ready	_	_
for	_	_
repeated	_	_
use	_	_
.	_	_

#203
5	_	_
Additional	_	_
Experiments	_	_

#204
As	_	_
part	_	_
of	_	_
development	_	_
,	_	_
we	_	_
experimented	_	_
with	_	_
multiple	_	_
architectures	_	_
and	_	_
their	_	_
learning	_	_
policies	_	_
.	_	_

#205
Initially	_	_
,	_	_
we	_	_
experimented	_	_
with	_	_
shallower	_	_
architectures	_	_
similar	_	_
to	_	_
[	_	_
18	_	_
,	_	_
20	_	_
]	_	_
and	_	_
[	_	_
21	_	_
]	_	_
;	_	_
however	_	_
,	_	_
these	_	_
struggled	_	_
to	_	_
meet	_	_
Chambolle’s	_	_
low-dose	_	_
benchmark	_	_
in	_	_
table	_	_
1	_	_
.	_	_

#206
Consequently	_	_
,	_	_
we	_	_
switched	_	_
to	_	_
the	_	_
deeper	_	_
Xception-based	_	_
architecture	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#207
In	_	_
this	_	_
section	_	_
we	_	_
present	_	_
some	_	_
of	_	_
the	_	_
experiments	_	_
we	_	_
performed	_	_
to	_	_
fine-tune	_	_
it	_	_
.	_	_

#208
5.1	_	_
Batch	_	_
Size	_	_
2	_	_

#209
Initial	_	_
experiments	_	_
with	_	_
a	_	_
batch	_	_
size	_	_
of	_	_
2	_	_
are	_	_
summarized	_	_
in	_	_
fig	_	_
.	_	_

#210
9	_	_
.	_	_

#211
This	_	_
includes	_	_
experiments	_	_
with	_	_
different	_	_
decay	_	_
rates	_	_
for	_	_
the	_	_
1st	_	_
moment	_	_
of	_	_
the	_	_
momentum	_	_
,	_	_
β1	_	_
,	_	_
used	_	_
by	_	_
the	_	_
ADAM	_	_
solver	_	_
.	_	_

#212
Our	_	_
results	_	_
show	_	_
that	_	_
training	_	_
converges	_	_
faster	_	_
and	_	_
more	_	_
stably	_	_
for	_	_
β1	_	_
=	_	_
0.2	_	_
than	_	_
the	_	_
recommended	_	_
[	_	_
31	_	_
,	_	_
54	_	_
]	_	_
β1	_	_
=	_	_
0.9.	_	_
β1	_	_
=	_	_
0.9	_	_
being	_	_
too	_	_
high	_	_
seems	_	_
to	_	_
be	_	_
a	_	_
theme	_	_
in	_	_
high	_	_
resolution	_	_
,	_	_
low	_	_
batch	_	_
size	_	_
applications	_	_
e.g.	_	_
[	_	_
79	_	_
]	_	_
.	_	_

#213
Our	_	_
batch	_	_
size	_	_
2	_	_
experiments	_	_
also	_	_
revealed	_	_
that	_	_
training	_	_
is	_	_
faster	_	_
and	_	_
more	_	_
stable	_	_
with	_	_
ReLU6	_	_
[	_	_
51	_	_
]	_	_
than	_	_
with	_	_
0	_	_
20	_	_
40	_	_
60	_	_
80	_	_
100	_	_
120	_	_
Batches	_	_
×103	_	_
3.25	_	_
3.00	_	_
2.75	_	_
2.50	_	_
2.25	_	_
2.00	_	_
1.75	_	_
1.50	_	_
Lo	_	_
g	_	_
1	_	_
0	_	_
(	_	_
M	_	_
SE	_	_
)	_	_
Clipping	_	_
,	_	_
No	_	_
SSIM	_	_
-	_	_
Train	_	_
Clipping	_	_
,	_	_
No	_	_
SSIM	_	_
-	_	_
Val	_	_
No	_	_
Clipping	_	_
,	_	_
SSIM	_	_
-	_	_
Train	_	_
No	_	_
Clipping	_	_
,	_	_
SSIM	_	_
-	_	_
Val	_	_
No	_	_
Clipping	_	_
,	_	_
2.5	_	_
SSIM	_	_
-	_	_
Train	_	_
No	_	_
Clipping	_	_
,	_	_
2.5	_	_
SSIM	_	_
-	_	_
Val	_	_
No	_	_
Clipping	_	_
,	_	_
RMSProp	_	_
,	_	_
No	_	_
SSIM	_	_
-	_	_
Train	_	_
No	_	_
Clipping	_	_
,	_	_
RMSProp	_	_
,	_	_
No	_	_
SSIM	_	_
-	_	_
Val	_	_
Figure	_	_
10	_	_
:	_	_
Batch	_	_
size	_	_
10	_	_
mean	_	_
squared	_	_
error	_	_
(	_	_
MSE	_	_
)	_	_
learning	_	_
curves	_	_
for	_	_
training	_	_
with	_	_
and	_	_
without	_	_
clipping	_	_
,	_	_
extra	_	_
structural	_	_
similarity	_	_
index	_	_
[	_	_
77	_	_
]	_	_
(	_	_
SSIM	_	_
)	_	_
based	_	_
losses	_	_
and	_	_
RMSProp	_	_
[	_	_
78	_	_
]	_	_
rather	_	_
than	_	_
ADAM	_	_
[	_	_
54	_	_
]	_	_
optimization	_	_
.	_	_

#214
ReLU	_	_
[	_	_
76	_	_
]	_	_
activation	_	_
.	_	_

#215
We	_	_
think	_	_
this	_	_
is	_	_
because	_	_
ReLU6	_	_
is	_	_
especially	_	_
effective	_	_
in	_	_
the	_	_
decoder	_	_
where	_	_
it	_	_
does	_	_
not	_	_
allow	_	_
sparse	_	_
activations	_	_
to	_	_
grow	_	_
unbounded	_	_
.	_	_

#216
Only	_	_
ReLU-based	_	_
activation	_	_
functions	_	_
were	_	_
considered	_	_
as	_	_
the	_	_
non-saturation	_	_
of	_	_
their	_	_
gradients	_	_
accelerates	_	_
the	_	_
convergence	_	_
of	_	_
stochastic	_	_
gradient	_	_
descent	_	_
[	_	_
80	_	_
]	_	_
.	_	_

#217
Batch	_	_
renormalization	_	_
[	_	_
75	_	_
]	_	_
was	_	_
trailed	_	_
as	_	_
our	_	_
limited	_	_
hardware	_	_
made	_	_
large	_	_
batch	_	_
sizes	_	_
prohibitive	_	_
.	_	_

#218
We	_	_
also	_	_
wanted	_	_
to	_	_
test	_	_
how	_	_
well	_	_
it	_	_
would	_	_
work	_	_
with	_	_
batch	_	_
size	_	_
2	_	_
as	_	_
we	_	_
were	_	_
unable	_	_
to	_	_
find	_	_
quantitative	_	_
information	_	_
for	_	_
this	_	_
batch	_	_
size	_	_
.	_	_

#219
It	_	_
is	_	_
ineffective	_	_
:	_	_
Fig.	_	_
9	_	_
shows	_	_
that	_	_
the	_	_
extra	_	_
trainable	_	_
variables	_	_
it	_	_
introduces	_	_
destabilize	_	_
training	_	_
and	_	_
reduce	_	_
convergence	_	_
.	_	_

#220
This	_	_
is	_	_
not	_	_
surprising	_	_
as	_	_
it	_	_
is	_	_
not	_	_
designed	_	_
for	_	_
batches	_	_
this	_	_
small	_	_
and	_	_
can	_	_
take	_	_
time	_	_
to	_	_
become	_	_
effective	_	_
.	_	_

#221
Additionally	_	_
,	_	_
instability	_	_
introduced	_	_
by	_	_
applying	_	_
different	_	_
amounts	_	_
noise	_	_
to	_	_
training	_	_
examples	_	_
may	_	_
be	_	_
adding	_	_
to	_	_
the	_	_
inherent	_	_
batch	_	_
renormalization	_	_
instability	_	_
,	_	_
making	_	_
it	_	_
much	_	_
worse	_	_
than	_	_
it	_	_
might	options	_
be	_	_
for	_	_
other	_	_
optimizations	_	_
.	_	_

#222
5.2	_	_
Batch	_	_
Size	_	_
10	_	_

#223
We	_	_
tried	_	_
adding	_	_
multiples	_	_
of	_	_
the	_	_
distance	_	_
from	_	_
the	_	_
maximum	_	_
SSIM	_	_
,	_	_
1.0−SSIM	_	_
,	_	_
to	_	_
the	_	_
training	_	_
loss	_	_
to	_	_
optimize	_	_
our	_	_
network	_	_
for	_	_
an	_	_
additional	_	_
metric	_	_
.	_	_

#224
As	_	_
shown	_	_
by	_	_
fig	_	_
.	_	_

#225
10	_	_
,	_	_
adding	_	_
different	_	_
multiples	_	_
does	_	_
not	_	_
have	_	_
a	_	_
significant	_	_
effect	_	_
on	_	_
the	_	_
rate	_	_
of	_	_
MSE	_	_
convergence	_	_
.	_	_

#226
Regardless	_	_
,	_	_
we	_	_
decided	_	_
against	_	_
adding	_	_
this	_	_
loss	_	_
as	_	_
SSIMs	_	_
are	_	_
weighted	_	_
to	_	_
measure	_	_
a	_	_
human	_	_
perception	_	_
of	_	_
quality	_	_
and	_	_
we	_	_
did	_	_
not	_	_
want	_	_
to	_	_
introduce	_	_
this	_	_
slight	_	_
bias	_	_
.	_	_

#227
Nevertheless	_	_
,	_	_
we	_	_
have	_	_
shown	_	_
that	_	_
additionally	_	_
optimizing	_	_
the	_	_
SSIM	_	_
does	_	_
not	_	_
significantly	_	_
affect	_	_
training	_	_
.	_	_

#228
Outputs	_	_
were	_	_
clipped	_	_
between	_	_
0.0	_	_
and	_	_
1.0	_	_
before	_	_
calculating	_	_
training	_	_
losses	_	_
in	_	_
all	_	_
of	_	_
our	_	_
batch	_	_
size	_	_
2	_	_
and	_	_
the	_	_
first	_	_
of	_	_
our	_	_
batch	_	_
size	_	_
10	_	_
experiments	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#229
10	_	_
.	_	_

#230
Removing	_	_
this	_	_
clipping	_	_
significantly	_	_
decreases	_	_
the	_	_
rate	_	_
of	_	_
convergence	_	_
.	_	_

#231
Nonetheless	_	_
,	_	_
we	_	_
decided	_	_
to	_	_
remove	_	_
clipping	_	_
as	_	_
the	_	_
network	_	_
was	_	_
especially	_	_
prone	_	_
to	_	_
producing	_	_
artifacts	_	_
close	_	_
to	_	_
0.0	_	_
and	_	_
1.0	_	_
.	_	_

#232
This	_	_
ensured	_	_
that	_	_
these	_	_
artifacts	_	_
were	_	_
not	_	_
present	_	_
in	_	_
the	_	_
fully	_	_
trained	_	_
network’s	_	_
outputs	_	_
.	_	_

#233
Following	_	_
the	_	_
success	_	_
of	_	_
lower	_	_
β1	_	_
with	_	_
the	_	_
ADAM	_	_
optimizer	_	_
,	_	_
we	_	_
tried	_	_
other	_	_
momentum-based	_	_
optimizers	_	_
.	_	_

#234
The	_	_
start	_	_
of	_	_
a	_	_
learning	_	_
curve	_	_
for	_	_
simple	_	_
RMSProp	_	_
[	_	_
78	_	_
]	_	_
is	_	_
shown	_	_
in	_	_
fig	_	_
.	_	_

#235
10	_	_
.	_	_

#236
It	_	_
shows	_	_
that	_	_
training	_	_
is	_	_
significantly	_	_
less	_	_
stable	_	_
and	_	_
has	_	_
a	_	_
lower	_	_
rate	_	_
of	_	_
convergence	_	_
.	_	_

#237
We	_	_
also	_	_
experimented	_	_
with	_	_
Nesterov-accelerated	_	_
gradient	_	_
descent	_	_
[	_	_
81	_	_
,	_	_
82	_	_
]	_	_
with	_	_
similar	_	_
results	_	_
.	_	_

#238
6	_	_
Summary	_	_

#239
•	_	_
We	_	_
have	_	_
developed	_	_
a	_	_
deep	_	_
neural	_	_
network	_	_
for	_	_
electron	_	_
micrograph	_	_
denoising	_	_
and	_	_
shown	_	_
that	_	_
it	_	_
outperforms	_	_
existing	_	_
methods	_	_
for	_	_
low	_	_
and	_	_
ordinary	_	_
electron	_	_
doses	_	_
.	_	_

#240
•	_	_
Fully	_	_
trained	_	_
versions	_	_
of	_	_
our	_	_
network	_	_
have	_	_
been	_	_
made	_	_
available	_	_
for	_	_
low	_	_
and	_	_
ordinary	_	_
doses	_	_
with	_	_
example	_	_
usage	_	_
:	_	_
https	_	_
:	_	_
//github.com/Jeffrey-Ede/	_	_
Electron-Micrograph-Denoiser	_	_
•	_	_
A	_	_
new	_	_
dataset	_	_
of	_	_
17267	_	_
2048×2048	_	_
high-quality	_	_
micrographs	_	_
collected	_	_
to	_	_
train	_	_
our	_	_
network	_	_
has	_	_
been	_	_
made	_	_
publicly	_	_
available	_	_
.	_	_

#241
•	_	_
Example	_	_
applications	_	_
of	_	_
our	_	_
network	_	_
to	_	_
noisy	_	_
electron	_	_
micrographs	_	_
are	_	_
presented	_	_
.	_	_

#242
We	_	_
also	_	_
present	_	_
example	_	_
application	_	_
to	_	_
STEM	_	_
images	_	_
to	_	_
show	_	_
that	_	_
our	_	_
network	_	_
can	_	_
generalize	_	_
to	_	_
other	_	_
domains	_	_
.	_	_

#243
•	_	_
Our	_	_
network	_	_
architecture	_	_
,	_	_
training	_	_
hyperparameters	_	_
and	_	_
learning	_	_
protocols	_	_
are	_	_
detailed	_	_
.	_	_

#244
In	_	_
addition	_	_
,	_	_
details	_	_
of	_	_
several	_	_
of	_	_
our	_	_
initial	_	_
architecture	_	_
and	_	_
learning	_	_
policy	_	_
experiments	_	_
are	_	_
presented	_	_
.	_	_