#0
Zero-Shot	_	_
Object	_	_
Detection	_	_
:	_	_
Learning	_	_
to	_	_
Simultaneously	_	_
Recognize	_	_
and	_	_
Localize	_	_
Novel	_	_
Concepts	_	_
Shafin	_	_
Rahman†	_	_
?	_	_
,	_	_
Salman	_	_
Khan	_	_
?	_	_
†	_	_
and	_	_
Fatih	_	_
Porikli†	_	_
†Australian	_	_
National	_	_
University	_	_
?	_	_
DATA61	_	_
,	_	_
CSIRO	_	_
Abstract	_	_
.	_	_

#1
Current	_	_
Zero-Shot	_	_
Learning	_	_
(	_	_
ZSL	_	_
)	_	_
approaches	_	_
are	_	_
restricted	_	_
to	_	_
recognition	_	_
of	_	_
a	_	_
single	_	_
dominant	_	_
unseen	_	_
object	_	_
category	_	_
in	_	_
a	_	_
test	_	_
image	_	_
.	_	_

#2
We	_	_
hypothesize	_	_
that	_	_
this	_	_
setting	_	_
is	_	_
ill-suited	_	_
for	_	_
real-world	_	_
applications	_	_
where	_	_
unseen	_	_
objects	_	_
appear	_	_
only	_	_
as	_	_
a	_	_
part	_	_
of	_	_
a	_	_
complex	_	_
scene	_	_
,	_	_
warranting	_	_
both	_	_
the	_	_
‘recognition’	_	_
and	_	_
‘localization’	_	_
of	_	_
an	_	_
unseen	_	_
category	_	_
.	_	_

#3
To	_	_
address	_	_
this	_	_
limitation	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
new	_	_
‘Zero-Shot	_	_
Detection’	_	_
(	_	_
ZSD	_	_
)	_	_
problem	_	_
setting	_	_
,	_	_
which	_	_
aims	_	_
at	_	_
simultaneously	_	_
recognizing	_	_
and	_	_
locating	_	_
object	_	_
instances	_	_
belonging	_	_
to	_	_
novel	_	_
categories	_	_
without	_	_
any	_	_
training	_	_
examples	_	_
.	_	_

#4
We	_	_
also	_	_
propose	_	_
a	_	_
new	_	_
experimental	_	_
protocol	_	_
for	_	_
ZSD	_	_
based	_	_
on	_	_
the	_	_
highly	_	_
challenging	_	_
ILSVRC	_	_
dataset	_	_
,	_	_
adhering	_	_
to	_	_
practical	_	_
issues	_	_
,	_	_
e.g.	_	_
,	_	_
the	_	_
rarity	_	_
of	_	_
unseen	_	_
objects	_	_
.	_	_

#5
To	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
this	_	_
is	_	_
the	_	_
first	_	_
end-to-end	_	_
deep	_	_
network	_	_
for	_	_
ZSD	_	_
that	_	_
jointly	_	_
models	_	_
the	_	_
interplay	_	_
between	_	_
visual	_	_
and	_	_
semantic	_	_
domain	_	_
information	_	_
.	_	_

#6
To	_	_
overcome	_	_
the	_	_
noise	_	_
in	_	_
the	_	_
automatically	_	_
derived	_	_
semantic	_	_
descriptions	_	_
,	_	_
we	_	_
utilize	_	_
the	_	_
concept	_	_
of	_	_
meta-classes	_	_
to	_	_
design	_	_
an	_	_
original	_	_
loss	_	_
function	_	_
that	_	_
achieves	_	_
synergy	_	_
between	_	_
max-margin	_	_
class	_	_
separation	_	_
and	_	_
semantic	_	_
space	_	_
clustering	_	_
.	_	_

#7
Furthermore	_	_
,	_	_
we	_	_
present	_	_
a	_	_
baseline	_	_
approach	_	_
extended	_	_
from	_	_
recognition	_	_
to	_	_
detection	_	_
setting	_	_
.	_	_

#8
Our	_	_
extensive	_	_
experiments	_	_
show	_	_
significant	_	_
performance	_	_
boost	_	_
over	_	_
the	_	_
baseline	_	_
on	_	_
the	_	_
imperative	_	_
yet	_	_
difficult	_	_
ZSD	_	_
problem	_	_
.	_	_

#9
Keywords	_	_
:	_	_
Zero-shot	_	_
learning	_	_
,	_	_
Object	_	_
detection	_	_
,	_	_
Zero-shot	_	_
detection	_	_

#10
1	_	_
Introduction	_	_

#11
Since	_	_
its	_	_
inception	_	_
,	_	_
zero-shot	_	_
learning	_	_
research	_	_
has	_	_
been	_	_
dominated	_	_
by	_	_
the	_	_
object	_	_
classification	_	_
problem	_	_
[	_	_
2,5,10,18,21,22,29,33,39,47,51,52,53	_	_
]	_	_
.	_	_

#12
Although	_	_
it	_	_
still	_	_
remains	_	_
as	_	_
a	_	_
challenging	_	_
task	_	_
,	_	_
the	_	_
zero-shot	_	_
recognition	_	_
has	_	_
a	_	_
number	_	_
of	_	_
limitations	_	_
that	_	_
render	_	_
it	_	_
unusable	_	_
in	_	_
real-life	_	_
scenarios	_	_
.	_	_

#13
First	_	_
,	_	_
it	_	_
is	_	_
destined	_	_
to	_	_
work	_	_
for	_	_
simpler	_	_
cases	_	_
where	_	_
only	_	_
a	_	_
single	_	_
dominant	_	_
object	_	_
is	_	_
present	_	_
in	_	_
an	_	_
image	_	_
.	_	_

#14
Second	_	_
,	_	_
the	_	_
attributes	_	_
and	_	_
semantic	_	_
descriptions	_	_
are	_	_
relevant	_	_
to	_	_
individual	_	_
objects	_	_
instead	_	_
of	_	_
the	_	_
entire	_	_
scene	_	_
composition	_	_
.	_	_

#15
Third	_	_
,	_	_
zero-shot	_	_
recognition	_	_
provides	_	_
an	_	_
answer	_	_
to	_	_
unseen	_	_
categories	_	_
in	_	_
elementary	_	_
tasks	_	_
,	_	_
e.g.	_	_
,	_	_
classification	_	_
and	_	_
retrieval	_	_
,	_	_
yet	_	_
it	_	_
is	_	_
unable	_	_
to	_	_
scale	_	_
to	_	_
advanced	_	_
tasks	_	_
such	_	_
as	_	_
scene	_	_
interpretation	_	_
and	_	_
contextual	_	_
modeling	_	_
,	_	_
which	_	_
require	_	_
a	_	_
fundamental	_	_
reasoning	_	_
about	_	_
all	_	_
salient	_	_
objects	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#16
9v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
6	_	_
M	_	_
ar	_	_
2	_	_

#17
2	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#18
Fig.	_	_
1	_	_
.	_	_

#19
ZSD	_	_
deals	_	_
with	_	_
a	_	_
more	_	_
complex	_	_
label	_	_
space	_	_
(	_	_
object	_	_
labels	_	_
and	_	_
locations	_	_
)	_	_
with	_	_
considerably	_	_
less	_	_
supervision	_	_
(	_	_
i.e.	_	_
,	_	_
no	_	_
examples	_	_
of	_	_
unseen	_	_
classes	_	_
)	_	_
.	_	_

#20
(	_	_
a	_	_
)	_	_
Traditional	_	_
recognition	_	_
task	_	_
only	_	_
predicts	_	_
seen	_	_
class	_	_
labels	_	_
.	_	_

#21
(	_	_
b	_	_
)	_	_
Traditional	_	_
detection	_	_
task	_	_
predicts	_	_
both	_	_
seen	_	_
class	_	_
labels	_	_
and	_	_
bounding	_	_
boxes	_	_
.	_	_

#22
(	_	_
c	_	_
)	_	_
Traditional	_	_
zero-shot	_	_
recognition	_	_
task	_	_
only	_	_
predicts	_	_
unseen	_	_
class	_	_
labels	_	_
.	_	_

#23
(	_	_
d	_	_
)	_	_
The	_	_
proposed	_	_
ZSD	_	_
predicts	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
and	_	_
their	_	_
bounding	_	_
boxes	_	_
.	_	_

#24
in	_	_
the	_	_
scene	_	_
.	_	_

#25
Fourth	_	_
,	_	_
global	_	_
attributes	_	_
are	_	_
more	_	_
susceptible	_	_
to	_	_
background	_	_
variations	_	_
,	_	_
viewpoint	_	_
,	_	_
appearance	_	_
and	_	_
scale	_	_
changes	_	_
and	_	_
practical	_	_
factors	_	_
such	_	_
as	_	_
occlusions	_	_
and	_	_
clutter	_	_
.	_	_

#26
As	_	_
a	_	_
result	_	_
,	_	_
image-level	_	_
ZSL	_	_
fails	_	_
for	_	_
the	_	_
case	_	_
of	_	_
complex	_	_
scenes	_	_
where	_	_
a	_	_
diverse	_	_
set	_	_
of	_	_
competing	_	_
attributes	_	_
that	_	_
do	_	_
not	_	_
belong	_	_
to	_	_
a	_	_
single	_	_
image-level	_	_
category	_	_
would	_	_
exist	_	_
.	_	_

#27
To	_	_
address	_	_
these	_	_
challenges	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
new	_	_
problem	_	_
setting	_	_
called	_	_
the	_	_
zero-shot	_	_
object	_	_
detection	_	_
.	_	_

#28
As	_	_
illustrated	_	_
in	_	_
Fig.	_	_
1	_	_
,	_	_
instead	_	_
of	_	_
merely	_	_
classifying	_	_
images	_	_
,	_	_
our	_	_
goal	_	_
is	_	_
to	_	_
simultaneously	_	_
detect	_	_
and	_	_
localize	_	_
each	_	_
individual	_	_
instance	_	_
of	_	_
new	_	_
object	_	_
classes	_	_
,	_	_
even	_	_
in	_	_
the	_	_
absence	_	_
of	_	_
any	_	_
visual	_	_
examples	_	_
of	_	_
those	_	_
classes	_	_
during	_	_
the	_	_
training	_	_
phase	_	_
.	_	_

#29
In	_	_
this	_	_
regard	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
new	_	_
zero-shot	_	_
detection	_	_
protocol	_	_
built	_	_
on	_	_
top	_	_
of	_	_
the	_	_
ILSVRC	_	_
-	_	_
Object	_	_
Detection	_	_
Challenge	_	_
[	_	_
40	_	_
]	_	_
.	_	_

#30
The	_	_
resulting	_	_
dataset	_	_
is	_	_
very	_	_
demanding	_	_
due	_	_
to	_	_
its	_	_
large	_	_
scale	_	_
,	_	_
diversity	_	_
,	_	_
and	_	_
unconstrained	_	_
nature	_	_
,	_	_
and	_	_
also	_	_
unique	_	_
due	_	_
to	_	_
its	_	_
leveraging	_	_
on	_	_
WordNet	_	_
semantic	_	_
hierarchy	_	_
[	_	_
31	_	_
]	_	_
.	_	_

#31
Taking	_	_
advantage	_	_
of	_	_
semantic	_	_
relationships	_	_
between	_	_
object	_	_
classes	_	_
,	_	_
we	_	_
use	_	_
the	_	_
concept	_	_
of	_	_
‘meta-classes’1	_	_
and	_	_
introduce	_	_
a	_	_
novel	_	_
approach	_	_
to	_	_
update	_	_
the	_	_
semantic	_	_
embeddings	_	_
automatically	_	_
.	_	_

#32
Raw	_	_
semantic	_	_
embeddings	_	_
are	_	_
learned	_	_
in	_	_
an	_	_
unsupervised	_	_
manner	_	_
using	_	_
text	_	_
mining	_	_
and	_	_
therefore	_	_
they	_	_
have	_	_
considerable	_	_
noise	_	_
.	_	_

#33
Our	_	_
optimization	_	_
of	_	_
the	_	_
class	_	_
embeddings	_	_
proves	_	_
to	_	_
be	_	_
an	_	_
effective	_	_
way	_	_
to	_	_
reduce	_	_
this	_	_
noise	_	_
and	_	_
learn	_	_
robust	_	_
semantic	_	_
representations	_	_
.	_	_

#34
ZSD	_	_
has	_	_
numerous	_	_
applications	_	_
in	_	_
novel	_	_
object	_	_
localization	_	_
,	_	_
retrieval	_	_
,	_	_
tracking	_	_
,	_	_
and	_	_
reasoning	_	_
about	_	_
object’s	_	_
relationships	_	_
with	_	_
its	_	_
environment	_	_
using	_	_
only	_	_
available	_	_
semantics	_	_
,	_	_
e.g.	_	_
,	_	_
an	_	_
object	_	_
name	_	_
or	_	_
a	_	_
natural	_	_
language	_	_
description	_	_
.	_	_

#35
Although	_	_
a	_	_
critical	_	_
problem	_	_
,	_	_
ZSD	_	_
is	_	_
remarkably	_	_
difficult	_	_
compared	_	_
to	_	_
its	_	_
classification	_	_
counterpart	_	_
.	_	_

#36
While	_	_
the	_	_
zero-shot	_	_
recognition	_	_
problem	_	_
assumes	_	_
only	_	_
a	_	_
single	_	_
primary	_	_
object	_	_
in	_	_
an	_	_
image	_	_
and	_	_
attempts	_	_
to	_	_
predict	_	_
its	_	_
category	_	_
,	_	_
the	_	_
ZSD	_	_
task	_	_
has	_	_
to	_	_
predict	_	_
both	_	_
the	_	_
multi-class	_	_
category	_	_
label	_	_
and	_	_
precise	_	_
location	_	_
of	_	_
each	_	_
instance	_	_
in	_	_
the	_	_
given	_	_
image	_	_
.	_	_

#37
Since	_	_
there	_	_
can	_	_
be	_	_
a	_	_
prohibitively	_	_
huge	_	_
number	_	_
of	_	_
possible	_	_
locations	_	_
for	_	_
each	_	_
object	_	_
in	_	_
an	_	_
image	_	_
and	_	_
because	_	_
the	_	_
semantic	_	_
class	_	_
descriptions	_	_
are	_	_
noisy	_	_
,	_	_
a	_	_
detection	_	_
approach	_	_
is	_	_
much	_	_
more	_	_
susceptible	_	_
to	_	_
incorrect	_	_
predictions	_	_
compared	_	_
to	_	_
classification	_	_
.	_	_

#38
Therefore	_	_
,	_	_
it	_	_
would	_	_
be	_	_
expected	_	_
that	_	_
a	_	_
ZSD	_	_
method	_	_
predicts	_	_
a	_	_
class	_	_
label	_	_
that	_	_
might	options	_
be	_	_
incorrect	_	_
but	_	_
visually	_	_
and	_	_
semantically	_	_
similar	_	_
to	_	_
the	_	_
corresponding	_	_
true	_	_
class	_	_
.	_	_

#39
For	_	_
example	_	_
,	_	_
wrongly	_	_
predicting	_	_
a	_	_
‘spider’	_	_
as	_	_
‘scorpion’	_	_
where	_	_
both	_	_
are	_	_
semantically	_	_
similar	_	_
because	_	_
of	_	_
being	_	_
invertebrates	_	_
.	_	_

#40
To	_	_
address	_	_
this	_	_
issue	_	_
,	_	_
we	_	_
relax	_	_
the	_	_
original	_	_
detection	_	_
problem	_	_
to	_	_
independently	_	_
study	_	_
the	_	_
confusions	_	_
emanating	_	_
from	_	_
the	_	_
visual	_	_
and	_	_
semantic	_	_
resemblance	_	_
between	_	_
closely	_	_
linked	_	_
classes	_	_
.	_	_

#41
For	_	_
this	_	_
purpose	_	_
,	_	_
alongside	_	_
the	_	_
ZSD	_	_
,	_	_
we	_	_
evaluate	_	_
on	_	_
zero-shot	_	_
meta-class	_	_
detection	_	_
,	_	_
zero-shot	_	_
tagging	_	_
,	_	_
and	_	_
zero-shot	_	_
meta	_	_
class	_	_
tagging	_	_
.	_	_

#42
Notably	_	_
,	_	_
the	_	_
proposed	_	_
network	_	_
is	_	_
trained	_	_
only	_	_
‘once’	_	_
for	_	_
ZSD	_	_
task	_	_
and	_	_
the	_	_
additional	_	_
tasks	_	_
are	_	_
used	_	_
during	_	_
evaluations	_	_
only	_	_
.	_	_

#43
Although	_	_
deep	_	_
network	_	_
based	_	_
solutions	_	_
have	_	_
been	_	_
proposed	_	_
for	_	_
zero-shot	_	_
recognition	_	_
[	_	_
10,22,51	_	_
]	_	_
,	_	_
to	_	_
the	_	_
best	_	_
of	_	_
our	_	_
knowledge	_	_
,	_	_
we	_	_
propose	_	_
the	_	_
first	_	_
end-to-end	_	_
trainable	_	_
network	_	_
for	_	_
the	_	_
ZSD	_	_
problem	_	_
that	_	_
concurrently	_	_
relates	_	_
visual	_	_
image	_	_
features	_	_
with	_	_
the	_	_
semantic	_	_
label	_	_
information	_	_
.	_	_

#44
This	_	_
network	_	_
considers	_	_
semantic	_	_
embedding	_	_
vector	_	_
of	_	_
classes	_	_
as	_	_
a	_	_
fixed	_	_
embedding	_	_
within	_	_
the	_	_
network	_	_
to	_	_
produce	_	_
prediction	_	_
scores	_	_
for	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
.	_	_

#45
We	_	_
propose	_	_
a	_	_
novel	_	_
loss	_	_
formulation	_	_
that	_	_
incorporates	_	_
max-margin	_	_
learning	_	_
[	_	_
53	_	_
]	_	_
and	_	_
a	_	_
semantic	_	_
clustering	_	_
loss	_	_
based	_	_
on	_	_
class-scores	_	_
of	_	_
different	_	_
meta-classes	_	_
.	_	_

#46
While	_	_
the	_	_
max-margin	_	_
loss	_	_
tries	_	_
to	_	_
separate	_	_
individual	_	_
classes	_	_
,	_	_
semantic	_	_
clustering	_	_
loss	_	_
tries	_	_
to	_	_
reduce	_	_
the	_	_
noise	_	_
in	_	_
semantic	_	_
vectors	_	_
by	_	_
positioning	_	_
similar	_	_
classes	_	_
together	_	_
and	_	_
dis-similar	_	_
classes	_	_
far	_	_
apart	_	_
.	_	_

#47
Notably	_	_
,	_	_
our	_	_
proposed	_	_
formulation	_	_
assumes	_	_
predefined	_	_
unseen	_	_
classes	_	_
to	_	_
explore	_	_
the	_	_
semantic	_	_
relationships	_	_
during	_	_
model	_	_
learning	_	_
phase	_	_
.	_	_

#48
This	_	_
assumption	_	_
is	_	_
consistent	_	_
with	_	_
recent	_	_
efforts	_	_
in	_	_
the	_	_
literature	_	_
which	_	_
consider	_	_
class	_	_
semantics	_	_
to	_	_
solve	_	_
the	_	_
domain	_	_
shift	_	_
problem	_	_
in	_	_
ZSL	_	_
[	_	_
7,12	_	_
]	_	_
and	_	_
does	_	_
not	_	_
a	_	_
constitute	_	_
transductive	_	_
setting	_	_
[	_	_
8,11,18	_	_
]	_	_
.	_	_

#49
Based	_	_
on	_	_
the	_	_
premise	_	_
that	_	_
unseen	_	_
class	_	_
semantics	_	_
may	_	_
be	_	_
unknown	_	_
during	_	_
training	_	_
in	_	_
several	_	_
practical	_	_
zero-shot	_	_
scenarios	_	_
,	_	_
we	_	_
also	_	_
propose	_	_
a	_	_
variant	_	_
of	_	_
our	_	_
approach	_	_
that	_	_
can	_	_
be	_	_
trained	_	_
without	_	_
predefined	_	_
unseen	_	_
classes	_	_
.	_	_

#50
Finally	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
comparison	_	_
method	_	_
for	_	_
ZSD	_	_
by	_	_
extending	_	_
a	_	_
popular	_	_
zero-shot	_	_
recognition	_	_
framework	_	_
named	_	_
ConSE	_	_
[	_	_
33	_	_
]	_	_
using	_	_
Faster-RCNN	_	_
[	_	_
38	_	_
]	_	_
.	_	_

#51
In	_	_
summary	_	_
,	_	_
this	_	_
paper	_	_
reports	_	_
the	_	_
following	_	_
advances	_	_
:	_	_
–	_	_
We	_	_
introduce	_	_
a	_	_
new	_	_
problem	_	_
for	_	_
zero-shot	_	_
learning	_	_
,	_	_
which	_	_
aims	_	_
to	_	_
jointly	_	_
recognize	_	_
and	_	_
localize	_	_
novel	_	_
objects	_	_
in	_	_
complex	_	_
scenes	_	_
.	_	_

#52
–	_	_
We	_	_
present	_	_
a	_	_
new	_	_
experimental	_	_
protocol	_	_
and	_	_
design	_	_
a	_	_
novel	_	_
baseline	_	_
solution	_	_
extended	_	_
from	_	_
conventional	_	_
recognition	_	_
to	_	_
the	_	_
detection	_	_
task	_	_
.	_	_

#53
–	_	_
We	_	_
propose	_	_
an	_	_
end-to-end	_	_
trainable	_	_
deep	_	_
architecture	_	_
that	_	_
simultaneously	_	_
considers	_	_
both	_	_
visual	_	_
and	_	_
semantic	_	_
information	_	_
.	_	_

#54
–	_	_
We	_	_
design	_	_
a	_	_
novel	_	_
loss	_	_
function	_	_
that	_	_
achieves	_	_
synergistic	_	_
effects	_	_
for	_	_
max-margin	_	_
class	_	_
separation	_	_
and	_	_
semantic	_	_
clustering	_	_
based	_	_
on	_	_
meta-classes	_	_
.	_	_

#55
Beside	_	_
that	_	_
,	_	_
our	_	_
approach	_	_
can	_	_
automatically	_	_
tune	_	_
noisy	_	_
semantic	_	_
embeddings	_	_
.	_	_

#56
2	_	_
Problem	_	_
Description	_	_

#57
Given	_	_
a	_	_
set	_	_
of	_	_
images	_	_
for	_	_
seen	_	_
object	_	_
categories	_	_
,	_	_
ZSD	_	_
aims	_	_
at	_	_
the	_	_
recognition	_	_
and	_	_
localization	_	_
of	_	_
previously	_	_
unseen	_	_
object	_	_
categories	_	_
.	_	_

#58
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
formally	_	_

#59
4	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#60
describe	_	_
the	_	_
ZSD	_	_
problem	_	_
and	_	_
its	_	_
associated	_	_
challenges	_	_
.	_	_

#61
We	_	_
also	_	_
introduce	_	_
variants	_	_
of	_	_
the	_	_
detection	_	_
task	_	_
,	_	_
which	_	_
are	_	_
natural	_	_
extensions	_	_
of	_	_
the	_	_
original	_	_
problem	_	_
.	_	_

#62
First	_	_
,	_	_
we	_	_
describe	_	_
the	_	_
notations	_	_
used	_	_
in	_	_
the	_	_
following	_	_
discussion	_	_
.	_	_

#63
Preliminaries	_	_
:	_	_
Consider	_	_
a	_	_
set	_	_
of	_	_
‘seen’	_	_
classes	_	_
denoted	_	_
by	_	_
S	_	_
=	_	_
{	_	_
1	_	_
,	_	_
.	_	_

#64
.	_	_

#65
.	_	_

#66
,	_	_
S	_	_
}	_	_
,	_	_
whose	_	_
examples	_	_
are	_	_
available	_	_
during	_	_
the	_	_
training	_	_
stage	_	_
and	_	_
S	_	_
represents	_	_
their	_	_
total	_	_
number	_	_
.	_	_

#67
There	_	_
exists	_	_
another	_	_
set	_	_
of	_	_
‘unseen’	_	_
classes	_	_
U	_	_
=	_	_
{	_	_
S+1	_	_
,	_	_
.	_	_

#68
.	_	_

#69
.	_	_

#70
,	_	_
S+U	_	_
}	_	_
,	_	_
whose	_	_
instances	_	_
are	_	_
only	_	_
available	_	_
during	_	_
the	_	_
test	_	_
phase	_	_
.	_	_

#71
We	_	_
denote	_	_
the	_	_
set	_	_
of	_	_
all	_	_
object	_	_
classes	_	_
by	_	_
C	_	_
=	_	_
S	_	_
∪	_	_
U	_	_
,	_	_
such	_	_
that	_	_
C	_	_
=	_	_
S	_	_
+	_	_
U	_	_
denote	_	_
the	_	_
cardinality	_	_
of	_	_
the	_	_
label	_	_
space	_	_
.	_	_

#72
We	_	_
define	_	_
a	_	_
set	_	_
of	_	_
meta	_	_
(	_	_
or	_	_
super	_	_
)	_	_
classes	_	_
by	_	_
grouping	_	_
similar	_	_
object	_	_
classes	_	_
into	_	_
a	_	_
single	_	_
meta	_	_
category	_	_
.	_	_

#73
These	_	_
meta-classes	_	_
are	_	_
denoted	_	_
by	_	_
M	_	_
=	_	_
{	_	_
zm	_	_
:	_	_
m	_	_
∈	_	_
[	_	_
1	_	_
,	_	_
M	_	_
]	_	_
}	_	_
,	_	_
where	_	_
M	_	_
denote	_	_
the	_	_
total	_	_
number	_	_
of	_	_
meta-classes	_	_
and	_	_
zm	_	_
=	_	_
{	_	_
k	_	_
∈	_	_
C	_	_
s.t.	_	_
,	_	_
g	_	_
(	_	_
k	_	_
)	_	_
=	_	_
m	_	_
}	_	_
.	_	_

#74
Here	_	_
,	_	_
g	_	_
(	_	_
k	_	_
)	_	_
is	_	_
a	_	_
mapping	_	_
function	_	_
which	_	_
maps	_	_
each	_	_
class	_	_
k	_	_
to	_	_
its	_	_
corresponding	_	_
meta-class	_	_
zg	_	_
(	_	_
k	_	_
)	_	_
.	_	_

#75
Note	_	_
that	_	_
the	_	_
meta-classes	_	_
are	_	_
mutually	_	_
exclusive	_	_
i.e.	_	_
,	_	_
∩Mm=1zm	_	_
=	_	_
φ	_	_
and	_	_
∪Mm=1zm	_	_
=	_	_
C.	_	_
The	_	_
set	_	_
of	_	_
all	_	_
training	_	_
images	_	_
is	_	_
denoted	_	_
by	_	_
X	_	_
s	_	_
,	_	_
which	_	_
contains	_	_
examples	_	_
of	_	_
all	_	_
seen	_	_
object	_	_
classes	_	_
.	_	_

#76
The	_	_
set	_	_
of	_	_
all	_	_
test	_	_
images	_	_
containing	_	_
samples	_	_
of	_	_
unseen	_	_
object	_	_
classes	_	_
is	_	_
denoted	_	_
by	_	_
X	_	_
u	_	_
.	_	_

#77
Each	_	_
test	_	_
image	_	_
x	_	_
∈	_	_
X	_	_
u	_	_
contains	_	_
at	_	_
least	_	_
one	_	_
instance	_	_
of	_	_
an	_	_
unseen	_	_
class	_	_
.	_	_

#78
Notably	_	_
,	_	_
no	_	_
unseen	_	_
class	_	_
object	_	_
is	_	_
present	_	_
in	_	_
X	_	_
s	_	_
,	_	_
but	_	_
X	_	_
u	_	_
may	_	_
contain	_	_
seen	_	_
objects	_	_
.	_	_

#79
We	_	_
define	_	_
a	_	_
d	_	_
dimensional	_	_
word	_	_
vector	_	_
vc	_	_
(	_	_
word2vec	_	_
or	_	_
GloVe	_	_
)	_	_
for	_	_
every	_	_
class	_	_
c	_	_
∈	_	_
C.	_	_
The	_	_
ground-truth	_	_
label	_	_
for	_	_
an	_	_
ith	_	_
bounding	_	_
box	_	_
is	_	_
denoted	_	_
by	_	_
yi	_	_
.	_	_

#80
The	_	_
object	_	_
detection	_	_
task	_	_
also	_	_
involves	_	_
identifying	_	_
the	_	_
background	_	_
class	_	_
for	_	_
negative	_	_
object	_	_
proposals	_	_
,	_	_
we	_	_
introduce	_	_
the	_	_
extended	_	_
label	_	_
sets	_	_
:	_	_
S	_	_
′	_	_
=	_	_
S	_	_
∪	_	_
ybg	_	_
,	_	_
C′	_	_
=	_	_
C	_	_
∪	_	_
ybg	_	_
andM′	_	_
=M∪	_	_
ybg	_	_
,	_	_
where	_	_
ybg	_	_
=	_	_
{	_	_
C	_	_
+	_	_
1	_	_
}	_	_
is	_	_
a	_	_
singleton	_	_
set	_	_
denoting	_	_
the	_	_
background	_	_
label	_	_
.	_	_

#81
Task	_	_
Definitions	_	_
:	_	_
Given	_	_
the	_	_
observed	_	_
space	_	_
of	_	_
images	_	_
X	_	_
=	_	_
X	_	_
s	_	_
∪	_	_
X	_	_
u	_	_
and	_	_
the	_	_
output	_	_
label	_	_
space	_	_
C′	_	_
,	_	_
our	_	_
goal	_	_
is	_	_
to	_	_
learn	_	_
a	_	_
mapping	_	_
function	_	_
f	_	_
:	_	_
X	_	_
7→	_	_
C′	_	_
which	_	_
gives	_	_
the	_	_
minimum	_	_
regularized	_	_
empirical	_	_
risk	_	_
(	_	_
R̂	_	_
)	_	_
as	_	_
follows	_	_
:	_	_
arg	_	_
min	_	_
f∈F	_	_
R̂	_	_
(	_	_
f	_	_
(	_	_
x	_	_
;	_	_
Θ	_	_
)	_	_
)	_	_
+Ω	_	_
(	_	_
Θ	_	_
)	_	_
,	_	_
(	_	_
1	_	_
)	_	_
where	_	_
,	_	_
x	_	_
∈	_	_
X	_	_
s	_	_
during	_	_
training	_	_
,	_	_
Θ	_	_
denotes	_	_
the	_	_
set	_	_
of	_	_
parameters	_	_
and	_	_
Ω	_	_
(	_	_
Θ	_	_
)	_	_
denotes	_	_
the	_	_
regularization	_	_
on	_	_
the	_	_
learned	_	_
weights	_	_
.	_	_

#82
The	_	_
mapping	_	_
function	_	_
has	_	_
the	_	_
following	_	_
form	_	_
:	_	_
f	_	_
(	_	_
x	_	_
;	_	_
Θ	_	_
)	_	_
=	_	_
arg	_	_
max	_	_
y∈C	_	_
max	_	_
b∈B	_	_
(	_	_
x	_	_
)	_	_
F	_	_
(	_	_
x	_	_
,	_	_
y	_	_
,	_	_
b	_	_
;	_	_
Θ	_	_
)	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
F	_	_
(	_	_
·	_	_
)	_	_
is	_	_
a	_	_
compatibility	_	_
function	_	_
,	_	_
B	_	_
(	_	_
x	_	_
)	_	_
is	_	_
the	_	_
set	_	_
of	_	_
all	_	_
bounding	_	_
box	_	_
proposals	_	_
in	_	_
a	_	_
given	_	_
image	_	_
x	_	_
.	_	_

#83
Intuitively	_	_
,	_	_
Eq.	_	_
2	_	_
finds	_	_
the	_	_
best	_	_
scoring	_	_
bounding	_	_
boxes	_	_
for	_	_
each	_	_
object	_	_
category	_	_
and	_	_
assigns	_	_
them	_	_
the	_	_
maximum	_	_
scoring	_	_
object	_	_
category	_	_
.	_	_

#84
Next	_	_
,	_	_
we	_	_
define	_	_
the	_	_
zero-shot	_	_
learning	_	_
tasks	_	_
which	_	_
go	_	_
beyond	_	_
a	_	_
single	_	_
unseen	_	_
category	_	_
recognition	_	_
in	_	_
images	_	_
.	_	_

#85
Notably	_	_
,	_	_
the	_	_
training	_	_
is	_	_
framed	_	_
as	_	_
the	_	_
challenging	_	_
ZSD	_	_
problem	_	_
,	_	_
however	_	_
the	_	_
remaining	_	_
task	_	_
descriptions	_	_
are	_	_
used	_	_
during	_	_
evaluation	_	_
to	_	_
relax	_	_
the	_	_
original	_	_
problem	_	_
:	_	_
Zero-Shot	_	_
Object	_	_
Detection	_	_
5	_	_
Fig.	_	_
2	_	_
.	_	_

#86
Network	_	_
Architecture	_	_
-	_	_
Left	_	_
:	_	_
Image	_	_
level	_	_
feature	_	_
maps	_	_
are	_	_
used	_	_
to	_	_
propose	_	_
candidate	_	_
object	_	_
boxes	_	_
and	_	_
their	_	_
corresponding	_	_
features	_	_
.	_	_

#87
Right	_	_
:	_	_
The	_	_
features	_	_
are	_	_
used	_	_
for	_	_
classification	_	_
and	_	_
localization	_	_
of	_	_
new	_	_
classes	_	_
by	_	_
utilizing	_	_
their	_	_
semantic	_	_
concepts	_	_
.	_	_

#88
T1	_	_
Zero-shot	_	_
detection	_	_
(	_	_
ZSD	_	_
)	_	_
:	_	_
Given	_	_
a	_	_
test	_	_
image	_	_
x	_	_
∈	_	_
X	_	_
u	_	_
,	_	_
the	_	_
goal	_	_
is	_	_
to	_	_
categorize	_	_
and	_	_
localize	_	_
each	_	_
instance	_	_
of	_	_
an	_	_
unseen	_	_
object	_	_
class	_	_
u	_	_
∈	_	_
U	_	_
.	_	_

#89
T2	_	_
Zero-shot	_	_
meta-class	_	_
detection	_	_
(	_	_
ZSMD	_	_
)	_	_
:	_	_
Given	_	_
a	_	_
test	_	_
image	_	_
x	_	_
∈	_	_
X	_	_
u	_	_
,	_	_
the	_	_
goal	_	_
is	_	_
to	_	_
localize	_	_
each	_	_
instance	_	_
of	_	_
an	_	_
unseen	_	_
object	_	_
class	_	_
u	_	_
∈	_	_
U	_	_
and	_	_
categorize	_	_
it	_	_
into	_	_
one	_	_
of	_	_
the	_	_
super-classes	_	_
m	_	_
∈M	_	_
.	_	_

#90
T3	_	_
Zero-shot	_	_
tagging	_	_
(	_	_
ZST	_	_
)	_	_
:	_	_
To	_	_
recognize	_	_
one	_	_
or	_	_
more	_	_
unseen	_	_
classes	_	_
in	_	_
a	_	_
test	_	_
image	_	_
x	_	_
∈	_	_
X	_	_
u	_	_
,	_	_
without	_	_
identifying	_	_
their	_	_
location	_	_
.	_	_

#91
T4	_	_
Zero-shot	_	_
meta-class	_	_
tagging	_	_
(	_	_
ZSMT	_	_
)	_	_
:	_	_
To	_	_
recognize	_	_
one	_	_
or	_	_
more	_	_
meta-classes	_	_
in	_	_
a	_	_
test	_	_
image	_	_
x	_	_
∈	_	_
X	_	_
u	_	_
,	_	_
without	_	_
identifying	_	_
their	_	_
location	_	_
.	_	_

#92
Among	_	_
the	_	_
above	_	_
mentioned	_	_
tasks	_	_
,	_	_
the	_	_
ZSD	_	_
is	_	_
the	_	_
most	_	_
difficult	_	_
problem	_	_
and	_	_
difficulty	_	_
level	_	_
decreases	_	_
as	_	_
we	_	_
go	_	_
down	_	_
the	_	_
list	_	_
.	_	_

#93
The	_	_
goal	_	_
of	_	_
the	_	_
later	_	_
tasks	_	_
is	_	_
to	_	_
distill	_	_
the	_	_
main	_	_
challenges	_	_
in	_	_
ZSD	_	_
by	_	_
investigating	_	_
two	_	_
ways	_	_
to	_	_
relax	_	_
the	_	_
original	_	_
problem	_	_
:	_	_
(	_	_
a	_	_
)	_	_
The	_	_
effect	_	_
of	_	_
reducing	_	_
the	_	_
unseen	_	_
object	_	_
classes	_	_
by	_	_
clustering	_	_
similar	_	_
unseen	_	_
classes	_	_
into	_	_
a	_	_
single	_	_
super-class	_	_
(	_	_
T2	_	_
and	_	_
T4	_	_
)	_	_
.	_	_

#94
(	_	_
b	_	_
)	_	_
The	_	_
effect	_	_
of	_	_
removing	_	_
the	_	_
localization	_	_
constraint	_	_
.	_	_

#95
To	_	_
this	_	_
end	_	_
we	_	_
investigate	_	_
the	_	_
zero-shot	_	_
tagging	_	_
problem	_	_
,	_	_
where	_	_
the	_	_
goal	_	_
is	_	_
to	_	_
only	_	_
recognize	_	_
all	_	_
object	_	_
categories	_	_
in	_	_
an	_	_
image	_	_
(	_	_
T3	_	_
and	_	_
T4	_	_
)	_	_
.	_	_

#96
The	_	_
state-of-the-art	_	_
in	_	_
zero-shot	_	_
learning	_	_
deals	_	_
with	_	_
only	_	_
recognition/tagging	_	_
.	_	_

#97
The	_	_
proposed	_	_
problem	_	_
settings	_	_
add	_	_
the	_	_
missing	_	_
detection	_	_
task	_	_
which	_	_
indirectly	_	_
encapsulates	_	_
traditional	_	_
recognition	_	_
and	_	_
tagging	_	_
task	_	_
.	_	_

#98
3	_	_
Zero-Shot	_	_
Detection	_	_

#99
Our	_	_
proposed	_	_
model	_	_
uses	_	_
Faster-RCNN	_	_
[	_	_
38	_	_
]	_	_
as	_	_
a	_	_
backbone	_	_
architecture	_	_
,	_	_
due	_	_
to	_	_
its	_	_
superior	_	_
performance	_	_
among	_	_
competitive	_	_
end-to-end	_	_
detection	_	_
models	_	_
[	_	_
17,28,37	_	_
]	_	_
.	_	_

#100
We	_	_
first	_	_
provide	_	_
an	_	_
overview	_	_
of	_	_
our	_	_
proposed	_	_
model	_	_
architecture	_	_
and	_	_
then	_	_
discuss	_	_
network	_	_
learning	_	_
.	_	_

#101
Finally	_	_
,	_	_
we	_	_
extend	_	_
a	_	_
popular	_	_
ZSL	_	_
approach	_	_
to	_	_
the	_	_
detection	_	_
problem	_	_
,	_	_
against	_	_
which	_	_
we	_	_
compare	_	_
our	_	_
performance	_	_
in	_	_
the	_	_
experiments	_	_
.	_	_

#102
6	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#103
3.1	_	_
Model	_	_
Architecture	_	_

#104
The	_	_
overall	_	_
architecture	_	_
is	_	_
illustrated	_	_
in	_	_
Fig	_	_
2	_	_
.	_	_

#105
It	_	_
has	_	_
two	_	_
main	_	_
components	_	_
marked	_	_
in	_	_
color	_	_
:	_	_
the	_	_
first	_	_
provides	_	_
object-level	_	_
feature	_	_
descriptions	_	_
and	_	_
the	_	_
second	_	_
integrates	_	_
visual	_	_
information	_	_
with	_	_
the	_	_
semantic	_	_
embeddings	_	_
to	_	_
perform	_	_
zero-shot	_	_
detection	_	_
.	_	_

#106
We	_	_
explain	_	_
these	_	_
in	_	_
detail	_	_
next	_	_
.	_	_

#107
Object-level	_	_
Feature	_	_
Encoding	_	_
:	_	_
For	_	_
an	_	_
input	_	_
image	_	_
x	_	_
,	_	_
a	_	_
deep	_	_
network	_	_
(	_	_
VGG	_	_
or	_	_
ResNet	_	_
)	_	_
is	_	_
used	_	_
to	_	_
obtain	_	_
the	_	_
intermediate	_	_
convolutional	_	_
activations	_	_
.	_	_

#108
These	_	_
activations	_	_
are	_	_
treated	_	_
as	_	_
feature	_	_
maps	_	_
,	_	_
which	_	_
are	_	_
forwarded	_	_
to	_	_
the	_	_
Region	_	_
Proposal	_	_
Network	_	_
(	_	_
RPN	_	_
)	_	_
.	_	_

#109
The	_	_
RPN	_	_
generates	_	_
a	_	_
set	_	_
of	_	_
candidate	_	_
object	_	_
proposals	_	_
by	_	_
automatically	_	_
ranking	_	_
the	_	_
anchor	_	_
boxes	_	_
at	_	_
each	_	_
sliding	_	_
window	_	_
location	_	_
.	_	_

#110
The	_	_
high-scoring	_	_
candidate	_	_
proposals	_	_
can	_	_
be	_	_
of	_	_
different	_	_
sizes	_	_
,	_	_
which	_	_
are	_	_
mapped	_	_
to	_	_
fixed	_	_
sized	_	_
representation	_	_
using	_	_
a	_	_
RoI	_	_
pooling	_	_
layer	_	_
which	_	_
operates	_	_
on	_	_
the	_	_
initial	_	_
feature	_	_
maps	_	_
and	_	_
the	_	_
proposals	_	_
generated	_	_
by	_	_
the	_	_
RPN	_	_
.	_	_

#111
The	_	_
resulting	_	_
object	_	_
level	_	_
features	_	_
for	_	_
each	_	_
candidate	_	_
are	_	_
denoted	_	_
as	_	_
‘f	_	_
’	_	_
.	_	_

#112
Note	_	_
that	_	_
the	_	_
RPN	_	_
generates	_	_
object	_	_
proposal	_	_
based	_	_
on	_	_
the	_	_
objectness	_	_
measure	_	_
.	_	_

#113
Thus	_	_
,	_	_
a	_	_
trained	_	_
RPN	_	_
on	_	_
seen	_	_
objects	_	_
can	_	_
generate	_	_
proposals	_	_
for	_	_
unseen	_	_
objects	_	_
also	_	_
.	_	_

#114
In	_	_
the	_	_
second	_	_
block	_	_
of	_	_
our	_	_
architecture	_	_
,	_	_
these	_	_
feature	_	_
representations	_	_
are	_	_
used	_	_
alongside	_	_
the	_	_
semantic	_	_
embeddings	_	_
to	_	_
learn	_	_
useful	_	_
representations	_	_
for	_	_
both	_	_
the	_	_
seen	_	_
and	_	_
unseen	_	_
objectcategories	_	_
.	_	_

#115
Integrating	_	_
Visual	_	_
and	_	_
Semantic	_	_
Contexts	_	_
:	_	_
The	_	_
object-level	_	_
feature	_	_
f	_	_
is	_	_
forwarded	_	_
to	_	_
two	_	_
branches	_	_
in	_	_
the	_	_
second	_	_
module	_	_
.	_	_

#116
The	_	_
top	_	_
branch	_	_
is	_	_
trained	_	_
to	_	_
predict	_	_
the	_	_
object	_	_
category	_	_
for	_	_
each	_	_
candidate	_	_
box	_	_
.	_	_

#117
Note	_	_
that	_	_
this	_	_
can	_	_
assign	_	_
a	_	_
class	_	_
c	_	_
∈	_	_
C′	_	_
,	_	_
which	_	_
can	_	_
be	_	_
a	_	_
seen	_	_
,	_	_
unseen	_	_
or	_	_
background	_	_
category	_	_
.	_	_

#118
The	_	_
branch	_	_
consists	_	_
of	_	_
two	_	_
main	_	_
sub-networks	_	_
,	_	_
which	_	_
are	_	_
key	_	_
to	_	_
learning	_	_
the	_	_
semantic	_	_
relationships	_	_
between	_	_
seen	_	_
and	_	_
unseen	_	_
object	_	_
classes	_	_
.	_	_

#119
The	_	_
first	_	_
component	_	_
is	_	_
the	_	_
‘Semantic	_	_
Alignment	_	_
Network	_	_
’	_	_
(	_	_
SAN	_	_
)	_	_
,	_	_
which	_	_
consist	_	_
of	_	_
an	_	_
adjustable	_	_
FC	_	_
layer	_	_
,	_	_
whose	_	_
parameters	_	_
are	_	_
denoted	_	_
as	_	_
W1	_	_
∈	_	_
Rd×d	_	_
,	_	_
that	_	_
projects	_	_
the	_	_
input	_	_
visual	_	_
feature	_	_
vectors	_	_
to	_	_
a	_	_
semantic	_	_
space	_	_
with	_	_
d	_	_
dimensions	_	_
.	_	_

#120
The	_	_
resulting	_	_
feature	_	_
maps	_	_
are	_	_
then	_	_
projected	_	_
onto	_	_
the	_	_
fixed	_	_
semantic	_	_
embeddings	_	_
,	_	_
denoted	_	_
by	_	_
W2	_	_
∈	_	_
Rd×	_	_
(	_	_
C+1	_	_
)	_	_
,	_	_
which	_	_
are	_	_
obtained	_	_
in	_	_
an	_	_
unsupervised	_	_
manner	_	_
by	_	_
text	_	_
mining	_	_
(	_	_
e.g.	_	_
,	_	_
Word2vec	_	_
and	_	_
GloVe	_	_
embeddings	_	_
)	_	_
.	_	_

#121
Note	_	_
that	_	_
,	_	_
here	_	_
we	_	_
consider	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
semantic	_	_
vectors	_	_
which	_	_
require	_	_
unseen	_	_
classes	_	_
to	_	_
be	_	_
predefined	_	_
.	_	_

#122
This	_	_
consideration	_	_
is	_	_
inline	_	_
with	_	_
a	_	_
very	_	_
recent	_	_
effort	_	_
[	_	_
12	_	_
]	_	_
which	_	_
adopt	_	_
this	_	_
setting	_	_
to	_	_
explore	_	_
the	_	_
cluster	_	_
manifold	_	_
structure	_	_
of	_	_
the	_	_
semantic	_	_
embedding	_	_
space	_	_
and	_	_
address	_	_
domain	_	_
shift	_	_
issue	_	_
.	_	_

#123
Given	_	_
a	_	_
feature	_	_
representation	_	_
input	_	_
to	_	_
SAN	_	_
in	_	_
the	_	_
top	_	_
branch	_	_
,	_	_
f	_	_
t	_	_
,	_	_
the	_	_
overall	_	_
operation	_	_
can	_	_
be	_	_
represented	_	_
as	_	_
:	_	_
o	_	_
=	_	_
(	_	_
W1W2	_	_
)	_	_
T	_	_
f	_	_
t.	_	_
(	_	_
3	_	_
)	_	_
Here	_	_
,	_	_
o	_	_
is	_	_
the	_	_
output	_	_
prediction	_	_
score	_	_
.	_	_

#124
The	_	_
W2	_	_
is	_	_
formed	_	_
by	_	_
stacking	_	_
semantic	_	_
vectors	_	_
for	_	_
all	_	_
classes	_	_
,	_	_
including	_	_
the	_	_
background	_	_
class	_	_
.	_	_

#125
For	_	_
background	_	_
class	_	_
,	_	_
we	_	_
use	_	_
the	_	_
mean	_	_
word	_	_
vectors	_	_
vb	_	_
=	_	_
1	_	_
C	_	_
∑C	_	_
c=1	_	_
vc	_	_
as	_	_
its	_	_
embedding	_	_
in	_	_
W2	_	_
.	_	_

#126
Notably	_	_
,	_	_
a	_	_
non-linear	_	_
activation	_	_
function	_	_
is	_	_
not	_	_
applied	_	_
between	_	_
the	_	_
adjustable	_	_
and	_	_
fixed	_	_
semantic	_	_
embeddings	_	_
in	_	_
the	_	_
SAN	_	_
.	_	_

#127
Therefore	_	_
,	_	_
the	_	_
two	_	_
projections	_	_
can	_	_
be	_	_
understood	_	_
as	_	_
a	_	_
single	_	_
learnable	_	_
projection	_	_
on	_	_
to	_	_
the	_	_
semantic	_	_
Zero-Shot	_	_
Object	_	_
Detection	_	_
7	_	_
embeddings	_	_
of	_	_
object	_	_
classes	_	_
.	_	_

#128
This	_	_
helps	_	_
in	_	_
automatically	_	_
updating	_	_
the	_	_
semantic	_	_
embeddings	_	_
to	_	_
make	_	_
them	_	_
compatible	_	_
with	_	_
the	_	_
visual	_	_
feature	_	_
domain	_	_
.	_	_

#129
It	_	_
is	_	_
highly	_	_
valuable	_	_
because	_	_
the	_	_
original	_	_
semantic	_	_
embeddings	_	_
are	_	_
often	_	_
noisy	_	_
due	_	_
to	_	_
the	_	_
ambiguous	_	_
nature	_	_
of	_	_
closely	_	_
related	_	_
semantic	_	_
concepts	_	_
and	_	_
the	_	_
unsupervised	_	_
procedure	_	_
used	_	_
for	_	_
their	_	_
calculation	_	_
.	_	_

#130
In	_	_
Fig.	_	_
3	_	_
,	_	_
we	_	_
visualize	_	_
modified	_	_
embedding	_	_
space	_	_
when	_	_
different	_	_
loss	_	_
functions	_	_
are	_	_
applied	_	_
during	_	_
training	_	_
.	_	_

#131
The	_	_
bottom	_	_
branch	_	_
is	_	_
for	_	_
bounding	_	_
box	_	_
regression	_	_
to	_	_
add	_	_
suitable	_	_
offsets	_	_
to	_	_
the	_	_
proposals	_	_
to	_	_
align	_	_
them	_	_
with	_	_
the	_	_
ground-truths	_	_
such	_	_
that	_	_
the	_	_
precise	_	_
location	_	_
of	_	_
objects	_	_
can	_	_
be	_	_
predicted	_	_
.	_	_

#132
This	_	_
branch	_	_
is	_	_
set	_	_
up	_	_
in	_	_
the	_	_
same	_	_
manner	_	_
as	_	_
in	_	_
Faster-RCNN	_	_
[	_	_
38	_	_
]	_	_
.	_	_

#133
3.2	_	_
Training	_	_
and	_	_
Inference	_	_

#134
We	_	_
follow	_	_
a	_	_
two	_	_
step	_	_
training	_	_
approach	_	_
to	_	_
learn	_	_
the	_	_
model	_	_
parameters	_	_
.	_	_

#135
The	_	_
first	_	_
part	_	_
involves	_	_
training	_	_
the	_	_
backbone	_	_
Faster-RCNN	_	_
for	_	_
only	_	_
seen	_	_
classes	_	_
using	_	_
the	_	_
training	_	_
set	_	_
X	_	_
s.	_	_
This	_	_
training	_	_
involves	_	_
initializing	_	_
weights	_	_
of	_	_
shared	_	_
layers	_	_
with	_	_
a	_	_
pre-trained	_	_
Vgg/ResNet	_	_
model	_	_
,	_	_
followed	_	_
by	_	_
learning	_	_
the	_	_
RPN	_	_
,	_	_
classification	_	_
and	_	_
detection	_	_
networks	_	_
.	_	_

#136
In	_	_
the	_	_
second	_	_
step	_	_
,	_	_
we	_	_
modify	_	_
the	_	_
Faster-RCNN	_	_
model	_	_
by	_	_
replacing	_	_
the	_	_
last	_	_
layer	_	_
of	_	_
Faster-RCNN	_	_
classification	_	_
branch	_	_
with	_	_
the	_	_
proposed	_	_
semantic	_	_
alignment	_	_
network	_	_
and	_	_
an	_	_
updated	_	_
loss	_	_
function	_	_
(	_	_
see	_	_
Fig.	_	_
2	_	_
)	_	_
.	_	_

#137
While	_	_
rest	_	_
of	_	_
the	_	_
network	_	_
weights	_	_
are	_	_
used	_	_
from	_	_
the	_	_
first	_	_
step	_	_
,	_	_
the	_	_
weights	_	_
W1	_	_
are	_	_
randomly	_	_
initialized	_	_
and	_	_
the	_	_
W2	_	_
are	_	_
fixed	_	_
to	_	_
semantic	_	_
vectors	_	_
of	_	_
the	_	_
object	_	_
classes	_	_
and	_	_
not	_	_
updated	_	_
during	_	_
training	_	_
.	_	_

#138
While	_	_
training	_	_
in	_	_
second	_	_
step	_	_
,	_	_
we	_	_
keep	_	_
the	_	_
shared	_	_
layers	_	_
trainable	_	_
but	_	_
fix	_	_
the	_	_
layers	_	_
specific	_	_
to	_	_
RPN	_	_
since	_	_
the	_	_
object	_	_
proposals	_	_
requirements	_	_
are	_	_
not	_	_
changed	_	_
from	_	_
the	_	_
previous	_	_
step	_	_
.	_	_

#139
The	_	_
same	_	_
seen	_	_
class	_	_
images	_	_
X	_	_
s	_	_
are	_	_
used	_	_
for	_	_
training	_	_
,	_	_
consistent	_	_
with	_	_
the	_	_
first	_	_
step	_	_
.	_	_

#140
For	_	_
each	_	_
given	_	_
image	_	_
,	_	_
we	_	_
obtain	_	_
the	_	_
output	_	_
of	_	_
RPN	_	_
which	_	_
consists	_	_
of	_	_
a	_	_
total	_	_
of	_	_
‘R’	_	_
ROIs	_	_
belonging	_	_
to	_	_
both	_	_
positive	_	_
and	_	_
negative	_	_
object	_	_
proposals	_	_
.	_	_

#141
Each	_	_
proposal	_	_
has	_	_
a	_	_
corresponding	_	_
ground-truth	_	_
label	_	_
given	_	_
by	_	_
yi	_	_
∈	_	_
S	_	_
′	_	_
.	_	_

#142
Positive	_	_
proposals	_	_
belong	_	_
to	_	_
any	_	_
of	_	_
the	_	_
seen	_	_
class	_	_
S	_	_
and	_	_
negative	_	_
proposals	_	_
contain	_	_
only	_	_
background	_	_
.	_	_

#143
In	_	_
our	_	_
implementation	_	_
,	_	_
we	_	_
use	_	_
an	_	_
equal	_	_
number	_	_
of	_	_
positive	_	_
and	_	_
negative	_	_
proposals	_	_
.	_	_

#144
Now	_	_
,	_	_
when	_	_
object	_	_
proposals	_	_
are	_	_
passed	_	_
through	_	_
ROI-Pooling	_	_
and	_	_
subsequent	_	_
dense	_	_
layers	_	_
,	_	_
a	_	_
feature	_	_
representation	_	_
fi	_	_
is	_	_
calculated	_	_
for	_	_
each	_	_
ROI	_	_
.	_	_

#145
This	_	_
feature	_	_
is	_	_
forwarded	_	_
to	_	_
two	_	_
branches	_	_
,	_	_
the	_	_
classification	_	_
branch	_	_
and	_	_
regression	_	_
branch	_	_
.	_	_

#146
The	_	_
overall	_	_
loss	_	_
is	_	_
the	_	_
summation	_	_
of	_	_
the	_	_
respective	_	_
losses	_	_
in	_	_
these	_	_
two	_	_
branches	_	_
,	_	_
i.e.	_	_
,	_	_
classification	_	_
loss	_	_
and	_	_
bounding	_	_
box	_	_
regression	_	_
loss	_	_
.	_	_

#147
L	_	_
(	_	_
oi	_	_
,	_	_
bi	_	_
,	_	_
yi	_	_
,	_	_
b	_	_
∗	_	_
i	_	_
)	_	_
=	_	_
arg	_	_
min	_	_
Θ	_	_
T	_	_
∑	_	_
i	_	_
(	_	_
Lcls	_	_
(	_	_
oi	_	_
,	_	_
yi	_	_
)	_	_
+	_	_
Lreg	_	_
(	_	_
bi	_	_
,	_	_
b	_	_
∗	_	_
i	_	_
)	_	_
)	_	_
where	_	_
Θ	_	_
denotes	_	_
the	_	_
parameters	_	_
of	_	_
the	_	_
network	_	_
,	_	_
oi	_	_
is	_	_
the	_	_
classification	_	_
branch	_	_
output	_	_
,	_	_
T	_	_
=	_	_
N×R	_	_
represents	_	_
the	_	_
total	_	_
number	_	_
of	_	_
ROIs	_	_
in	_	_
the	_	_
training	_	_
set	_	_
with	_	_
N	_	_
images	_	_
.	_	_

#148
bi	_	_
and	_	_
b∗i	_	_
are	_	_
parameterized	_	_
coordinates	_	_
of	_	_
predicted	_	_
and	_	_
ground-truth	_	_
bounding	_	_
boxes	_	_
respectively	_	_
and	_	_
yi	_	_
represents	_	_
the	_	_
true	_	_
class	_	_
label	_	_
of	_	_
the	_	_
ith	_	_
object	_	_
proposal	_	_
.	_	_

#149
8	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#150
Fig.	_	_
3	_	_
.	_	_

#151
The	_	_
2D	_	_
tSNE	_	_
embedding	_	_
of	_	_
modified	_	_
word	_	_
vectors	_	_
W1W2	_	_
using	_	_
only	_	_
max-margin	_	_
loss	_	_
,	_	_
Lmm	_	_
(	_	_
left	_	_
)	_	_
and	_	_
with	_	_
clustering	_	_
loss	_	_
,	_	_
Lmm	_	_
+Lmc	_	_
(	_	_
right	_	_
)	_	_
.	_	_

#152
Semantically	_	_
similar	_	_
classes	_	_
are	_	_
embedded	_	_
more	_	_
closely	_	_
in	_	_
cluster	_	_
based	_	_
loss	_	_
.	_	_

#153
Classification	_	_
loss	_	_
:	_	_
This	_	_
loss	_	_
deals	_	_
with	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
.	_	_

#154
It	_	_
has	_	_
two	_	_
components	_	_
:	_	_
a	_	_
max-margin	_	_
loss	_	_
(	_	_
Lmm	_	_
)	_	_
and	_	_
a	_	_
meta-class	_	_
clustering	_	_
loss	_	_
(	_	_
Lmc	_	_
)	_	_
.	_	_

#155
Lcls	_	_
(	_	_
oi	_	_
,	_	_
yi	_	_
)	_	_
=	_	_
λLmm	_	_
(	_	_
oi	_	_
,	_	_
yi	_	_
)	_	_
+	_	_
(	_	_
1−	_	_
λ	_	_
)	_	_
Lmc	_	_
(	_	_
oi	_	_
,	_	_
g	_	_
(	_	_
yi	_	_
)	_	_
)	_	_
,	_	_
(	_	_
4	_	_
)	_	_
where	_	_
,	_	_
λ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
that	_	_
controls	_	_
the	_	_
trade-off	_	_
between	_	_
the	_	_
two	_	_
losses	_	_
.	_	_

#156
We	_	_
define	_	_
,	_	_
Lmm	_	_
(	_	_
oi	_	_
,	_	_
yi	_	_
)	_	_
=	_	_
|C′	_	_
\	_	_
yi|	_	_
∑	_	_
c∈C′\yi	_	_
log	_	_
(	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
oc	_	_
−	_	_
oyi	_	_
)	_	_
)	_	_
,	_	_
and	_	_
Lmc	_	_
(	_	_
oi	_	_
,	_	_
g	_	_
(	_	_
yi	_	_
)	_	_
)	_	_
=	_	_
|M′	_	_
\	_	_
zg	_	_
(	_	_
yi	_	_
)	_	_
||zg	_	_
(	_	_
yi	_	_
)	_	_
|	_	_
∑	_	_
c∈M′\zg	_	_
(	_	_
yi	_	_
)	_	_
∑	_	_
j∈zg	_	_
(	_	_
yi	_	_
)	_	_
log	_	_
(	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
oc	_	_
−	_	_
oj	_	_
)	_	_
)	_	_
where	_	_
,	_	_
ok	_	_
represents	_	_
the	_	_
prediction	_	_
response	_	_
of	_	_
class	_	_
k	_	_
∈	_	_
S.	_	_
Lmm	_	_
tries	_	_
to	_	_
separate	_	_
the	_	_
prediction	_	_
response	_	_
of	_	_
true	_	_
class	_	_
from	_	_
rest	_	_
of	_	_
the	_	_
classes	_	_
.	_	_

#157
In	_	_
contrast	_	_
,	_	_
Lmc	_	_
tries	_	_
to	_	_
cluster	_	_
together	_	_
the	_	_
members	_	_
of	_	_
each	_	_
super-class	_	_
and	_	_
pulls	_	_
further	_	_
apart	_	_
the	_	_
classes	_	_
belonging	_	_
to	_	_
different	_	_
meta-classes	_	_
.	_	_

#158
We	_	_
illustrate	_	_
the	_	_
effect	_	_
of	_	_
clustering	_	_
loss	_	_
on	_	_
the	_	_
learned	_	_
embeddings	_	_
in	_	_
Fig.	_	_
3	_	_
.	_	_

#159
The	_	_
use	_	_
of	_	_
Lmc	_	_
enables	_	_
us	_	_
to	_	_
cluster	_	_
semantically	_	_
similar	_	_
classes	_	_
together	_	_
which	_	_
results	_	_
in	_	_
improved	_	_
embeddings	_	_
in	_	_
the	_	_
semantic	_	_
space	_	_
.	_	_

#160
For	_	_
example	_	_
,	_	_
all	_	_
animals	_	_
related	_	_
meta-classes	_	_
are	_	_
in	_	_
close	_	_
position	_	_
whereas	_	_
food	_	_
and	_	_
vehicle	_	_
are	_	_
far	_	_
apart	_	_
.	_	_

#161
Such	_	_
a	_	_
clear	_	_
separation	_	_
in	_	_
semantic	_	_
space	_	_
helps	_	_
in	_	_
obtaining	_	_
a	_	_
better	_	_
ZSD	_	_
performance	_	_
.	_	_

#162
Moreover	_	_
,	_	_
meta-class	_	_
based	_	_
clustering	_	_
loss	_	_
does	_	_
not	_	_
harm	_	_
fine-grained	_	_
detection	_	_
because	_	_
the	_	_
hype-parameter	_	_
λ	_	_
is	_	_
used	_	_
to	_	_
put	_	_
more	_	_
emphasis	_	_
on	_	_
the	_	_
max-margin	_	_
loss	_	_
(	_	_
Lmm	_	_
)	_	_
as	_	_
compared	_	_
to	_	_
the	_	_
clustering	_	_
part	_	_
(	_	_
Lmc	_	_
)	_	_
of	_	_
the	_	_
overall	_	_
loss	_	_
(	_	_
Lcls	_	_
)	_	_
.	_	_

#163
Still	_	_
,	_	_
the	_	_
clustering	_	_
loss	_	_
provides	_	_
enough	_	_
guidance	_	_
to	_	_
the	_	_
noisy	_	_
semantic	_	_
embeddings	_	_
(	_	_
e.g.	_	_
,	_	_
unsupervised	_	_
w2v/glove	_	_
)	_	_
such	_	_
that	_	_
similar	_	_
classes	_	_
are	_	_
clustered	_	_
together	_	_
as	_	_
illustrated	_	_
in	_	_
Fig.	_	_
3	_	_
.	_	_

#164
Note	_	_
that	_	_
w2v/glove	_	_
try	_	_
to	_	_
place	_	_
similar	_	_
words	_	_
nearby	_	_
with	_	_
respect	_	_
to	_	_
millions	_	_
of	_	_
text	_	_
corpus	_	_
,	_	_
it	_	_
is	_	_
therefore	_	_
not	_	_
fine-tuned	_	_
for	_	_
just	_	_
200	_	_
class	_	_
recognition	_	_
setting	_	_
.	_	_

#165
Regression	_	_
loss	_	_
:	_	_
This	_	_
part	_	_
of	_	_
the	_	_
loss	_	_
is	_	_
similar	_	_
to	_	_
faster-RCNN	_	_
regression	_	_
loss	_	_
which	_	_
fine-tunes	_	_
the	_	_
bounding	_	_
box	_	_
for	_	_
each	_	_
seen	_	_
class	_	_
ROI	_	_
.	_	_

#166
For	_	_
each	_	_
fi	_	_
,	_	_
we	_	_
get	_	_
4×S	_	_
values	_	_
representing	_	_
4	_	_
parameterized	_	_
co-ordinates	_	_
of	_	_
the	_	_
bounding	_	_
box	_	_
of	_	_
each	_	_
object	_	_
instance	_	_
.	_	_

#167
The	_	_
regression	_	_
loss	_	_
is	_	_
calculated	_	_
based	_	_
on	_	_
these	_	_
co-ordinates	_	_
and	_	_
parameterized	_	_
ground	_	_
truth	_	_
co-ordinates	_	_
.	_	_

#168
During	_	_
training	_	_
,	_	_
no	_	_
bounding	_	_
box	_	_
Zero-Shot	_	_
Object	_	_
Detection	_	_
9	_	_
prediction	_	_
is	_	_
done	_	_
for	_	_
background	_	_
and	_	_
unseen	_	_
classes	_	_
due	_	_
to	_	_
unavailability	_	_
of	_	_
visual	_	_
examples	_	_
.	_	_

#169
As	_	_
an	_	_
alternate	_	_
approach	_	_
,	_	_
we	_	_
approximate	_	_
the	_	_
bounding	_	_
box	_	_
for	_	_
an	_	_
unseen	_	_
object	_	_
through	_	_
the	_	_
box	_	_
proposal	_	_
for	_	_
a	_	_
closely	_	_
related	_	_
seen	_	_
object	_	_
that	_	_
achieves	_	_
maximum	_	_
response	_	_
.	_	_

#170
This	_	_
is	_	_
a	_	_
reasonable	_	_
approximation	_	_
because	_	_
visual	_	_
features	_	_
of	_	_
unseen	_	_
classes	_	_
are	_	_
related	_	_
to	_	_
that	_	_
of	_	_
similar	_	_
seen	_	_
classes	_	_
.	_	_

#171
Prediction	_	_
:	_	_
We	_	_
normalize	_	_
each	_	_
output	_	_
prediction	_	_
value	_	_
of	_	_
classification	_	_
branch	_	_
using	_	_
ôc	_	_
=	_	_
oc	_	_
‖vc‖2‖f	_	_
t‖2	_	_
.	_	_

#172
It	_	_
basically	_	_
calculates	_	_
the	_	_
cosine	_	_
similarity	_	_
between	_	_
modified	_	_
word	_	_
vectors	_	_
and	_	_
image	_	_
features	_	_
.	_	_

#173
This	_	_
normalization	_	_
maps	_	_
the	_	_
prediction	_	_
values	_	_
within	_	_
0	_	_
to	_	_
1	_	_
range	_	_
.	_	_

#174
We	_	_
classify	_	_
an	_	_
object	_	_
proposal	_	_
as	_	_
background	_	_
if	_	_
maximum	_	_
responds	_	_
among	_	_
ôc	_	_
where	_	_
c	_	_
∈	_	_
C′	_	_
belongs	_	_
to	_	_
ybg	_	_
.	_	_

#175
Otherwise	_	_
,	_	_
we	_	_
detect	_	_
an	_	_
object	_	_
proposal	_	_
as	_	_
unseen	_	_
object	_	_
if	_	_
its	_	_
maximum	_	_
prediction	_	_
response	_	_
among	_	_
ôu	_	_
where	_	_
u	_	_
∈	_	_
U	_	_
is	_	_
above	_	_
a	_	_
threshold	_	_
α.	_	_
yu	_	_
=	_	_
arg	_	_
max	_	_
u∈U	_	_
ôu	_	_
s.t.	_	_
,	_	_
ôu	_	_
>	_	_
α	_	_
.	_	_

#176
(	_	_
5	_	_
)	_	_
The	_	_
other	_	_
detection	_	_
branch	_	_
finds	_	_
bi	_	_
which	_	_
is	_	_
the	_	_
parameterized	_	_
co-ordinates	_	_
of	_	_
bounding	_	_
boxes	_	_
corresponds	_	_
to	_	_
S	_	_
seen	_	_
classes	_	_
.	_	_

#177
Among	_	_
them	_	_
,	_	_
we	_	_
choose	_	_
a	_	_
bounding	_	_
box	_	_
corresponding	_	_
to	_	_
the	_	_
class	_	_
having	_	_
the	_	_
maximum	_	_
prediction	_	_
response	_	_
in	_	_
ôs	_	_
where	_	_
s	_	_
∈	_	_
S	_	_
for	_	_
the	_	_
classified	_	_
unseen	_	_
class	_	_
yu	_	_
.	_	_

#178
For	_	_
the	_	_
tagging	_	_
tasks	_	_
,	_	_
we	_	_
simply	_	_
use	_	_
the	_	_
mapping	_	_
function	_	_
g	_	_
(	_	_
.	_	_
)	_	_

#179
to	_	_
assign	_	_
a	_	_
meta-class	_	_
for	_	_
any	_	_
unseen	_	_
label	_	_
.	_	_

#180
3.3	_	_
ZSD	_	_
without	_	_
Pre-defined	_	_
Unseen	_	_

#181
While	_	_
applying	_	_
clustering	_	_
loss	_	_
in	_	_
Sec	_	_
.	_	_

#182
3.2	_	_
,	_	_
the	_	_
meta-class	_	_
assignment	_	_
adds	_	_
high-level	_	_
supervision	_	_
in	_	_
the	_	_
semantic	_	_
space	_	_
.	_	_

#183
While	_	_
doing	_	_
this	_	_
assignment	_	_
,	_	_
we	_	_
consider	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
.	_	_

#184
Similarly	_	_
,	_	_
the	_	_
max-margin	_	_
loss	_	_
considers	_	_
the	_	_
set	_	_
C′	_	_
consisting	_	_
of	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
.	_	_

#185
This	_	_
problem	_	_
setting	_	_
helps	_	_
to	_	_
identify	_	_
the	_	_
clustering	_	_
structure	_	_
of	_	_
the	_	_
semantic	_	_
embeddings	_	_
to	_	_
address	_	_
domain	_	_
adaptation	_	_
for	_	_
zero-shot	_	_
detection	_	_
.	_	_

#186
However	_	_
,	_	_
in	_	_
several	_	_
practical	_	_
scenarios	_	_
,	_	_
unseen	_	_
classes	_	_
may	_	_
not	_	_
be	_	_
known	_	_
during	_	_
training	_	_
.	_	_

#187
Here	_	_
,	_	_
we	_	_
report	_	_
a	_	_
simplified	_	_
variant	_	_
of	_	_
our	_	_
approach	_	_
to	_	_
train	_	_
the	_	_
proposed	_	_
network	_	_
without	_	_
pre-defined	_	_
unseen	_	_
classes	_	_
.	_	_

#188
For	_	_
this	_	_
problem	_	_
setting	_	_
,	_	_
we	_	_
use	_	_
only	_	_
seen+bg	_	_
word	_	_
vectors	_	_
(	_	_
instead	_	_
of	_	_
seen+unseen+bg	_	_
vectors	_	_
)	_	_
as	_	_
the	_	_
fixed	_	_
embedding	_	_
W2	_	_
∈	_	_
Rd×	_	_
(	_	_
S+1	_	_
)	_	_
to	_	_
train	_	_
the	_	_
whole	_	_
framework	_	_
with	_	_
only	_	_
the	_	_
max-margin	_	_
loss	_	_
,	_	_
L′mm	_	_
,	_	_
defined	_	_
as	_	_
follows	_	_
:	_	_
L′mm	_	_
(	_	_
oi	_	_
,	_	_
yi	_	_
)	_	_
=	_	_
1	_	_
|S′\yi|	_	_
∑	_	_
c∈S′\yi	_	_
log	_	_
(	_	_
1	_	_
+	_	_
exp	_	_
(	_	_
oc	_	_
−	_	_
oyi	_	_
)	_	_
)	_	_
.	_	_

#189
Since	_	_
the	_	_
output	_	_
classification	_	_
layer	_	_
can	_	_
not	_	_
make	_	_
predictions	_	_
for	_	_
unseen	_	_
classes	_	_
,	_	_
we	_	_
apply	_	_
a	_	_
procedure	_	_
similar	_	_
to	_	_
ConSE	_	_
during	_	_
the	_	_
testing	_	_
phase	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#190
The	_	_
choice	_	_
of	_	_
[	_	_
33	_	_
]	_	_
here	_	_
is	_	_
made	_	_
due	_	_
to	_	_
two	_	_
main	_	_
reasons	_	_
:	_	_
(	_	_
a	_	_
)	_	_
In	_	_
contrast	_	_
to	_	_
other	_	_
ZSL	_	_
methods	_	_
which	_	_
train	_	_
separate	_	_
models	_	_
for	_	_
each	_	_
class	_	_
[	_	_
5,36	_	_
]	_	_
,	_	_
ConSE	_	_
can	_	_
work	_	_
on	_	_
the	_	_
prediction	_	_
score	_	_
of	_	_
a	_	_
single	_	_
end-to-end	_	_
framework	_	_
.	_	_

#191
(	_	_
b	_	_
)	_	_
It	_	_
is	_	_
straight-forward	_	_
to	_	_
extend	_	_
a	_	_
single	_	_
network	_	_
to	_	_
ZSD	_	_
along	_	_
with	_	_
ConSE	_	_
,	_	_
since	_	_
[	_	_
33	_	_
]	_	_
uses	_	_
semantic	_	_
embeddings	_	_
only	_	_
during	_	_
the	_	_
test	_	_
phase	_	_
.	_	_

#192
Suppose	_	_
,	_	_
for	_	_
an	_	_
object	_	_
proposal	_	_
,	_	_
o	_	_
∈	_	_
RS+1	_	_
is	_	_
the	_	_
vector	_	_
containing	_	_
final	_	_
probability	_	_
values	_	_
of	_	_
only	_	_
seen	_	_
classes	_	_
and	_	_
background	_	_
.	_	_

#193
As	_	_
described	_	_
earlier	_	_
,	_	_
we	_	_

#194
10	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#195
ignore	_	_
the	_	_
object	_	_
proposal	_	_
if	_	_
the	_	_
background	_	_
class	_	_
get	_	_
highest	_	_
probability	_	_
score	_	_
.	_	_

#196
For	_	_
other	_	_
cases	_	_
,	_	_
we	_	_
sort	_	_
the	_	_
vector	_	_
o	_	_
in	_	_
descending	_	_
order	_	_
to	_	_
compute	_	_
a	_	_
list	_	_
of	_	_
indices	_	_
l	_	_
and	_	_
the	_	_
sorted	_	_
list	_	_
ô	_	_
:	_	_
ô	_	_
,	_	_
l	_	_
=	_	_
sort	_	_
(	_	_
o	_	_
)	_	_
s.t.	_	_
,	_	_
oj	_	_
=	_	_
ôlj	_	_
.	_	_

#197
(	_	_
6	_	_
)	_	_
Then	_	_
,	_	_
top	_	_
K	_	_
score	_	_
values	_	_
(	_	_
s.t.	_	_
,	_	_
K	_	_
≤	_	_
S	_	_
)	_	_
from	_	_
ô	_	_
are	_	_
combined	_	_
with	_	_
their	_	_
corresponding	_	_
word	_	_
vectors	_	_
using	_	_
the	_	_
equation	_	_
:	_	_
ei	_	_
=	_	_
∑K	_	_
k=1	_	_
ôkvlk	_	_
.	_	_

#198
We	_	_
consider	_	_
ei	_	_
as	_	_
a	_	_
semantic	_	_
space	_	_
projection	_	_
of	_	_
an	_	_
object	_	_
proposal	_	_
which	_	_
is	_	_
a	_	_
combination	_	_
of	_	_
word	_	_
vectors	_	_
weighted	_	_
by	_	_
top	_	_
K	_	_
seen	_	_
class	_	_
probabilities	_	_
.	_	_

#199
The	_	_
final	_	_
prediction	_	_
is	_	_
made	_	_
by	_	_
finding	_	_
the	_	_
maximum	_	_
cosine	_	_
similarity	_	_
among	_	_
ei	_	_
and	_	_
all	_	_
unseen	_	_
word	_	_
vectors	_	_
,	_	_
yu	_	_
=	_	_
arg	_	_
max	_	_
u∈U	_	_
cos	_	_
(	_	_
ei	_	_
,	_	_
vu	_	_
)	_	_
.	_	_

#200
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
use	_	_
K	_	_
=	_	_
10	_	_
as	_	_
proposed	_	_
in	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#201
For	_	_
bounding	_	_
box	_	_
detection	_	_
,	_	_
we	_	_
choose	_	_
the	_	_
box	_	_
for	_	_
which	_	_
corresponding	_	_
seen	_	_
class	_	_
gets	_	_
maximum	_	_
score	_	_
.	_	_

#202
4	_	_
Experiments	_	_

#203
4.1	_	_
Dataset	_	_
and	_	_
Experiment	_	_
Protocol	_	_

#204
Dataset	_	_
:	_	_
We	_	_
evaluate	_	_
our	_	_
approach	_	_
on	_	_
the	_	_
standard	_	_
ILSVRC-2017	_	_
detection	_	_
dataset	_	_
[	_	_
40	_	_
]	_	_
.	_	_

#205
This	_	_
dataset	_	_
contains	_	_
200	_	_
object	_	_
categories	_	_
.	_	_

#206
For	_	_
training	_	_
,	_	_
it	_	_
includes	_	_
456,567	_	_
images	_	_
and	_	_
478,807	_	_
bounding	_	_
box	_	_
annotations	_	_
around	_	_
object	_	_
instances	_	_
.	_	_

#207
The	_	_
validation	_	_
dataset	_	_
contains	_	_
20,121	_	_
images	_	_
fully	_	_
annotated	_	_
with	_	_
the	_	_
200	_	_
object	_	_
categories	_	_
which	_	_
include	_	_
55,502	_	_
object	_	_
instances	_	_
.	_	_

#208
A	_	_
category	_	_
hierarchy	_	_
has	_	_
been	_	_
defined	_	_
in	_	_
[	_	_
40	_	_
]	_	_
,	_	_
where	_	_
some	_	_
objects	_	_
have	_	_
multiple	_	_
parents	_	_
.	_	_

#209
Since	_	_
,	_	_
we	_	_
also	_	_
evaluate	_	_
our	_	_
approach	_	_
on	_	_
meta-class	_	_
detection	_	_
and	_	_
tagging	_	_
,	_	_
we	_	_
define	_	_
a	_	_
single	_	_
parent	_	_
for	_	_
each	_	_
category	_	_
(	_	_
see	_	_
supplementary	_	_
material	_	_
for	_	_
detail	_	_
)	_	_
.	_	_

#210
Seen/unseen	_	_
split	_	_
:	_	_
Due	_	_
to	_	_
lack	_	_
of	_	_
an	_	_
existing	_	_
ZSD	_	_
protocol	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
challenging	_	_
seen/unseen	_	_
split	_	_
for	_	_
ILSVRC-2017	_	_
detection	_	_
dataset	_	_
.	_	_

#211
Among	_	_
200	_	_
object	_	_
categories	_	_
,	_	_
we	_	_
randomly	_	_
select	_	_
23	_	_
categories	_	_
as	_	_
unseen	_	_
and	_	_
rest	_	_
of	_	_
the	_	_
177	_	_
categories	_	_
are	_	_
considered	_	_
as	_	_
seen	_	_
.	_	_

#212
This	_	_
split	_	_
is	_	_
designed	_	_
to	_	_
follows	_	_
the	_	_
following	_	_
practical	_	_
considerations	_	_
:	_	_
(	_	_
a	_	_
)	_	_
unseen	_	_
classes	_	_
are	_	_
rare	_	_
,	_	_
(	_	_
b	_	_
)	_	_
test	_	_
categories	_	_
should	deontic	_
be	_	_
diverse	_	_
,	_	_
(	_	_
c	_	_
)	_	_
the	_	_
unseen	_	_
classes	_	_
should	deontic	_
be	_	_
semantically	_	_
similar	_	_
with	_	_
at	_	_
least	_	_
some	_	_
of	_	_
the	_	_
seen	_	_
classes	_	_
.	_	_

#213
The	_	_
details	_	_
of	_	_
split	_	_
are	_	_
provided	_	_
in	_	_
supplementary	_	_
material	_	_
.	_	_

#214
Train/test	_	_
set	_	_
:	_	_
A	_	_
zero-shot	_	_
setting	_	_
does	_	_
not	_	_
allow	_	_
any	_	_
visual	_	_
example	_	_
of	_	_
an	_	_
unseen	_	_
class	_	_
during	_	_
training	_	_
.	_	_

#215
Therefore	_	_
,	_	_
we	_	_
customize	_	_
the	_	_
training	_	_
set	_	_
of	_	_
ILSVRC	_	_
such	_	_
that	_	_
images	_	_
containing	_	_
any	_	_
unseen	_	_
instance	_	_
are	_	_
removed	_	_
.	_	_

#216
This	_	_
results	_	_
in	_	_
a	_	_
total	_	_
of	_	_
315,731	_	_
training	_	_
images	_	_
with	_	_
449,469	_	_
annotated	_	_
bounding	_	_
boxes	_	_
.	_	_

#217
For	_	_
testing	_	_
,	_	_
the	_	_
traditional	_	_
zero-shot	_	_
recognition	_	_
setting	_	_
is	_	_
used	_	_
which	_	_
considers	_	_
only	_	_
unseen	_	_
classes	_	_
.	_	_

#218
As	_	_
the	_	_
test	_	_
set	_	_
annotations	_	_
are	_	_
not	_	_
available	_	_
to	_	_
us	_	_
,	_	_
we	_	_
can	_	_
not	_	_
separate	_	_
unseen	_	_
classes	_	_
for	_	_
evaluation	_	_
.	_	_

#219
Therefore	_	_
,	_	_
our	_	_
test	_	_
set	_	_
is	_	_
composed	_	_
of	_	_
the	_	_
left	_	_
out	_	_
data	_	_
from	_	_
ILSVRC	_	_
training	_	_
dataset	_	_
plus	_	_
validation	_	_
images	_	_
having	_	_
at	_	_
least	_	_
one	_	_
unseen	_	_
bounding	_	_
box	_	_
.	_	_

#220
The	_	_
resulting	_	_
test	_	_
set	_	_
has	_	_
19,008	_	_
images	_	_
and	_	_
19,931	_	_
bounding	_	_
boxes	_	_
.	_	_

#221
Zero-Shot	_	_
Object	_	_
Detection	_	_
11	_	_
Network	_	_
ZSD	_	_
ZSMD	_	_
ZST	_	_
ZSMT	_	_
Baseline	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
Baseline	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
Baseline	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
Baseline	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
R+w2v	_	_
12.7	_	_
15.0	_	_
16.0	_	_
13.7	_	_
15.4	_	_
15.4	_	_
23.3	_	_
27.5	_	_
30.0	_	_
28.8	_	_
33.4	_	_
39.3	_	_
R+glo	_	_
12.0	_	_
12.3	_	_
14.6	_	_
12.9	_	_
14.1	_	_
16.1	_	_
22.3	_	_
24.5	_	_
26.2	_	_
29.2	_	_
31.5	_	_
36.3	_	_
V+w2v	_	_
10.2	_	_
12.7	_	_
11.8	_	_
11.4	_	_
12.5	_	_
11.8	_	_
23.3	_	_
25.6	_	_
26.2	_	_
29.0	_	_
31.3	_	_
36.0	_	_
V+glo	_	_
9.0	_	_
10.8	_	_
11.6	_	_
9.7	_	_
11.3	_	_
11.8	_	_
20.3	_	_
22.9	_	_
23.9	_	_
27.3	_	_
29.2	_	_
34.2	_	_
Table	_	_
1.	_	_
mAP	_	_
of	_	_
the	_	_
unseen	_	_
classes	_	_
.	_	_

#222
Ours	_	_
(	_	_
with	_	_
L′	_	_
mm	_	_
)	_	_
and	_	_
Ours	_	_
(	_	_
with	_	_
Lcls	_	_
)	_	_
denote	_	_
the	_	_
performance	_	_
without	_	_
predefined	_	_
unseen	_	_
and	_	_
with	_	_
cluster	_	_
loss	_	_
respectively	_	_
(	_	_
Sec	_	_
.	_	_

#223
3.3	_	_
and	_	_
Sec	_	_
.	_	_

#224
3.2	_	_
)	_	_
.	_	_

#225
For	_	_
cluster	_	_
case	_	_
,	_	_
λ	_	_
=	_	_
0.8	_	_
.	_	_

#226
Semantic	_	_
embedding	_	_
:	_	_
Traditionally	_	_
ZSL	_	_
methods	_	_
report	_	_
performance	_	_
on	_	_
both	_	_
supervised	_	_
attributes	_	_
and	_	_
unsupervised	_	_
word2vec/glove	_	_
as	_	_
semantic	_	_
embeddings	_	_
.	_	_

#227
As	_	_
manually	_	_
labeled	_	_
supervised	_	_
attributes	_	_
are	_	_
hard	_	_
to	_	_
obtain	_	_
,	_	_
only	_	_
small-scale	_	_
datasets	_	_
with	_	_
these	_	_
annotations	_	_
are	_	_
available	_	_
[	_	_
9,20	_	_
]	_	_
.	_	_

#228
ILSVRC-2017	_	_
detection	_	_
dataset	_	_
used	_	_
in	_	_
the	_	_
current	_	_
work	_	_
is	_	_
quite	_	_
huge	_	_
and	_	_
does	_	_
not	_	_
provide	_	_
attribute	_	_
annotations	_	_
.	_	_

#229
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
work	_	_
on	_	_
`2	_	_
normalized	_	_
500	_	_
and	_	_
300	_	_
dimensional	_	_
unsupervised	_	_
word2vec	_	_
[	_	_
30	_	_
]	_	_
and	_	_
GloVe	_	_
[	_	_
35	_	_
]	_	_
vector	_	_
respectively	_	_
to	_	_
describe	_	_
the	_	_
classes	_	_
.	_	_

#230
These	_	_
word	_	_
vectors	_	_
are	_	_
obtained	_	_
by	_	_
training	_	_
on	_	_
several	_	_
billion	_	_
words	_	_
from	_	_
Wikipedia	_	_
dump	_	_
corpus	_	_
.	_	_

#231
Evaluation	_	_
Metric	_	_
:	_	_
We	_	_
report	_	_
average	_	_
precision	_	_
(	_	_
AP	_	_
)	_	_
of	_	_
individual	_	_
unseen	_	_
classes	_	_
and	_	_
mean	_	_
average	_	_
precision	_	_
(	_	_
mAP	_	_
)	_	_
for	_	_
the	_	_
overall	_	_
performance	_	_
of	_	_
unseen	_	_
classes	_	_
.	_	_

#232
Implementation	_	_
Details	_	_
:	_	_
Unlike	_	_
Faster-RCNN	_	_
,	_	_
our	_	_
first	_	_
step	_	_
is	_	_
trained	_	_
in	_	_
one	_	_
step	_	_
:	_	_
after	_	_
initializing	_	_
shared	_	_
layer	_	_
with	_	_
pre-trained	_	_
weights	_	_
,	_	_
RPN	_	_
and	_	_
detection	_	_
network	_	_
of	_	_
Fast-RCNN	_	_
layers	_	_
are	_	_
learned	_	_
together	_	_
.	_	_

#233
Some	_	_
other	_	_
settings	_	_
includes	_	_
rescaling	_	_
shorter	_	_
size	_	_
of	_	_
image	_	_
as	_	_
600	_	_
pixels	_	_
,	_	_
RPN	_	_
stride	_	_
=	_	_
16	_	_
,	_	_
three	_	_
anchor	_	_
box	_	_
scale	_	_
128	_	_
,	_	_
256	_	_
and	_	_
512	_	_
pixels	_	_
,	_	_
three	_	_
aspect	_	_
ratios	_	_
1:1	_	_
,	_	_
1:2	_	_
and	_	_
2:1	_	_
,	_	_
non-maximum	_	_
suppression	_	_
(	_	_
NMS	_	_
)	_	_
on	_	_
proposals	_	_
class	_	_
probability	_	_
with	_	_
IoU	_	_
threshold	_	_
=	_	_
0.7	_	_
.	_	_

#234
Each	_	_
mini-batch	_	_
is	_	_
obtained	_	_
from	_	_
a	_	_
single	_	_
image	_	_
having	_	_
16	_	_
positive	_	_
and	_	_
16	_	_
negative	_	_
(	_	_
background	_	_
)	_	_
proposals	_	_
.	_	_

#235
Adam	_	_
optimizer	_	_
with	_	_
learning	_	_
rate	_	_
10−5	_	_
,	_	_
β1	_	_
=	_	_
0.9	_	_
and	_	_
β2	_	_
=	_	_
0.999	_	_
is	_	_
used	_	_
in	_	_
both	_	_
state	_	_
training	_	_
.	_	_

#236
First	_	_
step	_	_
is	_	_
trained	_	_
over	_	_
10	_	_
million	_	_
mini-batches	_	_
without	_	_
any	_	_
data	_	_
augmentation	_	_
,	_	_
but	_	_
data	_	_
augmentation	_	_
through	_	_
repetition	_	_
of	_	_
object	_	_
proposals	_	_
is	_	_
used	_	_
in	_	_
second	_	_
step	_	_
(	_	_
details	_	_
in	_	_
supplementary	_	_
material	_	_
)	_	_
.	_	_

#237
During	_	_
testing	_	_
,	_	_
the	_	_
prediction	_	_
score	_	_
threshold	_	_
was	_	_
0.1	_	_
for	_	_
baseline	_	_
and	_	_
Ours	_	_
(	_	_
with	_	_
L′mm	_	_
)	_	_
and	_	_
0.2	_	_
for	_	_
clustering	_	_
method	_	_
(	_	_
Ours	_	_
with	_	_
Lcls	_	_
)	_	_
.	_	_

#238
We	_	_
implement	_	_
our	_	_
model	_	_
in	_	_
Keras	_	_
.	_	_

#239
4.2	_	_
ZSD	_	_
Performance	_	_

#240
We	_	_
compare	_	_
different	_	_
versions	_	_
of	_	_
our	_	_
method	_	_
(	_	_
with	_	_
loss	_	_
configurations	_	_
L′mm	_	_
and	_	_
Lcls	_	_
respectively	_	_
)	_	_
to	_	_
a	_	_
baseline	_	_
approach	_	_
.	_	_

#241
Note	_	_
that	_	_
the	_	_
baseline	_	_
is	_	_
a	_	_
simple	_	_
extension	_	_
of	_	_
Faster-RCNN	_	_
[	_	_
38	_	_
]	_	_
and	_	_
ConSE	_	_
[	_	_
33	_	_
]	_	_
.	_	_

#242
We	_	_
apply	_	_
the	_	_
inference	_	_
strategy	_	_
mentioned	_	_
in	_	_
Sec	_	_
.	_	_

#243
3.3	_	_
after	_	_
first	_	_
step	_	_
training	_	_
as	_	_
we	_	_
can	_	_
still	_	_
get	_	_
a	_	_
vector	_	_
o	_	_
∈	_	_
RS+1	_	_
on	_	_
the	_	_
classification	_	_
layer	_	_
of	_	_
Faster-RCNN	_	_
network	_	_
.	_	_

#244
We	_	_
use	_	_
two	_	_
different	_	_
architectures	_	_
i.e.	_	_
,	_	_
VGG-16	_	_
(	_	_
V	_	_
)	_	_
[	_	_
42	_	_
]	_	_
and	_	_
ResNet-50	_	_
(	_	_
R	_	_
)	_	_
[	_	_
13	_	_
]	_	_
as	_	_
the	_	_
backbone	_	_
of	_	_
the	_	_
Faster-RCNN	_	_
during	_	_
the	_	_
first	_	_
training	_	_
step	_	_
.	_	_

#245
In	_	_
second	_	_
step	_	_
,	_	_
we	_	_
experiment	_	_

#246
12	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#247
O	_	_
V	_	_
E	_	_
R	_	_
A	_	_
L	_	_
L	_	_
p	_	_
.b	_	_
ox	_	_
sy	_	_
ri	_	_
n	_	_
g	_	_
e	_	_
h	_	_
a	_	_
rm	_	_
o	_	_
n	_	_
ic	_	_
a	_	_
m	_	_
a	_	_
ra	_	_
ca	_	_
b	_	_
u	_	_
rr	_	_
it	_	_
o	_	_
p	_	_
in	_	_
ea	_	_
p	_	_
p	_	_
le	_	_
b	_	_
ow	_	_
ti	_	_
e	_	_
s.	_	_
tr	_	_
u	_	_
n	_	_
k	_	_
d	_	_
.w	_	_
a	_	_
sh	_	_
er	_	_
ca	_	_
n	_	_
o	_	_
p	_	_
en	_	_
er	_	_
p	_	_
.r	_	_
a	_	_
ck	_	_
b	_	_
en	_	_
ch	_	_
e.	_	_
fa	_	_
n	_	_
iP	_	_
o	_	_
d	_	_
sc	_	_
o	_	_
rp	_	_
io	_	_
n	_	_
sn	_	_
a	_	_
il	_	_
h	_	_
a	_	_
m	_	_
st	_	_
er	_	_
ti	_	_
g	_	_
er	_	_
ra	_	_
y	_	_
tr	_	_
a	_	_
in	_	_
u	_	_
n	_	_
ic	_	_
y	_	_
cl	_	_
e	_	_
g	_	_
o	_	_
lf	_	_
b	_	_
a	_	_
ll	_	_
h	_	_
.b	_	_
a	_	_
r	_	_
Similar	_	_
classes	_	_
NOT	_	_
present	_	_
Similar	_	_
classes	_	_
present	_	_
ZSD	_	_
Baseline	_	_
=	_	_
6.3	_	_
,	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
=	_	_
6.5	_	_
,	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
=	_	_
4.4	_	_
ZSD	_	_
Baseline	_	_
=	_	_
18.6	_	_
,	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
=	_	_
22.7	_	_
,	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
=	_	_
27.4	_	_
Zero-Shot	_	_
Detection	_	_
(	_	_
ZSD	_	_
)	_	_
Baseline	_	_
12.7	_	_
0.0	_	_
3.9	_	_
0.5	_	_
0.0	_	_
36.3	_	_
2.7	_	_
1.8	_	_
1.7	_	_
12.2	_	_
2.7	_	_
7.0	_	_
1.0	_	_
0.6	_	_
22.0	_	_
19.0	_	_
1.9	_	_
40.9	_	_
75.3	_	_
0.3	_	_
28.4	_	_
17.9	_	_
12.0	_	_
4.0	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
15.0	_	_
0.0	_	_
8.0	_	_
0.2	_	_
0.2	_	_
39.2	_	_
2.3	_	_
1.9	_	_
3.2	_	_
11.7	_	_
4.8	_	_
0.0	_	_
0.0	_	_
7.1	_	_
23.3	_	_
25.7	_	_
5.0	_	_
50.5	_	_
75.3	_	_
0.0	_	_
44.8	_	_
7.8	_	_
28.9	_	_
4.5	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
16.4	_	_
5.6	_	_
1.0	_	_
0.1	_	_
0.0	_	_
27.8	_	_
1.7	_	_
1.5	_	_
1.6	_	_
7.2	_	_
2.2	_	_
0.0	_	_
4.1	_	_
5.3	_	_
26.7	_	_
65.6	_	_
4.0	_	_
47.3	_	_
71.5	_	_
21.5	_	_
51.1	_	_
3.7	_	_
26.2	_	_
1.2	_	_
Zero-Shot	_	_
Tagging	_	_
(	_	_
ZST	_	_
)	_	_
Baseline	_	_
23.3	_	_
2.9	_	_
13.4	_	_
9.6	_	_
3.1	_	_
61.7	_	_
20.7	_	_
16.3	_	_
7.5	_	_
29.4	_	_
8.6	_	_
12.2	_	_
8.5	_	_
4.9	_	_
46.2	_	_
30.7	_	_
11.0	_	_
51.8	_	_
77.6	_	_
9.0	_	_
46.1	_	_
39.0	_	_
12.7	_	_
12.6	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
27.5	_	_
2.9	_	_
20.8	_	_
10.5	_	_
3.3	_	_
72.5	_	_
27.7	_	_
16.7	_	_
7.9	_	_
22.9	_	_
14.3	_	_
2.8	_	_
6.7	_	_
14.5	_	_
46.8	_	_
42.6	_	_
16.0	_	_
59.1	_	_
80.0	_	_
12.9	_	_
67.3	_	_
34.1	_	_
34.0	_	_
17.1	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
30.6	_	_
12.6	_	_
10.2	_	_
11.9	_	_
4.9	_	_
48.9	_	_
21.8	_	_
17.9	_	_
29.1	_	_
32.2	_	_
10.0	_	_
4.1	_	_
20.7	_	_
10.7	_	_
52.2	_	_
82.6	_	_
12.3	_	_
58.5	_	_
75.5	_	_
48.9	_	_
72.2	_	_
16.9	_	_
33.9	_	_
15.5	_	_
Meta-class	_	_
Indoor	_	_
Musical	_	_
Food	_	_
Clothing	_	_
Appli	_	_
.	_	_

#248
Kitchen	_	_
Furn	_	_
.	_	_

#249
Electronic	_	_
Invertebra	_	_
.	_	_

#250
Mammal	_	_
Fish	_	_
Vehicle	_	_
Sport	_	_
Zero-Shot	_	_
Meta	_	_
Detection	_	_
(	_	_
ZSMD	_	_
)	_	_
Baseline	_	_
13.7	_	_
3.3	_	_
0.3	_	_
24.0	_	_
4.0	_	_
12.2	_	_
2.1	_	_
1.0	_	_
12.1	_	_
17.0	_	_
70.7	_	_
0.3	_	_
22.1	_	_
8.5	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
15.4	_	_
8.1	_	_
0.1	_	_
18.4	_	_
2.3	_	_
11.7	_	_
3.0	_	_
0.0	_	_
14.3	_	_
27.8	_	_
73.6	_	_
0.0	_	_
32.1	_	_
9.0	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
15.6	_	_
3.5	_	_
0.1	_	_
10.0	_	_
1.9	_	_
7.2	_	_
1.2	_	_
4.1	_	_
15.3	_	_
31.4	_	_
66.8	_	_
21.5	_	_
31.2	_	_
9.3	_	_
Zero-Shot	_	_
Meta-class	_	_
Tagging	_	_
(	_	_
ZSMT	_	_
)	_	_
Baseline	_	_
28.8	_	_
15.2	_	_
12.0	_	_
55.6	_	_
25.2	_	_
29.4	_	_
10.7	_	_
8.5	_	_
31.5	_	_
36.5	_	_
75.8	_	_
9.0	_	_
48.4	_	_
17.0	_	_
Ours	_	_
(	_	_
L′	_	_
mm	_	_
)	_	_
33.4	_	_
24.1	_	_
13.6	_	_
55.9	_	_
31.3	_	_
22.9	_	_
14.7	_	_
6.7	_	_
33.0	_	_
49.4	_	_
82.6	_	_
12.9	_	_
64.2	_	_
23.2	_	_
Ours	_	_
(	_	_
Lcls	_	_
)	_	_
39.9	_	_
19.2	_	_
15.5	_	_
45.6	_	_
38.5	_	_
32.2	_	_
12.4	_	_
20.7	_	_
40.3	_	_
58.2	_	_
84.8	_	_
48.9	_	_
74.7	_	_
27.1	_	_
Table	_	_
2	_	_
.	_	_

#251
Average	_	_
precision	_	_
of	_	_
individual	_	_
unseen	_	_
classes	_	_
using	_	_
ResNet+w2v	_	_
and	_	_
loss	_	_
configurations	_	_
L′	_	_
mm	_	_
and	_	_
Lcls	_	_
(	_	_
cluster	_	_
based	_	_
loss	_	_
with	_	_
λ	_	_
=	_	_
0.6	_	_
)	_	_
.	_	_

#252
We	_	_
have	_	_
grouped	_	_
unseen	_	_
classes	_	_
into	_	_
two	_	_
groups	_	_
based	_	_
on	_	_
whether	_	_
visually	_	_
similar	_	_
classes	_	_
present	_	_
in	_	_
the	_	_
seen	_	_
class	_	_
set	_	_
or	_	_
not	_	_
.	_	_

#253
Our	_	_
proposed	_	_
method	_	_
achieve	_	_
significant	_	_
performance	_	_
improvement	_	_
for	_	_
the	_	_
group	_	_
where	_	_
similar	_	_
classes	_	_
are	_	_
present	_	_
in	_	_
the	_	_
seen	_	_
set	_	_
.	_	_

#254
with	_	_
both	_	_
Word2vec	_	_
and	_	_
GloVe	_	_
as	_	_
the	_	_
semantic	_	_
embedding	_	_
vectors	_	_
used	_	_
to	_	_
define	_	_
W2	_	_
.	_	_

#255
Fig.	_	_
4	_	_
illustrates	_	_
some	_	_
qualitative	_	_
ZSD	_	_
examples	_	_
.	_	_

#256
More	_	_
performance	_	_
results	_	_
of	_	_
ZSD	_	_
on	_	_
other	_	_
datasets	_	_
is	_	_
provided	_	_
in	_	_
the	_	_
supplementary	_	_
material	_	_
.	_	_

#257
Overall	_	_
results	_	_
:	_	_
Table	_	_
1	_	_
reports	_	_
the	_	_
mAP	_	_
for	_	_
all	_	_
approaches	_	_
on	_	_
four	_	_
tasks	_	_
:	_	_
ZSD	_	_
,	_	_
ZSMD	_	_
,	_	_
ZST	_	_
,	_	_
and	_	_
ZSMT	_	_
across	_	_
different	_	_
combinations	_	_
of	_	_
network	_	_
architectures	_	_
.	_	_

#258
We	_	_
can	_	_
make	_	_
following	_	_
observations	_	_
:	_	_
(	_	_
1	_	_
)	_	_
Our	_	_
cluster	_	_
based	_	_
method	_	_
outperforms	_	_
other	_	_
competitors	_	_
on	_	_
all	_	_
four	_	_
tasks	_	_
because	_	_
its	_	_
loss	_	_
utilizes	_	_
high-level	_	_
semantic	_	_
relationships	_	_
from	_	_
meta-class	_	_
definitions	_	_
which	_	_
are	_	_
not	_	_
present	_	_
in	_	_
other	_	_
methods	_	_
.	_	_

#259
(	_	_
2	_	_
)	_	_
Performances	_	_
get	_	_
improved	_	_
from	_	_
baseline	_	_
to	_	_
Ours	_	_
(	_	_
with	_	_
L′mm	_	_
)	_	_
across	_	_
all	_	_
zero-shot	_	_
tasks	_	_
.	_	_

#260
The	_	_
reason	_	_
is	_	_
baseline	_	_
method	_	_
did	_	_
not	_	_
consider	_	_
word	_	_
vectors	_	_
during	_	_
the	_	_
training	_	_
.	_	_

#261
Thus	_	_
,	_	_
overall	_	_
detection	_	_
could	capability	_
not	_	_
get	_	_
enough	_	_
supervision	_	_
about	_	_
the	_	_
semantic	_	_
embeddings	_	_
of	_	_
classes	_	_
.	_	_

#262
In	_	_
contrast	_	_
,	_	_
L′mm	_	_
loss	_	_
formulation	_	_
considers	_	_
word	_	_
vectors	_	_
.	_	_

#263
(	_	_
3	_	_
)	_	_
Performances	_	_
get	_	_
improved	_	_
from	_	_
ZST	_	_
to	_	_
ZSMT	_	_
across	_	_
all	_	_
methods	_	_
whereas	_	_
similar	_	_
improvement	_	_
is	_	_
not	_	_
common	_	_
from	_	_
ZSD	_	_
to	_	_
ZSMD	_	_
.	_	_

#264
It’s	_	_
not	_	_
surprising	_	_
because	_	_
ZSMD	_	_
can	_	_
get	_	_
some	_	_
benefit	_	_
if	_	_
meta-class	_	_
of	_	_
the	_	_
predicted	_	_
class	_	_
is	_	_
same	_	_
as	_	_
the	_	_
meta-class	_	_
of	_	_
true	_	_
class	_	_
.	_	_

#265
If	_	_
this	_	_
is	_	_
violated	_	_
frequently	_	_
,	_	_
we	_	_
can	_	_
not	_	_
expect	_	_
significant	_	_
performance	_	_
improvement	_	_
in	_	_
ZSMD	_	_
.	_	_

#266
(	_	_
4	_	_
)	_	_
In	_	_
comparison	_	_
of	_	_
traditional	_	_
object	_	_
detection	_	_
results	_	_
,	_	_
ZSD	_	_
achieved	_	_
significantly	_	_
lower	_	_
performance	_	_
.	_	_

#267
Remarkably	_	_
,	_	_
even	_	_
the	_	_
state-of-the-art	_	_
zero-shot	_	_
classification	_	_
approaches	_	_
perform	_	_
quite	_	_
low	_	_
e.g.	_	_
,	_	_
a	_	_
recent	_	_
ZSL	_	_
method	_	_
[	_	_
51	_	_
]	_	_
reported	_	_
11	_	_
%	_	_
hit	_	_
@	_	_
1	_	_
rate	_	_
on	_	_
ILSVRC	_	_
2010/12	_	_
.	_	_

#268
This	_	_
trend	_	_
does	_	_
not	_	_
undermine	_	_
to	_	_
significance	_	_
of	_	_
ZSD	_	_
,	_	_
rather	_	_
highlights	_	_
the	_	_
underlying	_	_
challenges	_	_
.	_	_

#269
Individual	_	_
class	_	_
detection	_	_
:	_	_
Performances	_	_
of	_	_
individual	_	_
unseen	_	_
classes	_	_
indicate	_	_
the	_	_
challenges	_	_
for	_	_
ZSD	_	_
.	_	_

#270
In	_	_
Table	_	_
2	_	_
,	_	_
we	_	_
show	_	_
performances	_	_
of	_	_
individual	_	_
unseen	_	_
classes	_	_
across	_	_
all	_	_
tasks	_	_
with	_	_
our	_	_
best	_	_
(	_	_
R+w2v	_	_
)	_	_
network	_	_
.	_	_

#271
We	_	_
observe	_	_
that	_	_
the	_	_
unseen	_	_
classes	_	_
for	_	_
which	_	_
visually	_	_
similar	_	_
classes	_	_
are	_	_
present	_	_
in	_	_
their	_	_
metaZeroShot	_	_
Object	_	_
Detection	_	_
13	_	_
Top1	_	_
Accuracy	_	_
Network	_	_
w2v	_	_
glo	_	_
Akata’16	_	_
[	_	_
1	_	_
]	_	_
V	_	_
33.90	_	_
DMaPI’17	_	_
[	_	_
24	_	_
]	_	_
G+V	_	_
26.38	_	_
30.34	_	_
SCoRe’17	_	_
[	_	_
32	_	_
]	_	_
G	_	_
31.51	_	_
Akata’15	_	_
[	_	_
3	_	_
]	_	_
G	_	_
28.40	_	_
24.20	_	_
LATEM’16	_	_
[	_	_
46	_	_
]	_	_
G	_	_
31.80	_	_
32.50	_	_
DMaP-I’17	_	_
[	_	_
24	_	_
]	_	_
G	_	_
26.28	_	_
23.69	_	_
Ours	_	_
R	_	_
36.77	_	_
36.82	_	_
Table	_	_
3	_	_
.	_	_

#272
Zero	_	_
shot	_	_
recognition	_	_
on	_	_
CUB	_	_
using	_	_
λ	_	_
=	_	_
1	_	_
because	_	_
no	_	_
meta-class	_	_
assignment	_	_
is	_	_
done	_	_
here	_	_
.	_	_

#273
For	_	_
fairness	_	_
,	_	_
we	_	_
only	_	_
compared	_	_
our	_	_
result	_	_
with	_	_
the	_	_
inductive	_	_
setting	_	_
of	_	_
other	_	_
methods	_	_
without	_	_
per	_	_
image	_	_
part	_	_
annotation	_	_
and	_	_
description	_	_
.	_	_

#274
We	_	_
refer	_	_
V=VGG	_	_
,	_	_
R=ResNet	_	_
,	_	_
G=GoogLeNet	_	_
.	_	_

#275
classes	_	_
achieve	_	_
better	_	_
detection	_	_
performance	_	_
(	_	_
ZSD	_	_
mAP	_	_
18.6	_	_
,	_	_
22.7	_	_
,	_	_
27.4	_	_
)	_	_
than	_	_
those	_	_
which	_	_
do	_	_
not	_	_
have	_	_
similar	_	_
classes	_	_
(	_	_
ZSD	_	_
mAP	_	_
6.3	_	_
,	_	_
6.5	_	_
,	_	_
4.4	_	_
)	_	_
for	_	_
the	_	_
all	_	_
methods	_	_
(	_	_
baseline	_	_
,	_	_
our’s	_	_
with	_	_
L′mm	_	_
and	_	_
Lcls	_	_
)	_	_
.	_	_

#276
Our	_	_
proposed	_	_
cluster	_	_
method	_	_
with	_	_
loss	_	_
Lcls	_	_
outperforms	_	_
the	_	_
other	_	_
versions	_	_
significantly	_	_
for	_	_
the	_	_
case	_	_
when	_	_
visually	_	_
similar	_	_
classes	_	_
are	_	_
present	_	_
.	_	_

#277
For	_	_
the	_	_
all	_	_
classes	_	_
,	_	_
our	_	_
cluster	_	_
method	_	_
is	_	_
still	_	_
the	_	_
best	_	_
(	_	_
mAP	_	_
:	_	_
cluster	_	_
16.4	_	_
vs.	_	_
baseline	_	_
12.7	_	_
)	_	_
.	_	_

#278
However	_	_
,	_	_
our’s	_	_
with	_	_
L′mm	_	_
method	_	_
performs	_	_
better	_	_
for	_	_
when	_	_
case	_	_
similar	_	_
classes	_	_
are	_	_
not	_	_
present	_	_
(	_	_
mAP	_	_
6.5	_	_
vs	_	_
4.4	_	_
)	_	_
.	_	_

#279
For	_	_
the	_	_
easier	_	_
tagging	_	_
tasks	_	_
(	_	_
ZST	_	_
and	_	_
ZSMT	_	_
)	_	_
,	_	_
the	_	_
cluster	_	_
method	_	_
gets	_	_
superior	_	_
performance	_	_
in	_	_
most	_	_
of	_	_
the	_	_
cases	_	_
.	_	_

#280
This	_	_
indicates	_	_
that	_	_
one	_	_
potential	_	_
reason	_	_
for	_	_
the	_	_
failure	_	_
cases	_	_
of	_	_
our	_	_
cluster	_	_
method	_	_
for	_	_
ZSD	_	_
might	speculation	_
be	_	_
confusions	_	_
during	_	_
localization	_	_
of	_	_
objects	_	_
due	_	_
to	_	_
ambiguities	_	_
in	_	_
visual	_	_
appearance	_	_
of	_	_
unseen	_	_
classes	_	_
.	_	_

#281
In	_	_
Fig.	_	_
4	_	_
,	_	_
we	_	_
illustrate	_	_
the	_	_
effect	_	_
of	_	_
varying	_	_
λ	_	_
on	_	_
four	_	_
zero-shot	_	_
tasks	_	_
for	_	_
R+w2v	_	_
and	_	_
R+glo	_	_
.	_	_

#282
It	_	_
shows	_	_
that	_	_
performances	_	_
has	_	_
less	_	_
variation	_	_
in	_	_
the	_	_
range	_	_
of	_	_
λ	_	_
=	_	_
.5	_	_
to	_	_
.9	_	_
than	_	_
λ	_	_
=	_	_
.9	_	_
to	_	_
1	_	_
.	_	_

#283
For	_	_
a	_	_
larger	_	_
λ	_	_
,	_	_
mAP	_	_
starts	_	_
dropping	_	_
since	_	_
the	_	_
impact	_	_
of	_	_
Lmc	_	_
decreases	_	_
significantly	_	_
.	_	_

#284
4.3	_	_
Zero	_	_
Shot	_	_
Recognition	_	_
(	_	_
ZSR	_	_
)	_	_

#285
Being	_	_
a	_	_
detection	_	_
model	_	_
,	_	_
the	_	_
proposed	_	_
network	_	_
can	_	_
also	_	_
perform	_	_
traditional	_	_
ZSR	_	_
.	_	_

#286
We	_	_
evaluate	_	_
ZSR	_	_
performance	_	_
on	_	_
popular	_	_
Caltech-UCSD	_	_
Birds-200-2011	_	_
(	_	_
CUB	_	_
)	_	_
dataset	_	_
[	_	_
44	_	_
]	_	_
.	_	_

#287
This	_	_
dataset	_	_
contains	_	_
11,788	_	_
images	_	_
from	_	_
200	_	_
classes	_	_
and	_	_
provides	_	_
single	_	_
bounding	_	_
boxes	_	_
per	_	_
image	_	_
.	_	_

#288
Following	_	_
standard	_	_
train/test	_	_
split	_	_
[	_	_
47	_	_
]	_	_
,	_	_
we	_	_
use	_	_
150	_	_
seen	_	_
and	_	_
50	_	_
unseen	_	_
classes	_	_
for	_	_
experiments	_	_
.	_	_

#289
For	_	_
semantics	_	_
embedding	_	_
,	_	_
we	_	_
use	_	_
400-d	_	_
word2vec	_	_
(	_	_
w2v	_	_
)	_	_
and	_	_
GloVe	_	_
(	_	_
glo	_	_
)	_	_
vector	_	_
[	_	_
46	_	_
]	_	_
.	_	_

#290
Note	_	_
that	_	_
,	_	_
we	_	_
do	_	_
not	_	_
use	_	_
per	_	_
image	_	_
part	_	_
annotation	_	_
(	_	_
like	_	_
[	_	_
1	_	_
]	_	_
)	_	_
and	_	_
descriptions	_	_
(	_	_
like	_	_
[	_	_
51	_	_
]	_	_
)	_	_
to	_	_
enrich	_	_
semantic	_	_
embedding	_	_
.	_	_

#291
For	_	_
a	_	_
given	_	_
test	_	_
image	_	_
,	_	_
our	_	_
network	_	_
predicts	_	_
unseen	_	_
class	_	_
bounding	_	_
boxes	_	_
.	_	_

#292
We	_	_
pick	_	_
only	_	_
one	_	_
label	_	_
with	_	_
the	_	_
highest	_	_
prediction	_	_
score	_	_
per	_	_
image	_	_
.	_	_

#293
In	_	_
this	_	_
way	_	_
,	_	_
we	_	_
report	_	_
the	_	_
mean	_	_
Top1	_	_
accuracy	_	_
of	_	_
all	_	_
unseen	_	_
classes	_	_
in	_	_
Table	_	_
3	_	_
.	_	_

#294
One	_	_
can	_	_
find	_	_
our	_	_
proposed	_	_
solution	_	_
achieve	_	_
significant	_	_
performance	_	_
improvement	_	_
in	_	_
comparison	_	_
with	_	_
state-of-the-art	_	_
methods	_	_
.	_	_

#295
14	_	_
Shafin	_	_
Rahman	_	_
,	_	_
Salman	_	_
Khan	_	_
and	_	_
Fatih	_	_
Porikli	_	_

#296
Fig.	_	_
5	_	_
.	_	_

#297
Selected	_	_
examples	_	_
of	_	_
ZSD	_	_
of	_	_
our	_	_
cluster	_	_
(	_	_
λ	_	_
=	_	_
.6	_	_
)	_	_
method	_	_
with	_	_
R+w2v	_	_
,	_	_
using	_	_
the	_	_
prediction	_	_
score	_	_
threshold	_	_
=	_	_
0.3	_	_
.	_	_

#298
(	_	_
See	_	_
supplementary	_	_
material	_	_
for	_	_
more	_	_
examples	_	_
)	_	_

#299
4.4	_	_
Challenges	_	_
and	_	_
New	_	_
Directions	_	_

#300
ZSD	_	_
is	_	_
challenging	_	_
:	_	_
Our	_	_
empirical	_	_
evaluations	_	_
show	_	_
that	_	_
ZSD	_	_
needs	_	_
to	_	_
deal	_	_
with	_	_
the	_	_
following	_	_
challenges	_	_
:	_	_
(	_	_
1	_	_
)	_	_
Unseen	_	_
classes	_	_
are	_	_
rare	_	_
compared	_	_
to	_	_
seen	_	_
classes	_	_
;	_	_
(	_	_
2	_	_
)	_	_
Small	_	_
unseen	_	_
objects	_	_
are	_	_
hard	_	_
to	_	_
detect	_	_
and	_	_
harder	_	_
to	_	_
relate	_	_
with	_	_
their	_	_
semantics	_	_
;	_	_
(	_	_
3	_	_
)	_	_
The	_	_
scarcity	_	_
of	_	_
similar	_	_
seen	_	_
class	_	_
leads	_	_
to	_	_
an	_	_
inadequate	_	_
description	_	_
of	_	_
an	_	_
unseen	_	_
class	_	_
;	_	_
(	_	_
4	_	_
)	_	_
As	_	_
derived	_	_
in	_	_
an	_	_
unsupervised	_	_
manner	_	_
,	_	_
the	_	_
noise	_	_
of	_	_
semantic	_	_
space	_	_
affects	_	_
ZSD	_	_
.	_	_

#301
These	_	_
issues	_	_
are	_	_
discussed	_	_
in	_	_
detail	_	_
in	_	_
supplementary	_	_
material	_	_
.	_	_

#302
Future	_	_
challenges	_	_
:	_	_
The	_	_
ZSD	_	_
problem	_	_
warrants	_	_
further	_	_
investigation	_	_
.	_	_

#303
(	_	_
1	_	_
)	_	_
Unlink	_	_
current	_	_
work	_	_
one	_	_
can	_	_
consider	_	_
fine-tuning	_	_
the	_	_
bounding	_	_
box	_	_
of	_	_
the	_	_
both	_	_
seen	_	_
and	_	_
unseen	_	_
classes	_	_
based	_	_
on	_	_
visual	_	_
and	_	_
semantic	_	_
correspondences	_	_
.	_	_

#304
(	_	_
2	_	_
)	_	_
Rather	_	_
mapping	_	_
image	_	_
feature	_	_
to	_	_
the	_	_
semantic	_	_
space	_	_
,	_	_
the	_	_
reverse	_	_
mapping	_	_
may	_	_
help	_	_
ZSD	_	_
similar	_	_
to	_	_
ZSR	_	_
used	_	_
in	_	_
[	_	_
19,51	_	_
]	_	_
.	_	_

#305
(	_	_
3	_	_
)	_	_
One	_	_
can	_	_
consider	_	_
the	_	_
fusion	_	_
of	_	_
different	_	_
word	_	_
vectors	_	_
(	_	_
word2vec	_	_
and	_	_
GloVe	_	_
)	_	_
to	_	_
improve	_	_
ZSD	_	_
.	_	_

#306
(	_	_
4	_	_
)	_	_
Like	_	_
generalized	_	_
ZSL	_	_
[	_	_
48,47,24	_	_
]	_	_
,	_	_
one	_	_
can	_	_
extend	_	_
it	_	_
to	_	_
a	_	_
more	_	_
realistic	_	_
generalized	_	_
ZSD	_	_
.	_	_

#307
Moreover	_	_
,	_	_
weakly	_	_
supervised	_	_
or	_	_
semi-supervised	_	_
version	_	_
of	_	_
zero	_	_
shot	_	_
problems	_	_
is	_	_
also	_	_
possible	_	_
while	_	_
performing	_	_
ZSD/GZSD	_	_
.	_	_

#308
5	_	_
Conclusion	_	_

#309
While	_	_
traditional	_	_
ZSL	_	_
research	_	_
focuses	_	_
on	_	_
only	_	_
object	_	_
recognition	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
extend	_	_
the	_	_
problem	_	_
to	_	_
object	_	_
detection	_	_
(	_	_
ZSD	_	_
)	_	_
.	_	_

#310
To	_	_
this	_	_
end	_	_
,	_	_
we	_	_
offer	_	_
a	_	_
new	_	_
experimental	_	_
protocol	_	_
with	_	_
ILSVRC-2017	_	_
dataset	_	_
specifying	_	_
the	_	_
seen-unseen	_	_
,	_	_
train-test	_	_
split	_	_
.	_	_

#311
We	_	_
also	_	_
develop	_	_
an	_	_
end-to-end	_	_
trainable	_	_
CNN	_	_
model	_	_
to	_	_
solve	_	_
this	_	_
problem	_	_
.	_	_

#312
We	_	_
show	_	_
that	_	_
our	_	_
solution	_	_
is	_	_
better	_	_
than	_	_
a	_	_
strong	_	_
baseline	_	_
.	_	_

#313
Zero-Shot	_	_
Object	_	_
Detection	_	_
15	_	_
Overall	_	_
,	_	_
this	_	_
research	_	_
throws	_	_
some	_	_
new	_	_
challenges	_	_
to	_	_
ZSL	_	_
community	_	_
.	_	_

#314
To	_	_
make	_	_
a	_	_
long-standing	_	_
progress	_	_
in	_	_
ZSL	_	_
,	_	_
the	_	_
community	_	_
needs	_	_
to	_	_
move	_	_
forward	_	_
in	_	_
the	_	_
detection	_	_
setting	_	_
rather	_	_
than	_	_
merely	_	_
recognition	_	_
.	_	_