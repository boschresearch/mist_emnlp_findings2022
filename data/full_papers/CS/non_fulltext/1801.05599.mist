#0
Additive	_	_
Margin	_	_
Softmax	_	_
for	_	_
Face	_	_
Verification	_	_
Feng	_	_
Wang	_	_
UESTC	_	_
feng.wff	_	_
@	_	_
gmail.com	_	_
Weiyang	_	_
Liu	_	_
Georgia	_	_
Tech	_	_
wyliu	_	_
@	_	_
gatech.edu	_	_
Haijun	_	_
Liu	_	_
UESTC	_	_
haijun	_	_
liu	_	_
@	_	_
126.com	_	_
Jian	_	_
Cheng	_	_
UESTC	_	_
chengjian	_	_
@	_	_
uestc.edu.cn	_	_

#1
Abstract	_	_

#2
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
conceptually	_	_
simple	_	_
and	_	_
geometrically	_	_
interpretable	_	_
objective	_	_
function	_	_
,	_	_
i.e.	_	_
additive	_	_
margin	_	_
Softmax	_	_
(	_	_
AM-Softmax	_	_
)	_	_
,	_	_
for	_	_
deep	_	_
face	_	_
verification	_	_
.	_	_

#3
In	_	_
general	_	_
,	_	_
the	_	_
face	_	_
verification	_	_
task	_	_
can	_	_
be	_	_
viewed	_	_
as	_	_
a	_	_
metric	_	_
learning	_	_
problem	_	_
,	_	_
so	_	_
learning	_	_
large-margin	_	_
face	_	_
features	_	_
whose	_	_
intra-class	_	_
variation	_	_
is	_	_
small	_	_
and	_	_
inter-class	_	_
difference	_	_
is	_	_
large	_	_
is	_	_
of	_	_
great	_	_
importance	_	_
in	_	_
order	_	_
to	_	_
achieve	_	_
good	_	_
performance	_	_
.	_	_

#4
Recently	_	_
,	_	_
Large-margin	_	_
Softmax	_	_
[	_	_
10	_	_
]	_	_
and	_	_
Angular	_	_
Softmax	_	_
[	_	_
9	_	_
]	_	_
have	_	_
been	_	_
proposed	_	_
to	_	_
incorporate	_	_
the	_	_
angular	_	_
margin	_	_
in	_	_
a	_	_
multiplicative	_	_
manner	_	_
.	_	_

#5
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
novel	_	_
additive	_	_
angular	_	_
margin	_	_
for	_	_
the	_	_
Softmax	_	_
loss	_	_
,	_	_
which	_	_
is	_	_
intuitively	_	_
appealing	_	_
and	_	_
more	_	_
interpretable	_	_
than	_	_
the	_	_
existing	_	_
works	_	_
.	_	_

#6
We	_	_
also	_	_
emphasize	_	_
and	_	_
discuss	_	_
the	_	_
importance	_	_
of	_	_
feature	_	_
normalization	_	_
in	_	_
the	_	_
paper	_	_
.	_	_

#7
Most	_	_
importantly	_	_
,	_	_
our	_	_
experiments	_	_
on	_	_
LFW	_	_
and	_	_
MegaFace	_	_
show	_	_
that	_	_
our	_	_
additive	_	_
margin	_	_
softmax	_	_
loss	_	_
consistently	_	_
performs	_	_
better	_	_
than	_	_
the	_	_
current	_	_
state-of-the-art	_	_
methods	_	_
using	_	_
the	_	_
same	_	_
network	_	_
architecture	_	_
and	_	_
training	_	_
dataset	_	_
.	_	_

#8
Our	_	_
code	_	_
has	_	_
also	_	_
been	_	_
made	_	_
available1	_	_
.	_	_

#9
1	_	_
.	_	_

#10
Introduction	_	_
Face	_	_
verification	_	_
is	_	_
widely	_	_
used	_	_
for	_	_
identity	_	_
authentication	_	_
in	_	_
enormous	_	_
areas	_	_
such	_	_
as	_	_
finance	_	_
,	_	_
military	_	_
,	_	_
public	_	_
security	_	_
and	_	_
so	_	_
on	_	_
.	_	_

#11
Nowadays	_	_
,	_	_
most	_	_
face	_	_
verification	_	_
models	_	_
are	_	_
built	_	_
upon	_	_
Deep	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
and	_	_
supervised	_	_
by	_	_
classification	_	_
loss	_	_
functions	_	_
[	_	_
18	_	_
,	_	_
20	_	_
,	_	_
19	_	_
,	_	_
9	_	_
]	_	_
,	_	_
metric	_	_
learning	_	_
loss	_	_
functions	_	_
[	_	_
16	_	_
]	_	_
or	_	_
both	_	_
[	_	_
17	_	_
,	_	_
13	_	_
]	_	_
.	_	_

#12
Metric	_	_
learning	_	_
loss	_	_
functions	_	_
such	_	_
as	_	_
contrastive	_	_
loss	_	_
[	_	_
17	_	_
]	_	_
or	_	_
triplet	_	_
loss	_	_
[	_	_
16	_	_
]	_	_
usually	_	_
require	_	_
carefully	_	_
designed	_	_
sample	_	_
mining	_	_
strategies	_	_
and	_	_
the	_	_
final	_	_
performance	_	_
is	_	_
very	_	_
sensitive	_	_
to	_	_
these	_	_
strategies	_	_
,	_	_
so	_	_
increasingly	_	_
more	_	_
researchers	_	_
shift	_	_
their	_	_
attentions	_	_
to	_	_
building	_	_
deep	_	_
face	_	_
verification	_	_
models	_	_
based	_	_
on	_	_
improved	_	_
classification	_	_
loss	_	_
functions	_	_
[	_	_
20	_	_
,	_	_
19	_	_
,	_	_
9	_	_
]	_	_
.	_	_

#13
Current	_	_
prevailing	_	_
classification	_	_
loss	_	_
functions	_	_
for	_	_
deep	_	_
face	_	_
recognition	_	_
are	_	_
mostly	_	_
based	_	_
on	_	_
the	_	_
widely-used	_	_
soft-max	_	_
loss	_	_
.	_	_

#14
The	_	_
softmax	_	_
loss	_	_
is	_	_
typically	_	_
good	_	_
at	_	_
optimizing	_	_
1https	_	_
:	_	_
//github.com/happynear/AMSoftmax	_	_
W1	_	_
W2	_	_
Decision	_	_
Boundary	_	_
W1	_	_
W2	_	_
Decision	_	_
Boundary	_	_
for	_	_
Class	_	_
1	_	_
Decision	_	_
Boundary	_	_
for	_	_
Class	_	_
2	_	_
Class	_	_
1	_	_
Class	_	_
2	_	_
Class	_	_
1	_	_
Class	_	_
2	_	_
Original	_	_
Softmax	_	_
Additive	_	_
Margin	_	_
Softmax	_	_
Figure	_	_
1	_	_
.	_	_

#15
Comparison	_	_
between	_	_
the	_	_
original	_	_
softmax	_	_
loss	_	_
and	_	_
the	_	_
additive	_	_
margin	_	_
softmax	_	_
loss	_	_
.	_	_

#16
Note	_	_
that	_	_
,	_	_
the	_	_
angular	_	_
softmax	_	_
[	_	_
9	_	_
]	_	_
can	_	_
only	_	_
impose	_	_
unfixed	_	_
angular	_	_
margin	_	_
,	_	_
while	_	_
the	_	_
additive	_	_
margin	_	_
softmax	_	_
incorporates	_	_
the	_	_
fixed	_	_
hard	_	_
angular	_	_
margin	_	_
.	_	_

#17
the	_	_
inter-class	_	_
difference	_	_
(	_	_
i.e.	_	_
,	_	_
separating	_	_
different	_	_
classes	_	_
)	_	_
,	_	_
but	_	_
not	_	_
good	_	_
at	_	_
reducing	_	_
the	_	_
intra-class	_	_
variation	_	_
(	_	_
i.e.	_	_
,	_	_
making	_	_
features	_	_
of	_	_
the	_	_
same	_	_
class	_	_
compact	_	_
)	_	_
.	_	_

#18
To	_	_
address	_	_
this	_	_
,	_	_
lots	_	_
of	_	_
new	_	_
loss	_	_
functions	_	_
are	_	_
proposed	_	_
to	_	_
minimize	_	_
the	_	_
intra-class	_	_
variation	_	_
.	_	_

#19
[	_	_
20	_	_
]	_	_
proposed	_	_
to	_	_
add	_	_
a	_	_
regularization	_	_
term	_	_
to	_	_
penalize	_	_
the	_	_
feature-to-center	_	_
distances	_	_
.	_	_

#20
In	_	_
[	_	_
19	_	_
,	_	_
12	_	_
,	_	_
15	_	_
]	_	_
,	_	_
researchers	_	_
proposed	_	_
to	_	_
use	_	_
a	_	_
scale	_	_
parameter	_	_
to	_	_
control	_	_
the	_	_
”temperature”	_	_
[	_	_
2	_	_
]	_	_
of	_	_
the	_	_
softmax	_	_
loss	_	_
,	_	_
producing	_	_
higher	_	_
gradients	_	_
to	_	_
the	_	_
well-separated	_	_
samples	_	_
to	_	_
further	_	_
shrink	_	_
the	_	_
intra-class	_	_
variance	_	_
.	_	_

#21
In	_	_
[	_	_
9	_	_
,	_	_
10	_	_
]	_	_
,	_	_
the	_	_
authors	_	_
introduced	_	_
an	_	_
conceptually	_	_
appealing	_	_
angular	_	_
margin	_	_
to	_	_
push	_	_
the	_	_
classification	_	_
boundary	_	_
closer	_	_
to	_	_
the	_	_
weight	_	_
vector	_	_
of	_	_
each	_	_
class	_	_
.	_	_

#22
[	_	_
9	_	_
]	_	_
also	_	_
provided	_	_
a	_	_
theoretical	_	_
guidance	_	_
of	_	_
training	_	_
a	_	_
deep	_	_
model	_	_
for	_	_
metric	_	_
learning	_	_
tasks	_	_
using	_	_
the	_	_
classification	_	_
loss	_	_
functions	_	_
.	_	_

#23
[	_	_
6	_	_
,	_	_
12	_	_
,	_	_
15	_	_
]	_	_
also	_	_
improved	_	_
the	_	_
softmax	_	_
loss	_	_
by	_	_
incorporating	_	_
differnet	_	_
kinds	_	_
of	_	_
margins	_	_
.	_	_

#24
In	_	_
this	_	_
work	_	_
,	_	_
we	_	_
propose	_	_
a	_	_
novel	_	_
and	_	_
more	_	_
interpretable	_	_
way	_	_
to	_	_
import	_	_
the	_	_
angular	_	_
margin	_	_
into	_	_
the	_	_
softmax	_	_
loss	_	_
.	_	_

#25
We	_	_
formulate	_	_
an	_	_
additive	_	_
margin	_	_
via	_	_
cos	_	_
θ−m	_	_
,	_	_
which	_	_
is	_	_
simpler	_	_
than	_	_
[	_	_
9	_	_
]	_	_
and	_	_
yields	_	_
better	_	_
performance	_	_
.	_	_

#26
From	_	_
Equation	_	_
(	_	_
3	_	_
)	_	_
,	_	_
we	_	_
can	_	_
see	_	_
thatm	_	_
is	_	_
multiplied	_	_
to	_	_
the	_	_
target	_	_
angle	_	_
θyi	_	_
in	_	_
[	_	_
9	_	_
]	_	_
,	_	_
so	_	_
this	_	_
type	_	_
of	_	_
margin	_	_
is	_	_
incorporated	_	_
in	_	_
a	_	_
multiplicative	_	_
manner	_	_
.	_	_

#27
Since	_	_
our	_	_
margin	_	_
is	_	_
a	_	_
scalar	_	_
subtracted	_	_
from	_	_
cosθ	_	_
,	_	_
we	_	_
call	_	_
our	_	_
loss	_	_
function	_	_
Additive	_	_
Margin	_	_
Softmax	_	_
(	_	_
AMSoftmax	_	_
)	_	_
.	_	_

#28
Experiments	_	_
on	_	_
LFW	_	_
BLUFR	_	_
protocol	_	_
[	_	_
7	_	_
]	_	_
and	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
show	_	_
that	_	_
our	_	_
loss	_	_
function	_	_
with	_	_
the	_	_
same	_	_
network	_	_
architecture	_	_
achieves	_	_
better	_	_
results	_	_
than	_	_
the	_	_
current	_	_
state-of-the-art	_	_
approaches	_	_
.	_	_

#29
ar	_	_
X	_	_
iv	_	_
:1	_	_
1	_	_
.	_	_

#30
9v	_	_
4	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
3	_	_
0	_	_
M	_	_
ay	_	_
2	_	_
2	_	_
.	_	_

#31
Preliminaries	_	_
To	_	_
better	_	_
understand	_	_
the	_	_
proposed	_	_
AM-Softmax	_	_
loss	_	_
,	_	_
we	_	_
will	_	_
first	_	_
give	_	_
a	_	_
brief	_	_
review	_	_
of	_	_
the	_	_
original	_	_
softmax	_	_
and	_	_
the	_	_
A-softmax	_	_
loss	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#32
The	_	_
formulation	_	_
of	_	_
the	_	_
original	_	_
softmax	_	_
loss	_	_
is	_	_
given	_	_
by	_	_
LS	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
eW	_	_
T	_	_
yi	_	_
fi∑c	_	_
j=1	_	_
e	_	_
WT	_	_
j	_	_
fi	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
e‖Wyi	_	_
‖‖fi‖cos	_	_
(	_	_
θyi	_	_
)	_	_
∑c	_	_
j=1	_	_
e	_	_
‖Wj‖‖fi‖cos	_	_
(	_	_
θj	_	_
)	_	_
,	_	_
(	_	_
1	_	_
)	_	_
where	_	_
f	_	_
is	_	_
the	_	_
input	_	_
of	_	_
the	_	_
last	_	_
fully	_	_
connected	_	_
layer	_	_
(	_	_
fi	_	_
denotes	_	_
the	_	_
the	_	_
i-th	_	_
sample	_	_
)	_	_
,	_	_
Wj	_	_
is	_	_
the	_	_
j-th	_	_
column	_	_
of	_	_
the	_	_
last	_	_
fully	_	_
connected	_	_
layer	_	_
.	_	_

#33
The	_	_
WT	_	_
yifi	_	_
is	_	_
also	_	_
called	_	_
as	_	_
the	_	_
target	_	_
logit	_	_
[	_	_
14	_	_
]	_	_
of	_	_
the	_	_
i-th	_	_
sample	_	_
.	_	_

#34
In	_	_
the	_	_
A-softmax	_	_
loss	_	_
,	_	_
the	_	_
authors	_	_
proposed	_	_
to	_	_
normalize	_	_
the	_	_
weight	_	_
vectors	_	_
(	_	_
making	_	_
‖Wi‖	_	_
to	_	_
be	_	_
1	_	_
)	_	_
and	_	_
generalize	_	_
the	_	_
target	_	_
logit	_	_
from	_	_
‖fi‖cos	_	_
(	_	_
θyi	_	_
)	_	_
to	_	_
‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
,	_	_
LAS	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
e‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
e‖fi‖ψ	_	_
(	_	_
θyi	_	_
)	_	_
+	_	_
∑c	_	_
j=1	_	_
,	_	_
j	_	_
6=yi	_	_
e	_	_
‖fi‖cos	_	_
(	_	_
θj	_	_
)	_	_
,	_	_
(	_	_
2	_	_
)	_	_
where	_	_
the	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
is	_	_
usually	_	_
a	_	_
piece-wise	_	_
function	_	_
defined	_	_
as	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
(	_	_
−1	_	_
)	_	_
k	_	_
cos	_	_
(	_	_
mθ	_	_
)	_	_
−	_	_
2k	_	_
+	_	_
λcos	_	_
(	_	_
θ	_	_
)	_	_
1	_	_
+	_	_
λ	_	_
,	_	_
θ	_	_
∈	_	_
[	_	_
kπ	_	_
m	_	_
,	_	_
(	_	_
k	_	_
+	_	_
1	_	_
)	_	_
π	_	_
m	_	_
]	_	_
,	_	_
(	_	_
3	_	_
)	_	_
where	_	_
m	_	_
is	_	_
usually	_	_
an	_	_
integer	_	_
larger	_	_
than	_	_
1	_	_
and	_	_
λ	_	_
is	_	_
a	_	_
hyper-parameter	_	_
to	_	_
control	_	_
how	_	_
hard	_	_
the	_	_
classification	_	_
boundary	_	_
should	deontic	_
be	_	_
pushed	_	_
.	_	_

#35
During	_	_
training	_	_
,	_	_
the	_	_
λ	_	_
is	_	_
annealing	_	_
from	_	_
1	_	_
,	_	_
000	_	_
to	_	_
a	_	_
small	_	_
value	_	_
to	_	_
make	_	_
the	_	_
angular	_	_
space	_	_
of	_	_
each	_	_
class	_	_
become	_	_
more	_	_
and	_	_
more	_	_
compact	_	_
.	_	_

#36
In	_	_
their	_	_
experiments	_	_
,	_	_
they	_	_
set	_	_
the	_	_
minimum	_	_
value	_	_
of	_	_
λ	_	_
to	_	_
be	_	_
5	_	_
and	_	_
m	_	_
=	_	_
4	_	_
,	_	_
which	_	_
is	_	_
approximately	_	_
equivalent	_	_
to	_	_
m	_	_
=	_	_
1.5	_	_
(	_	_
Figure	_	_
2	_	_
)	_	_
.	_	_

#37
3	_	_
.	_	_

#38
Additive	_	_
Margin	_	_
Softmax	_	_
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
first	_	_
describe	_	_
the	_	_
definition	_	_
of	_	_
the	_	_
proposed	_	_
loss	_	_
function	_	_
.	_	_

#39
Then	_	_
we	_	_
will	_	_
discuss	_	_
about	_	_
the	_	_
intuition	_	_
and	_	_
interpretation	_	_
of	_	_
the	_	_
loss	_	_
function	_	_
.	_	_

#40
3.1	_	_
.	_	_

#41
Definition	_	_
[	_	_
10	_	_
]	_	_
defines	_	_
a	_	_
general	_	_
function	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
to	_	_
introduce	_	_
the	_	_
large	_	_
margin	_	_
property	_	_
.	_	_

#42
Motivated	_	_
by	_	_
that	_	_
,	_	_
we	_	_
further	_	_
propose	_	_
a	_	_
specific	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
that	_	_
introduces	_	_
an	_	_
additive	_	_
margin	_	_
to	_	_
the	_	_
softmax	_	_
loss	_	_
function	_	_
.	_	_

#43
The	_	_
formulation	_	_
is	_	_
given	_	_
by	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
=	_	_
cosθ	_	_
−m	_	_
.	_	_

#44
(	_	_
4	_	_
)	_	_
Compared	_	_
to	_	_
the	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
defined	_	_
in	_	_
L-Softmax	_	_
[	_	_
10	_	_
]	_	_
and	_	_
A-softmax	_	_
[	_	_
9	_	_
]	_	_
(	_	_
Equation	_	_
(	_	_
3	_	_
)	_	_
)	_	_
,	_	_
our	_	_
definition	_	_
is	_	_
more	_	_
simple	_	_
0°	_	_
20°	_	_
40°	_	_
60°	_	_
80°	_	_
100°	_	_
120°	_	_
140°	_	_
160°	_	_
180°	_	_
angle	_	_
-7	_	_
-6	_	_
-5	_	_
-4	_	_
-3	_	_
-2	_	_
-1	_	_
ta	_	_
rg	_	_
et	_	_
lo	_	_
gi	_	_
t	_	_
Conventional	_	_
Softmax	_	_
Angular	_	_
Softmax	_	_
(	_	_
m=2	_	_
,	_	_
6=0	_	_
)	_	_
Angular	_	_
Softmax	_	_
(	_	_
m=4	_	_
,	_	_
6=0	_	_
)	_	_
Angular	_	_
Softmax	_	_
(	_	_
m=4	_	_
,	_	_
6=5	_	_
)	_	_
Additive	_	_
Margin	_	_
Softmax	_	_
(	_	_
m=0.35	_	_
)	_	_
Figure	_	_
2.	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
for	_	_
conventional	_	_
Softmax	_	_
,	_	_
Angular	_	_
Softmax	_	_
[	_	_
9	_	_
]	_	_
and	_	_
our	_	_
proposed	_	_
Hard	_	_
Margin	_	_
Softmax	_	_
.	_	_

#45
For	_	_
Angular	_	_
Softmax	_	_
,	_	_
we	_	_
plot	_	_
the	_	_
logit	_	_
curve	_	_
for	_	_
three	_	_
parameter	_	_
sets	_	_
.	_	_

#46
From	_	_
the	_	_
curves	_	_
we	_	_
can	_	_
infer	_	_
that	_	_
m	_	_
=	_	_
4	_	_
,	_	_
λ	_	_
=	_	_
5	_	_
lies	_	_
between	_	_
conventional	_	_
Softmax	_	_
and	_	_
Angular	_	_
Softmax	_	_
with	_	_
m	_	_
=	_	_
2	_	_
,	_	_
λ	_	_
=	_	_
0	_	_
,	_	_
which	_	_
means	_	_
it	_	_
is	_	_
approximately	_	_
m	_	_
=	_	_
1.5	_	_
.	_	_

#47
Our	_	_
proposed	_	_
Additive	_	_
Margin	_	_
Softmax	_	_
with	_	_
optimized	_	_
parameter	_	_
m	_	_
=	_	_
0.35	_	_
is	_	_
also	_	_
plotted	_	_
and	_	_
we	_	_
can	_	_
observe	_	_
that	_	_
it	_	_
is	_	_
similar	_	_
with	_	_
Angular	_	_
Softmax	_	_
withm	_	_
=	_	_
4	_	_
,	_	_
λ	_	_
=	_	_
5	_	_
in	_	_
the	_	_
range	_	_
[	_	_
0◦	_	_
,	_	_
90◦	_	_
]	_	_
,	_	_
in	_	_
which	_	_
most	_	_
of	_	_
the	_	_
real-world	_	_
θs	_	_
lie	_	_
.	_	_

#48
and	_	_
intuitive	_	_
.	_	_

#49
During	_	_
implementation	_	_
,	_	_
the	_	_
input	_	_
after	_	_
normalizing	_	_
both	_	_
the	_	_
feature	_	_
and	_	_
the	_	_
weight	_	_
is	_	_
actually	_	_
x	_	_
=	_	_
cosθyi	_	_
=	_	_
WT	_	_
yi	_	_
fi	_	_
‖Wyi	_	_
‖‖fi‖	_	_
,	_	_
so	_	_
in	_	_
the	_	_
forward	_	_
propagation	_	_
we	_	_
only	_	_
need	_	_
to	_	_
compute	_	_
Ψ	_	_
(	_	_
x	_	_
)	_	_
=	_	_
x−m	_	_
.	_	_

#50
(	_	_
5	_	_
)	_	_
In	_	_
this	_	_
margin	_	_
scheme	_	_
,	_	_
we	_	_
don’t	_	_
need	_	_
to	_	_
calculate	_	_
the	_	_
gradient	_	_
for	_	_
back-propagation	_	_
because	_	_
Ψ′	_	_
(	_	_
x	_	_
)	_	_
=	_	_
1	_	_
.	_	_

#51
It	_	_
is	_	_
much	_	_
easier	_	_
to	_	_
implement	_	_
compared	_	_
with	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#52
Since	_	_
we	_	_
use	_	_
cosine	_	_
as	_	_
the	_	_
similarity	_	_
to	_	_
compare	_	_
two	_	_
face	_	_
features	_	_
,	_	_
we	_	_
follow	_	_
[	_	_
19	_	_
,	_	_
11	_	_
,	_	_
12	_	_
]	_	_
to	_	_
apply	_	_
both	_	_
feature	_	_
normalization	_	_
and	_	_
weight	_	_
normalization	_	_
to	_	_
the	_	_
inner	_	_
product	_	_
layer	_	_
in	_	_
order	_	_
to	_	_
build	_	_
a	_	_
cosine	_	_
layer	_	_
.	_	_

#53
Then	_	_
we	_	_
scale	_	_
the	_	_
cosine	_	_
values	_	_
using	_	_
a	_	_
hyper-parameter	_	_
s	_	_
as	_	_
suggested	_	_
in	_	_
[	_	_
19	_	_
,	_	_
11	_	_
,	_	_
12	_	_
]	_	_
.	_	_

#54
Finally	_	_
,	_	_
the	_	_
loss	_	_
function	_	_
becomes	_	_
LAMS	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
es·	_	_
(	_	_
cosθyi−m	_	_
)	_	_
es·	_	_
(	_	_
cosθyi−m	_	_
)	_	_
+	_	_
∑c	_	_
j=1	_	_
,	_	_
j	_	_
6=yi	_	_
e	_	_
s·cosθj	_	_
=	_	_
−	_	_
1	_	_
n	_	_
n∑	_	_
i=1	_	_
log	_	_
es·	_	_
(	_	_
W	_	_
T	_	_
yi	_	_
fi−m	_	_
)	_	_
es·	_	_
(	_	_
W	_	_
T	_	_
yi	_	_
fi−m	_	_
)	_	_
+	_	_
∑c	_	_
j=1	_	_
,	_	_
j	_	_
6=yi	_	_
e	_	_
sWT	_	_
j	_	_
fi	_	_
.	_	_

#55
(	_	_
6	_	_
)	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
assume	_	_
that	_	_
the	_	_
norm	_	_
of	_	_
bothWi	_	_
and	_	_
f	_	_
are	_	_
normalized	_	_
to	_	_
1	_	_
if	_	_
not	_	_
specified	_	_
.	_	_

#56
In	_	_
[	_	_
19	_	_
]	_	_
,	_	_
the	_	_
authors	_	_
propose	_	_
to	_	_
let	_	_
the	_	_
scaling	_	_
factor	_	_
s	_	_
to	_	_
be	_	_
learned	_	_
through	_	_
backpropagation	_	_
.	_	_

#57
However	_	_
,	_	_
after	_	_
the	_	_
margin	_	_
is	_	_
introduced	_	_
into	_	_
the	_	_
loss	_	_
function	_	_
,	_	_
we	_	_
find	_	_
that	_	_
the	_	_
s	_	_
will	_	_
not	_	_
increase	_	_
and	_	_
the	_	_
network	_	_
converges	_	_
very	_	_
slowly	_	_
if	_	_
we	_	_
let	_	_
s	_	_
to	_	_
be	_	_
learned	_	_
.	_	_

#58
Thus	_	_
,	_	_
we	_	_
fix	_	_
s	_	_
to	_	_
be	_	_
a	_	_
large	_	_
enough	_	_
value	_	_
,	_	_
e.g.	_	_
30	_	_
,	_	_
to	_	_
accelerate	_	_
and	_	_
stablize	_	_
the	_	_
optimization	_	_
.	_	_

#59
As	_	_
described	_	_
in	_	_
Section	_	_
2	_	_
,	_	_
[	_	_
10	_	_
,	_	_
9	_	_
]	_	_
propose	_	_
to	_	_
use	_	_
an	_	_
annealing	_	_
strategy	_	_
to	_	_
set	_	_
the	_	_
hyper-parameter	_	_
λ	_	_
to	_	_
avoid	_	_
network	_	_
divergence	_	_
.	_	_

#60
However	_	_
,	_	_
to	_	_
set	_	_
the	_	_
annealing	_	_
curve	_	_
of	_	_
λ	_	_
,	_	_
lots	_	_
of	_	_
extra	_	_
parameters	_	_
are	_	_
introduced	_	_
,	_	_
which	_	_
are	_	_
more	_	_
or	_	_
less	_	_
confusing	_	_
for	_	_
starters	_	_
.	_	_

#61
Although	_	_
properly	_	_
tuning	_	_
those	_	_
hyper-parameters	_	_
for	_	_
λ	_	_
could	speculation	_
lead	_	_
to	_	_
impressive	_	_
results	_	_
,	_	_
the	_	_
hyper-parameters	_	_
are	_	_
still	_	_
quite	_	_
difficult	_	_
to	_	_
tune	_	_
.	_	_

#62
With	_	_
our	_	_
margin	_	_
scheme	_	_
,	_	_
we	_	_
find	_	_
that	_	_
we	_	_
no	_	_
longer	_	_
need	_	_
the	_	_
help	_	_
of	_	_
the	_	_
annealing	_	_
strategy	_	_
.	_	_

#63
The	_	_
network	_	_
can	_	_
converge	_	_
flexibly	_	_
even	_	_
if	_	_
we	_	_
fix	_	_
the	_	_
hyper-parameter	_	_
m	_	_
from	_	_
scratch	_	_
.	_	_

#64
Compared	_	_
to	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
,	_	_
our	_	_
additive	_	_
margin	_	_
scheme	_	_
is	_	_
more	_	_
friendly	_	_
to	_	_
those	_	_
who	_	_
are	_	_
not	_	_
familiar	_	_
with	_	_
the	_	_
effects	_	_
of	_	_
the	_	_
hyper-parameters	_	_
.	_	_

#65
Another	_	_
recently	_	_
proposed	_	_
additive	_	_
margin	_	_
is	_	_
also	_	_
described	_	_
in	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#66
Our	_	_
AM-Softmax	_	_
is	_	_
different	_	_
than	_	_
[	_	_
6	_	_
]	_	_
in	_	_
the	_	_
sense	_	_
that	_	_
our	_	_
feature	_	_
and	_	_
weight	_	_
are	_	_
normalized	_	_
to	_	_
a	_	_
predefined	_	_
constant	_	_
s.	_	_
The	_	_
normalization	_	_
is	_	_
the	_	_
key	_	_
to	_	_
the	_	_
angular	_	_
margin	_	_
property	_	_
.	_	_

#67
Without	_	_
the	_	_
normalization	_	_
,	_	_
the	_	_
margin	_	_
m	_	_
does	_	_
not	_	_
necessarily	_	_
lead	_	_
to	_	_
large	_	_
angular	_	_
margin	_	_
.	_	_

#68
3.2	_	_
.	_	_

#69
Discussion	_	_

#70
3.2.1	_	_
Geometric	_	_
Interpretation	_	_

#71
Our	_	_
additive	_	_
margin	_	_
scheme	_	_
has	_	_
a	_	_
clear	_	_
geometric	_	_
interpretation	_	_
on	_	_
the	_	_
hypersphere	_	_
manifold	_	_
.	_	_

#72
In	_	_
Figure	_	_
3	_	_
,	_	_
we	_	_
draw	_	_
a	_	_
schematic	_	_
diagram	_	_
to	_	_
show	_	_
the	_	_
decision	_	_
boundary	_	_
of	_	_
both	_	_
conventional	_	_
softmax	_	_
loss	_	_
and	_	_
our	_	_
AM-Softmax	_	_
.	_	_

#73
For	_	_
example	_	_
,	_	_
in	_	_
Figure	_	_
3	_	_
,	_	_
the	_	_
features	_	_
are	_	_
of	_	_
2	_	_
dimensions	_	_
.	_	_

#74
After	_	_
normalization	_	_
,	_	_
the	_	_
features	_	_
are	_	_
on	_	_
a	_	_
circle	_	_
and	_	_
the	_	_
decision	_	_
boundary	_	_
of	_	_
the	_	_
traditional	_	_
softmax	_	_
loss	_	_
is	_	_
denoted	_	_
as	_	_
the	_	_
vector	_	_
P0	_	_
.	_	_

#75
In	_	_
this	_	_
case	_	_
,	_	_
we	_	_
have	_	_
WT	_	_

#76
1	_	_
P0	_	_
=	_	_
WT	_	_

#77
2	_	_
P0	_	_
at	_	_
the	_	_

#78
decision	_	_
boundary	_	_
P0	_	_
.	_	_

#79
For	_	_
our	_	_
AM-Softmax	_	_
,	_	_
the	_	_
boundary	_	_
becomes	_	_
a	_	_
marginal	_	_
region	_	_
instead	_	_
of	_	_
a	_	_
single	_	_
vector	_	_
.	_	_

#80
At	_	_
the	_	_
new	_	_
boundary	_	_
P1	_	_
for	_	_
class	_	_
1	_	_
,	_	_
we	_	_
have	_	_
WT	_	_

#81
1	_	_
P1	_	_
−	_	_
m	_	_
=	_	_
WT	_	_

#82
2	_	_
P1	_	_
,	_	_
which	_	_

#83
gives	_	_
m	_	_
=	_	_
(	_	_
W1	_	_
−W2	_	_
)	_	_
TP1	_	_
=	_	_
cos	_	_
(	_	_
θW1	_	_
,	_	_
P1	_	_
)	_	_
−	_	_
cos	_	_
(	_	_
θW2	_	_
,	_	_
P1	_	_
)	_	_
.	_	_

#84
If	_	_
we	_	_
further	_	_
assume	_	_
that	_	_
all	_	_
the	_	_
classes	_	_
have	_	_
the	_	_
same	_	_
intra-class	_	_
variance	_	_
and	_	_
the	_	_
boundary	_	_
for	_	_
class	_	_
2	_	_
is	_	_
at	_	_
P2	_	_
,	_	_
we	_	_
can	_	_
get	_	_
cos	_	_
(	_	_
θW2	_	_
,	_	_
P1	_	_
)	_	_
=	_	_
cos	_	_
(	_	_
θW1	_	_
,	_	_
P2	_	_
)	_	_
(	_	_
Fig.	_	_
3	_	_
)	_	_
.	_	_

#85
Thus	_	_
,	_	_
m	_	_
=	_	_
cos	_	_
(	_	_
θW1	_	_
,	_	_
P1	_	_
)	_	_
−	_	_
cos	_	_
(	_	_
θW1	_	_
,	_	_
P2	_	_
)	_	_
,	_	_
which	_	_
is	_	_
the	_	_
difference	_	_
of	_	_
the	_	_
cosine	_	_
scores	_	_
for	_	_
class	_	_
1	_	_
between	_	_
the	_	_
two	_	_
sides	_	_
of	_	_
the	_	_
margin	_	_
region	_	_
.	_	_

#86
3.2.2	_	_
Angular	_	_
Margin	_	_
or	_	_
Cosine	_	_
Margin	_	_

#87
In	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
,	_	_
the	_	_
margin	_	_
m	_	_
is	_	_
multiplied	_	_
to	_	_
θ	_	_
,	_	_
so	_	_
the	_	_
angular	_	_
margin	_	_
is	_	_
incorporated	_	_
into	_	_
the	_	_
loss	_	_
in	_	_
a	_	_
multiplicative	_	_
way	_	_
.	_	_

#88
In	_	_
our	_	_
proposed	_	_
loss	_	_
function	_	_
,	_	_
the	_	_
margin	_	_
is	_	_
enforced	_	_
by	_	_
subtracting	_	_
m	_	_
from	_	_
cos	_	_
θ	_	_
,	_	_
so	_	_
our	_	_
margin	_	_
is	_	_
incorporated	_	_
into	_	_
the	_	_
loss	_	_
in	_	_
an	_	_
additive	_	_
way	_	_
,	_	_
which	_	_
is	_	_
one	_	_
of	_	_
the	_	_
most	_	_
significant	_	_
differences	_	_
than	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#89
It	_	_
is	_	_
also	_	_
worth	_	_
mentioning	_	_
that	_	_
despite	_	_
the	_	_
difference	_	_
of	_	_
enforcing	_	_
margin	_	_
,	_	_
these	_	_
two	_	_
types	_	_
of	_	_
margin	_	_
formulations	_	_
are	_	_
also	_	_
different	_	_
in	_	_
the	_	_
base	_	_
values	_	_
.	_	_

#90
Specifically	_	_
,	_	_
one	_	_
is	_	_
θ	_	_
and	_	_
the	_	_
other	_	_
is	_	_
cos	_	_
θ	_	_
.	_	_

#91
Although	_	_
usually	_	_
the	_	_
cosine	_	_
margin	_	_
has	_	_
an	_	_
one-to-one	_	_
mapping	_	_
to	_	_
the	_	_
angular	_	_
margin	_	_
,	_	_
there	_	_
will	_	_
still	_	_
be	_	_
some	_	_
difference	_	_
while	_	_
optimizing	_	_
them	_	_
due	_	_
to	_	_
the	_	_
non-linearity	_	_
induced	_	_
by	_	_
the	_	_
cosine	_	_
function	_	_
.	_	_

#92
Whether	_	_
we	_	_
should	deontic	_
use	_	_
the	_	_
cosine	_	_
margin	_	_
depends	_	_
on	_	_
which	_	_
similarity	_	_
measurement	_	_
(	_	_
or	_	_
distance	_	_
)	_	_
the	_	_
final	_	_
loss	_	_
function	_	_
is	_	_
optimizing	_	_
.	_	_

#93
Obviously	_	_
,	_	_
our	_	_
modified	_	_
softmax	_	_
loss	_	_
function	_	_
is	_	_
optimizing	_	_
the	_	_
cosine	_	_
similarity	_	_
,	_	_
not	_	_
the	_	_
angle	_	_
.	_	_

#94
This	_	_
may	options	_
not	_	_
be	_	_
a	_	_
problem	_	_
if	_	_
we	_	_
are	_	_
using	_	_
the	_	_
conventional	_	_
softmax	_	_
loss	_	_
because	_	_
the	_	_
decision	_	_
boundaries	_	_
are	_	_
the	_	_
same	_	_
in	_	_
these	_	_
two	_	_
forms	_	_
(	_	_
cos	_	_
θ1	_	_
=	_	_
cos	_	_
θ2	_	_
⇒	_	_
θ1	_	_
=	_	_
θ2	_	_
)	_	_
.	_	_

#95
However	_	_
,	_	_
when	_	_
we	_	_
are	_	_
trying	_	_
to	_	_
push	_	_
the	_	_
boundary	_	_
,	_	_
we	_	_
will	_	_
face	_	_
a	_	_
problem	_	_
that	_	_
these	_	_
two	_	_
similarities	_	_
(	_	_
distances	_	_
)	_	_
have	_	_
different	_	_
densities	_	_
.	_	_

#96
Cosine	_	_
values	_	_
are	_	_
more	_	_
dense	_	_
when	_	_
the	_	_
angles	_	_
are	_	_
near	_	_
0	_	_
or	_	_
π	_	_
.	_	_

#97
If	_	_
we	_	_
want	_	_
to	_	_
optimize	_	_
the	_	_
angle	_	_
,	_	_
an	_	_
arccos	_	_
operation	_	_
may	options	_
be	_	_
required	_	_
after	_	_
the	_	_
value	_	_
of	_	_
the	_	_
inner	_	_
product	_	_
WTf	_	_
is	_	_
obtained	_	_
.	_	_

#98
It	_	_
will	_	_
potentially	_	_
be	_	_
more	_	_
computationally	_	_
expensive	_	_
.	_	_

#99
In	_	_
general	_	_
,	_	_
angular	_	_
margin	_	_
is	_	_
conceptually	_	_
better	_	_
than	_	_
the	_	_
cosine	_	_
margin	_	_
,	_	_
but	_	_
considering	_	_
the	_	_
computational	_	_
cost	_	_
,	_	_
cosine	_	_
margin	_	_
is	_	_
more	_	_
appealing	_	_
in	_	_
the	_	_
sense	_	_
that	_	_
it	_	_
could	capability	_
achieve	_	_
the	_	_
same	_	_
goal	_	_
with	_	_
less	_	_
efforts	_	_
.	_	_

#100
3.2.3	_	_
Feature	_	_
Normalization	_	_

#101
In	_	_
the	_	_
SphereFace	_	_
model	_	_
[	_	_
9	_	_
]	_	_
,	_	_
the	_	_
authors	_	_
added	_	_
the	_	_
weight	_	_
normalization	_	_
based	_	_
on	_	_
Large	_	_
Margin	_	_
Softmax	_	_
[	_	_
10	_	_
]	_	_
,	_	_
leaving	_	_
the	_	_
feature	_	_
still	_	_
not	_	_
normalized	_	_
.	_	_

#102
Our	_	_
loss	_	_
function	_	_
,	_	_
following	_	_
[	_	_
19	_	_
,	_	_
12	_	_
,	_	_
15	_	_
]	_	_
,	_	_
applies	_	_
feature	_	_
normalization	_	_
and	_	_
uses	_	_
a	_	_
global	_	_
scale	_	_
factor	_	_
s	_	_
to	_	_
replace	_	_
the	_	_
sample-dependent	_	_
feature	_	_
norm	_	_
in	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#103
One	_	_
question	_	_
arises	_	_
:	_	_
when	_	_
should	deontic	_
we	_	_
add	_	_
the	_	_
feature	_	_
normalization	_	_
?	_	_

#104
Our	_	_
answer	_	_
is	_	_
that	_	_
it	_	_
depends	_	_
on	_	_
the	_	_
image	_	_
quality	_	_
.	_	_

#105
In	_	_
[	_	_
15	_	_
]	_	_
’s	_	_
Figure	_	_
1	_	_
,	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
feature	_	_
norm	_	_
is	_	_
highly	_	_
correlated	_	_
with	_	_
the	_	_
quality	_	_
of	_	_
the	_	_
image	_	_
.	_	_

#106
Note	_	_
that	_	_
back	_	_
propagation	_	_
has	_	_
a	_	_
property	_	_
that	_	_
,	_	_
y	_	_
=	_	_
x	_	_
α	_	_
⇒	_	_
dy	_	_
dx	_	_
=	_	_
α	_	_
.	_	_

#107
(	_	_
7	_	_
)	_	_
Thus	_	_
,	_	_
after	_	_
normalization	_	_
,	_	_
features	_	_
with	_	_
small	_	_
norms	_	_
will	_	_
get	_	_
much	_	_
bigger	_	_
gradient	_	_
compared	_	_
with	_	_
features	_	_
that	_	_
have	_	_
big	_	_
norms	_	_
(	_	_
Figure	_	_
5	_	_
)	_	_
.	_	_

#108
By	_	_
back-propagation	_	_
,	_	_
the	_	_
network	_	_
will	_	_
pay	_	_
more	_	_
attention	_	_
to	_	_
the	_	_
low-quality	_	_
face	_	_
images	_	_
,	_	_
which	_	_
usually	_	_
have	_	_
small	_	_
norms	_	_
.	_	_

#109
Its	_	_
effect	_	_
is	_	_
very	_	_
similar	_	_
with	_	_
hard	_	_
sample	_	_
mining	_	_
[	_	_
16	_	_
,	_	_
8	_	_
]	_	_
.	_	_

#110
The	_	_
advantages	_	_
of	_	_
feature	_	_
normalization	_	_
are	_	_
also	_	_
revealed	_	_
in	_	_
[	_	_
11	_	_
]	_	_
.	_	_

#111
As	_	_
a	_	_
conclusion	_	_
,	_	_
feature	_	_
normalization	_	_
is	_	_
most	_	_
suitable	_	_
for	_	_
tasks	_	_
whose	_	_
image	_	_
quality	_	_
is	_	_
very	_	_
low	_	_
.	_	_

#112
From	_	_
Figure	_	_
5	_	_
we	_	_
can	_	_
see	_	_
that	_	_
the	_	_
gradient	_	_
norm	_	_
may	options	_
be	_	_
extremely	_	_
big	_	_
when	_	_
the	_	_
feature	_	_
norm	_	_
is	_	_
very	_	_
small	_	_
.	_	_

#113
This	_	_
potentially	_	_
increases	_	_
the	_	_
risk	_	_
of	_	_
gradient	_	_
explosion	_	_
,	_	_
even	_	_
though	_	_
we	_	_
may	speculation	_
not	_	_
come	_	_
across	_	_
many	_	_
samples	_	_
with	_	_
very	_	_
small	_	_
feature	_	_
norm	_	_
.	_	_

#114
Maybe	_	_
some	_	_
re-weighting	_	_
strategy	_	_
whose	_	_
feature-gradient	_	_
norm	_	_
curve	_	_
is	_	_
between	_	_
the	_	_
two	_	_
curves	_	_
in	_	_
Figure	_	_
5	_	_
could	speculation	_
potentially	_	_
work	_	_
better	_	_
.	_	_

#115
This	_	_
is	_	_
an	_	_
interesting	_	_
topic	_	_
to	_	_
be	_	_
studied	_	_
in	_	_
the	_	_
future	_	_
.	_	_

#116
3.2.4	_	_
Feature	_	_
Distribution	_	_
Visualization	_	_

#117
To	_	_
better	_	_
understand	_	_
the	_	_
effect	_	_
of	_	_
our	_	_
loss	_	_
function	_	_
,	_	_
we	_	_
designed	_	_
a	_	_
toy	_	_
experiment	_	_
to	_	_
visualize	_	_
the	_	_
feature	_	_
distributions	_	_
trained	_	_
by	_	_
several	_	_
loss	_	_
functions	_	_
.	_	_

#118
We	_	_
used	_	_
Fashion	_	_
MNIST	_	_
[	_	_
21	_	_
]	_	_
(	_	_
10	_	_
classes	_	_
)	_	_
to	_	_
train	_	_
several	_	_
7-layer	_	_
CNN	_	_
models	_	_
which	_	_
output	_	_
3-dimensional	_	_
features	_	_
.	_	_

#119
These	_	_
networks	_	_
are	_	_
supervised	_	_
by	_	_
different	_	_
loss	_	_
functions	_	_
.	_	_

#120
After	_	_
we	_	_
obtain	_	_
the	_	_
3-dimensional	_	_
features	_	_
,	_	_
we	_	_
normalize	_	_
and	_	_
plot	_	_
them	_	_
on	_	_
a	_	_
hypersphere	_	_
(	_	_
ball	_	_
)	_	_
in	_	_
the	_	_
3	_	_
dimensional	_	_
space	_	_
(	_	_
Figure	_	_
4	_	_
)	_	_
.	_	_

#121
From	_	_
the	_	_
visualization	_	_
,	_	_
we	_	_
can	_	_
empirically	_	_
show	_	_
that	_	_
our	_	_
AM-Softmax	_	_
performs	_	_
similarly	_	_
with	_	_
the	_	_
best	_	_
SphereFace	_	_
[	_	_
9	_	_
]	_	_
(	_	_
A-Softmax	_	_
)	_	_
model	_	_
when	_	_
we	_	_
set	_	_
s	_	_
=	_	_
10	_	_
,	_	_
m	_	_
=	_	_
0.2	_	_
.	_	_

#122
Moreover	_	_
,	_	_
our	_	_
loss	_	_
function	_	_
can	_	_
further	_	_
shrink	_	_
the	_	_
intra-class	_	_
variance	_	_
by	_	_
setting	_	_
a	_	_
larger	_	_
m.	_	_
Compared	_	_
to	_	_
A-Softmax	_	_
[	_	_
9	_	_
]	_	_
,	_	_
the	_	_
AM-Softmax	_	_
loss	_	_
also	_	_
converges	_	_
easier	_	_
with	_	_
proper	_	_
scaling	_	_
factor	_	_
s	_	_
.	_	_

#123
The	_	_
visualized	_	_
3D	_	_
features	_	_
well	_	_
demonstrates	_	_
that	_	_
AM-Softmax	_	_
could	capability	_
bring	_	_
the	_	_
large	_	_
margin	_	_
property	_	_
to	_	_
the	_	_
features	_	_
without	_	_
tuning	_	_
too	_	_
many	_	_
hyper-parameters	_	_
.	_	_

#124
4.	_	_
Experiment	_	_

#125
In	_	_
this	_	_
section	_	_
,	_	_
we	_	_
will	_	_
firstly	_	_
describe	_	_
the	_	_
experimental	_	_
settings	_	_
.	_	_

#126
Then	_	_
we	_	_
will	_	_
discuss	_	_
the	_	_
overlapping	_	_
problem	_	_
of	_	_
the	_	_
modern	_	_
in-the-wild	_	_
face	_	_
datasets	_	_
.	_	_

#127
Finally	_	_
we	_	_
will	_	_
compare	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
loss	_	_
function	_	_
with	_	_
several	_	_
previous	_	_
state-of-the-art	_	_
loss	_	_
functions	_	_
.	_	_

#128
4.1	_	_
.	_	_

#129
Implementation	_	_
Details	_	_
Our	_	_
loss	_	_
function	_	_
is	_	_
implemented	_	_
using	_	_
Caffe	_	_
framework	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#130
We	_	_
follow	_	_
all	_	_
the	_	_
experimental	_	_
settings	_	_
from	_	_
[	_	_
9	_	_
]	_	_
,	_	_
including	_	_
the	_	_
image	_	_
resolution	_	_
,	_	_
preprocessing	_	_
method	_	_
and	_	_
the	_	_
network	_	_
structure	_	_
.	_	_

#131
Specifically	_	_
speaking	_	_
,	_	_
we	_	_
use	_	_
MTCNN	_	_
[	_	_
24	_	_
]	_	_
to	_	_
detect	_	_
faces	_	_
and	_	_
facial	_	_
landmarks	_	_
in	_	_
images	_	_
.	_	_

#132
Then	_	_
LFW	_	_
[	_	_
3	_	_
]	_	_
LFW	_	_
BLUFR	_	_
[	_	_
7	_	_
]	_	_
LFW	_	_
BLUFR	_	_
[	_	_
7	_	_
]	_	_
LFW	_	_
BLUFR	_	_
[	_	_
7	_	_
]	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
Loss	_	_
Function	_	_
m	_	_
6	_	_
,	_	_
000	_	_
pairs	_	_
VR	_	_
@	_	_
FAR=0.01	_	_
%	_	_
VR	_	_
@	_	_
FAR=0.1	_	_
%	_	_
DIR	_	_
@	_	_
FAR=1	_	_
%	_	_
Rank1	_	_
@	_	_
1e6	_	_
VR	_	_
@	_	_
FAR=1e−6	_	_
Softmax	_	_
-	_	_
97.08	_	_
%	_	_
60.26	_	_
%	_	_
78.26	_	_
%	_	_
50.85	_	_
%	_	_
45.26	_	_
%	_	_
50.12	_	_
%	_	_
Softmax+75	_	_
%	_	_
dropout	_	_
-	_	_
98.62	_	_
%	_	_
77.64	_	_
%	_	_
90.91	_	_
%	_	_
63.72	_	_
%	_	_
57.32	_	_
%	_	_
65.58	_	_
%	_	_
Center	_	_
Loss	_	_
[	_	_
20	_	_
]	_	_
-	_	_
99.00	_	_
%	_	_
83.30	_	_
%	_	_
94.50	_	_
%	_	_
65.46	_	_
%	_	_
63.38	_	_
%	_	_
75.68	_	_
%	_	_
NormFace	_	_
[	_	_
19	_	_
]	_	_
-	_	_
98.98	_	_
%	_	_
88.15	_	_
%	_	_
96.16	_	_
%	_	_
75.22	_	_
%	_	_
65.03	_	_
%	_	_
75.88	_	_
%	_	_
A-Softmax	_	_
[	_	_
9	_	_
]	_	_
∼1.5	_	_
99.08	_	_
%	_	_
91.26	_	_
%	_	_
97.06	_	_
%	_	_
81.93	_	_
%	_	_
67.41	_	_
%	_	_
78.19	_	_
%	_	_
AM-Softmax	_	_
0.25	_	_
99.13	_	_
%	_	_
91.97	_	_
%	_	_
97.13	_	_
%	_	_
81.42	_	_
%	_	_
70.81	_	_
%	_	_
83.01	_	_
%	_	_
AM-Softmax	_	_
0.3	_	_
99.08	_	_
%	_	_
93.18	_	_
%	_	_
97.56	_	_
%	_	_
84.02	_	_
%	_	_
72.01	_	_
%	_	_
83.29	_	_
%	_	_
AM-Softmax	_	_
0.35	_	_
98.98	_	_
%	_	_
93.51	_	_
%	_	_
97.69	_	_
%	_	_
84.82	_	_
%	_	_
72.47	_	_
%	_	_
84.44	_	_
%	_	_
AM-Softmax	_	_
0.4	_	_
99.17	_	_
%	_	_
93.60	_	_
%	_	_
97.71	_	_
%	_	_
84.51	_	_
%	_	_
72.44	_	_
%	_	_
83.50	_	_
%	_	_
AM-Softmax	_	_
0.45	_	_
99.03	_	_
%	_	_
93.44	_	_
%	_	_
97.60	_	_
%	_	_
84.59	_	_
%	_	_
72.22	_	_
%	_	_
83.00	_	_
%	_	_
AM-Softmax	_	_
0.5	_	_
99.10	_	_
%	_	_
92.33	_	_
%	_	_
97.28	_	_
%	_	_
83.38	_	_
%	_	_
71.56	_	_
%	_	_
82.49	_	_
%	_	_
AM-Softmax	_	_
w/o	_	_
FN	_	_
0.35	_	_
99.08	_	_
%	_	_
93.86	_	_
%	_	_
97.63	_	_
%	_	_
87.58	_	_
%	_	_
70.71	_	_
%	_	_
82.66	_	_
%	_	_
AM-Softmax	_	_
w/o	_	_
FN	_	_
0.4	_	_
99.12	_	_
%	_	_
94.48	_	_
%	_	_
97.96	_	_
%	_	_
87.31	_	_
%	_	_
70.96	_	_
%	_	_
83.11	_	_
%	_	_
Table	_	_
1	_	_
.	_	_

#133
Performance	_	_
on	_	_
modified	_	_
ResNet-20	_	_
with	_	_
various	_	_
loss	_	_
functions	_	_
.	_	_

#134
Note	_	_
that	_	_
,	_	_
for	_	_
Center	_	_
Loss	_	_
[	_	_
20	_	_
]	_	_
and	_	_
NormFace	_	_
[	_	_
19	_	_
]	_	_
,	_	_
we	_	_
used	_	_
modified	_	_
ResNet-28	_	_
[	_	_
20	_	_
]	_	_
because	_	_
we	_	_
failed	_	_
to	_	_
train	_	_
a	_	_
model	_	_
using	_	_
Center	_	_
Loss	_	_
on	_	_
modified	_	_
ResNet-20	_	_
[	_	_
9	_	_
]	_	_
and	_	_
the	_	_
NormFace	_	_
model	_	_
was	_	_
fine-tuned	_	_
based	_	_
on	_	_
the	_	_
Center	_	_
Loss	_	_
model	_	_
.	_	_

#135
the	_	_
faces	_	_
are	_	_
aligned	_	_
according	_	_
to	_	_
the	_	_
detected	_	_
landmarks	_	_
.	_	_

#136
The	_	_
aligned	_	_
face	_	_
images	_	_
are	_	_
of	_	_
size	_	_
112	_	_
×	_	_
96	_	_
,	_	_
and	_	_
are	_	_
normalized	_	_
by	_	_
subtracting	_	_
128	_	_
and	_	_
dividing	_	_
128	_	_
.	_	_

#137
Our	_	_
network	_	_
structure	_	_
follows	_	_
[	_	_
9	_	_
]	_	_
,	_	_
which	_	_
is	_	_
a	_	_
modified	_	_
ResNet	_	_
[	_	_
1	_	_
]	_	_
with	_	_
20	_	_
layers	_	_
that	_	_
is	_	_
adapted	_	_
to	_	_
face	_	_
recognition	_	_
.	_	_

#138
All	_	_
the	_	_
networks	_	_
are	_	_
trained	_	_
from	_	_
scratch	_	_
.	_	_

#139
We	_	_
set	_	_
the	_	_
weight	_	_
decay	_	_
parameter	_	_
to	_	_
be	_	_
5e−4	_	_
.	_	_

#140
The	_	_
batch	_	_
size	_	_
is	_	_
256	_	_
and	_	_
the	_	_
learning	_	_
rate	_	_
begins	_	_
with	_	_
0.1	_	_
and	_	_
is	_	_
divided	_	_
by	_	_
10	_	_
at	_	_
the	_	_
16K	_	_
,	_	_
24K	_	_
and	_	_
28K	_	_
iterations	_	_
.	_	_

#141
The	_	_
training	_	_
is	_	_
finished	_	_
at	_	_
30K	_	_
iterations	_	_
.	_	_

#142
During	_	_
training	_	_
,	_	_
we	_	_
only	_	_
use	_	_
image	_	_
mirror	_	_
to	_	_
augment	_	_
the	_	_
dataset	_	_
.	_	_

#143
In	_	_
testing	_	_
phase	_	_
,	_	_
We	_	_
feed	_	_
both	_	_
frontal	_	_
face	_	_
images	_	_
and	_	_
mirror	_	_
face	_	_
images	_	_
and	_	_
extract	_	_
the	_	_
features	_	_
from	_	_
the	_	_
output	_	_
of	_	_
the	_	_
first	_	_
inner-product	_	_
layer	_	_
.	_	_

#144
Then	_	_
the	_	_
two	_	_
features	_	_
are	_	_
summed	_	_
together	_	_
as	_	_
the	_	_
representation	_	_
of	_	_
the	_	_
face	_	_
image	_	_
.	_	_

#145
When	_	_
comparing	_	_
two	_	_
face	_	_
images	_	_
,	_	_
cosine	_	_
similarity	_	_
is	_	_
utilized	_	_
as	_	_
the	_	_
measurement	_	_
.	_	_

#146
4.2	_	_
.	_	_

#147
Dataset	_	_
Overlap	_	_
Removal	_	_
The	_	_
dataset	_	_
we	_	_
use	_	_
for	_	_
training	_	_
is	_	_
CASIA-Webface	_	_
[	_	_
22	_	_
]	_	_
,	_	_
which	_	_
contains	_	_
494,414	_	_
training	_	_
images	_	_
from	_	_
10,575	_	_
identities	_	_
.	_	_

#148
To	_	_
perform	_	_
open-set	_	_
evaluations	_	_
,	_	_
we	_	_
carefully	_	_
remove	_	_
the	_	_
overlapped	_	_
identities	_	_
between	_	_
training	_	_
dataset	_	_
(	_	_
CASIAWebface	_	_
[	_	_
22	_	_
]	_	_
)	_	_
and	_	_
testing	_	_
datasets	_	_
(	_	_
LFW	_	_
[	_	_
3	_	_
]	_	_
and	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
)	_	_
.	_	_

#149
Finally	_	_
,	_	_
we	_	_
find	_	_
17	_	_
overlapped	_	_
identities	_	_
between	_	_
CASIA-Webface	_	_
and	_	_
LFW	_	_
,	_	_
and	_	_
42	_	_
overlapped	_	_
identities	_	_
between	_	_
CASIA-Webface	_	_
and	_	_
MegaFace	_	_
set1	_	_
.	_	_

#150
Note	_	_
that	_	_
there	_	_
are	_	_
only	_	_
80	_	_
identities	_	_
in	_	_
MegaFace	_	_
set1	_	_
,	_	_
i.e.	_	_
over	_	_
half	_	_
of	_	_
the	_	_
identities	_	_
are	_	_
already	_	_
in	_	_
the	_	_
training	_	_
dataset	_	_
.	_	_

#151
The	_	_
effect	_	_
of	_	_
overlap	_	_
removal	_	_
is	_	_
remarkable	_	_
for	_	_
MegaFace	_	_
(	_	_
Table	_	_
4.2	_	_
)	_	_
.	_	_

#152
To	_	_
be	_	_
rigorous	_	_
,	_	_
all	_	_
the	_	_
experiments	_	_
in	_	_
this	_	_
paper	_	_
are	_	_
based	_	_
on	_	_
the	_	_
cleaned	_	_
dataset	_	_
.	_	_

#153
We	_	_
have	_	_
made	_	_
our	_	_
overlap	_	_
checking	_	_
code	_	_
publicly	_	_
available2	_	_
to	_	_
encourage	_	_
researchers	_	_
to	_	_
clean	_	_
2https	_	_
:	_	_
//github.com/happynear/FaceDatasets	_	_
their	_	_
training	_	_
datasets	_	_
before	_	_
experiments	_	_
.	_	_

#154
Loss	_	_
Overlap	_	_
MegaFace	_	_
MegaFace	_	_
Function	_	_
Removal	_	_
?	_	_

#155
Rank1	_	_
VR	_	_
AM-Softmax	_	_
No	_	_
75.23	_	_
%	_	_
87.06	_	_
%	_	_
AM-Softmax	_	_
Yes	_	_
72.47	_	_
%	_	_
84.44	_	_
%	_	_
Table	_	_
2	_	_
.	_	_

#156
Effect	_	_
of	_	_
Overlap	_	_
Removal	_	_
on	_	_
modified	_	_
ResNet-20	_	_
In	_	_
our	_	_
paper	_	_
,	_	_
we	_	_
re-train	_	_
some	_	_
of	_	_
the	_	_
previous	_	_
loss	_	_
functions	_	_
on	_	_
the	_	_
cleaned	_	_
dataset	_	_
as	_	_
the	_	_
baselines	_	_
for	_	_
comparison	_	_
.	_	_

#157
Note	_	_
that	_	_
,	_	_
we	_	_
make	_	_
our	_	_
experiments	_	_
fair	_	_
by	_	_
using	_	_
the	_	_
same	_	_
network	_	_
architecture	_	_
and	_	_
training	_	_
dataset	_	_
for	_	_
every	_	_
compared	_	_
methods	_	_
.	_	_

#158
4.3	_	_
.	_	_

#159
Effect	_	_
of	_	_
Hyper-parameter	_	_
m	_	_
There	_	_
are	_	_
two	_	_
hyper-parameters	_	_
in	_	_
our	_	_
proposed	_	_
loss	_	_
function	_	_
,	_	_
one	_	_
is	_	_
the	_	_
scale	_	_
s	_	_
and	_	_
another	_	_
is	_	_
the	_	_
margin	_	_
m.	_	_
The	_	_
scale	_	_
s	_	_
has	_	_
already	_	_
been	_	_
discussed	_	_
sufficiently	_	_
in	_	_
several	_	_
previous	_	_
works	_	_
[	_	_
19	_	_
,	_	_
12	_	_
,	_	_
15	_	_
]	_	_
.	_	_

#160
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
directly	_	_
fixed	_	_
it	_	_
to	_	_
30	_	_
and	_	_
will	_	_
not	_	_
discuss	_	_
its	_	_
effect	_	_
anymore	_	_
.	_	_

#161
The	_	_
main	_	_
hyper-parameter	_	_
in	_	_
our	_	_
loss	_	_
function	_	_
is	_	_
the	_	_
margin	_	_
m.	_	_
In	_	_
Table	_	_
4	_	_
,	_	_
we	_	_
list	_	_
the	_	_
performance	_	_
of	_	_
our	_	_
proposed	_	_
AM-Softmax	_	_
loss	_	_
function	_	_
with	_	_
m	_	_
varies	_	_
from	_	_
0.25	_	_
to	_	_
0.5	_	_
.	_	_

#162
From	_	_
the	_	_
table	_	_
we	_	_
can	_	_
see	_	_
that	_	_
from	_	_
m	_	_
=	_	_
0.25	_	_
to	_	_
0.3	_	_
,	_	_
the	_	_
performance	_	_
improves	_	_
significantly	_	_
,	_	_
and	_	_
the	_	_
performance	_	_
become	_	_
the	_	_
best	_	_
when	_	_
m	_	_
=	_	_
0.35	_	_
to	_	_
m	_	_
=	_	_
0.4	_	_
.	_	_

#163
We	_	_
also	_	_
provide	_	_
the	_	_
result	_	_
for	_	_
the	_	_
loss	_	_
function	_	_
without	_	_
feature	_	_
normalization	_	_
(	_	_
noted	_	_
as	_	_
w/o	_	_
FN	_	_
)	_	_
and	_	_
the	_	_
scale	_	_
s.	_	_
As	_	_
we	_	_
explained	_	_
before	_	_
,	_	_
feature	_	_
normalization	_	_
performs	_	_
better	_	_
on	_	_
low	_	_
quality	_	_
images	_	_
like	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
,	_	_
and	_	_
using	_	_
the	_	_
original	_	_
feature	_	_
norm	_	_
performs	_	_
better	_	_
on	_	_
high	_	_
quality	_	_
images	_	_
like	_	_
LFW	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#164
In	_	_
Figure	_	_
6	_	_
,	_	_
we	_	_
draw	_	_
both	_	_
of	_	_
the	_	_
CMC	_	_
curves	_	_
to	_	_
evaluate	_	_
the	_	_
performance	_	_
of	_	_
identification	_	_
and	_	_
ROC	_	_
curves	_	_
to	_	_
evaluate	_	_
the	_	_
performance	_	_
of	_	_
verification	_	_
.	_	_

#165
From	_	_
this	_	_
figure	_	_
,	_	_
we	_	_
Rank	_	_
Id	_	_
e	_	_
n	_	_
ti	_	_
fi	_	_
c	_	_
a	_	_
tio	_	_
n	_	_
R	_	_
a	_	_
te	_	_
%	_	_
Identification	_	_
with	_	_
1M	_	_
Distractors	_	_
Softmax	_	_
Softmax+Dropout	_	_
Center	_	_
Loss	_	_
NormFace	_	_
SphereFace	_	_
(	_	_
m	_	_
1.5	_	_
)	_	_
AM-Softmax	_	_
(	_	_
m=0.35	_	_
)	_	_
-6	_	_
-5	_	_
-4	_	_
-3	_	_
-2	_	_
-1	_	_
False	_	_
Positive	_	_
Rate	_	_
0.5	_	_
0.55	_	_
0.6	_	_
0.65	_	_
0.7	_	_
0.75	_	_
0.8	_	_
0.85	_	_
0.9	_	_
0.95	_	_
T	_	_
ru	_	_
e	_	_
P	_	_
o	_	_
s	_	_
it	_	_
iv	_	_
e	_	_
R	_	_
a	_	_
te	_	_
Verification	_	_
with	_	_
1M	_	_
Distractors	_	_
Softmax	_	_
Softmax+Dropout	_	_
Center	_	_
Loss	_	_
NormFace	_	_
SphereFace	_	_
(	_	_
m	_	_
1.5	_	_
)	_	_
AM-Softmax	_	_
(	_	_
m=0.35	_	_
)	_	_
Figure	_	_
6	_	_
.	_	_

#166
Left	_	_
:	_	_
CMC	_	_
curves	_	_
of	_	_
different	_	_
loss	_	_
functions	_	_
with	_	_
1M	_	_
distractors	_	_
on	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
Set	_	_
1	_	_
.	_	_

#167
Right	_	_
:	_	_
ROC	_	_
curves	_	_
of	_	_
different	_	_
loss	_	_
functions	_	_
with	_	_
1M	_	_
distractors	_	_
on	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
Set	_	_
1	_	_
.	_	_

#168
Note	_	_
that	_	_
for	_	_
Center	_	_
Loss	_	_
and	_	_
NormFace	_	_
,	_	_
the	_	_
backend	_	_
network	_	_
is	_	_
ResNet-28	_	_
[	_	_
20	_	_
]	_	_
,	_	_
while	_	_
others	_	_
are	_	_
based	_	_
on	_	_
ResNet-20	_	_
[	_	_
9	_	_
]	_	_
.	_	_

#169
Even	_	_
though	_	_
the	_	_
curves	_	_
of	_	_
the	_	_
Center	_	_
Loss	_	_
model	_	_
and	_	_
the	_	_
NormFace	_	_
model	_	_
is	_	_
close	_	_
to	_	_
the	_	_
SphereFace	_	_
model	_	_
,	_	_
please	_	_
keep	_	_
in	_	_
mind	_	_
that	_	_
part	_	_
of	_	_
the	_	_
performance	_	_
comes	_	_
from	_	_
the	_	_
bigger	_	_
network	_	_
structure	_	_
.	_	_

#170
can	_	_
show	_	_
that	_	_
our	_	_
loss	_	_
function	_	_
performs	_	_
much	_	_
better	_	_
than	_	_
the	_	_
other	_	_
loss	_	_
functions	_	_
when	_	_
the	_	_
rank	_	_
or	_	_
false	_	_
positive	_	_
rate	_	_
is	_	_
very	_	_
low	_	_
.	_	_

#171
5	_	_
.	_	_

#172
Conclusion	_	_
and	_	_
Future	_	_
Work	_	_
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
propose	_	_
to	_	_
impose	_	_
an	_	_
additive	_	_
margin	_	_
strategy	_	_
to	_	_
the	_	_
target	_	_
logit	_	_
of	_	_
softmax	_	_
loss	_	_
with	_	_
feature	_	_
and	_	_
weights	_	_
normalized	_	_
.	_	_

#173
Our	_	_
loss	_	_
function	_	_
is	_	_
built	_	_
upon	_	_
the	_	_
previous	_	_
margin	_	_
schemes	_	_
[	_	_
9	_	_
,	_	_
10	_	_
]	_	_
,	_	_
but	_	_
it	_	_
is	_	_
more	_	_
simple	_	_
and	_	_
interpretable	_	_
.	_	_

#174
Comprehensive	_	_
experiments	_	_
show	_	_
that	_	_
our	_	_
loss	_	_
function	_	_
performs	_	_
better	_	_
than	_	_
A-Softmax	_	_
[	_	_
9	_	_
]	_	_
on	_	_
LFW	_	_
BLUFR	_	_
[	_	_
7	_	_
]	_	_
and	_	_
MegaFace	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#175
There	_	_
is	_	_
still	_	_
lots	_	_
of	_	_
potentials	_	_
for	_	_
the	_	_
research	_	_
of	_	_
the	_	_
large	_	_
margin	_	_
strategies	_	_
.	_	_

#176
There	_	_
could	speculation	_
be	_	_
more	_	_
creative	_	_
way	_	_
of	_	_
specifying	_	_
the	_	_
function	_	_
ψ	_	_
(	_	_
θ	_	_
)	_	_
other	_	_
than	_	_
multiplication	_	_
and	_	_
addition	_	_
.	_	_

#177
In	_	_
our	_	_
AM-Softmax	_	_
loss	_	_
,	_	_
the	_	_
margin	_	_
is	_	_
a	_	_
manually	_	_
tuned	_	_
global	_	_
hyper-parameter	_	_
.	_	_

#178
How	_	_
to	_	_
automatically	_	_
determine	_	_
the	_	_
margin	_	_
and	_	_
how	_	_
to	_	_
incorporate	_	_
class-specific	_	_
or	_	_
sample-specific	_	_
margins	_	_
remain	_	_
open	_	_
questions	_	_
and	_	_
are	_	_
worth	_	_
studying	_	_
.	_	_