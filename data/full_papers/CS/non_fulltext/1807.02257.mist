#0
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
Guided	_	_
by	_	_
Natural	_	_
Language	_	_
Queries	_	_
Edgar	_	_
Margffoy-Tuay	_	_
,	_	_
Juan	_	_
C.	_	_
Pérez	_	_
,	_	_
Emilio	_	_
Botero	_	_
,	_	_
and	_	_
Pablo	_	_
Arbeláez	_	_
Universidad	_	_
de	_	_
los	_	_
Andes	_	_
,	_	_
Colombia	_	_
{	_	_
ea.margffoy10	_	_
,	_	_
jc.perez13	_	_
,	_	_
e.botero10	_	_
,	_	_
pa.arbelaez	_	_
}	_	_
@	_	_
uniandes.edu.co	_	_
Abstract	_	_
.	_	_

#1
We	_	_
address	_	_
the	_	_
problem	_	_
of	_	_
segmenting	_	_
an	_	_
object	_	_
given	_	_
a	_	_
natural	_	_
language	_	_
expression	_	_
that	_	_
describes	_	_
it	_	_
.	_	_

#2
Current	_	_
techniques	_	_
tackle	_	_
this	_	_
task	_	_
by	_	_
either	_	_
(	_	_
i	_	_
)	_	_
directly	_	_
or	_	_
recursively	_	_
merging	_	_
linguistic	_	_
and	_	_
visual	_	_
information	_	_
in	_	_
the	_	_
channel	_	_
dimension	_	_
and	_	_
then	_	_
performing	_	_
convolutions	_	_
;	_	_
or	_	_
by	_	_
(	_	_
ii	_	_
)	_	_
mapping	_	_
the	_	_
expression	_	_
to	_	_
a	_	_
space	_	_
in	_	_
which	_	_
it	_	_
can	_	_
be	_	_
thought	_	_
of	_	_
as	_	_
a	_	_
filter	_	_
,	_	_
whose	_	_
response	_	_
is	_	_
directly	_	_
related	_	_
to	_	_
the	_	_
presence	_	_
of	_	_
the	_	_
object	_	_
at	_	_
a	_	_
given	_	_
spatial	_	_
coordinate	_	_
in	_	_
the	_	_
image	_	_
,	_	_
so	_	_
that	_	_
a	_	_
convolution	_	_
can	_	_
be	_	_
applied	_	_
to	_	_
look	_	_
for	_	_
the	_	_
object	_	_
.	_	_

#3
We	_	_
propose	_	_
a	_	_
novel	_	_
method	_	_
that	_	_
integrates	_	_
these	_	_
two	_	_
insights	_	_
in	_	_
order	_	_
to	_	_
fully	_	_
exploit	_	_
the	_	_
recursive	_	_
nature	_	_
of	_	_
language	_	_
.	_	_

#4
Additionally	_	_
,	_	_
during	_	_
the	_	_
upsampling	_	_
process	_	_
,	_	_
we	_	_
take	_	_
advantage	_	_
of	_	_
the	_	_
intermediate	_	_
information	_	_
generated	_	_
when	_	_
downsampling	_	_
the	_	_
image	_	_
,	_	_
so	_	_
that	_	_
detailed	_	_
segmentations	_	_
can	_	_
be	_	_
obtained	_	_
.	_	_

#5
We	_	_
compare	_	_
our	_	_
method	_	_
against	_	_
the	_	_
state-of-the-art	_	_
approaches	_	_
in	_	_
four	_	_
standard	_	_
datasets	_	_
,	_	_
in	_	_
which	_	_
it	_	_
surpasses	_	_
all	_	_
previous	_	_
methods	_	_
in	_	_
six	_	_
of	_	_
eight	_	_
of	_	_
the	_	_
splits	_	_
for	_	_
this	_	_
task	_	_
.	_	_

#6
Keywords	_	_
:	_	_
Referring	_	_
expressions	_	_
,	_	_
instance	_	_
segmentation	_	_
,	_	_
multimodal	_	_
interaction	_	_
,	_	_
dynamic	_	_
convolutional	_	_
filters	_	_
,	_	_
natural	_	_
language	_	_
processing	_	_
.	_	_

#7
1	_	_
Introduction	_	_

#8
Consider	_	_
the	_	_
task	_	_
of	_	_
retrieving	_	_
specific	_	_
object	_	_
instances	_	_
from	_	_
an	_	_
image	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
descriptions	_	_
,	_	_
as	_	_
illustrated	_	_
in	_	_
Fig.	_	_
1	_	_
.	_	_

#9
In	_	_
contrast	_	_
to	_	_
traditional	_	_
instance	_	_
segmentation	_	_
,	_	_
in	_	_
which	_	_
the	_	_
goal	_	_
is	_	_
to	_	_
label	_	_
all	_	_
pixels	_	_
belonging	_	_
to	_	_
instances	_	_
in	_	_
the	_	_
image	_	_
for	_	_
a	_	_
set	_	_
of	_	_
predefined	_	_
semantic	_	_
classes	_	_
[	_	_
1	_	_
]	_	_
[	_	_
2	_	_
]	_	_
,	_	_
segmenting	_	_
instances	_	_
described	_	_
by	_	_
a	_	_
natural	_	_
language	_	_
expression	_	_
is	_	_
a	_	_
task	_	_
that	_	_
humans	_	_
are	_	_
able	_	_
to	_	_
perform	_	_
without	_	_
specifically	_	_
focusing	_	_
on	_	_
a	_	_
limited	_	_
set	_	_
of	_	_
categories	_	_
:	_	_
we	_	_
simply	_	_
associate	_	_
a	_	_
referring	_	_
expression	_	_
such	_	_
as	_	_
“Man	_	_
on	_	_
the	_	_
right”	_	_
with	_	_
what	_	_
we	_	_
see	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
1	_	_
.	_	_

#10
To	_	_
learn	_	_
such	_	_
an	_	_
association	_	_
is	_	_
the	_	_
main	_	_
goal	_	_
of	_	_
this	_	_
paper	_	_
.	_	_

#11
In	_	_
this	_	_
task	_	_
,	_	_
the	_	_
main	_	_
labels	_	_
to	_	_
be	_	_
assigned	_	_
are	_	_
related	_	_
to	_	_
query	_	_
and	_	_
background	_	_
.	_	_

#12
Thus	_	_
,	_	_
the	_	_
set	_	_
of	_	_
possible	_	_
segmentation	_	_
masks	_	_
has	_	_
few	_	_
constraints	_	_
,	_	_
as	_	_
a	_	_
mask	_	_
can	_	_
be	_	_
anything	_	_
one	_	_
might	capability-speculation	_
observe	_	_
in	_	_
the	_	_
image	_	_
,	_	_
in	_	_
all	_	_
the	_	_
ways	_	_
natural	_	_
language	_	_
allows	_	_
an	_	_
object	_	_
to	_	_
be	_	_
referred	_	_
to	_	_
.	_	_

#13
An	_	_
algorithm	_	_
to	_	_
tackle	_	_
this	_	_
problem	_	_
must	deontic	_
then	_	_
make	_	_
sense	_	_
of	_	_
the	_	_
query	_	_
and	_	_
relate	_	_
it	_	_
to	_	_
what	_	_
it	_	_
sees	_	_
and	_	_
recognizes	_	_
in	_	_
the	_	_
image	_	_
,	_	_
to	_	_
finally	_	_
output	_	_
an	_	_
instance	_	_
segmentation	_	_
map	_	_
.	_	_

#14
Therefore	_	_
,	_	_
attempting	_	_
to	_	_
naively	_	_
use	_	_
Convolutional	_	_
Neural	_	_
Networks	_	_
(	_	_
CNNs	_	_
)	_	_
for	_	_
this	_	_
task	_	_
falls	_	_
short	_	_
,	_	_
since	_	_
such	_	_
networks	_	_
do	_	_
not	_	_
model	_	_
sequential	_	_
information	_	_
by	_	_
nature	_	_
,	_	_
as	_	_
is	_	_
required	_	_
when	_	_
processing	_	_
natural	_	_
language	_	_
.	_	_

#15
Given	_	_
that	_	_
the	_	_
cornerstone	_	_
of	_	_
this	_	_
task	_	_
is	_	_
the	_	_
proper	_	_
combination	_	_
of	_	_
information	_	_
retrieved	_	_
from	_	_
multiple	_	_
,	_	_
dissimilar	_	_
domains	_	_
,	_	_
we	_	_
expect	_	_
traditional	_	_
architectures	_	_
,	_	_
like	_	_
CNNs	_	_
and	_	_
Recurrent	_	_
Neural	_	_
Networks	_	_
(	_	_
RNNs	_	_
)	_	_
,	_	_
to	_	_
be	_	_
useful	_	_
modules	_	_
,	_	_
but	_	_
we	_	_
still	_	_
need	_	_
to	_	_
design	_	_
an	_	_
overall	_	_
architecture	_	_
that	_	_
fully	_	_
exploits	_	_
their	_	_
complementary	_	_
nature	_	_
.	_	_

#16
In	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
introduce	_	_
a	_	_
modular	_	_
neural	_	_
network	_	_
architecture	_	_
that	_	_
divides	_	_
the	_	_
task	_	_
into	_	_
several	_	_
sub-tasks	_	_
,	_	_
each	_	_
handling	_	_
a	_	_
different	_	_
type	_	_
of	_	_
information	_	_
in	_	_
a	_	_
specific	_	_
manner	_	_
.	_	_

#17
Our	_	_
approach	_	_
is	_	_
similar	_	_
to	_	_
[	_	_
3	_	_
]	_	_
,	_	_
[	_	_
4	_	_
]	_	_
and	_	_
[	_	_
5	_	_
]	_	_
in	_	_
that	_	_
we	_	_
extract	_	_
visual	_	_
and	_	_
natural	_	_
language	_	_
information	_	_
in	_	_
an	_	_
independent	_	_
manner	_	_
by	_	_
employing	_	_
networks	_	_
commonly	_	_
used	_	_
for	_	_
these	_	_
types	_	_
of	_	_
data	_	_
,	_	_
i.e.	_	_
,	_	_
CNNs	_	_
and	_	_
RNNs	_	_
,	_	_
and	_	_
then	_	_
focus	_	_
on	_	_
processing	_	_
this	_	_
multi-domain	_	_
information	_	_
by	_	_
means	_	_
of	_	_
another	_	_
neural	_	_
network	_	_
,	_	_
yielding	_	_
an	_	_
end-to-end	_	_
trainable	_	_
architecture	_	_
.	_	_

#18
However	_	_
,	_	_
our	_	_
method	_	_
also	_	_
introduces	_	_
the	_	_
usage	_	_
of	_	_
Simple	_	_
Recurrent	_	_
Units	_	_
(	_	_
SRUs	_	_
)	_	_
for	_	_
efficient	_	_
segmentation	_	_
based	_	_
on	_	_
referring	_	_
expressions	_	_
,	_	_
a	_	_
Synthesis	_	_
Module	_	_
that	_	_
processes	_	_
the	_	_
linguistic	_	_
and	_	_
visual	_	_
information	_	_
jointly	_	_
,	_	_
and	_	_
an	_	_
Upsampling	_	_
Module	_	_
that	_	_
outputs	_	_
highly	_	_
detailed	_	_
segmentation	_	_
maps	_	_
.	_	_

#19
Our	_	_
network	_	_
,	_	_
which	_	_
we	_	_
refer	_	_
to	_	_
as	_	_
Dynamic	_	_
Multimodal	_	_
Network	_	_
(	_	_
DMN	_	_
)	_	_
,	_	_
is	_	_
composed	_	_
of	_	_
several	_	_
modules	_	_
,	_	_
as	_	_
depicted	_	_
in	_	_
Fig.	_	_
2	_	_
:	_	_
(	_	_
i	_	_
)	_	_
a	_	_
Visual	_	_
Module	_	_
(	_	_
VM	_	_
)	_	_
that	_	_
produces	_	_
an	_	_
adequate	_	_
representation	_	_
of	_	_
the	_	_
image	_	_
,	_	_
(	_	_
ii	_	_
)	_	_
a	_	_
Language	_	_
Module	_	_
(	_	_
LM	_	_
)	_	_
that	_	_
outputs	_	_
an	_	_
appropriate	_	_
representation	_	_
of	_	_
the	_	_
meaning	_	_
of	_	_
the	_	_
query	_	_
up	_	_
to	_	_
a	_	_
given	_	_
word	_	_
,	_	_
(	_	_
iii	_	_
)	_	_
a	_	_
Synthesis	_	_
Module	_	_
(	_	_
SM	_	_
)	_	_
that	_	_
merges	_	_
the	_	_
information	_	_
provided	_	_
by	_	_
the	_	_
VM	_	_
and	_	_
LM	_	_
at	_	_
each	_	_
time	_	_
step	_	_
and	_	_
produces	_	_
a	_	_
single	_	_
output	_	_
for	_	_
the	_	_
whole	_	_
expression	_	_
and	_	_
,	_	_
finally	_	_
,	_	_
(	_	_
iv	_	_
)	_	_
an	_	_
Upsampling	_	_
Module	_	_
(	_	_
UM	_	_
)	_	_
that	_	_
incrementally	_	_
upsamples	_	_
the	_	_
output	_	_
of	_	_
the	_	_
SM	_	_
by	_	_
using	_	_
the	_	_
feature	_	_
maps	_	_
proDynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
3	_	_
Input	_	_
Image	_	_
Visual	_	_
Module	_	_
second	_	_
from	_	_
right	_	_
Woman	_	_
Language	_	_
Module	_	_
S	_	_
y	_	_
n	_	_
th	_	_
e	_	_
s	_	_
is	_	_
M	_	_
o	_	_
d	_	_
u	_	_
le	_	_
Upsample	_	_
Module	_	_
Output	_	_
Segmentation	_	_
Language	_	_
Module	_	_
Language	_	_
Module	_	_
Language	_	_
Module	_	_
Query	_	_
Fig.	_	_
2	_	_
:	_	_
Overview	_	_
of	_	_
our	_	_
Dynamic	_	_
Multimodal	_	_
Network	_	_
(	_	_
DMN	_	_
)	_	_
,	_	_
involving	_	_
four	_	_
different	_	_
modules	_	_
:	_	_
Visual	_	_
Module	_	_
(	_	_
VM	_	_
)	_	_
,	_	_
Language	_	_
Module	_	_
(	_	_
LM	_	_
)	_	_
,	_	_
Synthesis	_	_
Module	_	_
(	_	_
SM	_	_
)	_	_
,	_	_
and	_	_
Upsampling	_	_
Module	_	_
(	_	_
UM	_	_
)	_	_
.	_	_

#20
duced	_	_
by	_	_
the	_	_
VM	_	_
.	_	_

#21
Our	_	_
approach	_	_
is	_	_
a	_	_
fully	_	_
differentiable	_	_
,	_	_
end-to-end	_	_
trainable	_	_
neural	_	_
network	_	_
for	_	_
segmentation	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
queries	_	_
.	_	_

#22
Our	_	_
main	_	_
contributions	_	_
are	_	_
the	_	_
following	_	_
:	_	_
–	_	_
The	_	_
use	_	_
of	_	_
Simple	_	_
Recurrent	_	_
Units	_	_
(	_	_
SRUs	_	_
)	_	_
[	_	_
6	_	_
]	_	_
as	_	_
language	_	_
and	_	_
multi-modal	_	_
processors	_	_
instead	_	_
of	_	_
standard	_	_
LSTMs	_	_
[	_	_
7	_	_
]	_	_
.	_	_

#23
We	_	_
empirically	_	_
show	_	_
that	_	_
they	_	_
are	_	_
efficient	_	_
while	_	_
providing	_	_
high	_	_
performance	_	_
for	_	_
the	_	_
task	_	_
at	_	_
hand	_	_
.	_	_

#24
–	_	_
A	_	_
Synthesis	_	_
Module	_	_
that	_	_
takes	_	_
visual	_	_
and	_	_
linguistic	_	_
information	_	_
and	_	_
merges	_	_
them	_	_
by	_	_
generating	_	_
“scores”	_	_
for	_	_
the	_	_
referring	_	_
expression	_	_
in	_	_
a	_	_
visual	_	_
space	_	_
.	_	_

#25
–	_	_
The	_	_
Synthesis	_	_
Module	_	_
then	_	_
takes	_	_
this	_	_
representation	_	_
as	_	_
well	_	_
as	_	_
additional	_	_
features	_	_
,	_	_
and	_	_
exploits	_	_
the	_	_
spatial	_	_
and	_	_
sequential	_	_
nature	_	_
of	_	_
both	_	_
types	_	_
of	_	_
information	_	_
to	_	_
produce	_	_
a	_	_
low	_	_
resolution	_	_
segmentation	_	_
map	_	_
.	_	_

#26
–	_	_
A	_	_
high	_	_
resolution	_	_
upsampling	_	_
module	_	_
that	_	_
takes	_	_
advantage	_	_
of	_	_
visual	_	_
features	_	_
during	_	_
the	_	_
upsampling	_	_
procedure	_	_
in	_	_
order	_	_
to	_	_
recover	_	_
fine	_	_
scale	_	_
details	_	_
.	_	_

#27
We	_	_
validate	_	_
our	_	_
method	_	_
by	_	_
performing	_	_
experiments	_	_
on	_	_
all	_	_
standard	_	_
datasets	_	_
,	_	_
and	_	_
show	_	_
that	_	_
DMN	_	_
outperforms	_	_
all	_	_
the	_	_
previous	_	_
methods	_	_
in	_	_
various	_	_
splits	_	_
for	_	_
instance	_	_
segmentation	_	_
based	_	_
on	_	_
referring	_	_
expressions	_	_
,	_	_
and	_	_
obtains	_	_
state-of-the	_	_
art	_	_
results	_	_
.	_	_

#28
Additionally	_	_
,	_	_
in	_	_
order	_	_
to	_	_
ensure	_	_
reproducibility	_	_
,	_	_
we	_	_
provide	_	_
full	_	_
implementation	_	_
of	_	_
our	_	_
method	_	_
and	_	_
training	_	_
routines	_	_
,	_	_
written	_	_
in	_	_
PyTorch1	_	_
[	_	_
8	_	_
]	_	_
.	_	_

#29
2	_	_
Related	_	_
Work	_	_

#30
The	_	_
intersection	_	_
of	_	_
Computer	_	_
Vision	_	_
(	_	_
CV	_	_
)	_	_
and	_	_
Natural	_	_
Language	_	_
Understanding	_	_
(	_	_
NLU	_	_
)	_	_
is	_	_
an	_	_
active	_	_
area	_	_
of	_	_
research	_	_
that	_	_
includes	_	_
multiple	_	_
tasks	_	_
such	_	_
as	_	_
object	_	_
detection	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
expressions	_	_
[	_	_
9	_	_
]	_	_
[	_	_
10	_	_
]	_	_
,	_	_
image	_	_
captioning	_	_
1	_	_
https	_	_
:	_	_
//github.com/BCV-Uniandes/query-objseg	_	_
4	_	_
Edgar	_	_
Margffoy-Tuay	_	_
,	_	_
Juan	_	_
C.	_	_
Pérez	_	_
,	_	_
Emilio	_	_
Botero	_	_
,	_	_
Pablo	_	_
Arbeláez	_	_
[	_	_
11	_	_
]	_	_
[	_	_
12	_	_
]	_	_
[	_	_
13	_	_
]	_	_
[	_	_
14	_	_
]	_	_
and	_	_
visual	_	_
question	_	_
answering	_	_
(	_	_
VQA	_	_
)	_	_
[	_	_
15	_	_
]	_	_
[	_	_
16	_	_
]	_	_
[	_	_
17	_	_
]	_	_
[	_	_
18	_	_
]	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#31
Since	_	_
visual	_	_
and	_	_
linguistic	_	_
data	_	_
have	_	_
properties	_	_
that	_	_
make	_	_
them	_	_
fundamentally	_	_
different	_	_
,	_	_
i.e.	_	_
,	_	_
the	_	_
former	_	_
has	_	_
spatial	_	_
meaning	_	_
and	_	_
no	_	_
sequentiality	_	_
while	_	_
the	_	_
latter	_	_
does	_	_
not	_	_
contemplate	_	_
space	_	_
but	_	_
has	_	_
a	_	_
sequential	_	_
nature	_	_
,	_	_
optimally	_	_
processing	_	_
both	_	_
types	_	_
of	_	_
information	_	_
is	_	_
still	_	_
an	_	_
open	_	_
question	_	_
.	_	_

#32
Hence	_	_
,	_	_
each	_	_
work	_	_
in	_	_
this	_	_
sub-field	_	_
has	_	_
proposed	_	_
a	_	_
particular	_	_
way	_	_
of	_	_
addressing	_	_
each	_	_
task	_	_
.	_	_

#33
The	_	_
task	_	_
studied	_	_
in	_	_
this	_	_
paper	_	_
is	_	_
closest	_	_
in	_	_
nature	_	_
to	_	_
object	_	_
detection	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
expressions	_	_
,	_	_
mirroring	_	_
how	_	_
semantic	_	_
segmentation	_	_
arose	_	_
from	_	_
object	_	_
detection	_	_
[	_	_
20	_	_
]	_	_
.	_	_

#34
Indeed	_	_
,	_	_
in	_	_
[	_	_
3	_	_
]	_	_
,	_	_
object	_	_
detection	_	_
with	_	_
NLU	_	_
evolved	_	_
into	_	_
instance	_	_
segmentation	_	_
using	_	_
referring	_	_
expressions	_	_
.	_	_

#35
We	_	_
review	_	_
the	_	_
state-of-the-art	_	_
on	_	_
the	_	_
task	_	_
of	_	_
segmentation	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
expressions	_	_
[	_	_
3	_	_
]	_	_
[	_	_
4	_	_
]	_	_
[	_	_
5	_	_
]	_	_
,	_	_
highlighting	_	_
the	_	_
main	_	_
contributions	_	_
in	_	_
the	_	_
fusion	_	_
of	_	_
multimodal	_	_
information	_	_
,	_	_
and	_	_
then	_	_
compare	_	_
them	_	_
against	_	_
our	_	_
approach	_	_
.	_	_

#36
Segmentation	_	_
from	_	_
Natural	_	_
Language	_	_
Expressions	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#37
This	_	_
work	_	_
processes	_	_
visual	_	_
and	_	_
natural	_	_
language	_	_
information	_	_
through	_	_
separate	_	_
neural	_	_
networks	_	_
:	_	_
a	_	_
CNN	_	_
extracts	_	_
visual	_	_
features	_	_
from	_	_
the	_	_
image	_	_
while	_	_
an	_	_
LSTM	_	_
scans	_	_
the	_	_
query	_	_
.	_	_

#38
Strided	_	_
convolutions	_	_
and	_	_
pooling	_	_
operations	_	_
in	_	_
the	_	_
CNN	_	_
downsample	_	_
the	_	_
feature	_	_
maps	_	_
to	_	_
a	_	_
low	_	_
resolution	_	_
output	_	_
while	_	_
producing	_	_
large	_	_
receptive	_	_
fields	_	_
for	_	_
neurons	_	_
in	_	_
the	_	_
final	_	_
layers	_	_
.	_	_

#39
Additionally	_	_
,	_	_
to	_	_
explicitly	_	_
model	_	_
spatial	_	_
information	_	_
,	_	_
relative	_	_
coordinates	_	_
are	_	_
concatenated	_	_
at	_	_
each	_	_
spatial	_	_
location	_	_
in	_	_
the	_	_
feature	_	_
map	_	_
obtained	_	_
by	_	_
the	_	_
CNN	_	_
.	_	_

#40
Merging	_	_
of	_	_
visual	_	_
and	_	_
natural	_	_
language	_	_
information	_	_
is	_	_
done	_	_
by	_	_
concatenating	_	_
the	_	_
LSTM’s	_	_
output	_	_
to	_	_
the	_	_
visual	_	_
feature	_	_
map	_	_
at	_	_
each	_	_
spatial	_	_
location	_	_
.	_	_

#41
Convolution	_	_
layers	_	_
with	_	_
ReLU	_	_
[	_	_
21	_	_
]	_	_
nonlinearities	_	_
are	_	_
applied	_	_
for	_	_
final	_	_
classification	_	_
.	_	_

#42
The	_	_
loss	_	_
is	_	_
defined	_	_
as	_	_
the	_	_
average	_	_
over	_	_
the	_	_
per-pixel	_	_
weighed	_	_
logistic	_	_
regression	_	_
loss	_	_
.	_	_

#43
Training	_	_
has	_	_
two	_	_
stages	_	_
:	_	_
a	_	_
low	_	_
resolution	_	_
stage	_	_
,	_	_
in	_	_
which	_	_
the	_	_
ground	_	_
truth	_	_
mask	_	_
is	_	_
downsampled	_	_
to	_	_
have	_	_
the	_	_
same	_	_
dimensions	_	_
as	_	_
the	_	_
output	_	_
,	_	_
and	_	_
a	_	_
high	_	_
resolution	_	_
stage	_	_
that	_	_
trains	_	_
a	_	_
deconvolution	_	_
layer	_	_
to	_	_
upsample	_	_
the	_	_
low	_	_
resolution	_	_
output	_	_
to	_	_
yield	_	_
the	_	_
final	_	_
segmentation	_	_
mask	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#44
This	_	_
seminal	_	_
method	_	_
does	_	_
not	_	_
fully	_	_
exploit	_	_
the	_	_
sequential	_	_
nature	_	_
of	_	_
language	_	_
,	_	_
as	_	_
it	_	_
does	_	_
not	_	_
make	_	_
use	_	_
of	_	_
the	_	_
learned	_	_
word	_	_
embeddings	_	_
,	_	_
it	_	_
merges	_	_
visual	_	_
and	_	_
linguistic	_	_
information	_	_
by	_	_
concatenation	_	_
,	_	_
and	_	_
it	_	_
uses	_	_
deconvolution	_	_
layers	_	_
for	_	_
upsampling	_	_
,	_	_
which	_	_
have	_	_
been	_	_
shown	_	_
to	_	_
introduce	_	_
checkerboard	_	_
artifacts	_	_
in	_	_
images	_	_
[	_	_
22	_	_
]	_	_
.	_	_

#45
Recurrent	_	_
Multimodal	_	_
Interaction	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#46
This	_	_
paper	_	_
argues	_	_
that	_	_
segmenting	_	_
the	_	_
image	_	_
based	_	_
only	_	_
on	_	_
a	_	_
final	_	_
,	_	_
memorized	_	_
representation	_	_
of	_	_
the	_	_
sentence	_	_
does	_	_
not	_	_
fully	_	_
take	_	_
advantage	_	_
of	_	_
the	_	_
sequential	_	_
nature	_	_
of	_	_
language	_	_
.	_	_

#47
Consequently	_	_
,	_	_
the	_	_
paper	_	_
proposes	_	_
to	_	_
perform	_	_
segmentation	_	_
multiple	_	_
times	_	_
in	_	_
the	_	_
pipeline	_	_
.	_	_

#48
The	_	_
method	_	_
produces	_	_
image	_	_
features	_	_
at	_	_
every	_	_
time	_	_
step	_	_
by	_	_
generating	_	_
a	_	_
representation	_	_
that	_	_
involves	_	_
visual	_	_
,	_	_
spatial	_	_
and	_	_
linguistic	_	_
features	_	_
.	_	_

#49
Such	_	_
multimodal	_	_
representation	_	_
is	_	_
obtained	_	_
by	_	_
concatenating	_	_
the	_	_
hidden	_	_
state	_	_
of	_	_
the	_	_
LSTM	_	_
that	_	_
processed	_	_
the	_	_
query	_	_
at	_	_
every	_	_
spatial	_	_
location	_	_
of	_	_
the	_	_
visual	_	_
representation	_	_
.	_	_

#50
The	_	_
segmentation	_	_
mask	_	_
is	_	_
obtained	_	_
by	_	_
applying	_	_
a	_	_
multimodal	_	_
LSTM	_	_
(	_	_
mLSTM	_	_
)	_	_
to	_	_
the	_	_
joint	_	_
representation	_	_
and	_	_
then	_	_
performing	_	_
regular	_	_
convolutions	_	_
to	_	_
combine	_	_
the	_	_
channels	_	_
that	_	_
were	_	_
produced	_	_
by	_	_
the	_	_
mLSTM	_	_
.	_	_

#51
The	_	_
mLSTM	_	_
is	_	_
defined	_	_
as	_	_
a	_	_
convolutional	_	_
LSTM	_	_
that	_	_
shares	_	_
weights	_	_
both	_	_
across	_	_
spatial	_	_
location	_	_
and	_	_
time	_	_
step	_	_
,	_	_
and	_	_
is	_	_
implemented	_	_
as	_	_
a	_	_
1	_	_
×	_	_
1	_	_
convolution	_	_
that	_	_
merges	_	_
all	_	_
these	_	_
types	_	_
of	_	_
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
5	_	_
information	_	_
.	_	_

#52
Bilinear	_	_
upsampling	_	_
is	_	_
performed	_	_
to	_	_
the	_	_
network’s	_	_
output	_	_
at	_	_
test	_	_
time	_	_
to	_	_
produce	_	_
a	_	_
mask	_	_
with	_	_
the	_	_
same	_	_
dimensions	_	_
of	_	_
the	_	_
ground-truth	_	_
mask	_	_
.	_	_

#53
This	_	_
method	_	_
reduces	_	_
strides	_	_
of	_	_
convolutional	_	_
layers	_	_
and	_	_
uses	_	_
atrous	_	_
convolution	_	_
in	_	_
the	_	_
final	_	_
layers	_	_
of	_	_
the	_	_
CNN	_	_
to	_	_
compensate	_	_
for	_	_
the	_	_
downsampling	_	_
.	_	_

#54
Such	_	_
modification	_	_
reduces	_	_
the	_	_
upsampling	_	_
process	_	_
to	_	_
bilinear	_	_
interpolation	_	_
,	_	_
but	_	_
can	_	_
decrease	_	_
the	_	_
CNN’s	_	_
representation	_	_
capacity	_	_
while	_	_
also	_	_
increasing	_	_
the	_	_
number	_	_
of	_	_
computations	_	_
that	_	_
must	deontic	_
be	_	_
performed	_	_
by	_	_
the	_	_
mLSTM	_	_
.	_	_

#55
Tracking	_	_
by	_	_
Natural	_	_
Language	_	_
Specification	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#56
In	_	_
this	_	_
paper	_	_
,	_	_
the	_	_
main	_	_
task	_	_
is	_	_
object	_	_
tracking	_	_
in	_	_
video	_	_
sequences	_	_
.	_	_

#57
A	_	_
typical	_	_
user	_	_
interaction	_	_
in	_	_
tracking	_	_
consists	_	_
in	_	_
providing	_	_
the	_	_
bounding	_	_
box	_	_
of	_	_
the	_	_
object	_	_
of	_	_
interest	_	_
in	_	_
the	_	_
first	_	_
frame	_	_
.	_	_

#58
However	_	_
,	_	_
this	_	_
type	_	_
of	_	_
interaction	_	_
has	_	_
the	_	_
issue	_	_
that	_	_
,	_	_
for	_	_
the	_	_
duration	_	_
of	_	_
the	_	_
video	_	_
,	_	_
the	_	_
appearance	_	_
and	_	_
location	_	_
of	_	_
objects	_	_
may	_	_
change	_	_
,	_	_
rendering	_	_
the	_	_
initial	_	_
bounding	_	_
box	_	_
useless	_	_
in	_	_
some	_	_
cases	_	_
.	_	_

#59
The	_	_
main	_	_
idea	_	_
is	_	_
to	_	_
provide	_	_
an	_	_
alternative	_	_
to	_	_
this	_	_
approach	_	_
,	_	_
by	_	_
noting	_	_
that	_	_
(	_	_
i	_	_
)	_	_
the	_	_
semantic	_	_
meaning	_	_
of	_	_
the	_	_
object	_	_
being	_	_
tracked	_	_
does	_	_
not	_	_
vary	_	_
for	_	_
the	_	_
duration	_	_
of	_	_
the	_	_
video	_	_
as	_	_
much	_	_
as	_	_
the	_	_
appearance	_	_
,	_	_
and	_	_
(	_	_
ii	_	_
)	_	_
this	_	_
semantic	_	_
meaning	_	_
may	_	_
be	_	_
better	_	_
defined	_	_
by	_	_
a	_	_
linguistic	_	_
expression	_	_
.	_	_

#60
This	_	_
approach	_	_
is	_	_
substantially	_	_
different	_	_
from	_	_
[	_	_
4	_	_
]	_	_
and	_	_
[	_	_
3	_	_
]	_	_
:	_	_
visual	_	_
and	_	_
linguistic	_	_
information	_	_
is	_	_
never	_	_
merged	_	_
per	_	_
se	_	_
,	_	_
but	_	_
rather	_	_
the	_	_
linguistic	_	_
information	_	_
is	_	_
mapped	_	_
to	_	_
a	_	_
space	_	_
in	_	_
which	_	_
it	_	_
can	_	_
be	_	_
interpreted	_	_
as	_	_
having	_	_
visual	_	_
meaning	_	_
.	_	_

#61
The	_	_
visual	_	_
input	_	_
is	_	_
thus	_	_
processed	_	_
by	_	_
a	_	_
modified	_	_
VGG	_	_
[	_	_
23	_	_
]	_	_
to	_	_
yield	_	_
a	_	_
feature	_	_
map	_	_
.	_	_

#62
An	_	_
LSTM	_	_
scans	_	_
the	_	_
linguistic	_	_
input	_	_
,	_	_
and	_	_
a	_	_
single	_	_
layer	_	_
perceptron	_	_
is	_	_
applied	_	_
to	_	_
the	_	_
LSTM’s	_	_
last	_	_
hidden	_	_
state	_	_
to	_	_
generate	_	_
a	_	_
vector	_	_
that	_	_
can	_	_
be	_	_
interpreted	_	_
as	_	_
being	_	_
a	_	_
filter	_	_
for	_	_
a	_	_
2D	_	_
convolution	_	_
that	_	_
is	_	_
to	_	_
be	_	_
performed	_	_
on	_	_
the	_	_
feature	_	_
map	_	_
.	_	_

#63
The	_	_
dynamic	_	_
convolutional	_	_
visual	_	_
filter	_	_
,	_	_
generated	_	_
based	_	_
on	_	_
the	_	_
expression	_	_
,	_	_
is	_	_
computed	_	_
to	_	_
produce	_	_
a	_	_
strong	_	_
response	_	_
to	_	_
the	_	_
elements	_	_
being	_	_
referred	_	_
to	_	_
the	_	_
expression	_	_
,	_	_
and	_	_
a	_	_
weak	_	_
response	_	_
to	_	_
those	_	_
not	_	_
being	_	_
referred	_	_
to	_	_
.	_	_

#64
This	_	_
response	_	_
is	_	_
interpreted	_	_
as	_	_
a	_	_
“score”	_	_
for	_	_
the	_	_
referring	_	_
expression	_	_
,	_	_
so	_	_
that	_	_
a	_	_
segmentation	_	_
can	_	_
be	_	_
produced	_	_
.	_	_

#65
This	_	_
method	_	_
proposes	_	_
a	_	_
new	_	_
paradigm	_	_
for	_	_
combining	_	_
information	_	_
from	_	_
the	_	_
visual	_	_
and	_	_
linguistic	_	_
domains	_	_
,	_	_
but	_	_
assumes	_	_
a	_	_
non	_	_
linear	_	_
combination	_	_
of	_	_
the	_	_
last	_	_
hidden	_	_
state	_	_
is	_	_
sufficient	_	_
for	_	_
modeling	_	_
a	_	_
filter	_	_
that	_	_
responds	_	_
to	_	_
the	_	_
query	_	_
.	_	_

#66
Our	_	_
approach	_	_
.	_	_

#67
The	_	_
approach	_	_
of	_	_
[	_	_
3	_	_
]	_	_
merges	_	_
multi-domain	_	_
information	_	_
by	_	_
concatenation	_	_
of	_	_
linguistic	_	_
information	_	_
,	_	_
subsequent	_	_
1	_	_
×	_	_
1	_	_
convolutions	_	_
for	_	_
segmentation	_	_
and	_	_
a	_	_
deconvolution	_	_
layer	_	_
to	_	_
perform	_	_
upsampling	_	_
.	_	_

#68
The	_	_
method	_	_
in	_	_
[	_	_
4	_	_
]	_	_
follows	_	_
the	_	_
same	_	_
logic	_	_
as	_	_
[	_	_
3	_	_
]	_	_
but	_	_
introduces	_	_
recursion	_	_
into	_	_
the	_	_
approach	_	_
,	_	_
exploiting	_	_
the	_	_
linguistic	_	_
information	_	_
further	_	_
;	_	_
however	_	_
,	_	_
the	_	_
upsampling	_	_
module	_	_
is	_	_
an	_	_
interpolation	_	_
that	_	_
produces	_	_
rather	_	_
coarse	_	_
results	_	_
,	_	_
to	_	_
which	_	_
the	_	_
authors	_	_
apply	_	_
a	_	_
post-processing	_	_
DenseCRF	_	_
,	_	_
making	_	_
the	_	_
architecture	_	_
not	_	_
trainable	_	_
end-to-end	_	_
.	_	_

#69
Finally	_	_
,	_	_
[	_	_
5	_	_
]	_	_
has	_	_
a	_	_
different	_	_
approach	_	_
,	_	_
in	_	_
which	_	_
linguistic	_	_
information	_	_
is	_	_
never	_	_
merged	_	_
with	_	_
feature	_	_
maps	_	_
,	_	_
but	_	_
is	_	_
rather	_	_
transformed	_	_
so	_	_
that	_	_
it	_	_
can	_	_
detect	_	_
the	_	_
locations	_	_
in	_	_
the	_	_
image	_	_
to	_	_
which	_	_
the	_	_
referring	_	_
expression	_	_
has	_	_
strong	_	_
response	_	_
;	_	_
nonetheless	_	_
,	_	_
like	_	_
[	_	_
3	_	_
]	_	_
,	_	_
it	_	_
does	_	_
not	_	_
fully	_	_
exploit	_	_
linguistic	_	_
information	_	_
in	_	_
a	_	_
sequential	_	_
manner	_	_
.	_	_

#70
Moreover	_	_
,	_	_
all	_	_
these	_	_
methods	_	_
fail	_	_
to	_	_
utilize	_	_
information	_	_
acquired	_	_
in	_	_
the	_	_
downsampling	_	_
process	_	_
in	_	_
the	_	_
upsampling	_	_
process	_	_
.	_	_

#71
6	_	_
Edgar	_	_
Margffoy-Tuay	_	_
,	_	_
Juan	_	_
C.	_	_
Pérez	_	_
,	_	_
Emilio	_	_
Botero	_	_
,	_	_
Pablo	_	_
Arbeláez	_	_
Input	_	_
Image	_	_
I1	_	_
I2	_	_
I3	_	_
I4	_	_
IN	_	_
Visual	_	_
Module	_	_
CNN	_	_
Fig.	_	_
3	_	_
:	_	_
The	_	_
Visual	_	_
Module	_	_
outputs	_	_
feature	_	_
maps	_	_
at	_	_
N	_	_
different	_	_
scales	_	_
with	_	_
the	_	_
aim	_	_
of	_	_
using	_	_
them	_	_
in	_	_
the	_	_
segmentation	_	_
process	_	_
and	_	_
in	_	_
the	_	_
upsampling	_	_
.	_	_

#72
Our	_	_
approach	_	_
takes	_	_
advantage	_	_
of	_	_
the	_	_
previous	_	_
insights	_	_
,	_	_
and	_	_
consists	_	_
of	_	_
a	_	_
modularized	_	_
network	_	_
that	_	_
exploits	_	_
both	_	_
the	_	_
possibility	_	_
of	_	_
segmentation	_	_
based	_	_
on	_	_
combinations	_	_
of	_	_
multi-domain	_	_
information	_	_
,	_	_
and	_	_
the	_	_
feasibility	_	_
of	_	_
producing	_	_
filters	_	_
that	_	_
respond	_	_
to	_	_
objects	_	_
being	_	_
referred	_	_
to	_	_
by	_	_
processing	_	_
the	_	_
linguistic	_	_
information	_	_
.	_	_

#73
Following	_	_
the	_	_
spirit	_	_
of	_	_
[	_	_
24	_	_
]	_	_
[	_	_
25	_	_
]	_	_
[	_	_
26	_	_
]	_	_
,	_	_
we	_	_
use	_	_
skip	_	_
connections	_	_
between	_	_
the	_	_
downsampling	_	_
process	_	_
and	_	_
the	_	_
upsampling	_	_
module	_	_
to	_	_
output	_	_
finely-defined	_	_
segmentations	_	_
.	_	_

#74
We	_	_
employ	_	_
the	_	_
concatenation	_	_
strategy	_	_
of	_	_
[	_	_
3	_	_
]	_	_
but	_	_
include	_	_
richer	_	_
visual	_	_
and	_	_
language	_	_
features	_	_
.	_	_

#75
Furthermore	_	_
,	_	_
we	_	_
make	_	_
use	_	_
of	_	_
dynamic	_	_
filter	_	_
computation	_	_
,	_	_
like	_	_
[	_	_
5	_	_
]	_	_
,	_	_
but	_	_
in	_	_
a	_	_
sequential	_	_
manner	_	_
.	_	_

#76
Lastly	_	_
,	_	_
we	_	_
introduce	_	_
the	_	_
use	_	_
of	_	_
a	_	_
more	_	_
efficient	_	_
alternative	_	_
to	_	_
LSTMs	_	_
in	_	_
this	_	_
domain	_	_
,	_	_
namely	_	_
SRUs	_	_
.	_	_

#77
We	_	_
demonstrate	_	_
empirically	_	_
that	_	_
SRUs	_	_
can	_	_
be	_	_
used	_	_
for	_	_
modeling	_	_
language	_	_
and	_	_
multimodal	_	_
information	_	_
for	_	_
this	_	_
task	_	_
,	_	_
and	_	_
that	_	_
they	_	_
can	_	_
be	_	_
up	_	_
to	_	_
3×	_	_
faster	_	_
than	_	_
LSTMs	_	_
,	_	_
allowing	_	_
us	_	_
to	_	_
train	_	_
more	_	_
expressive	_	_
models	_	_
.	_	_

#78
3	_	_
Dynamic	_	_
Multimodal	_	_
Network	_	_

#79
3.1	_	_
Overall	_	_
Architecture	_	_

#80
Fig.	_	_
2	_	_
illustrates	_	_
our	_	_
overall	_	_
architecture	_	_
.	_	_

#81
Given	_	_
an	_	_
input	_	_
consisting	_	_
of	_	_
an	_	_
image	_	_
I	_	_
,	_	_
and	_	_
a	_	_
query	_	_
composed	_	_
of	_	_
T	_	_
words	_	_
,	_	_
{	_	_
wt	_	_
}	_	_
Tt=1	_	_
,	_	_
the	_	_
Visual	_	_
Module	_	_
(	_	_
VM	_	_
)	_	_
takes	_	_
I	_	_
as	_	_
input	_	_
and	_	_
produces	_	_
feature	_	_
maps	_	_
at	_	_
N	_	_
different	_	_
scales	_	_
:	_	_
{	_	_
In	_	_
}	_	_
Nn=1	_	_
.	_	_

#82
The	_	_
Language	_	_
Module	_	_
(	_	_
LM	_	_
)	_	_
processes	_	_
{	_	_
wt	_	_
}	_	_
Tt=1	_	_
and	_	_
yields	_	_
a	_	_
set	_	_
of	_	_
features	_	_
{	_	_
rt	_	_
}	_	_
Tt=1	_	_
and	_	_
a	_	_
set	_	_
of	_	_
dynamic	_	_
filters	_	_
{	_	_
{	_	_
fk	_	_
,	_	_
t	_	_
}	_	_
Kk=1	_	_
}	_	_
Tt=1	_	_
.	_	_

#83
Given	_	_
the	_	_
VM’s	_	_
last	_	_
output	_	_
,	_	_
IN	_	_
,	_	_
{	_	_
rt	_	_
}	_	_
Tt=1	_	_
,	_	_
and	_	_
{	_	_
{	_	_
fk	_	_
,	_	_
t	_	_
}	_	_
Kk=1	_	_
}	_	_
Tt=1	_	_
,	_	_
the	_	_
Synthesis	_	_
Module	_	_
(	_	_
SM	_	_
)	_	_
processes	_	_
this	_	_
information	_	_
and	_	_
produces	_	_
a	_	_
single	_	_
feature	_	_
map	_	_
for	_	_
the	_	_
entire	_	_
referring	_	_
expression	_	_
.	_	_

#84
This	_	_
output	_	_
,	_	_
along	_	_
with	_	_
the	_	_
feature	_	_
maps	_	_
given	_	_
by	_	_
the	_	_
VM	_	_
,	_	_
is	_	_
processed	_	_
by	_	_
the	_	_
Upsampling	_	_
Module	_	_
(	_	_
UM	_	_
)	_	_
,	_	_
that	_	_
outputs	_	_
a	_	_
heatmap	_	_
with	_	_
a	_	_
single	_	_
channel	_	_
,	_	_
to	_	_
which	_	_
a	_	_
sigmoid	_	_
activation	_	_
function	_	_
is	_	_
applied	_	_
in	_	_
order	_	_
to	_	_
produce	_	_
the	_	_
final	_	_
prediction	_	_
.	_	_

#85
3.2	_	_
Visual	_	_
Module	_	_
(	_	_
VM	_	_
)	_	_

#86
Fig.	_	_
3	_	_
depicts	_	_
the	_	_
Visual	_	_
Module	_	_
.	_	_

#87
We	_	_
extract	_	_
deep	_	_
visual	_	_
features	_	_
from	_	_
the	_	_
image	_	_
using	_	_
as	_	_
backbone	_	_
a	_	_
Dual	_	_
Path	_	_
Network	_	_
92	_	_
(	_	_
DPN92	_	_
)	_	_
[	_	_
27	_	_
]	_	_
,	_	_
which	_	_
has	_	_
shown	_	_
competitive	_	_
performance	_	_
in	_	_
various	_	_
tasks	_	_
,	_	_
and	_	_
is	_	_
efficient	_	_
in	_	_
parameter	_	_
usage	_	_
.	_	_

#88
The	_	_
VM	_	_
can	_	_
be	_	_
written	_	_
as	_	_
a	_	_
function	_	_
returning	_	_
a	_	_
tuple	_	_
:	_	_
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
7	_	_
second	_	_
from	_	_
right	_	_
EMB	_	_
HID	_	_
FILTER	_	_
SRU	_	_
SRU	_	_
SRU	_	_
SRU	_	_
Language	_	_
Module	_	_
Woman	_	_
EMB	_	_
|	_	_
HID	_	_
Fig.	_	_
4	_	_
:	_	_
The	_	_
Language	_	_
Module	_	_
uses	_	_
an	_	_
SRU	_	_
,	_	_
instead	_	_
of	_	_
the	_	_
traditional	_	_
LSTM	_	_
,	_	_
to	_	_
output	_	_
enriched	_	_
features	_	_
of	_	_
the	_	_
query	_	_
and	_	_
dynamic	_	_
filters	_	_
based	_	_
on	_	_
such	_	_
features	_	_
.	_	_

#89
{	_	_
In	_	_
}	_	_
Nn=1	_	_
=	_	_
VM	_	_
(	_	_
I	_	_
)	_	_
(	_	_
1	_	_
)	_	_
Where	_	_
I	_	_
is	_	_
the	_	_
original	_	_
image	_	_
,	_	_
and	_	_
In	_	_
,	_	_
n	_	_
∈	_	_
{	_	_
1	_	_
,	_	_
.	_	_

#90
.	_	_

#91
.	_	_

#92
,	_	_
N	_	_
}	_	_
are	_	_
the	_	_
downsampled	_	_
feature	_	_
maps	_	_
of	_	_
dimensions	_	_
equal	_	_
to	_	_
1	_	_
2n	_	_
of	_	_
the	_	_
dimensions	_	_
of	_	_
I	_	_
.	_	_

#93
In	_	_
the	_	_
experiments	_	_
,	_	_
we	_	_
use	_	_
N	_	_
=	_	_
5	_	_
,	_	_
which	_	_
considers	_	_
all	_	_
convolutional	_	_
layers	_	_
in	_	_
the	_	_
visual	_	_
encoder	_	_
.	_	_

#94
Note	_	_
that	_	_
,	_	_
since	_	_
our	_	_
architecture	_	_
is	_	_
fully	_	_
convolutional	_	_
,	_	_
we	_	_
are	_	_
not	_	_
restricted	_	_
to	_	_
a	_	_
fixed	_	_
image	_	_
size	_	_
.	_	_

#95
3.3	_	_
Language	_	_
Module	_	_
(	_	_
LM	_	_
)	_	_

#96
Fig.	_	_
4	_	_
shows	_	_
a	_	_
diagram	_	_
of	_	_
the	_	_
Language	_	_
Module	_	_
.	_	_

#97
Given	_	_
an	_	_
expression	_	_
consisting	_	_
of	_	_
T	_	_
words	_	_
{	_	_
wt	_	_
}	_	_
Tt=1	_	_
,	_	_
each	_	_
word	_	_
is	_	_
represented	_	_
by	_	_
an	_	_
embedding	_	_
(	_	_
WE	_	_
)	_	_
,	_	_
et	_	_
=	_	_
WE	_	_
(	_	_
wt	_	_
)	_	_
(	_	_
EMB	_	_
in	_	_
Fig.	_	_
4	_	_
)	_	_
,	_	_
and	_	_
the	_	_
sentence	_	_
is	_	_
scanned	_	_
by	_	_
an	_	_
RNN	_	_
to	_	_
produce	_	_
a	_	_
hidden	_	_
state	_	_
ht	_	_
for	_	_
each	_	_
word	_	_
(	_	_
HID	_	_
in	_	_
Fig.	_	_
4	_	_
)	_	_
.	_	_

#98
Instead	_	_
of	_	_
using	_	_
LSTMs	_	_
as	_	_
recurrent	_	_
cells	_	_
,	_	_
we	_	_
employ	_	_
SRUs	_	_
[	_	_
6	_	_
]	_	_
,	_	_
which	_	_
allow	_	_
the	_	_
LM	_	_
to	_	_
process	_	_
the	_	_
natural	_	_
language	_	_
queries	_	_
more	_	_
efficiently	_	_
than	_	_
when	_	_
using	_	_
LSTMs	_	_
.	_	_

#99
The	_	_
SRU	_	_
is	_	_
defined	_	_
by	_	_
:	_	_
x̃t	_	_
=	_	_
Wxt	_	_
(	_	_
2	_	_
)	_	_
f	_	_
′t	_	_
=	_	_
σ	_	_
(	_	_
Wfxt	_	_
+	_	_
bf	_	_
)	_	_
(	_	_
3	_	_
)	_	_
rt	_	_
=	_	_
σ	_	_
(	_	_
Wrxt	_	_
+	_	_
br	_	_
)	_	_
(	_	_
4	_	_
)	_	_
ct	_	_
=	_	_
f	_	_
′t	_	_
ct−1	_	_
+	_	_
(	_	_
1−	_	_
f	_	_
′t	_	_
)	_	_
x̃t	_	_
(	_	_
5	_	_
)	_	_
ht	_	_
=	_	_
rt	_	_
g	_	_
(	_	_
ct	_	_
)	_	_
+	_	_
(	_	_
1−	_	_
rt	_	_
)	_	_
xt	_	_
(	_	_
6	_	_
)	_	_
Where	_	_
is	_	_
the	_	_
element-wise	_	_
multiplication	_	_
.	_	_

#100
The	_	_
function	_	_
g	_	_
(	_	_
·	_	_
)	_	_
can	_	_
be	_	_
selected	_	_
based	_	_
on	_	_
the	_	_
task	_	_
;	_	_
here	_	_
we	_	_
choose	_	_
g	_	_
(	_	_
·	_	_
)	_	_
to	_	_
be	_	_
the	_	_
sigmoid	_	_
function	_	_
.	_	_

#101
For	_	_
further	_	_
details	_	_
regarding	_	_
the	_	_
SRU	_	_
definition	_	_
and	_	_
implementation	_	_
,	_	_
please	_	_
refer	_	_
to	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#102
We	_	_
concatenate	_	_
the	_	_
hidden	_	_
state	_	_
ht	_	_
with	_	_
the	_	_
word	_	_
embedding	_	_
et	_	_
to	_	_
produce	_	_
the	_	_
final	_	_
language	_	_
output	_	_
:	_	_
rt	_	_
=	_	_
[	_	_
et	_	_
,	_	_
ht	_	_
]	_	_
.	_	_

#103
This	_	_
procedure	_	_
yields	_	_
an	_	_
enriched	_	_
language	_	_
representation	_	_
of	_	_
the	_	_
concept	_	_
of	_	_
the	_	_
sentence	_	_
up	_	_
to	_	_
word	_	_
t.	_	_
Moreover	_	_
,	_	_
we	_	_
compute	_	_
a	_	_
set	_	_
of	_	_
dynamic	_	_
filters	_	_
fk	_	_
,	_	_
t	_	_
based	_	_
on	_	_
rt	_	_
,	_	_
defined	_	_
by	_	_
:	_	_
fk	_	_
,	_	_
t	_	_
=	_	_
σ	_	_
(	_	_
Wfkrt	_	_
+	_	_
bfk	_	_
)	_	_
,	_	_
k	_	_
=	_	_
1	_	_
,	_	_
...	_	_
,	_	_
K	_	_
(	_	_
7	_	_
)	_	_
8	_	_
Edgar	_	_
Margffoy-Tuay	_	_
,	_	_
Juan	_	_
C.	_	_
Pérez	_	_
,	_	_
Emilio	_	_
Botero	_	_
,	_	_
Pablo	_	_
Arbeláez	_	_
EMB	_	_
|	_	_
HID	_	_
|	_	_
RESP	_	_
|	_	_
LOC	_	_
|	_	_
IN	_	_
IN	_	_
LOC	_	_
FILTER	_	_
mSRU	_	_
mSRU	_	_
mSRU	_	_
mSRU	_	_
Low-Res	_	_
Synthesis	_	_
Module	_	_
CONV	_	_
CONV	_	_
CONV	_	_
CONV	_	_
1x1	_	_
@	_	_
0	_	_
LOC	_	_
|	_	_
IN	_	_
RESP	_	_
*	_	_
*	_	_
*	_	_
*	_	_
Fig.	_	_
5	_	_
:	_	_
The	_	_
Synthesis	_	_
Module	_	_
takes	_	_
into	_	_
account	_	_
the	_	_
response	_	_
to	_	_
dynamic	_	_
filters	_	_
,	_	_
language	_	_
features	_	_
,	_	_
spatial	_	_
coordinates	_	_
representation	_	_
,	_	_
and	_	_
visual	_	_
features	_	_
in	_	_
a	_	_
recurrent	_	_
manner	_	_
to	_	_
output	_	_
a	_	_
single	_	_
response	_	_
map	_	_
.	_	_

#104
thus	_	_
,	_	_
we	_	_
define	_	_
the	_	_
LM	_	_
formally	_	_
as	_	_
:	_	_
(	_	_
{	_	_
rt	_	_
}	_	_
Tt=1	_	_
,	_	_
{	_	_
{	_	_
fk	_	_
,	_	_
t	_	_
}	_	_
Kk=1	_	_
}	_	_
Tt=1	_	_
)	_	_
=	_	_
LM	_	_
(	_	_
{	_	_
wt	_	_
}	_	_
Tt=1	_	_
)	_	_
(	_	_
8	_	_
)	_	_

#105
3.4	_	_
Synthesis	_	_
Module	_	_
(	_	_
SM	_	_
)	_	_

#106
Fig.	_	_
5	_	_
illustrates	_	_
the	_	_
Synthesis	_	_
Module	_	_
.	_	_

#107
The	_	_
SM	_	_
is	_	_
the	_	_
core	_	_
of	_	_
our	_	_
architecture	_	_
,	_	_
as	_	_
it	_	_
is	_	_
responsible	_	_
for	_	_
merging	_	_
multimodal	_	_
information	_	_
.	_	_

#108
We	_	_
first	_	_
concatenate	_	_
IN	_	_
and	_	_
a	_	_
representation	_	_
of	_	_
the	_	_
spatial	_	_
coordinates	_	_
(	_	_
LOC	_	_
in	_	_
Fig.	_	_
5	_	_
)	_	_
,	_	_
following	_	_
the	_	_
implementation	_	_
of	_	_
[	_	_
3	_	_
]	_	_
,	_	_
and	_	_
convolve	_	_
this	_	_
result	_	_
with	_	_
each	_	_
of	_	_
the	_	_
filters	_	_
computed	_	_
by	_	_
the	_	_
LM	_	_
to	_	_
generate	_	_
a	_	_
response	_	_
map	_	_
(	_	_
RESP	_	_
in	_	_
Fig.	_	_
5	_	_
)	_	_
consisting	_	_
of	_	_
K	_	_
channels	_	_
:	_	_
Ft	_	_
=	_	_
{	_	_
fk	_	_
,	_	_
t	_	_
∗	_	_
IN	_	_
}	_	_
Kk=1	_	_
.	_	_

#109
Next	_	_
,	_	_
we	_	_
concatenate	_	_
IN	_	_
,	_	_
LOC	_	_
,	_	_
and	_	_
Ft	_	_
along	_	_
the	_	_
channel	_	_
dimension	_	_
to	_	_
obtain	_	_
a	_	_
representation	_	_
I	_	_
′	_	_
,	_	_
to	_	_
which	_	_
rt	_	_
is	_	_
concatenated	_	_
at	_	_
each	_	_
spatial	_	_
location	_	_
,	_	_
as	_	_
to	_	_
have	_	_
all	_	_
the	_	_
multimodal	_	_
information	_	_
in	_	_
a	_	_
single	_	_
tensor	_	_
.	_	_

#110
Finally	_	_
,	_	_
we	_	_
apply	_	_
a	_	_
1×	_	_
1	_	_
convolutional	_	_
layer	_	_
that	_	_
merges	_	_
all	_	_
the	_	_
multimodal	_	_
information	_	_
,	_	_
providing	_	_
tan	_	_
output	_	_
corresponding	_	_
to	_	_
each	_	_
time	_	_
step	_	_
t	_	_
,	_	_
denoted	_	_
by	_	_
Mt	_	_
.	_	_

#111
Formally	_	_
,	_	_
Mt	_	_
is	_	_
defined	_	_
by	_	_
:	_	_
Mt	_	_
=	_	_
Conv1×1	_	_
(	_	_
[	_	_
IN	_	_
,	_	_
Ft	_	_
,	_	_
LOC	_	_
,	_	_
rt	_	_
]	_	_
)	_	_
(	_	_
9	_	_
)	_	_
Next	_	_
,	_	_
in	_	_
the	_	_
pursuit	_	_
of	_	_
performing	_	_
a	_	_
recurrent	_	_
operation	_	_
that	_	_
takes	_	_
into	_	_
account	_	_
the	_	_
sequentiality	_	_
of	_	_
the	_	_
set	_	_
and	_	_
also	_	_
the	_	_
information	_	_
of	_	_
each	_	_
of	_	_
the	_	_
channels	_	_
in	_	_
Mt	_	_
,	_	_
we	_	_
propose	_	_
the	_	_
use	_	_
of	_	_
a	_	_
multimodal	_	_
SRU	_	_
(	_	_
mSRU	_	_
)	_	_
,	_	_
which	_	_
we	_	_
define	_	_
as	_	_
a	_	_
1	_	_
×	_	_
1	_	_
convolution	_	_
,	_	_
similar	_	_
to	_	_
[	_	_
4	_	_
]	_	_
but	_	_
using	_	_
SRUs	_	_
.	_	_

#112
We	_	_
apply	_	_
the	_	_
mSRU	_	_
to	_	_
the	_	_
whole	_	_
set	_	_
{	_	_
Mt	_	_
}	_	_
Tt=1	_	_
,	_	_
so	_	_
that	_	_
all	_	_
the	_	_
information	_	_
in	_	_
each	_	_
Mt	_	_
,	_	_
including	_	_
the	_	_
sequentiality	_	_
of	_	_
the	_	_
set	_	_
,	_	_
is	_	_
used	_	_
in	_	_
the	_	_
segmentation	_	_
process	_	_
.	_	_

#113
The	_	_
final	_	_
hidden	_	_
states	_	_
are	_	_
gathered	_	_
to	_	_
produce	_	_
a	_	_
3D	_	_
tensor	_	_
that	_	_
is	_	_
interpreted	_	_
as	_	_
a	_	_
feature	_	_
map	_	_
.	_	_

#114
This	_	_
tensor	_	_
,	_	_
which	_	_
we	_	_
denominate	_	_
RN	_	_
,	_	_
due	_	_
to	_	_
its	_	_
size	_	_
being	_	_
1	_	_
2N	_	_
of	_	_
the	_	_
image’s	_	_
original	_	_
size	_	_
,	_	_
has	_	_
the	_	_
same	_	_
dimensions	_	_
as	_	_
Mt	_	_
and	_	_
has	_	_
as	_	_
many	_	_
channels	_	_
as	_	_
there	_	_
are	_	_
entries	_	_
in	_	_
the	_	_
hidden	_	_
state	_	_
of	_	_
the	_	_
mSRU	_	_
.	_	_

#115
We	_	_
define	_	_
the	_	_
SM	_	_
as	_	_
a	_	_
function	_	_
returning	_	_
RN	_	_
:	_	_
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
9	_	_
Low-Res	_	_
IN	_	_
Output	_	_
Segmentation	_	_
CONV	_	_
+	_	_
Bilinear	_	_
3x3	_	_
@	_	_
1	_	_
IN-1	_	_
I1	_	_
CONV	_	_
+	_	_
Bilinear	_	_
3x3	_	_
@	_	_
1	_	_
CONV	_	_
1x1	_	_
@	_	_
0	_	_
Fig.	_	_
6	_	_
:	_	_
The	_	_
Upsampling	_	_
Module	_	_
makes	_	_
use	_	_
of	_	_
all	_	_
the	_	_
feature	_	_
maps	_	_
that	_	_
were	_	_
generated	_	_
in	_	_
the	_	_
feature	_	_
extraction	_	_
process	_	_
to	_	_
provide	_	_
more	_	_
detailed	_	_
segmentations	_	_
.	_	_

#116
RN	_	_
=	_	_
SM	_	_
(	_	_
{	_	_
Mt	_	_
}	_	_
Tt=1	_	_
)	_	_
=	_	_
mSRU	_	_
(	_	_
{	_	_
Mt	_	_
}	_	_
Tt=1	_	_
)	_	_
,	_	_
(	_	_
10	_	_
)	_	_
where	_	_
Mt	_	_
is	_	_
reshaped	_	_
appropriately	_	_
to	_	_
make	_	_
sense	_	_
of	_	_
the	_	_
sequential	_	_
nature	_	_
of	_	_
the	_	_
information	_	_
at	_	_
each	_	_
time	_	_
step	_	_
.	_	_

#117
3.5	_	_
Upsampling	_	_
Module	_	_
(	_	_
UM	_	_
)	_	_

#118
Finally	_	_
,	_	_
the	_	_
Upsampling	_	_
Module	_	_
is	_	_
shown	_	_
in	_	_
Fig.	_	_
6	_	_
.	_	_

#119
Inspired	_	_
by	_	_
skip	_	_
connections	_	_
[	_	_
24	_	_
]	_	_
[	_	_
25	_	_
]	_	_
[	_	_
28	_	_
]	_	_
,	_	_
we	_	_
construct	_	_
an	_	_
upsampling	_	_
architecture	_	_
that	_	_
takes	_	_
into	_	_
account	_	_
the	_	_
feature	_	_
maps	_	_
{	_	_
In	_	_
}	_	_
Nn=1	_	_
at	_	_
all	_	_
stages	_	_
in	_	_
order	_	_
to	_	_
recover	_	_
fine-scale	_	_
details	_	_
.	_	_

#120
At	_	_
each	_	_
stage	_	_
we	_	_
concatenate	_	_
Rn	_	_
with	_	_
In	_	_
,	_	_
perform	_	_
3×3	_	_
convolution	_	_
over	_	_
this	_	_
result	_	_
,	_	_
and	_	_
then	_	_
scale	_	_
the	_	_
size	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
2	_	_
via	_	_
bilinear	_	_
interpolation	_	_
to	_	_
generate	_	_
Rn−1	_	_
.	_	_

#121
We	_	_
apply	_	_
this	_	_
process	_	_
log2	_	_
(	_	_
N	_	_
)	_	_
times	_	_
,	_	_
to	_	_
produce	_	_
an	_	_
output	_	_
mask	_	_
of	_	_
the	_	_
same	_	_
size	_	_
of	_	_
the	_	_
input	_	_
R1	_	_
.	_	_

#122
We	_	_
apply	_	_
1	_	_
×	_	_
1	_	_
convolution	_	_
over	_	_
R1	_	_
to	_	_
generate	_	_
a	_	_
single	_	_
channel	_	_
and	_	_
,	_	_
finally	_	_
,	_	_
a	_	_
sigmoid	_	_
layer	_	_
to	_	_
obtain	_	_
scores	_	_
between	_	_
0	_	_
and	_	_
1	_	_
.	_	_

#123
4	_	_
Experimental	_	_
Setup	_	_

#124
4.1	_	_
Datasets	_	_

#125
We	_	_
conduct	_	_
experiments	_	_
on	_	_
the	_	_
four	_	_
standard	_	_
datasets	_	_
for	_	_
this	_	_
task	_	_
:	_	_
ReferIt	_	_
,	_	_
UNC	_	_
,	_	_
UNC+	_	_
[	_	_
29	_	_
]	_	_
,	_	_
and	_	_
GRef	_	_
[	_	_
30	_	_
]	_	_
.	_	_

#126
UNC	_	_
,	_	_
UNC+	_	_
and	_	_
GRef	_	_
are	_	_
based	_	_
on	_	_
MS	_	_
COCO	_	_
[	_	_
1	_	_
]	_	_
.	_	_

#127
The	_	_
type	_	_
of	_	_
objects	_	_
that	_	_
appear	_	_
in	_	_
the	_	_
referring	_	_
expressions	_	_
,	_	_
length	_	_
of	_	_
the	_	_
expressions	_	_
,	_	_
and	_	_
relative	_	_
size	_	_
of	_	_
the	_	_
referred	_	_
objects	_	_
are	_	_
the	_	_
main	_	_
differences	_	_
between	_	_
the	_	_
datasets	_	_
.	_	_

#128
The	_	_
high	_	_
variability	_	_
of	_	_
those	_	_
characteristics	_	_
across	_	_
the	_	_
datasets	_	_
evidences	_	_
the	_	_
challenge	_	_
of	_	_
constructing	_	_
models	_	_
for	_	_
this	_	_
task	_	_
that	_	_
are	_	_
capable	_	_
of	_	_
generalization	_	_
.	_	_

#129
ReferIt	_	_
[	_	_
29	_	_
]	_	_
is	_	_
a	_	_
crowd-sourced	_	_
database	_	_
that	_	_
contains	_	_
images	_	_
and	_	_
referring	_	_
expressions	_	_
to	_	_
objects	_	_
in	_	_
those	_	_
images	_	_
.	_	_

#130
Currently	_	_
it	_	_
has	_	_
130,525	_	_
expressions	_	_
,	_	_
referring	_	_
to	_	_
96,654	_	_
distinct	_	_
objects	_	_
,	_	_
in	_	_
19,894	_	_
photographs	_	_
of	_	_
natural	_	_
scenes	_	_
.	_	_

#131
UNC	_	_
[	_	_
31	_	_
]	_	_
,	_	_
was	_	_
collected	_	_
interactively	_	_
in	_	_
the	_	_
ReferIt	_	_
game	_	_
,	_	_
with	_	_
images	_	_
that	_	_
were	_	_
selected	_	_
to	_	_
contain	_	_
two	_	_
or	_	_
more	_	_
objects	_	_
of	_	_
the	_	_
same	_	_
object	_	_
category	_	_
[	_	_
31	_	_
]	_	_
,	_	_
which	_	_
means	_	_
that	_	_
an	_	_
expression	_	_
making	_	_
reference	_	_
to	_	_
a	_	_
determined	_	_
type	_	_
of	_	_
object	_	_
will	_	_
need	_	_
to	_	_
be	_	_
further	_	_
analysed	_	_
to	_	_
determine	_	_
which	_	_
object	_	_
the	_	_
query	_	_
is	_	_
referring	_	_
to	_	_
,	_	_
since	_	_
ambiguity	_	_
arises	_	_
when	_	_
only	_	_
guided	_	_
by	_	_
semantic	_	_
instance	_	_
class	_	_
cues	_	_
.	_	_

#132
It	_	_
consists	_	_
of	_	_
142,209	_	_
referring	_	_
expressions	_	_
for	_	_
50,000	_	_
objects	_	_
in	_	_
19,994	_	_
images	_	_
.	_	_

#133
UNC+	_	_
[	_	_
31	_	_
]	_	_
,	_	_
is	_	_
similar	_	_
to	_	_
UNC	_	_
but	_	_
has	_	_
an	_	_
additional	_	_
restriction	_	_
regarding	_	_
words	_	_
describing	_	_
location	_	_
:	_	_
expressions	_	_
must	deontic	_
be	_	_
based	_	_
only	_	_
on	_	_
appearance	_	_
rather	_	_
than	_	_
location	_	_
.	_	_

#134
Such	_	_
restriction	_	_
implies	_	_
that	_	_
the	_	_
expression	_	_
will	_	_
depend	_	_
on	_	_
the	_	_
perspective	_	_
of	_	_
the	_	_
scene	_	_
and	_	_
the	_	_
semantic	_	_
class	_	_
of	_	_
the	_	_
object	_	_
.	_	_

#135
GRef	_	_
[	_	_
30	_	_
]	_	_
,	_	_
was	_	_
collected	_	_
on	_	_
Amazon’s	_	_
Mechanical	_	_
Turk	_	_
and	_	_
contains	_	_
85,474	_	_
referring	_	_
expressions	_	_
for	_	_
54,822	_	_
objects	_	_
in	_	_
26,711	_	_
images	_	_
selected	_	_
to	_	_
contain	_	_
between	_	_
two	_	_
and	_	_
four	_	_
objects	_	_
of	_	_
the	_	_
same	_	_
class	_	_
,	_	_
and	_	_
thus	_	_
,	_	_
it	_	_
presents	_	_
similar	_	_
challenges	_	_
to	_	_
those	_	_
of	_	_
UNC	_	_
.	_	_

#136
4.2	_	_
Performance	_	_
Metrics	_	_

#137
We	_	_
use	_	_
the	_	_
standard	_	_
metrics	_	_
from	_	_
the	_	_
literature	_	_
to	_	_
allow	_	_
for	_	_
direct	_	_
comparison	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
state-of-the-art	_	_
.	_	_

#138
We	_	_
perform	_	_
experiments	_	_
with	_	_
the	_	_
proposed	_	_
method	_	_
on	_	_
the	_	_
four	_	_
standard	_	_
datasets	_	_
described	_	_
above	_	_
by	_	_
training	_	_
on	_	_
the	_	_
training	_	_
set	_	_
and	_	_
evaluating	_	_
the	_	_
performance	_	_
in	_	_
each	_	_
of	_	_
the	_	_
validation	_	_
or	_	_
test	_	_
sets	_	_
.	_	_

#139
We	_	_
evaluate	_	_
results	_	_
by	_	_
using	_	_
two	_	_
standard	_	_
metrics	_	_
:	_	_
(	_	_
i	_	_
)	_	_
mean	_	_
Intersection	_	_
over	_	_
Union	_	_
(	_	_
mIoU	_	_
)	_	_
,	_	_
defined	_	_
as	_	_
the	_	_
total	_	_
intersection	_	_
area	_	_
between	_	_
the	_	_
output	_	_
and	_	_
the	_	_
Ground	_	_
Truth	_	_
(	_	_
GT	_	_
)	_	_
mask	_	_
,	_	_
divided	_	_
by	_	_
the	_	_
total	_	_
union	_	_
area	_	_
between	_	_
the	_	_
output	_	_
and	_	_
the	_	_
GT	_	_
mask	_	_
,	_	_
added	_	_
over	_	_
all	_	_
the	_	_
images	_	_
in	_	_
the	_	_
evaluation	_	_
set	_	_
,	_	_
and	_	_
(	_	_
ii	_	_
)	_	_
Precision	_	_
@	_	_
X	_	_
,	_	_
or	_	_
Pr	_	_
@	_	_
X	_	_
,	_	_
(	_	_
X	_	_
∈	_	_
{	_	_
0.5	_	_
,	_	_
0.6	_	_
,	_	_
0.7	_	_
,	_	_
0.8	_	_
,	_	_
0.9	_	_
}	_	_
)	_	_
,	_	_
defined	_	_
as	_	_
the	_	_
percentage	_	_
of	_	_
images	_	_
with	_	_
IoU	_	_
higher	_	_
than	_	_
X	_	_
.	_	_

#140
We	_	_
report	_	_
mIoU	_	_
in	_	_
the	_	_
validation	_	_
and	_	_
test	_	_
splits	_	_
of	_	_
each	_	_
dataset	_	_
,	_	_
when	_	_
available	_	_
,	_	_
using	_	_
optimal	_	_
thresholds	_	_
from	_	_
the	_	_
training	_	_
or	_	_
validation	_	_
splits	_	_
,	_	_
respectively	_	_
.	_	_

#141
4.3	_	_
Implementation	_	_
Details	_	_

#142
All	_	_
the	_	_
models	_	_
are	_	_
defined	_	_
and	_	_
trained	_	_
with	_	_
DPN92	_	_
[	_	_
27	_	_
]	_	_
as	_	_
the	_	_
backbone	_	_
,	_	_
which	_	_
outputs	_	_
2688	_	_
channels	_	_
in	_	_
the	_	_
last	_	_
layer	_	_
.	_	_

#143
We	_	_
use	_	_
N	_	_
=	_	_
5	_	_
scales	_	_
in	_	_
the	_	_
VM	_	_
.	_	_

#144
We	_	_
use	_	_
the	_	_
following	_	_
hyperparameters	_	_
,	_	_
which	_	_
we	_	_
optimized	_	_
on	_	_
the	_	_
UNC+	_	_
val	_	_
set	_	_
:	_	_
WE	_	_
size	_	_
of	_	_
1000	_	_
,	_	_
2-layered	_	_
SRU	_	_
with	_	_
hidden	_	_
state	_	_
size	_	_
of	_	_
1000	_	_
,	_	_
K	_	_
=	_	_
10	_	_
filters	_	_
,	_	_
1000	_	_
1×	_	_
1	_	_
convolution	_	_
filters	_	_
in	_	_
the	_	_
SM	_	_
,	_	_
3-layered	_	_
mSRU’s	_	_
hidden	_	_
state	_	_
size	_	_
of	_	_
1000	_	_
.	_	_

#145
The	_	_
increased	_	_
number	_	_
of	_	_
layers	_	_
presented	_	_
here	_	_
in	_	_
both	_	_
the	_	_
SRU	_	_
and	_	_
the	_	_
mSRU	_	_
,	_	_
with	_	_
respect	_	_
to	_	_
usual	_	_
number	_	_
of	_	_
layers	_	_
in	_	_
LSTMs	_	_
,	_	_
are	_	_
in	_	_
response	_	_
to	_	_
the	_	_
increased	_	_
need	_	_
of	_	_
layers	_	_
for	_	_
an	_	_
SRU	_	_
to	_	_
work	_	_
as	_	_
expected	_	_
,	_	_
according	_	_
to	_	_
[	_	_
6	_	_
]	_	_
.	_	_

#146
We	_	_
train	_	_
our	_	_
method	_	_
in	_	_
two	_	_
stages	_	_
:	_	_
at	_	_
low	_	_
resolution	_	_
(	_	_
i.e.	_	_
,	_	_
without	_	_
using	_	_
the	_	_
UM	_	_
)	_	_
and	_	_
then	_	_
finetune	_	_
the	_	_
UM	_	_
to	_	_
obtain	_	_
high	_	_
resolution	_	_
segmentation	_	_
maps	_	_
.	_	_

#147
Training	_	_
is	_	_
done	_	_
with	_	_
Adam	_	_
optimizer	_	_
[	_	_
32	_	_
]	_	_
with	_	_
an	_	_
initial	_	_
learning	_	_
rate	_	_
of	_	_
1×	_	_
10−5	_	_
,	_	_
a	_	_
scheduler	_	_
that	_	_
waits	_	_
2	_	_
epochs	_	_
for	_	_
loss	_	_
stagnation	_	_
to	_	_
reduce	_	_
the	_	_
learning	_	_
rate	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
10	_	_
,	_	_
and	_	_
batch	_	_
size	_	_
of	_	_
1	_	_
image-query	_	_
pair	_	_
.	_	_

#148
5	_	_
Results	_	_

#149
5.1	_	_
Control	_	_
Experiments	_	_

#150
We	_	_
assess	_	_
the	_	_
relative	_	_
importance	_	_
of	_	_
our	_	_
modules	_	_
in	_	_
the	_	_
final	_	_
result	_	_
by	_	_
performing	_	_
ablation	_	_
experiments	_	_
.	_	_

#151
The	_	_
control	_	_
experiments	_	_
were	_	_
trained	_	_
until	_	_
convergence	_	_
on	_	_
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
11	_	_
the	_	_
UNC	_	_
dataset	_	_
.	_	_

#152
Accordingly	_	_
,	_	_
we	_	_
compare	_	_
them	_	_
to	_	_
a	_	_
version	_	_
of	_	_
our	_	_
full	_	_
method	_	_
trained	_	_
for	_	_
a	_	_
similar	_	_
amount	_	_
of	_	_
time	_	_
.	_	_

#153
Table	_	_
1	_	_
presents	_	_
the	_	_
results	_	_
.	_	_

#154
The	_	_
“Only	_	_
VM”	_	_
experiment	_	_
in	_	_
row	_	_
1	_	_
consists	_	_
on	_	_
training	_	_
only	_	_
the	_	_
VM	_	_
and	_	_
upsampling	_	_
the	_	_
low	_	_
resolution	_	_
output	_	_
with	_	_
bilinear	_	_
interpolation	_	_
,	_	_
without	_	_
using	_	_
the	_	_
query	_	_
.	_	_

#155
At	_	_
test	_	_
time	_	_
,	_	_
the	_	_
VM	_	_
processes	_	_
the	_	_
image	_	_
and	_	_
the	_	_
resulting	_	_
segmentation	_	_
map	_	_
is	_	_
upsampled	_	_
using	_	_
the	_	_
UM	_	_
and	_	_
compared	_	_
to	_	_
the	_	_
GT	_	_
mask	_	_
.	_	_

#156
Results	_	_
show	_	_
how	_	_
this	_	_
method	_	_
performs	_	_
poorly	_	_
in	_	_
comparison	_	_
to	_	_
our	_	_
full	_	_
approach	_	_
,	_	_
which	_	_
confirms	_	_
our	_	_
hypothesis	_	_
that	_	_
naively	_	_
using	_	_
a	_	_
CNN	_	_
falls	_	_
short	_	_
for	_	_
the	_	_
task	_	_
addressed	_	_
in	_	_
this	_	_
paper	_	_
.	_	_

#157
However	_	_
,	_	_
it	_	_
is	_	_
interesting	_	_
that	_	_
performance	_	_
is	_	_
rather	_	_
high	_	_
for	_	_
a	_	_
method	_	_
that	_	_
does	_	_
not	_	_
use	_	_
linguistic	_	_
information	_	_
at	_	_
all	_	_
.	_	_

#158
This	_	_
result	_	_
reveals	_	_
that	_	_
many	_	_
of	_	_
the	_	_
objects	_	_
annotated	_	_
in	_	_
this	_	_
dataset	_	_
are	_	_
salient	_	_
,	_	_
and	_	_
so	_	_
the	_	_
network	_	_
is	_	_
able	_	_
to	_	_
learn	_	_
to	_	_
segment	_	_
salient	_	_
objects	_	_
without	_	_
help	_	_
from	_	_
the	_	_
query	_	_
.	_	_

#159
The	_	_
experiment	_	_
in	_	_
row	_	_
2	_	_
consists	_	_
of	_	_
defining	_	_
rt	_	_
=	_	_
ht	_	_
,	_	_
instead	_	_
of	_	_
using	_	_
the	_	_
concatenation	_	_
of	_	_
ht	_	_
and	_	_
et	_	_
,	_	_
which	_	_
affects	_	_
both	_	_
the	_	_
LM	_	_
(	_	_
when	_	_
computing	_	_
the	_	_
dynamic	_	_
filters	_	_
)	_	_
and	_	_
the	_	_
SM	_	_
.	_	_

#160
Results	_	_
show	_	_
that	_	_
using	_	_
the	_	_
learned	_	_
embeddings	_	_
provides	_	_
a	_	_
small	_	_
gain	_	_
in	_	_
the	_	_
full	_	_
method	_	_
,	_	_
particularly	_	_
for	_	_
stricter	_	_
overlap	_	_
thresholds	_	_
.	_	_

#161
Next	_	_
,	_	_
in	_	_
row	_	_
3	_	_
we	_	_
assess	_	_
the	_	_
importance	_	_
of	_	_
skip	_	_
connections	_	_
in	_	_
the	_	_
UM	_	_
,	_	_
which	_	_
is	_	_
a	_	_
measure	_	_
of	_	_
the	_	_
usefulness	_	_
of	_	_
features	_	_
extracted	_	_
in	_	_
the	_	_
downsampling	_	_
process	_	_
for	_	_
the	_	_
upsampling	_	_
module	_	_
.	_	_

#162
The	_	_
large	_	_
drop	_	_
in	_	_
performance	_	_
with	_	_
respect	_	_
to	_	_
the	_	_
full	_	_
method	_	_
shows	_	_
that	_	_
the	_	_
skip	_	_
connections	_	_
allow	_	_
the	_	_
network	_	_
to	_	_
exploit	_	_
finer	_	_
details	_	_
that	_	_
are	_	_
otherwise	_	_
lost	_	_
,	_	_
showing	_	_
how	_	_
the	_	_
upsampling	_	_
strategy	_	_
can	_	_
benefit	_	_
from	_	_
performing	_	_
convolutions	_	_
followed	_	_
by	_	_
bilinear	_	_
interpolations	_	_
instead	_	_
of	_	_
deconvolutions	_	_
,	_	_
as	_	_
done	_	_
in	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#163
We	_	_
next	_	_
study	_	_
the	_	_
effects	_	_
of	_	_
removing	_	_
features	_	_
from	_	_
Mt	_	_
.	_	_

#164
In	_	_
rows	_	_
4	_	_
and	_	_
5	_	_
we	_	_
remove	_	_
the	_	_
set	_	_
of	_	_
responses	_	_
to	_	_
the	_	_
dynamic	_	_
filters	_	_
F	_	_
,	_	_
as	_	_
well	_	_
as	_	_
the	_	_
concatenation	_	_
of	_	_
rt	_	_
in	_	_
the	_	_
SM	_	_
,	_	_
respectively	_	_
.	_	_

#165
We	_	_
observe	_	_
that	_	_
the	_	_
dynamic	_	_
filters	_	_
generate	_	_
useful	_	_
scores	_	_
for	_	_
the	_	_
natural	_	_
language	_	_
queries	_	_
in	_	_
the	_	_
visual	_	_
space	_	_
,	_	_
and	_	_
that	_	_
reusing	_	_
features	_	_
from	_	_
the	_	_
LM	_	_
in	_	_
the	_	_
SM	_	_
does	_	_
not	_	_
help	_	_
the	_	_
network	_	_
significantly	_	_
.	_	_

#166
Our	_	_
results	_	_
show	_	_
that	_	_
the	_	_
key	_	_
components	_	_
of	_	_
our	_	_
network	_	_
have	_	_
significant	_	_
impact	_	_
in	_	_
overall	_	_
performance	_	_
.	_	_

#167
High	_	_
performance	_	_
is	_	_
not	_	_
achieved	_	_
by	_	_
either	_	_
using	_	_
only	_	_
linguistic	_	_
information	_	_
(	_	_
rt	_	_
)	_	_
or	_	_
the	_	_
response	_	_
to	_	_
filters	_	_
(	_	_
F	_	_
)	_	_
:	_	_
both	_	_
must	deontic	_
be	_	_
properly	_	_
combined	_	_
.	_	_

#168
Additionally	_	_
,	_	_
the	_	_
UM	_	_
allows	_	_
the	_	_
network	_	_
to	_	_
properly	_	_
exploit	_	_
features	_	_
from	_	_
the	_	_
downsampling	_	_
stage	_	_
and	_	_
perform	_	_
detailed	_	_
segmentation	_	_
.	_	_

#169
5.2	_	_
Comparison	_	_
with	_	_
the	_	_
State-of-the-Art	_	_

#170
Next	_	_
,	_	_
we	_	_
proceed	_	_
to	_	_
compare	_	_
our	_	_
full	_	_
method	_	_
with	_	_
the	_	_
state-of-the-art	_	_
,	_	_
for	_	_
which	_	_
we	_	_
evaluate	_	_
on	_	_
all	_	_
the	_	_
datasets	_	_
described	_	_
above	_	_
.	_	_

#171
Table	_	_
2	_	_
compares	_	_
the	_	_
mIoU	_	_
of	_	_
our	_	_
method	_	_
with	_	_
the	_	_
state-of-the-art	_	_
in	_	_
this	_	_
task	_	_
[	_	_
3	_	_
]	_	_
[	_	_
4	_	_
]	_	_
[	_	_
5	_	_
]	_	_
.	_	_

#172
The	_	_
results	_	_
show	_	_
that	_	_
our	_	_
method	_	_
outperforms	_	_
all	_	_
other	_	_
methods	_	_
in	_	_
six	_	_
out	_	_
of	_	_
eight	_	_
splits	_	_
of	_	_
the	_	_
datasets	_	_
.	_	_

#173
By	_	_
including	_	_
enriched	_	_
linguistic	_	_
features	_	_
at	_	_
several	_	_
stages	_	_
of	_	_
the	_	_
process	_	_
,	_	_
and	_	_
by	_	_
combining	_	_
them	_	_
in	_	_
different	_	_
ways	_	_
,	_	_
our	_	_
network	_	_
learns	_	_
appropriate	_	_
associations	_	_
between	_	_
queries	_	_
and	_	_
the	_	_
instances	_	_
they	_	_
refer	_	_
to	_	_
.	_	_

#174
Interestingly	_	_
,	_	_
the	_	_
performance	_	_
gain	_	_
in	_	_
the	_	_
testB	_	_
splits	_	_
of	_	_
UNC	_	_
and	_	_
UNC+	_	_
is	_	_
not	_	_
as	_	_
large	_	_
as	_	_
in	_	_
testA	_	_
.	_	_

#175
One	_	_
possible	_	_
reason	_	_
for	_	_
the	_	_
smaller	_	_
performance	_	_
gain	_	_
across	_	_
splits	_	_
is	_	_
their	_	_
difference	_	_
:	_	_
visual	_	_
inspection	_	_
of	_	_
results	_	_
shows	_	_
how	_	_
testA	_	_
splits	_	_
are	_	_
biased	_	_
towards	_	_
queries	_	_
related	_	_
to	_	_
segmenting	_	_
persons	_	_
.	_	_

#176
The	_	_
testB	_	_
splits	_	_
,	_	_
however	_	_
,	_	_
contain	_	_
more	_	_
varied	_	_
queries	_	_
and	_	_
objects	_	_
,	_	_
which	_	_
is	_	_
why	_	_
the	_	_
increase	_	_
in	_	_
mIoU	_	_
is	_	_
not	_	_
as	_	_
marked	_	_
.	_	_

#177
This	_	_
behavior	_	_
can	_	_
also	_	_
be	_	_
observed	_	_
for	_	_
the	_	_
method	_	_
proposed	_	_
by	_	_
[	_	_
4	_	_
]	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
the	_	_
second-to-last	_	_
line	_	_
of	_	_
Table	_	_
2	_	_
.	_	_

#178
5.3	_	_
Efficiency	_	_
Comparison	_	_
:	_	_
SRU	_	_
vs.	_	_
LSTM	_	_
In	_	_
order	_	_
to	_	_
assess	_	_
the	_	_
efficiency	_	_
and	_	_
the	_	_
performance	_	_
of	_	_
SRUs	_	_
when	_	_
compared	_	_
to	_	_
the	_	_
more	_	_
commonly	_	_
used	_	_
LSTMs	_	_
,	_	_
both	_	_
as	_	_
language	_	_
and	_	_
multi-modal	_	_
processors	_	_
,	_	_
we	_	_
conduct	_	_
an	_	_
experiment	_	_
in	_	_
which	_	_
we	_	_
replaced	_	_
the	_	_
SRUs	_	_
with	_	_
LSTMs	_	_
in	_	_
our	_	_
final	_	_
system	_	_
,	_	_
both	_	_
in	_	_
the	_	_
LM	_	_
and	_	_
the	_	_
SM	_	_
,	_	_
we	_	_
trained	_	_
on	_	_
the	_	_
UNC	_	_
dataset	_	_
,	_	_
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
13	_	_
(	_	_
a	_	_
)	_	_
yellow	_	_
shirt	_	_
(	_	_
b	_	_
)	_	_
man	_	_
alone	_	_
on	_	_
the	_	_
right	_	_
(	_	_
c	_	_
)	_	_
batter	_	_
(	_	_
d	_	_
)	_	_
catcher	_	_
Fig.	_	_
8	_	_
:	_	_
Qualitative	_	_
examples	_	_
of	_	_
the	_	_
output	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#179
From	_	_
left	_	_
to	_	_
right	_	_
in	_	_
each	_	_
subfigure	_	_
:	_	_
original	_	_
image	_	_
,	_	_
heatmap	_	_
produced	_	_
by	_	_
our	_	_
method	_	_
,	_	_
and	_	_
ground-truth	_	_
mask	_	_
.	_	_

#180
Each	_	_
caption	_	_
is	_	_
the	_	_
query	_	_
that	_	_
produced	_	_
the	_	_
output	_	_
.	_	_

#181
and	_	_
we	_	_
measured	_	_
performance	_	_
on	_	_
the	_	_
testA	_	_
split	_	_
.	_	_

#182
In	_	_
terms	_	_
of	_	_
model	_	_
complexity	_	_
,	_	_
when	_	_
using	_	_
SRUs	_	_
,	_	_
the	_	_
LM	_	_
and	_	_
the	_	_
SM	_	_
have	_	_
9M	_	_
and	_	_
10M	_	_
trainable	_	_
parameters	_	_
,	_	_
respectively	_	_
.	_	_

#183
When	_	_
switching	_	_
to	_	_
LSTMs	_	_
,	_	_
the	_	_
number	_	_
of	_	_
parameters	_	_
increases	_	_
to	_	_
24M	_	_
and	_	_
24.2M	_	_
,	_	_
respectively	_	_
,	_	_
multiplying	_	_
training	_	_
time	_	_
by	_	_
a	_	_
factor	_	_
of	_	_
three	_	_
,	_	_
as	_	_
shown	_	_
in	_	_
Fig.	_	_
7a	_	_
.	_	_

#184
Regarding	_	_
accuracy	_	_
,	_	_
Fig.	_	_
7b	_	_
.	_	_

#185
shows	_	_
that	_	_
both	_	_
systems	_	_
perform	_	_
similarly	_	_
,	_	_
with	_	_
a	_	_
small	_	_
advantage	_	_
for	_	_
SRUs	_	_
.	_	_

#186
Therefore	_	_
,	_	_
when	_	_
compared	_	_
to	_	_
LSTMs	_	_
,	_	_
SRUs	_	_
allow	_	_
us	_	_
to	_	_
design	_	_
architectures	_	_
that	_	_
are	_	_
more	_	_
compact	_	_
,	_	_
train	_	_
significantly	_	_
faster	_	_
,	_	_
and	_	_
generalize	_	_
better	_	_
.	_	_

#187
5.4	_	_
Qualitative	_	_
Results	_	_

#188
Fig.	_	_
8	_	_
shows	_	_
qualitative	_	_
results	_	_
in	_	_
which	_	_
the	_	_
network	_	_
performed	_	_
well	_	_
.	_	_

#189
These	_	_
examples	_	_
demonstrate	_	_
DMN’s	_	_
flexibility	_	_
for	_	_
segmenting	_	_
based	_	_
on	_	_
different	_	_
information	_	_
about	_	_
a	_	_
particular	_	_
class	_	_
or	_	_
instance	_	_
:	_	_
attributes	_	_
,	_	_
location	_	_
or	_	_
role	_	_
.	_	_

#190
We	_	_
emphasize	_	_
that	_	_
understanding	_	_
a	_	_
role	_	_
is	_	_
not	_	_
trivial	_	_
,	_	_
as	_	_
it	_	_
is	_	_
related	_	_
to	_	_
the	_	_
object’s	_	_
context	_	_
and	_	_
appearance	_	_
.	_	_

#191
Additionally	_	_
,	_	_
a	_	_
semantic	_	_
difficulty	_	_
that	_	_
our	_	_
network	_	_
seems	_	_
to	_	_
overcome	_	_
is	_	_
that	_	_
the	_	_
role	_	_
coexists	_	_
with	_	_
the	_	_
object’s	_	_
class	_	_
:	_	_
an	_	_
instance	_	_
can	_	_
be	_	_
“batter”	_	_
and	_	_
be	_	_
“person”	_	_
.	_	_

#192
Notice	_	_
in	_	_
Fig.	_	_
8	_	_
that	_	_
thanks	_	_
to	_	_
the	_	_
upsampling	_	_
module	_	_
,	_	_
our	_	_
network	_	_
segments	_	_
fine	_	_
details	_	_
such	_	_
as	_	_
legs	_	_
,	_	_
heads	_	_
and	_	_
hands	_	_
.	_	_

#193
In	_	_
Fig.	_	_
8a	_	_
the	_	_
query	_	_
refers	_	_
to	_	_
the	_	_
kid	_	_
by	_	_
one	_	_
of	_	_
his	_	_
attributes	_	_
:	_	_
the	_	_
color	_	_
of	_	_
his	_	_
shirt	_	_
;	_	_
in	_	_
Fig.	_	_
8b	_	_
the	_	_
man	_	_
is	_	_
defined	_	_
by	_	_
his	_	_
location	_	_
and	_	_
the	_	_
fact	_	_
that	_	_
he	_	_
is	_	_
alone	_	_
(	_	_
although	_	_
that	_	_
could	_	_
be	_	_
dropped	_	_
,	_	_
as	_	_
there	_	_
is	_	_
no	_	_
ambiguity	_	_
)	_	_
;	_	_
in	_	_
Figs.	_	_
8c	_	_
and	_	_
d	_	_
the	_	_
reference	_	_
is	_	_
based	_	_
on	_	_
the	_	_
person’s	_	_
role	_	_
.	_	_

#194
Typical	_	_
failure	_	_
cases	_	_
are	_	_
depicted	_	_
in	_	_
Fig.	_	_
9	_	_
.	_	_

#195
In	_	_
Fig.	_	_
9a	_	_
the	_	_
network	_	_
segments	_	_
(	_	_
arguably	_	_
)	_	_
the	_	_
incorrect	_	_
person	_	_
,	_	_
since	_	_
the	_	_
correct	_	_
segmentation	_	_
was	_	_
the	_	_
person	_	_
at	_	_
the	_	_
border	_	_
of	_	_
the	_	_
image	_	_
whose	_	_
face	_	_
is	_	_
partially	_	_
shown	_	_
.	_	_

#196
Several	_	_
failure	_	_
cases	_	_
we	_	_
found	_	_
had	_	_
exactly	_	_
the	_	_
same	_	_
issue	_	_
:	_	_
ambiguity	_	_
in	_	_
the	_	_
expression	_	_
that	_	_
could	_	_
confuse	_	_
even	_	_
a	_	_
human	_	_
.	_	_

#197
Fig.	_	_
9b	_	_
shows	_	_
an	_	_
example	_	_
of	_	_
strong	_	_
failure	_	_
,	_	_
in	_	_
which	_	_
a	_	_
weak	_	_
segmentation	_	_
is	_	_
produced	_	_
.	_	_

#198
The	_	_
model	_	_
appears	_	_
to	_	_
have	_	_
only	_	_
focused	_	_
on	_	_
the	_	_
word	_	_
“right”	_	_
.	_	_

#199
We	_	_
attribute	_	_
this	_	_
failure	_	_
to	_	_
the	_	_
network’s	_	_
inability	_	_
to	_	_
make	_	_
14	_	_
Edgar	_	_
Margffoy-Tuay	_	_
,	_	_
Juan	_	_
C.	_	_
Pérez	_	_
,	_	_
Emilio	_	_
Botero	_	_
,	_	_
Pablo	_	_
Arbeláez	_	_
(	_	_
a	_	_
)	_	_
person	_	_
on	_	_
left	_	_
(	_	_
b	_	_
)	_	_
person	_	_
sitting	_	_
on	_	_
the	_	_
right	_	_
with	_	_
a	_	_
hat	_	_
that	_	_
has	_	_
a	_	_
white	_	_
stripe	_	_
(	_	_
c	_	_
)	_	_
guy	_	_
in	_	_
gray	_	_
shirt	_	_
standing	_	_
(	_	_
d	_	_
)	_	_
hand	_	_
on	_	_
remote	_	_
Fig.	_	_
9	_	_
:	_	_
Negative	_	_
examples	_	_
of	_	_
the	_	_
output	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#200
From	_	_
left	_	_
to	_	_
right	_	_
in	_	_
each	_	_
Subfigure	_	_
:	_	_
original	_	_
image	_	_
,	_	_
heatmap	_	_
produced	_	_
by	_	_
our	_	_
method	_	_
,	_	_
and	_	_
ground-truth	_	_
mask	_	_
.	_	_

#201
Each	_	_
caption	_	_
is	_	_
the	_	_
query	_	_
that	_	_
produced	_	_
the	_	_
output	_	_
.	_	_

#202
sense	_	_
of	_	_
such	_	_
a	_	_
relatively	_	_
long	_	_
sentence	_	_
,	_	_
which	_	_
,	_	_
while	_	_
unambiguously	_	_
defining	_	_
an	_	_
object	_	_
,	_	_
is	_	_
a	_	_
confusing	_	_
way	_	_
of	_	_
referring	_	_
to	_	_
it	_	_
.	_	_

#203
Fig.	_	_
9c	_	_
is	_	_
an	_	_
interesting	_	_
example	_	_
of	_	_
the	_	_
network’s	_	_
confusion	_	_
.	_	_

#204
While	_	_
the	_	_
woman	_	_
is	_	_
not	_	_
segmented	_	_
,	_	_
two	_	_
subjects	_	_
that	_	_
share	_	_
several	_	_
attributes	_	_
(	_	_
guy	_	_
,	_	_
gray	_	_
and	_	_
shirt	_	_
)	_	_
are	_	_
confused	_	_
and	_	_
are	_	_
both	_	_
segmented	_	_
.	_	_

#205
However	_	_
,	_	_
the	_	_
network	_	_
does	_	_
not	_	_
manage	_	_
to	_	_
use	_	_
the	_	_
word	_	_
“standing”	_	_
to	_	_
resolve	_	_
the	_	_
ambiguity	_	_
.	_	_

#206
Finally	_	_
,	_	_
in	_	_
Fig.	_	_
9d	_	_
a	_	_
failure	_	_
is	_	_
observed	_	_
,	_	_
where	_	_
nothing	_	_
related	_	_
to	_	_
the	_	_
query	_	_
is	_	_
segmented	_	_
.	_	_

#207
The	_	_
mask	_	_
that	_	_
is	_	_
produced	_	_
only	_	_
reflects	_	_
a	_	_
weak	_	_
attempt	_	_
of	_	_
segmenting	_	_
the	_	_
red	_	_
object	_	_
,	_	_
while	_	_
ignoring	_	_
the	_	_
upper	_	_
part	_	_
of	_	_
the	_	_
image	_	_
,	_	_
in	_	_
which	_	_
both	_	_
the	_	_
hand	_	_
and	_	_
the	_	_
remote	_	_
were	_	_
present	_	_
.	_	_

#208
6	_	_
Conclusions	_	_

#209
We	_	_
propose	_	_
Dynamic	_	_
Multimodal	_	_
Network	_	_
,	_	_
a	_	_
novel	_	_
approach	_	_
for	_	_
segmentation	_	_
of	_	_
instances	_	_
based	_	_
on	_	_
natural	_	_
language	_	_
expressions	_	_
.	_	_

#210
DMN	_	_
integrates	_	_
insights	_	_
from	_	_
previous	_	_
works	_	_
into	_	_
a	_	_
modularized	_	_
network	_	_
,	_	_
in	_	_
which	_	_
each	_	_
module	_	_
has	_	_
the	_	_
responsibility	_	_
of	_	_
handling	_	_
information	_	_
from	_	_
a	_	_
specific	_	_
domain	_	_
.	_	_

#211
Our	_	_
Synthesis	_	_
Module	_	_
combines	_	_
the	_	_
outputs	_	_
from	_	_
previous	_	_
modules	_	_
and	_	_
handles	_	_
this	_	_
multi-modal	_	_
information	_	_
to	_	_
produce	_	_
features	_	_
that	_	_
can	_	_
be	_	_
used	_	_
by	_	_
the	_	_
Upsampling	_	_
Module	_	_
.	_	_

#212
Thanks	_	_
to	_	_
the	_	_
incremental	_	_
use	_	_
of	_	_
feature	_	_
maps	_	_
obtained	_	_
in	_	_
the	_	_
encoding	_	_
part	_	_
of	_	_
the	_	_
network	_	_
,	_	_
the	_	_
Upsampling	_	_
Module	_	_
delivers	_	_
great	_	_
detail	_	_
in	_	_
the	_	_
final	_	_
segmentations	_	_
.	_	_

#213
Our	_	_
method	_	_
outperforms	_	_
the	_	_
state-of-the-art	_	_
methods	_	_
in	_	_
six	_	_
of	_	_
the	_	_
eight	_	_
standard	_	_
dataset	_	_
splits	_	_
for	_	_
this	_	_
task	_	_
.	_	_

#214
Acknowledgments	_	_
The	_	_
authors	_	_
gratefully	_	_
thank	_	_
NVIDIA	_	_
for	_	_
donating	_	_
the	_	_
GPUs	_	_
used	_	_
in	_	_
this	_	_
work	_	_
.	_	_

#215
Dynamic	_	_
Multimodal	_	_
Instance	_	_
Segmentation	_	_
15	_	_