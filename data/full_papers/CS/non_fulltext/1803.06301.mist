#0
Improved	_	_
Part	_	_
Segmentation	_	_
Performance	_	_
by	_	_
Optimising	_	_
Realism	_	_
of	_	_
Synthetic	_	_
Images	_	_
using	_	_
Cycle	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
.	_	_

#1
R.	_	_
Bartha	_	_
,	_	_
,	_	_
̊	_	_
J.	_	_
Hemminga	_	_
,	_	_
E.J	_	_
.	_	_

#2
Van	_	_
Hentenb	_	_
aWageningen	_	_
University	_	_
&	_	_
Research	_	_
,	_	_
Greenhouse	_	_
Horticulture	_	_
,	_	_
P.O	_	_
.	_	_

#3
Box	_	_
644	_	_
,	_	_
6700	_	_
AP	_	_
,	_	_
Wageningen	_	_
,	_	_
The	_	_
Netherlands	_	_
bWageningen	_	_
University	_	_
&	_	_
Research	_	_
,	_	_
Farm	_	_
Technology	_	_
Group	_	_
,	_	_
Droevendaalsesteeg	_	_
1	_	_
,	_	_
6708	_	_
PB	_	_
,	_	_
Wageningen	_	_
,	_	_
The	_	_
Netherlands	_	_

#4
Abstract	_	_

#5
In	_	_
this	_	_
paper	_	_
we	_	_
report	_	_
on	_	_
improved	_	_
part	_	_
segmentation	_	_
performance	_	_
using	_	_
convolutional	_	_
neural	_	_
networks	_	_
to	_	_
reduce	_	_
the	_	_
dependency	_	_
on	_	_
the	_	_
large	_	_
amount	_	_
of	_	_
manually	_	_
annotated	_	_
empirical	_	_
images	_	_
.	_	_

#6
This	_	_
was	_	_
achieved	_	_
by	_	_
optimising	_	_
the	_	_
visual	_	_
realism	_	_
of	_	_
synthetic	_	_
agricultural	_	_
images	_	_
.	_	_

#7
In	_	_
Part	_	_
I	_	_
,	_	_
a	_	_
cycle	_	_
consistent	_	_
generative	_	_
adversarial	_	_
network	_	_
was	_	_
applied	_	_
to	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
with	_	_
the	_	_
objective	_	_
to	_	_
generate	_	_
more	_	_
realistic	_	_
synthetic	_	_
images	_	_
by	_	_
translating	_	_
them	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#8
We	_	_
first	_	_
hypothesise	_	_
and	_	_
confirm	_	_
that	_	_
plant	_	_
part	_	_
image	_	_
features	_	_
such	_	_
as	_	_
color	_	_
and	_	_
texture	_	_
become	_	_
more	_	_
similar	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
after	_	_
translation	_	_
of	_	_
the	_	_
synthetic	_	_
images	_	_
.	_	_

#9
Results	_	_
confirm	_	_
this	_	_
with	_	_
an	_	_
improved	_	_
mean	_	_
color	_	_
distribution	_	_
correlation	_	_
with	_	_
the	_	_
empirical	_	_
data	_	_
prior	_	_
of	_	_
0.62	_	_
and	_	_
post	_	_
translation	_	_
of	_	_
0.90	_	_
.	_	_

#10
Furthermore	_	_
,	_	_
the	_	_
mean	_	_
image	_	_
features	_	_
of	_	_
contrast	_	_
,	_	_
homogeneity	_	_
,	_	_
energy	_	_
and	_	_
entropy	_	_
moved	_	_
closer	_	_
to	_	_
the	_	_
empirical	_	_
mean	_	_
,	_	_
post	_	_
translation	_	_
.	_	_

#11
In	_	_
Part	_	_
II	_	_
,	_	_
7	_	_
experiments	_	_
were	_	_
performed	_	_
using	_	_
convolutional	_	_
neural	_	_
networks	_	_
with	_	_
different	_	_
combinations	_	_
of	_	_
synthetic	_	_
,	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
and	_	_
empirical	_	_
images	_	_
.	_	_

#12
We	_	_
hypothesised	_	_
that	_	_
the	_	_
translated	_	_
images	_	_
can	_	_
be	_	_
used	_	_
for	_	_
(	_	_
i	_	_
)	_	_
improved	_	_
learning	_	_
of	_	_
empirical	_	_
images	_	_
,	_	_
and	_	_
(	_	_
ii	_	_
)	_	_
that	_	_
learning	_	_
without	_	_
any	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
is	_	_
improved	_	_
by	_	_
bootstrapping	_	_
with	_	_
translated	_	_
images	_	_
over	_	_
bootstrapping	_	_
with	_	_
synthetic	_	_
images	_	_
.	_	_

#13
Results	_	_
confirm	_	_
our	_	_
second	_	_
and	_	_
third	_	_
hypotheses	_	_
.	_	_

#14
First	_	_
a	_	_
maximum	_	_
intersection-over-union	_	_
performance	_	_
was	_	_
achieved	_	_
of	_	_
0.52	_	_
when	_	_
bootstrapping	_	_
with	_	_
translated	_	_
images	_	_
and	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
;	_	_
an	_	_
8	_	_
%	_	_
increase	_	_
compared	_	_
to	_	_
only	_	_
using	_	_
synthetic	_	_
images	_	_
.	_	_

#15
Second	_	_
,	_	_
training	_	_
without	_	_
any	_	_
empirical	_	_
fine-tuning	_	_
resulted	_	_
in	_	_
an	_	_
average	_	_
IOU	_	_
of	_	_
0.31	_	_
;	_	_
a	_	_
55	_	_
%	_	_
performance	_	_
increase	_	_
over	_	_
previous	_	_
methods	_	_
that	_	_
only	_	_
used	_	_
synthetic	_	_
images	_	_
.	_	_

#16
The	_	_
work	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
an	_	_
important	_	_
step	_	_
towards	_	_
improved	_	_
sensing	_	_
for	_	_
agricultural	_	_
robotics	_	_
.	_	_

#17
˚Corresponding	_	_
author	_	_
URL	_	_
:	_	_
ruud.barth	_	_
@	_	_
wur.nl	_	_
(	_	_
R.	_	_
Barth	_	_
)	_	_
,	_	_
jochen.hemming	_	_
@	_	_
wur.nl	_	_
(	_	_
J.	_	_
Hemming	_	_
)	_	_
,	_	_
eldert.vanhenten	_	_
@	_	_
wur.nl	_	_
(	_	_
E.J	_	_
.	_	_

#18
Van	_	_
Henten	_	_
)	_	_
ar	_	_
X	_	_
iv	_	_
:1	_	_
3	_	_
.	_	_

#19
1v	_	_
1	_	_
[	_	_
cs	_	_
.C	_	_
V	_	_
]	_	_
1	_	_
6	_	_
M	_	_
ar	_	_
2	_	_
Keywords	_	_
:	_	_
Agriculture	_	_
,	_	_
Robotics	_	_
,	_	_
Computer	_	_
Vision	_	_
,	_	_
Semantic	_	_
Segmentation	_	_
,	_	_
Generative	_	_
Adversarial	_	_
Networks	_	_
1	_	_
.	_	_

#20
Introduction	_	_
A	_	_
key	_	_
success	_	_
factor	_	_
of	_	_
agricultural	_	_
robotics	_	_
performance	_	_
is	_	_
a	_	_
robust	_	_
underlying	_	_
perception	_	_
methodology	_	_
that	_	_
can	_	_
distinguish	_	_
and	_	_
localise	_	_
object	_	_
parts	_	_
[	_	_
1	_	_
,	_	_
13	_	_
,	_	_
2	_	_
]	_	_
.	_	_

#21
In	_	_
order	_	_
to	_	_
train	_	_
state-of-the-art	_	_
machine	_	_
learning	_	_
methods	_	_
that	_	_
can	_	_
achieve	_	_
this	_	_
feat	_	_
,	_	_
large	_	_
annotated	_	_
empirical	_	_
image	_	_
datasets	_	_
remain	_	_
required	_	_
.	_	_

#22
Synthetic	_	_
images	_	_
can	_	_
help	_	_
bootstrapping	_	_
such	_	_
methods	_	_
in	_	_
order	_	_
to	_	_
reduce	_	_
the	_	_
required	_	_
amount	_	_
of	_	_
annotated	_	_
empirical	_	_
data	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#23
However	_	_
,	_	_
a	_	_
gap	_	_
in	_	_
realism	_	_
remains	_	_
between	_	_
the	_	_
modelled	_	_
synthetic	_	_
images	_	_
and	_	_
the	_	_
empirical	_	_
ones	_	_
,	_	_
plausibly	_	_
restraining	_	_
synthetic	_	_
bootstrapping	_	_
performance	_	_
.	_	_

#24
The	_	_
long	_	_
term	_	_
objective	_	_
of	_	_
our	_	_
research	_	_
is	_	_
to	_	_
improve	_	_
plant	_	_
part	_	_
segmentation	_	_
performance	_	_
.	_	_

#25
Previous	_	_
work	_	_
performed	_	_
synthetically	_	_
bootstrapping	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
(	_	_
CNN	_	_
)	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#26
In	_	_
this	_	_
paper	_	_
we	_	_
report	_	_
on	_	_
optimising	_	_
the	_	_
realism	_	_
of	_	_
rendered	_	_
synthetic	_	_
images	_	_
modelled	_	_
from	_	_
empirical	_	_
photographical	_	_
data	_	_
[	_	_
3	_	_
]	_	_
that	_	_
was	_	_
used	_	_
in	_	_
our	_	_
previous	_	_
work	_	_
.	_	_

#27
We	_	_
first	_	_
hypothesise	_	_
that	_	_
the	_	_
dissimilarity	_	_
between	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
qualitatively	_	_
and	_	_
quantitatively	_	_
reduced	_	_
using	_	_
unpaired	_	_
image-to-image	_	_
translation	_	_
by	_	_
cycle-consistent	_	_
adversarial	_	_
networks	_	_
(	_	_
Cycle-GAN	_	_
)	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#28
Furthermore	_	_
,	_	_
we	_	_
secondly	_	_
hypothesise	_	_
that	_	_
the	_	_
synthetic	_	_
images	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
can	_	_
be	_	_
used	_	_
for	_	_
improved	_	_
learning	_	_
of	_	_
empirical	_	_
images	_	_
,	_	_
potentially	_	_
further	_	_
closing	_	_
the	_	_
performance	_	_
gap	_	_
that	_	_
remained	_	_
previously	_	_
when	_	_
bootstrapping	_	_
only	_	_
with	_	_
synthetic	_	_
data	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#29
Additionally	_	_
,	_	_
our	_	_
third	_	_
hypothesis	_	_
is	_	_
that	_	_
without	_	_
any	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
,	_	_
improved	_	_
learning	_	_
of	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
achieved	_	_
using	_	_
only	_	_
translated	_	_
images	_	_
as	_	_
opposed	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
images	_	_
.	_	_

#30
The	_	_
key	_	_
contributions	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
are	_	_
the	_	_
(	_	_
i	_	_
)	_	_
further	_	_
minimisation	_	_
of	_	_
the	_	_
dependency	_	_
on	_	_
annotated	_	_
empirical	_	_
data	_	_
for	_	_
image	_	_
segmentation	_	_
learning	_	_
,	_	_
and	_	_
(	_	_
ii	_	_
)	_	_
improving	_	_
the	_	_
performance	_	_
thereof	_	_
.	_	_

#31
This	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
an	_	_
important	_	_
step	_	_
towards	_	_
improved	_	_
sensing	_	_
for	_	_
agricultural	_	_
robotics	_	_
.	_	_

#32
1.1	_	_
.	_	_

#33
Theoretical	_	_
background	_	_
Convolutional	_	_
neural	_	_
networks	_	_
recently	_	_
have	_	_
shown	_	_
state-of-the-art	_	_
performance	_	_
on	_	_
many	_	_
image	_	_
segmentation	_	_
tasks	_	_
[	_	_
9	_	_
,	_	_
24	_	_
,	_	_
6	_	_
]	_	_
.	_	_

#34
However	_	_
,	_	_
CNNs	_	_
require	_	_
large	_	_
annotated	_	_
datasets	_	_
on	_	_
a	_	_
per-pixel	_	_
level	_	_
in	_	_
order	_	_
to	_	_
successfully	_	_
train	_	_
the	_	_
large	_	_
number	_	_
of	_	_
free	_	_
parameters	_	_
of	_	_
the	_	_
deep	_	_
network	_	_
.	_	_

#35
Moreover	_	_
,	_	_
in	_	_
agriculture	_	_
the	_	_
high	_	_
amount	_	_
of	_	_
image	_	_
variety	_	_
due	_	_
to	_	_
a	_	_
wide	_	_
range	_	_
of	_	_
species	_	_
,	_	_
illumination	_	_
conditions	_	_
and	_	_
morphological	_	_
seasonal	_	_
growth	_	_
differences	_	_
,	_	_
leads	_	_
to	_	_
an	_	_
increased	_	_
annotated	_	_
dataset	_	_
size	_	_
dependency	_	_
.	_	_

#36
Satisfying	_	_
this	_	_
requirement	_	_
can	_	_
quickly	_	_
become	_	_
a	_	_
bottleneck	_	_
for	_	_
learning	_	_
.	_	_

#37
One	_	_
solution	_	_
is	_	_
to	_	_
bootstrap	_	_
CNNs	_	_
with	_	_
synthetic	_	_
images	_	_
including	_	_
automatically	_	_
computed	_	_
ground	_	_
truths	_	_
[	_	_
10	_	_
,	_	_
27	_	_
]	_	_
.	_	_

#38
Consequently	_	_
,	_	_
the	_	_
bootstrapped	_	_
network	_	_
can	_	_
be	_	_
fine-tuned	_	_
with	_	_
a	_	_
small	_	_
set	_	_
of	_	_
empirical	_	_
images	_	_
,	_	_
which	_	_
can	_	_
result	_	_
in	_	_
increased	_	_
performance	_	_
over	_	_
methods	_	_
without	_	_
synthetic	_	_
bootstrapping	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#39
Previously	_	_
we	_	_
have	_	_
shown	_	_
methods	_	_
to	_	_
create	_	_
such	_	_
a	_	_
synthetic	_	_
dataset	_	_
by	_	_
realistically	_	_
rendering	_	_
3D	_	_
modelled	_	_
plants	_	_
[	_	_
3	_	_
]	_	_
.	_	_

#40
Despite	_	_
intensive	_	_
manual	_	_
optimisation	_	_
for	_	_
geometry	_	_
,	_	_
color	_	_
and	_	_
textures	_	_
,	_	_
we	_	_
have	_	_
shown	_	_
that	_	_
a	_	_
discrepancy	_	_
remains	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
.	_	_

#41
Although	_	_
this	_	_
dataset	_	_
can	_	_
be	_	_
used	_	_
for	_	_
successful	_	_
synthetic	_	_
bootstrapping	_	_
and	_	_
improved	_	_
empirical	_	_
learning	_	_
,	_	_
there	_	_
remained	_	_
a	_	_
difference	_	_
between	_	_
the	_	_
achieved	_	_
performance	_	_
and	_	_
the	_	_
theoretical	_	_
optimal	_	_
performance	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#42
Recently	_	_
,	_	_
the	_	_
advent	_	_
of	_	_
generative	_	_
adversarial	_	_
networks	_	_
(	_	_
GAN	_	_
)	_	_
introduced	_	_
another	_	_
method	_	_
of	_	_
image	_	_
data	_	_
generation	_	_
[	_	_
15	_	_
]	_	_
.	_	_

#43
In	_	_
GANs	_	_
two	_	_
deep	_	_
convolutional	_	_
neural	_	_
networks	_	_
are	_	_
trained	_	_
simultaneously	_	_
and	_	_
adversarially	_	_
:	_	_
a	_	_
generative	_	_
model	_	_
G	_	_
and	_	_
a	_	_
discriminative	_	_
model	_	_
D.	_	_
The	_	_
generative	_	_
model’s	_	_
goal	_	_
is	_	_
to	_	_
capture	_	_
the	_	_
feature	_	_
distribution	_	_
of	_	_
a	_	_
dataset	_	_
by	_	_
learning	_	_
to	_	_
generate	_	_
images	_	_
thereof	_	_
from	_	_
latent	_	_
variables	_	_
(	_	_
e.g.	_	_
random	_	_
noise	_	_
vectors	_	_
)	_	_
.	_	_

#44
The	_	_
discriminative	_	_
model	_	_
in	_	_
turn	_	_
evaluates	_	_
to	_	_
what	_	_
extent	_	_
the	_	_
generated	_	_
image	_	_
is	_	_
a	_	_
true	_	_
member	_	_
of	_	_
the	_	_
dataset	_	_
.	_	_

#45
In	_	_
other	_	_
words	_	_
,	_	_
model	_	_
G	_	_
is	_	_
optimised	_	_
to	_	_
trick	_	_
model	_	_
D	_	_
while	_	_
model	_	_
D	_	_
is	_	_
optimising	_	_
to	_	_
not	_	_
get	_	_
fooled	_	_
by	_	_
model	_	_
G.	_	_
In	_	_
Figure	_	_
1	_	_
a	_	_
schematic	_	_
overview	_	_
of	_	_
this	_	_
learning	_	_
process	_	_
is	_	_
shown	_	_
.	_	_

#46
As	_	_
both	_	_
models	_	_
can	_	_
be	_	_
implemented	_	_
as	_	_
CNNs	_	_
,	_	_
the	_	_
error	_	_
can	_	_
be	_	_
back-propagated	_	_
to	_	_
minimise	_	_
the	_	_
loss	_	_
of	_	_
both	_	_
models	_	_
Figure	_	_
1	_	_
:	_	_
Learning	_	_
schematic	_	_
of	_	_
a	_	_
generative	_	_
adversarial	_	_
network	_	_
.	_	_

#47
In	_	_
each	_	_
learning	_	_
step	_	_
,	_	_
a	_	_
discriminator	_	_
receives	_	_
either	_	_
a	_	_
random	_	_
real	_	_
domain	_	_
image	_	_
from	_	_
a	_	_
database	_	_
or	_	_
a	_	_
translated	_	_
image	_	_
from	_	_
the	_	_
generator	_	_
.	_	_

#48
The	_	_
discriminator	_	_
determines	_	_
if	_	_
this	_	_
image	_	_
is	_	_
real	_	_
or	_	_
translated	_	_
.	_	_

#49
Through	_	_
a	_	_
loss	_	_
function	_	_
,	_	_
feedback	_	_
to	_	_
both	_	_
the	_	_
discriminator	_	_
and	_	_
the	_	_
generator	_	_
is	_	_
given	_	_
to	_	_
optimise	_	_
their	_	_
tasks	_	_
.	_	_

#50
In	_	_
this	_	_
example	_	_
,	_	_
the	_	_
generator	_	_
learns	_	_
to	_	_
synthesise	_	_
empirical	_	_
photographic	_	_
images	_	_
from	_	_
random	_	_
vectors	_	_
.	_	_

#51
simultaneously	_	_
.	_	_

#52
The	_	_
result	_	_
after	_	_
training	_	_
is	_	_
a	_	_
model	_	_
G	_	_
that	_	_
can	_	_
generate	_	_
new	_	_
random	_	_
images	_	_
highly	_	_
similar	_	_
to	_	_
the	_	_
learned	_	_
dataset	_	_
.	_	_

#53
This	_	_
method	_	_
is	_	_
useful	_	_
if	_	_
one	_	_
wants	_	_
to	_	_
generate	_	_
more	_	_
similar	_	_
images	_	_
from	_	_
the	_	_
same	_	_
domain	_	_
.	_	_

#54
Given	_	_
that	_	_
this	_	_
does	_	_
not	_	_
provide	_	_
a	_	_
corresponding	_	_
ground	_	_
truth	_	_
,	_	_
this	_	_
method	_	_
was	_	_
not	_	_
pursued	_	_
for	_	_
this	_	_
paper	_	_
,	_	_
although	_	_
as	_	_
we’ll	_	_
see	_	_
later	_	_
,	_	_
it	_	_
does	_	_
provide	_	_
a	_	_
fundamental	_	_
building	_	_
block	_	_
towards	_	_
the	_	_
method	_	_
that	_	_
was	_	_
used	_	_
.	_	_

#55
In	_	_
later	_	_
approaches	_	_
,	_	_
GANs	_	_
were	_	_
conditioned	_	_
with	_	_
an	_	_
additional	_	_
input	_	_
image	_	_
from	_	_
another	_	_
domain	_	_
[	_	_
21	_	_
]	_	_
,	_	_
forming	_	_
an	_	_
image	_	_
pair	_	_
that	_	_
had	_	_
some	_	_
relation	_	_
with	_	_
each	_	_
other	_	_
(	_	_
e.g.	_	_
a	_	_
color	_	_
image	_	_
and	_	_
its	_	_
label	_	_
or	_	_
class	_	_
mapping	_	_
)	_	_
.	_	_

#56
The	_	_
generator	_	_
was	_	_
tasked	_	_
with	_	_
image-to-image	_	_
translation	_	_
to	_	_
create	_	_
a	_	_
coherent	_	_
image	_	_
(	_	_
e.g.	_	_
color	_	_
)	_	_
from	_	_
a	_	_
corresponding	_	_
pair	_	_
image	_	_
(	_	_
e.g.	_	_
label	_	_
map	_	_
)	_	_
.	_	_

#57
The	_	_
discriminator’s	_	_
goal	_	_
is	_	_
then	_	_
to	_	_
evaluate	_	_
if	_	_
input	_	_
pairs	_	_
are	_	_
either	_	_
real	_	_
or	_	_
generated	_	_
.	_	_

#58
The	_	_
loss	_	_
can	_	_
then	_	_
be	_	_
fed	_	_
back	_	_
to	_	_
both	_	_
the	_	_
discriminator	_	_
and	_	_
generator	_	_
to	_	_
improve	_	_
on	_	_
their	_	_
tasks	_	_
.	_	_

#59
The	_	_
result	_	_
after	_	_
training	_	_
is	_	_
a	_	_
generator	_	_
G	_	_
that	_	_
can	_	_
translate	_	_
images	_	_
from	_	_
one	_	_
domain	_	_
X	_	_
(	_	_
e.g.	_	_
color	_	_
images	_	_
)	_	_
to	_	_
images	_	_
in	_	_
another	_	_
domain	_	_
Y	_	_
(	_	_
e.g.	_	_
label	_	_
maps	_	_
)	_	_
or	_	_
more	_	_
formally	_	_
notated	_	_
as	_	_
G	_	_
:	_	_
XÑY	_	_
.	_	_

#60
In	_	_
Figure	_	_
2	_	_
a	_	_
schematic	_	_
overview	_	_
of	_	_
the	_	_
learning	_	_
process	_	_
is	_	_
shown	_	_
.	_	_

#61
Given	_	_
that	_	_
this	_	_
does	_	_
not	_	_
provide	_	_
additional	_	_
novel	_	_
training	_	_
pairs	_	_
,	_	_
we	_	_
also	_	_
did	_	_
not	_	_
pursue	_	_
this	_	_
method	_	_
for	_	_
this	_	_
paper	_	_
,	_	_
although	_	_
again	_	_
this	_	_
methodology	_	_
provides	_	_
a	_	_
useful	_	_
building	_	_
block	_	_
for	_	_
our	_	_
work	_	_
.	_	_

#62
A	_	_
requirement	_	_
for	_	_
image-to-image	_	_
translation	_	_
using	_	_
conditional	_	_
GANs	_	_
,	_	_
is	_	_
that	_	_
images	_	_
from	_	_
both	_	_
domains	_	_
are	_	_
geometrically	_	_
paired	_	_
.	_	_

#63
For	_	_
our	_	_
objective	_	_
of	_	_
translating	_	_
images	_	_
from	_	_
the	_	_
synthetic	_	_
domain	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
this	_	_
requirement	_	_
was	_	_
not	_	_
met	_	_
because	_	_
images	_	_
from	_	_
both	_	_
domains	_	_
did	_	_
not	_	_
geometrically	_	_
correspond	_	_
one-to-one	_	_
.	_	_

#64
A	_	_
recent	_	_
approach	_	_
aimed	_	_
to	_	_
dissolve	_	_
this	_	_
paired	_	_
geometry	_	_
requirement	_	_
by	_	_
investigating	_	_
unpaired	_	_
image-to-image	_	_
translation	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#65
In	_	_
cycle-consistent	_	_
adversarial	_	_
networks	_	_
(	_	_
Cycle-GAN	_	_
)	_	_
,	_	_
a	_	_
mapping	_	_
G	_	_
:	_	_
XÑY	_	_
is	_	_
learned	_	_
whilst	_	_
at	_	_
the	_	_
same	_	_
time	_	_
also	_	_
the	_	_
inverse	_	_
mapping	_	_
F	_	_
:	_	_
YÑX	_	_
is	_	_
learned	_	_
.	_	_

#66
Both	_	_
domains	_	_
X	_	_
and	_	_
Y	_	_
have	_	_
corresponding	_	_
discriminators	_	_
DX	_	_
and	_	_
DY	_	_
.	_	_

#67
Hence	_	_
,	_	_
DX	_	_
ensures	_	_
G	_	_
to	_	_
translate	_	_
X	_	_
similar	_	_
to	_	_
Y	_	_
whilst	_	_
DY	_	_
tries	_	_
to	_	_
safeguard	_	_
a	_	_
preferably	_	_
indistinguishable	_	_
conversion	_	_
of	_	_
Y	_	_
to	_	_
X	_	_
.	_	_

#68
However	_	_
since	_	_
the	_	_
domains	_	_
are	_	_
unpaired	_	_
,	_	_
the	_	_
translation	_	_
at	_	_
this	_	_
point	_	_
does	_	_
not	_	_
guarantee	_	_
that	_	_
an	_	_
individual	_	_
image	_	_
x	_	_
P	_	_
X	_	_
is	_	_
mapped	_	_
to	_	_
a	_	_
geometrically	_	_
similar	_	_
image	_	_
in	_	_
domain	_	_
Y	_	_
(	_	_
or	_	_
vice	_	_
versa	_	_
y	_	_
P	_	_
Y	_	_
to	_	_
X	_	_
)	_	_
.	_	_

#69
This	_	_
is	_	_
because	_	_
there	_	_
are	_	_
boundless	_	_
mappings	_	_
from	_	_
x	_	_
that	_	_
result	_	_
in	_	_
the	_	_
same	_	_
target	_	_
distribution	_	_
of	_	_
Y	_	_
.	_	_

#70
Therefore	_	_
the	_	_
mapping	_	_
needs	_	_
to	_	_
be	_	_
constrained	_	_
in	_	_
a	_	_
way	_	_
such	_	_
that	_	_
the	_	_
original	_	_
geometry	_	_
is	_	_
maintained	_	_
.	_	_

#71
Figure	_	_
2	_	_
:	_	_
Learning	_	_
schematic	_	_
of	_	_
a	_	_
conditional	_	_
generative	_	_
adversarial	_	_
network	_	_
.	_	_

#72
In	_	_
each	_	_
learning	_	_
step	_	_
,	_	_
a	_	_
discriminator	_	_
receives	_	_
either	_	_
a	_	_
real	_	_
pair	_	_
of	_	_
corresponding	_	_
images	_	_
in	_	_
different	_	_
domains	_	_
(	_	_
e.g.	_	_
colored	_	_
3D	_	_
render	_	_
and	_	_
a	_	_
map	_	_
)	_	_
or	_	_
a	_	_
fake	_	_
pair	_	_
of	_	_
which	_	_
one	_	_
domain	_	_
image	_	_
was	_	_
translated	_	_
by	_	_
the	_	_
generator	_	_
(	_	_
e.g.	_	_
colored	_	_
3D	_	_
render	_	_
)	_	_
from	_	_
an	_	_
image	_	_
in	_	_
the	_	_
other	_	_
domain	_	_
(	_	_
e.g.	_	_
map	_	_
)	_	_
.	_	_

#73
The	_	_
discriminator	_	_
determines	_	_
if	_	_
this	_	_
image	_	_
pair	_	_
is	_	_
real	_	_
or	_	_
fake	_	_
.	_	_

#74
Through	_	_
a	_	_
loss	_	_
function	_	_
feedback	_	_
to	_	_
both	_	_
the	_	_
discriminator	_	_
and	_	_
the	_	_
generator	_	_
is	_	_
given	_	_
to	_	_
optimise	_	_
their	_	_
tasks	_	_
.	_	_

#75
In	_	_
this	_	_
example	_	_
,	_	_
the	_	_
generator	_	_
learns	_	_
to	_	_
translate	_	_
render-like	_	_
color	_	_
images	_	_
from	_	_
class	_	_
maps	_	_
.	_	_

#76
To	_	_
achieve	_	_
that	_	_
,	_	_
a	_	_
cycle	_	_
consistency	_	_
loss	_	_
was	_	_
added	_	_
to	_	_
further	_	_
regularise	_	_
the	_	_
learning	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#77
Given	_	_
a	_	_
sample	_	_
x	_	_
P	_	_
X	_	_
and	_	_
y	_	_
P	_	_
Y	_	_
,	_	_
a	_	_
loss	_	_
was	_	_
added	_	_
to	_	_
the	_	_
optimisation	_	_
such	_	_
that	_	_
F	_	_
(	_	_
G	_	_
(	_	_
x	_	_
)	_	_
)	_	_
«	_	_
x	_	_
and	_	_
G	_	_
(	_	_
F	_	_
(	_	_
y	_	_
)	_	_
)	_	_
«	_	_
y	_	_
.	_	_

#78
Hence	_	_
,	_	_
the	_	_
learning	_	_
was	_	_
therefore	_	_
constrained	_	_
by	_	_
the	_	_
intuition	_	_
that	_	_
if	_	_
an	_	_
input	_	_
image	_	_
is	_	_
translated	_	_
from	_	_
one	_	_
domain	_	_
to	_	_
the	_	_
other	_	_
and	_	_
then	_	_
back	_	_
again	_	_
,	_	_
an	_	_
image	_	_
should	_	_
result	_	_
similar	_	_
to	_	_
the	_	_
original	_	_
input	_	_
.	_	_

#79
This	_	_
similarity	_	_
is	_	_
captured	_	_
by	_	_
the	_	_
cycle	_	_
consistency	_	_
loss	_	_
,	_	_
which	_	_
forces	_	_
the	_	_
generators	_	_
G	_	_
and	_	_
F	_	_
to	_	_
achieve	_	_
unpaired	_	_
geometrically	_	_
consistent	_	_
image-to-image	_	_
translation	_	_
from	_	_
one	_	_
domain	_	_
to	_	_
the	_	_
other	_	_
and	_	_
vice	_	_
versa	_	_
.	_	_

#80
In	_	_
Figure	_	_
3	_	_
a	_	_
schematic	_	_
is	_	_
shown	_	_
of	_	_
this	_	_
learning	_	_
process	_	_
.	_	_

#81
Note	_	_
that	_	_
this	_	_
method	_	_
was	_	_
pursued	_	_
for	_	_
this	_	_
paper	_	_
,	_	_
because	_	_
it	_	_
allows	_	_
to	_	_
create	_	_
a	_	_
large	_	_
dataset	_	_
of	_	_
images	_	_
in	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#82
Furthermore	_	_
,	_	_
the	_	_
key	_	_
utility	_	_
lies	_	_
in	_	_
the	_	_
image	_	_
pair	_	_
P	_	_
,	_	_
in	_	_
which	_	_
the	_	_
ground	_	_
truth	_	_
class	_	_
mapping	_	_
from	_	_
the	_	_
synthetic	_	_
images	_	_
could	feasibility	_
also	_	_
be	_	_
used	_	_
for	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#83
Moreover	_	_
,	_	_
the	_	_
method	_	_
does	_	_
not	_	_
require	_	_
any	_	_
annotations	_	_
of	_	_
empirical	_	_
images	_	_
.	_	_

#84
The	_	_
paper	_	_
is	_	_
structured	_	_
in	_	_
2	_	_
parts	_	_
,	_	_
each	_	_
with	_	_
their	_	_
corresponding	_	_
materials	_	_
,	_	_
methods	_	_
,	_	_
discussion	_	_
and	_	_
conclusion	_	_
sections	_	_
.	_	_

#85
Part	_	_
I	_	_
describes	_	_
and	_	_
evaluates	_	_
the	_	_
image-to-image	_	_
translation	_	_
from	_	_
the	_	_
synthetic	_	_
rendered	_	_
domain	_	_
to	_	_
the	_	_
empirical	_	_
photographic	_	_
domain	_	_
.	_	_

#86
In	_	_
Part	_	_
II	_	_
,	_	_
the	_	_
effect	_	_
on	_	_
segmentation	_	_
learning	_	_
was	_	_
investigated	_	_
using	_	_
the	_	_
translated	_	_
images	_	_
from	_	_
Part	_	_
I	_	_
.	_	_

#87
The	_	_
paper	_	_
ends	_	_
with	_	_
a	_	_
general	_	_
discussion	_	_
and	_	_
conclusion	_	_
.	_	_

#88
Figure	_	_
3	_	_
:	_	_
Learning	_	_
schematic	_	_
of	_	_
a	_	_
cycle	_	_
generative	_	_
adversarial	_	_
network	_	_
.	_	_

#89
In	_	_
each	_	_
learning	_	_
step	_	_
,	_	_
generator	_	_
G	_	_
receives	_	_
an	_	_
image	_	_
from	_	_
domain	_	_
X	_	_
and	_	_
generator	_	_
F	_	_
receives	_	_
an	_	_
image	_	_
from	_	_
domain	_	_
Y	_	_
.	_	_

#90
Each	_	_
generator	_	_
is	_	_
trained	_	_
to	_	_
transform	_	_
the	_	_
input	_	_
image	_	_
to	_	_
the	_	_
other	_	_
domain	_	_
.	_	_

#91
A	_	_
discriminator	_	_
Y	_	_
and	_	_
discriminator	_	_
X	_	_
for	_	_
each	_	_
corresponding	_	_
domain	_	_
is	_	_
trained	_	_
to	_	_
distinguish	_	_
between	_	_
generated	_	_
and	_	_
original	_	_
domain	_	_
images	_	_
.	_	_

#92
From	_	_
those	_	_
first	_	_
set	_	_
of	_	_
generated	_	_
images	_	_
,	_	_
the	_	_
opposing	_	_
generator	_	_
then	_	_
synthesizes	_	_
the	_	_
second	_	_
set	_	_
of	_	_
images	_	_
back	_	_
to	_	_
its	_	_
original	_	_
domain	_	_
(	_	_
which	_	_
ideally	_	_
should	_	_
result	_	_
in	_	_
the	_	_
original	_	_
domain	_	_
image	_	_
)	_	_
.	_	_

#93
A	_	_
cycle	_	_
consistency	_	_
loss	_	_
is	_	_
then	_	_
calculated	_	_
by	_	_
comparing	_	_
the	_	_
second	_	_
set	_	_
of	_	_
images	_	_
with	_	_
the	_	_
initial	_	_
input	_	_
image	_	_
.	_	_

#94
The	_	_
loss	_	_
of	_	_
both	_	_
discriminators	_	_
and	_	_
cycle	_	_
consistency	_	_
is	_	_
fed	_	_
back	_	_
to	_	_
both	_	_
generators	_	_
for	_	_
learning	_	_
.	_	_

#95
In	_	_
this	_	_
example	_	_
,	_	_
each	_	_
generator	_	_
learns	_	_
to	_	_
synthesise	_	_
an	_	_
image	_	_
to	_	_
the	_	_
opposing	_	_
domain	_	_
,	_	_
whilst	_	_
remaining	_	_
geometrically	_	_
consistent	_	_
.	_	_

#96
This	_	_
example	_	_
was	_	_
pursued	_	_
in	_	_
this	_	_
paper	_	_
to	_	_
obtain	_	_
image	_	_
pair	_	_
P	_	_
,	_	_
consisting	_	_
of	_	_
the	_	_
label	_	_
lx	_	_
that	_	_
corresponds	_	_
to	_	_
xy	_	_
;	_	_
the	_	_
translated	_	_
image	_	_
from	_	_
domain	_	_
X	_	_
to	_	_
Y	_	_
.	_	_

#97
2	_	_
.	_	_

#98
Part	_	_
I	_	_
:	_	_
Image-to-image	_	_
translation	_	_
In	_	_
this	_	_
first	_	_
part	_	_
of	_	_
the	_	_
paper	_	_
we	_	_
describe	_	_
and	_	_
evaluate	_	_
the	_	_
unpaired	_	_
image-to-image	_	_
translation	_	_
on	_	_
agricultural	_	_
images	_	_
from	_	_
the	_	_
synthetic	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
and	_	_
vice	_	_
versa	_	_
.	_	_

#99
The	_	_
main	_	_
objective	_	_
was	_	_
to	_	_
obtain	_	_
pairs	_	_
of	_	_
images	_	_
P	_	_
consisting	_	_
of	_	_
an	_	_
image	_	_
translated	_	_
from	_	_
the	_	_
synthetic	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
and	_	_
a	_	_
corresponding	_	_
ground	_	_
truth	_	_
map	_	_
(	_	_
see	_	_
Figure	_	_
3	_	_
)	_	_
.	_	_

#100
2.1	_	_
.	_	_

#101
Materials	_	_
2.1.1	_	_
.	_	_

#102
Image	_	_
dataset	_	_
The	_	_
unpaired	_	_
image	_	_
dataset	_	_
[	_	_
3	_	_
]	_	_
of	_	_
Capsicum	_	_
annuum	_	_
(	_	_
sweet-	_	_
or	_	_
bell	_	_
pepper	_	_
)	_	_
was	_	_
used	_	_
that	_	_
consists	_	_
of	_	_
50	_	_
empirical	_	_
images	_	_
of	_	_
a	_	_
crop	_	_
in	_	_
a	_	_
commercial	_	_
high-tech	_	_
greenhouse	_	_
and	_	_
10,500	_	_
corresponding	_	_
synthetic	_	_
images	_	_
,	_	_
modelled	_	_
to	_	_
approximate	_	_
the	_	_
empirical	_	_
set	_	_
visually	_	_
.	_	_

#103
In	_	_
both	_	_
sets	_	_
,	_	_
8	_	_
classes	_	_
were	_	_
annotated	_	_
on	_	_
a	_	_
per-pixel	_	_
level	_	_
,	_	_
either	_	_
manually	_	_
for	_	_
the	_	_
empirical	_	_
dataset	_	_
or	_	_
computed	_	_
automatically	_	_
for	_	_
the	_	_
synthetic	_	_
dataset	_	_
.	_	_

#104
In	_	_
Figure	_	_
4	_	_
examples	_	_
of	_	_
images	_	_
in	_	_
the	_	_
dataset	_	_
are	_	_
shown	_	_
.	_	_

#105
The	_	_
dataset	_	_
was	_	_
publicly	_	_
released	_	_
at	_	_
:	_	_
http	_	_
:	_	_
//dx.doi.org/10.4121/uuid:884958f5-b868-46e1-b3d8-a0b5d91b02c0	_	_
Both	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
were	_	_
first	_	_
cropped	_	_
to	_	_
424x424	_	_
pixels	_	_
to	_	_
exclude	_	_
the	_	_
robot	_	_
end-effector’s	_	_
suction	_	_
cup	_	_
in	_	_
the	_	_
image	_	_
,	_	_
because	_	_
initial	_	_
image-to-image	_	_
translation	_	_
experiments	_	_
showed	_	_
the	_	_
cup	_	_
was	_	_
replicated	_	_
undesirably	_	_
in	_	_
other	_	_
parts	_	_
of	_	_
the	_	_
image	_	_
.	_	_

#106
This	_	_
was	_	_
in	_	_
line	_	_
with	_	_
previous	_	_
findings	_	_
from	_	_
the	_	_
same	_	_
authors	_	_
where	_	_
color	_	_
and	_	_
texture	_	_
translation	_	_
often	_	_
succeeded	_	_
,	_	_
but	_	_
domains	_	_
with	_	_
a	_	_
large	_	_
geometric	_	_
variation	_	_
were	_	_
translated	_	_
with	_	_
less	_	_
success	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#107
Secondly	_	_
,	_	_
the	_	_
image	_	_
was	_	_
resampled	_	_
bilinearly	_	_
to	_	_
a	_	_
resolution	_	_
of	_	_
1000x1000	_	_
pixels	_	_
as	_	_
additional	_	_
experiments	_	_
during	_	_
Part	_	_
II	_	_
showed	_	_
upscaling	_	_
improved	_	_
the	_	_
learning	_	_
.	_	_

#108
From	_	_
the	_	_
Capsicum	_	_
annuum	_	_
dataset	_	_
,	_	_
the	_	_
synthetic	_	_
images	_	_
1-1000	_	_
were	_	_
used	_	_
for	_	_
training	_	_
the	_	_
translations	_	_
and	_	_
the	_	_
remainder	_	_
for	_	_
testing	_	_
.	_	_

#109
For	_	_
the	_	_
empirical	_	_
images	_	_
,	_	_
50	_	_
annotated	_	_
images	_	_
of	_	_
the	_	_
Capsicum	_	_
annuum	_	_
dataset	_	_
were	_	_
used	_	_
for	_	_
testing	_	_
,	_	_
whereas	_	_
for	_	_
training	_	_
175	_	_
non-annotated	_	_
images	_	_
were	_	_
used	_	_
that	_	_
were	_	_
not	_	_
part	_	_
of	_	_
the	_	_
released	_	_
dataset	_	_
,	_	_
but	_	_
were	_	_
collected	_	_
during	_	_
the	_	_
same	_	_
data	_	_
acquisition	_	_
experiment	_	_
.	_	_

#110
Figure	_	_
4	_	_
:	_	_
Uncropped	_	_
examples	_	_
of	_	_
empirical	_	_
(	_	_
top	_	_
row	_	_
)	_	_
and	_	_
synthetic	_	_
(	_	_
bottom	_	_
row	_	_
)	_	_
color	_	_
images	_	_
(	_	_
left	_	_
column	_	_
)	_	_
and	_	_
their	_	_
corresponding	_	_
ground	_	_
truth	_	_
labels	_	_
(	_	_
right	_	_
column	_	_
)	_	_
.	_	_

#111
Part	_	_
class	_	_
labels	_	_
:	_	_
background	_	_
,	_	_
leafs	_	_
,	_	_
peppers	_	_
,	_	_
peduncles	_	_
,	_	_
stems	_	_
,	_	_
shoots	_	_
and	_	_
leaf	_	_
stems	_	_
,	_	_
wires	_	_
and	_	_
cuts	_	_
where	_	_
pepper	_	_
where	_	_
harvested	_	_
.	_	_

#112
2.1.2	_	_
.	_	_

#113
Software	_	_
The	_	_
Berkeley	_	_
AI	_	_
Research	_	_
(	_	_
BAIR	_	_
)	_	_
laboratory	_	_
implementation	_	_
of	_	_
unpaired	_	_
image-to-image	_	_
translation	_	_
using	_	_
cycle-consistent	_	_
adversarial	_	_
networks	_	_
was	_	_
used	_	_
[	_	_
29	_	_
]	_	_
.	_	_

#114
2.1.3	_	_
.	_	_

#115
Hardware	_	_
Experiments	_	_
were	_	_
run	_	_
on	_	_
a	_	_
NVIDIA	_	_
DevBox	_	_
system	_	_
with	_	_
4	_	_
TITAN	_	_
X	_	_
Maxwell	_	_
12GB	_	_
GPUs	_	_
,	_	_
Intel	_	_
Core	_	_
i7-5930K	_	_
and	_	_
128GB	_	_
DDR4	_	_
RAM	_	_
running	_	_
Ubuntu	_	_
14.04	_	_
.	_	_

#116
2.2	_	_
.	_	_

#117
Methods	_	_
The	_	_
adversarial	_	_
learning	_	_
scheme	_	_
in	_	_
Figure	_	_
3	_	_
was	_	_
applied	_	_
with	_	_
synthetic	_	_
images	_	_
as	_	_
domain	_	_
X	_	_
and	_	_
empirical	_	_
images	_	_
as	_	_
domain	_	_
Y	_	_
.	_	_

#118
The	_	_
hyper-parameters	_	_
of	_	_
the	_	_
Cycle-GAN	_	_
were	_	_
manually	_	_
optimised	_	_
by	_	_
visually	_	_
evaluating	_	_
the	_	_
resulting	_	_
images	_	_
with	_	_
their	_	_
target	_	_
domain	_	_
.	_	_

#119
The	_	_
number	_	_
of	_	_
generative	_	_
and	_	_
discriminative	_	_
filters	_	_
were	_	_
set	_	_
to	_	_
50	_	_
and	_	_
the	_	_
learning	_	_
rate	_	_
was	_	_
set	_	_
to	_	_
0.0002	_	_
with	_	_
an	_	_
ADAM	_	_
[	_	_
23	_	_
]	_	_
momentum	_	_
term	_	_
of	_	_
0.5	_	_
.	_	_

#120
The	_	_
basic	_	_
discriminator	_	_
model	_	_
was	_	_
used	_	_
,	_	_
whereas	_	_
for	_	_
the	_	_
generator	_	_
the	_	_
RESNET	_	_
6	_	_
blocks	_	_
model	_	_
was	_	_
used	_	_
[	_	_
19	_	_
]	_	_
.	_	_

#121
Weights	_	_
for	_	_
the	_	_
cycle	_	_
loss	_	_
were	_	_
set	_	_
to	_	_
10	_	_
for	_	_
each	_	_
translation	_	_
direction	_	_
.	_	_

#122
2.2.1.	_	_
Quantitative	_	_
translation	_	_
evaluation	_	_

#123
Although	_	_
the	_	_
success	_	_
of	_	_
the	_	_
translation	_	_
is	_	_
already	_	_
quantitatively	_	_
captured	_	_
by	_	_
the	_	_
adversarial	_	_
loss	_	_
,	_	_
this	_	_
measure	_	_
is	_	_
biased	_	_
and	_	_
mathematically	_	_
obfuscated	_	_
.	_	_

#124
By	_	_
specifically	_	_
looking	_	_
at	_	_
key	_	_
image	_	_
features	_	_
like	_	_
color	_	_
,	_	_
contrast	_	_
,	_	_
homogeneity	_	_
,	_	_
energy	_	_
and	_	_
entropy	_	_
,	_	_
it	_	_
could	feasibility-options	_
be	_	_
derived	_	_
if	_	_
the	_	_
translated	_	_
images	_	_
improved	_	_
on	_	_
those	_	_
features	_	_
.	_	_

#125
This	_	_
would	_	_
provide	_	_
evidence	_	_
about	_	_
the	_	_
dissimilarity	_	_
gap	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
empirical	_	_
domains	_	_
.	_	_

#126
For	_	_
this	_	_
purpose	_	_
,	_	_
we	_	_
first	_	_
compared	_	_
for	_	_
each	_	_
object	_	_
part	_	_
class	_	_
the	_	_
synthetic	_	_
color	_	_
distribution	_	_
prior	_	_
and	_	_
post	_	_
translation	_	_
with	_	_
those	_	_
of	_	_
the	_	_
empirical	_	_
distribution	_	_
.	_	_

#127
The	_	_
color	_	_
spectrum	_	_
of	_	_
each	_	_
class	_	_
was	_	_
obtained	_	_
by	_	_
first	_	_
transforming	_	_
the	_	_
color	_	_
images	_	_
to	_	_
HSI	_	_
colorspace	_	_
.	_	_

#128
The	_	_
Hue	_	_
channel	_	_
in	_	_
the	_	_
transformed	_	_
image	_	_
represented	_	_
for	_	_
each	_	_
pixel	_	_
which	_	_
color	_	_
was	_	_
present	_	_
,	_	_
regardless	_	_
of	_	_
illumination	_	_
and	_	_
saturation	_	_
intensity	_	_
.	_	_

#129
The	_	_
histogram	_	_
of	_	_
this	_	_
channel	_	_
was	_	_
then	_	_
taken	_	_
to	_	_
count	_	_
the	_	_
relative	_	_
color	_	_
occurrence	_	_
per	_	_
class	_	_
.	_	_

#130
As	_	_
we	_	_
hypothesise	_	_
that	_	_
the	_	_
color	_	_
difference	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
the	_	_
empirical	_	_
domain	_	_
images	_	_
will	_	_
be	_	_
reduced	_	_
after	_	_
translation	_	_
of	_	_
the	_	_
synthetic	_	_
images	_	_
,	_	_
the	_	_
correlations	_	_
of	_	_
the	_	_
color	_	_
distributions	_	_
of	_	_
each	_	_
object	_	_
part	_	_
class	_	_
were	_	_
compared	_	_
for	_	_
i	_	_
)	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
the	_	_
synthetic	_	_
images	_	_
,	_	_
and	_	_
ii	_	_
)	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
the	_	_
translated	_	_
images	_	_
.	_	_

#131
Second	_	_
,	_	_
to	_	_
obtain	_	_
additional	_	_
image	_	_
features	_	_
,	_	_
first	_	_
an	_	_
average	_	_
gray	_	_
level	_	_
co-occurrence	_	_
matrix	_	_
(	_	_
GLCM	_	_
)	_	_
[	_	_
16	_	_
]	_	_
was	_	_
calculated	_	_
for	_	_
each	_	_
class	_	_
for	_	_
the	_	_
first	_	_
10	_	_
images	_	_
in	_	_
the	_	_
synthetic	_	_
,	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
and	_	_
empirical	_	_
sets	_	_
.	_	_

#132
The	_	_
GLCM	_	_
summarises	_	_
how	_	_
often	_	_
a	_	_
pixel	_	_
with	_	_
a	_	_
certain	_	_
intensity	_	_
value	_	_
i	_	_
occurs	_	_
in	_	_
a	_	_
specific	_	_
spatial	_	_
relationship	_	_
to	_	_
a	_	_
pixel	_	_
with	_	_
the	_	_
intensity	_	_
value	_	_
j	_	_
.	_	_

#133
This	_	_
relationship	_	_
was	_	_
set	_	_
to	_	_
address	_	_
horizontally	_	_
neighbouring	_	_
pixels	_	_
only	_	_
.	_	_

#134
From	_	_
the	_	_
GLCM	_	_
,	_	_
the	_	_
following	_	_
features	_	_
were	_	_
derived	_	_
:	_	_
Contrast	_	_
=	_	_
ř	_	_
i	_	_
,	_	_
j	_	_
|i	_	_
´	_	_
j|2	_	_
GLCMpi	_	_
,	_	_
jq	_	_
,	_	_
measuring	_	_
the	_	_
overall	_	_
difference	_	_
in	_	_
luminance	_	_
between	_	_
neighbouring	_	_
pixels	_	_
.	_	_

#135
Homogeneity	_	_
=	_	_
ř	_	_
i	_	_
,	_	_
j	_	_
GLCMpi	_	_
,	_	_
jq	_	_
1`|i´j|	_	_
,	_	_
a	_	_
value	_	_
that	_	_
measures	_	_
the	_	_
closeness	_	_
of	_	_
the	_	_
distribution	_	_
of	_	_
elements	_	_
in	_	_
the	_	_
GLCM	_	_
to	_	_
the	_	_
GLCM	_	_
diagonal	_	_
,	_	_
which	_	_
implies	_	_
that	_	_
high	_	_
values	_	_
of	_	_
homogeneity	_	_
reflect	_	_
the	_	_
absence	_	_
of	_	_
changes	_	_
in	_	_
the	_	_
image	_	_
and	_	_
indicates	_	_
a	_	_
locally	_	_
homogenous	_	_
distribution	_	_
in	_	_
image	_	_
textures	_	_
.	_	_

#136
Energy	_	_
=	_	_
ř	_	_
i	_	_
,	_	_
j	_	_
GLCMpi	_	_
,	_	_
jq2	_	_
,	_	_
a	_	_
measure	_	_
of	_	_
texture	_	_
crudeness	_	_
or	_	_
disorder	_	_
.	_	_

#137
Entropy	_	_
=	_	_
ř	_	_
i	_	_
,	_	_
j	_	_
´lnpGLCMpi	_	_
,	_	_
jqq	_	_
¨	_	_
GLCMpi	_	_
,	_	_
jq	_	_
,	_	_
measuring	_	_
the	_	_
amount	_	_
of	_	_
information	_	_
or	_	_
complexity	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#138
2.3	_	_
.	_	_

#139
Results	_	_
In	_	_
Figure	_	_
5	_	_
the	_	_
results	_	_
of	_	_
the	_	_
image-to-image	_	_
translations	_	_
are	_	_
shown	_	_
.	_	_

#140
The	_	_
second	_	_
column	_	_
is	_	_
of	_	_
most	_	_
interest	_	_
to	_	_
our	_	_
research	_	_
,	_	_
as	_	_
it	_	_
shows	_	_
the	_	_
set	_	_
Xy	_	_
of	_	_
synthetic	_	_
images	_	_
which	_	_
were	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#141
However	_	_
,	_	_
as	_	_
a	_	_
reference	_	_
also	_	_
the	_	_
translation	_	_
from	_	_
empirical	_	_
to	_	_
the	_	_
synthetic	_	_
domain	_	_
is	_	_
shown	_	_
in	_	_
the	_	_
third	_	_
column	_	_
.	_	_

#142
The	_	_
color	_	_
distributions	_	_
for	_	_
each	_	_
object	_	_
part	_	_
class	_	_
for	_	_
the	_	_
synthetic	_	_
,	_	_
empirical	_	_
and	_	_
translated	_	_
synthetic	_	_
images	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
6	_	_
.	_	_

#143
The	_	_
corresponding	_	_
correlations	_	_
between	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
the	_	_
synthetic	_	_
as	_	_
well	_	_
as	_	_
translated	_	_
synthetic	_	_
images	_	_
are	_	_
shown	_	_
in	_	_
Table	_	_
1	_	_
.	_	_

#144
For	_	_
the	_	_
image	_	_
features	_	_
contrast	_	_
,	_	_
homogeneity	_	_
,	_	_
energy	_	_
and	_	_
entropy	_	_
,	_	_
the	_	_
results	_	_
per	_	_
class	_	_
for	_	_
the	_	_
synthetic	_	_
,	_	_
empirical	_	_
and	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
images	_	_
are	_	_
shown	_	_
in	_	_
Figure	_	_
7	_	_
.	_	_

#145
The	_	_
difference	_	_
of	_	_
0.100	_	_
was	_	_
found	_	_
in	_	_
contrast	_	_
Figure	_	_
5	_	_
:	_	_
Image-to-image	_	_
translation	_	_
examples	_	_
using	_	_
Cycle-GAN	_	_
.	_	_

#146
Source	_	_
domain	_	_
images	_	_
prior	_	_
translation	_	_
are	_	_
shown	_	_
in	_	_
the	_	_
outer	_	_
columns	_	_
;	_	_
synthetic	_	_
images	_	_
(	_	_
left	_	_
)	_	_
and	_	_
empirical	_	_
images	_	_
(	_	_
right	_	_
)	_	_
.	_	_

#147
The	_	_
second	_	_
column	_	_
shows	_	_
the	_	_
set	_	_
of	_	_
interest	_	_
Xy	_	_
;	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#148
The	_	_
third	_	_
column	_	_
shows	_	_
empirical	_	_
images	_	_
translated	_	_
to	_	_
synthetic	_	_
domain	_	_
.	_	_

#149
Figure	_	_
6	_	_
:	_	_
Color	_	_
distributions	_	_
discretized	_	_
to	_	_
256	_	_
values	_	_
in	_	_
the	_	_
hue	_	_
channel	_	_
(	_	_
x-axis	_	_
)	_	_
per	_	_
class	_	_
of	_	_
the	_	_
synthetic	_	_
,	_	_
empirical	_	_
and	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
images	_	_
.	_	_

#150
Integral	_	_
per	_	_
distribution	_	_
amounts	_	_
to	_	_
1	_	_
(	_	_
y-axis	_	_
)	_	_
.	_	_

#151
backgr	_	_
.	_	_

#152
leafs	_	_
peppers	_	_
peduncles	_	_
stems	_	_
shoots	_	_
wires	_	_
cuts	_	_
mean	_	_
correlation	_	_
(	_	_
synthetic	_	_
,	_	_
empirical	_	_
)	_	_
0.25	_	_
0.78	_	_
0.42	_	_
0.93	_	_
0.76	_	_
0.83	_	_
0.45	_	_
0.48	_	_
0.62	_	_
correlation	_	_
(	_	_
syntheticÑempirical	_	_
,	_	_
empirical	_	_
)	_	_
0.86	_	_
0.94	_	_
0.93	_	_
0.93	_	_
0.92	_	_
0.98	_	_
0.81	_	_
0.79	_	_
0.90	_	_
Table	_	_
1	_	_
:	_	_
Average	_	_
color	_	_
distribution	_	_
correlations	_	_
per	_	_
object	_	_
part	_	_
class	_	_
between	_	_
i	_	_
)	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
synthetic	_	_
images	_	_
,	_	_
and	_	_
ii	_	_
)	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
.	_	_

#153
averaged	_	_
over	_	_
all	_	_
classes	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
empirical	_	_
set	_	_
,	_	_
whereas	_	_
this	_	_
difference	_	_
was	_	_
reduced	_	_
to	_	_
0.015	_	_
for	_	_
the	_	_
translated	_	_
and	_	_
the	_	_
empirical	_	_
set	_	_
.	_	_

#154
Similarly	_	_
,	_	_
for	_	_
homogeneity	_	_
this	_	_
was	_	_
reduced	_	_
from	_	_
0.028	_	_
to	_	_
0.015	_	_
.	_	_

#155
For	_	_
the	_	_
energy	_	_
feature	_	_
,	_	_
this	_	_
was	_	_
reduced	_	_
from	_	_
0.126	_	_
to	_	_
0.026	_	_
.	_	_

#156
Regarding	_	_
entropy	_	_
,	_	_
the	_	_
average	_	_
difference	_	_
was	_	_
reduced	_	_
from	_	_
0.364	_	_
to	_	_
0.003	_	_
.	_	_

#157
2.4	_	_
.	_	_

#158
Discussion	_	_
and	_	_
conclusion	_	_
Qualitative	_	_
visual	_	_
evaluation	_	_
of	_	_
the	_	_
results	_	_
in	_	_
Figure	_	_
5	_	_
showed	_	_
a	_	_
remarkable	_	_
translation	_	_
of	_	_
synthetic	_	_
images	_	_
to	_	_
empirical	_	_
looking	_	_
images	_	_
and	_	_
vice	_	_
versa	_	_
.	_	_

#159
Most	_	_
ba	_	_
ck	_	_
gr	_	_
ou	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
fru	_	_
it	_	_
pe	_	_
du	_	_
nc	_	_
le	_	_
s	_	_
m	_	_
ai	_	_
n	_	_
st	_	_
em	_	_
s	_	_
si	_	_
de	_	_
sh	_	_
oo	_	_
ts	_	_
a	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
te	_	_
m	_	_
s	_	_
w	_	_
ire	_	_
s	_	_
cu	_	_
ts	_	_
0	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
C	_	_
o	_	_
n	_	_
tr	_	_
a	_	_
s	_	_
t	_	_
Contrast	_	_
ba	_	_
ck	_	_
gr	_	_
ou	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
fru	_	_
it	_	_
pe	_	_
du	_	_
nc	_	_
le	_	_
s	_	_
m	_	_
ai	_	_
n	_	_
st	_	_
em	_	_
s	_	_
si	_	_
de	_	_
sh	_	_
oo	_	_
ts	_	_
a	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
te	_	_
m	_	_
s	_	_
w	_	_
ire	_	_
s	_	_
cu	_	_
ts	_	_
0.8	_	_
0.9	_	_
1	_	_
H	_	_
o	_	_
m	_	_
o	_	_
g	_	_
e	_	_
n	_	_
e	_	_
it	_	_
y	_	_
Homogeneity	_	_
ba	_	_
ck	_	_
gr	_	_
ou	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
fru	_	_
it	_	_
pe	_	_
du	_	_
nc	_	_
le	_	_
s	_	_
m	_	_
ai	_	_
n	_	_
st	_	_
em	_	_
s	_	_
si	_	_
de	_	_
sh	_	_
oo	_	_
ts	_	_
a	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
te	_	_
m	_	_
s	_	_
w	_	_
ire	_	_
s	_	_
cu	_	_
ts	_	_
0	_	_
0.2	_	_
0.4	_	_
0.6	_	_
0.8	_	_
1	_	_
E	_	_
n	_	_
e	_	_
rg	_	_
y	_	_
Energy	_	_
ba	_	_
ck	_	_
gr	_	_
ou	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
fru	_	_
it	_	_
pe	_	_
du	_	_
nc	_	_
le	_	_
s	_	_
m	_	_
ai	_	_
n	_	_
st	_	_
em	_	_
s	_	_
si	_	_
de	_	_
sh	_	_
oo	_	_
ts	_	_
a	_	_
nd	_	_
le	_	_
af	_	_
s	_	_
te	_	_
m	_	_
s	_	_
w	_	_
ire	_	_
s	_	_
cu	_	_
ts	_	_
0.5	_	_
1	_	_
1.5	_	_
2	_	_
E	_	_
n	_	_
tr	_	_
o	_	_
p	_	_
y	_	_
Entropy	_	_
Figure	_	_
7	_	_
:	_	_
Image	_	_
features	_	_
values	_	_
for	_	_
contrast	_	_
,	_	_
homogeneity	_	_
,	_	_
erergy	_	_
and	_	_
entropy	_	_
per	_	_
class	_	_
for	_	_
the	_	_
empirical	_	_
,	_	_
synthetic	_	_
and	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
˚	_	_
images	_	_
.	_	_

#160
Average	_	_
over	_	_
all	_	_
classes	_	_
is	_	_
represented	_	_
by	_	_
a	_	_
solid	_	_
line	_	_
for	_	_
the	_	_
empirical	_	_
set	_	_
,	_	_
a	_	_
dashed-dotted	_	_
line	_	_
for	_	_
the	_	_
synthetic	_	_
set	_	_
and	_	_
a	_	_
dashed	_	_
line	_	_
for	_	_
the	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
set	_	_
.	_	_

#161
notably	_	_
the	_	_
scattering	_	_
of	_	_
illumination	_	_
and	_	_
color	_	_
of	_	_
each	_	_
plant	_	_
part	_	_
were	_	_
converted	_	_
realistically	_	_
.	_	_

#162
It	_	_
also	_	_
appeared	_	_
that	_	_
the	_	_
model	_	_
learns	_	_
to	_	_
distinguish	_	_
plant	_	_
parts	_	_
without	_	_
any	_	_
supervised	_	_
information	_	_
,	_	_
as	_	_
the	_	_
(	_	_
partially	_	_
)	_	_
ripe	_	_
and	_	_
unripe	_	_
fruit	_	_
were	_	_
often	_	_
translated	_	_
to	_	_
the	_	_
other	_	_
domain	_	_
with	_	_
altered	_	_
maturity	_	_
levels	_	_
.	_	_

#163
A	_	_
difference	_	_
in	_	_
camera	_	_
focus	_	_
seemed	_	_
translated	_	_
properly	_	_
,	_	_
indicating	_	_
that	_	_
local	_	_
features	_	_
(	_	_
e.g.	_	_
edge	_	_
blur	_	_
and	_	_
texture	_	_
)	_	_
can	_	_
be	_	_
mapped	_	_
accurately	_	_
.	_	_

#164
Some	_	_
image	_	_
artifacts	_	_
did	_	_
arise	_	_
however	_	_
,	_	_
especially	_	_
the	_	_
translation	_	_
of	_	_
overexposed	_	_
areas	_	_
like	_	_
sunshine	_	_
or	_	_
fruit	_	_
reflections	_	_
.	_	_

#165
The	_	_
explanation	_	_
might	speculation	_
be	_	_
that	_	_
the	_	_
model	_	_
can	_	_
not	_	_
generate	_	_
this	_	_
information	_	_
correctly	_	_
because	_	_
any	_	_
information	_	_
beyond	_	_
overexposure	_	_
prior	_	_
translation	_	_
was	_	_
already	_	_
collapsed	_	_
to	_	_
a	_	_
single	_	_
maximum	_	_
value	_	_
(	_	_
e.g.	_	_
255	_	_
)	_	_
.	_	_

#166
Furthermore	_	_
,	_	_
an	_	_
overlay	_	_
of	_	_
a	_	_
checkerboard-like	_	_
texture	_	_
seems	_	_
to	_	_
have	_	_
been	_	_
added	_	_
to	_	_
the	_	_
translated	_	_
local	_	_
textures	_	_
.	_	_

#167
The	_	_
image-to-image	_	_
translation	_	_
method	_	_
appeared	_	_
not	_	_
to	_	_
be	_	_
suited	_	_
when	_	_
one	_	_
image	_	_
set	_	_
contained	_	_
additional	_	_
objects	_	_
or	_	_
parts	_	_
that	_	_
were	_	_
absent	_	_
in	_	_
the	_	_
other	_	_
set	_	_
,	_	_
such	_	_
as	_	_
the	_	_
presence	_	_
of	_	_
a	_	_
suction	_	_
cup	_	_
in	_	_
our	_	_
earlier	_	_
experiments	_	_
.	_	_

#168
We	_	_
noticed	_	_
in	_	_
previous	_	_
experiments	_	_
that	_	_
this	_	_
part	_	_
was	_	_
undesirably	_	_
replicated	_	_
in	_	_
other	_	_
areas	_	_
of	_	_
the	_	_
image	_	_
.	_	_

#169
In	_	_
Figure	_	_
5	_	_
we	_	_
can	_	_
also	_	_
see	_	_
that	_	_
large	_	_
morphological	_	_
features	_	_
(	_	_
e.g.	_	_
large	_	_
plant	_	_
part	_	_
shape	_	_
and	_	_
geometry	_	_
relatively	_	_
to	_	_
other	_	_
plant	_	_
parts	_	_
)	_	_
were	_	_
not	_	_
translated	_	_
,	_	_
indicating	_	_
a	_	_
limitation	_	_
of	_	_
the	_	_
Cycle-GAN	_	_
approach	_	_
.	_	_

#170
However	_	_
,	_	_
since	_	_
geometry	_	_
was	_	_
not	_	_
translated	_	_
,	_	_
this	_	_
did	_	_
allow	_	_
for	_	_
using	_	_
the	_	_
underlying	_	_
synthetic	_	_
ground	_	_
truth	_	_
labels	_	_
to	_	_
be	_	_
used	_	_
with	_	_
the	_	_
translated	_	_
images	_	_
for	_	_
Part	_	_
II	_	_
.	_	_

#171
If	_	_
also	_	_
the	_	_
geometry	_	_
would	_	_
have	_	_
been	_	_
translated	_	_
,	_	_
then	_	_
the	_	_
ground	_	_
truth	_	_
labels	_	_
would	_	_
not	_	_
have	_	_
been	_	_
translated	_	_
accordingly	_	_
.	_	_

#172
In	_	_
Figure	_	_
6	_	_
,	_	_
the	_	_
translation	_	_
effect	_	_
on	_	_
color	_	_
distribution	_	_
can	_	_
be	_	_
seen	_	_
for	_	_
each	_	_
plant	_	_
part	_	_
and	_	_
background	_	_
.	_	_

#173
Quantitatively	_	_
,	_	_
the	_	_
mean	_	_
color	_	_
correlation	_	_
of	_	_
0.62	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
increased	_	_
post	_	_
translation	_	_
to	_	_
0.90	_	_
(	_	_
See	_	_
Table	_	_
1	_	_
for	_	_
correlations	_	_
per	_	_
plant	_	_
part	_	_
and	_	_
the	_	_
mean	_	_
over	_	_
all	_	_
plant	_	_
parts	_	_
)	_	_
.	_	_

#174
Indeed	_	_
this	_	_
is	_	_
also	_	_
what	_	_
we	_	_
observe	_	_
in	_	_
Figure	_	_
5	_	_
,	_	_
where	_	_
for	_	_
example	_	_
the	_	_
color	_	_
of	_	_
the	_	_
fruit	_	_
in	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
matches	_	_
the	_	_
empirical	_	_
images	_	_
more	_	_
than	_	_
those	_	_
of	_	_
the	_	_
synthetic	_	_
images	_	_
.	_	_

#175
When	_	_
we	_	_
look	_	_
at	_	_
the	_	_
averages	_	_
of	_	_
the	_	_
image	_	_
texture	_	_
features	_	_
contrast	_	_
,	_	_
homogeneity	_	_
,	_	_
energy	_	_
and	_	_
entropy	_	_
,	_	_
they	_	_
were	_	_
closer	_	_
together	_	_
when	_	_
comparing	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
synthetic	_	_
translated	_	_
images	_	_
than	_	_
when	_	_
comparing	_	_
the	_	_
empirical	_	_
images	_	_
and	_	_
the	_	_
synthetic	_	_
images	_	_
.	_	_

#176
For	_	_
some	_	_
individual	_	_
classes	_	_
this	_	_
did	_	_
not	_	_
hold	_	_
however	_	_
,	_	_
e.g.	_	_
the	_	_
homogeneity	_	_
of	_	_
the	_	_
cuts	_	_
was	_	_
erroneously	_	_
doubled	_	_
instead	_	_
.	_	_

#177
In	_	_
Figure	_	_
5	_	_
it	_	_
can	_	_
indeed	_	_
be	_	_
observed	_	_
that	_	_
local	_	_
level	_	_
textures	_	_
of	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
have	_	_
become	_	_
more	_	_
similar	_	_
to	_	_
those	_	_
of	_	_
the	_	_
empirical	_	_
images	_	_
.	_	_

#178
For	_	_
example	_	_
,	_	_
the	_	_
smoothness	_	_
of	_	_
the	_	_
fruit	_	_
in	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
is	_	_
improved	_	_
towards	_	_
the	_	_
empirical	_	_
images	_	_
,	_	_
as	_	_
compared	_	_
to	_	_
the	_	_
the	_	_
more	_	_
coarse	_	_
and	_	_
grainy	_	_
surface	_	_
texture	_	_
of	_	_
the	_	_
fruit	_	_
in	_	_
the	_	_
synthetic	_	_
images	_	_
.	_	_

#179
Regarding	_	_
our	_	_
first	_	_
hypothesis	_	_
,	_	_
we	_	_
therefore	_	_
confirm	_	_
that	_	_
image	_	_
feature	_	_
differences	_	_
with	_	_
the	_	_
empirical	_	_
set	_	_
were	_	_
reduced	_	_
after	_	_
translation	_	_
of	_	_
the	_	_
synthetic	_	_
images	_	_
,	_	_
using	_	_
a	_	_
cycle-GAN	_	_
.	_	_

#180
This	_	_
part	_	_
of	_	_
the	_	_
work	_	_
contributed	_	_
to	_	_
the	_	_
field	_	_
of	_	_
computer	_	_
vision	_	_
(	_	_
e.g.	_	_
for	_	_
agricultural	_	_
robotics	_	_
)	_	_
by	_	_
providing	_	_
a	_	_
method	_	_
for	_	_
optimising	_	_
realism	_	_
in	_	_
synthetic	_	_
training	_	_
data	_	_
to	_	_
potentially	_	_
improve	_	_
state-of-the-art	_	_
machine	_	_
learning	_	_
methods	_	_
that	_	_
semantically	_	_
segment	_	_
plant	_	_
parts	_	_
,	_	_
as	_	_
evaluated	_	_
in	_	_
Part	_	_
II	_	_
of	_	_
this	_	_
paper	_	_
.	_	_

#181
3	_	_
.	_	_

#182
Part	_	_
II	_	_
:	_	_
Improving	_	_
semantic	_	_
segmentation	_	_
In	_	_
Part	_	_
II	_	_
,	_	_
the	_	_
effect	_	_
on	_	_
using	_	_
translated	_	_
images	_	_
on	_	_
object	_	_
part	_	_
segmentation	_	_
learning	_	_
was	_	_
investigated	_	_
by	_	_
using	_	_
the	_	_
translated	_	_
images	_	_
from	_	_
Part	_	_
I	_	_
instead	_	_
of	_	_
synthetic	_	_
images	_	_
.	_	_

#183
Our	_	_
second	_	_
hypothesis	_	_
states	_	_
that	_	_
by	_	_
bootstrapping	_	_
with	_	_
translated	_	_
images	_	_
and	_	_
empirical	_	_
fine-tuning	_	_
,	_	_
the	_	_
highest	_	_
empirical	_	_
performance	_	_
can	_	_
be	_	_
achieved	_	_
over	_	_
methods	_	_
that	_	_
bootstrap	_	_
with	_	_
limited	_	_
dataset	_	_
size	_	_
of	_	_
(	_	_
30	_	_
)	_	_
empirical	_	_
images	_	_
or	_	_
a	_	_
large	_	_
set	_	_
(	_	_
8750	_	_
)	_	_
of	_	_
synthetic	_	_
images	_	_
.	_	_

#184
With	_	_
our	_	_
third	_	_
hypothesis	_	_
in	_	_
this	_	_
paper	_	_
,	_	_
we	_	_
reckon	_	_
that	_	_
without	_	_
any	_	_
empirical	_	_
fine-tuning	_	_
,	_	_
learning	_	_
can	_	_
be	_	_
improved	_	_
with	_	_
translated	_	_
images	_	_
as	_	_
compared	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
bootstrapping	_	_
.	_	_

#185
3.1	_	_
.	_	_

#186
Materials	_	_
The	_	_
synthetic	_	_
and	_	_
empirical	_	_
datasets	_	_
as	_	_
described	_	_
in	_	_
Part	_	_
I	_	_
(	_	_
see	_	_
Section	_	_
2.1.1	_	_
)	_	_
were	_	_
used	_	_
as	_	_
well	_	_
as	_	_
the	_	_
obtained	_	_
image	_	_
pairs	_	_
P	_	_
plx	_	_
,	_	_
xyq	_	_
(	_	_
see	_	_
Figure	_	_
2	_	_
)	_	_
.	_	_

#187
3.1.1	_	_
.	_	_

#188
Software	_	_
The	_	_
publicly	_	_
available	_	_
semantic	_	_
segmentation	_	_
framework	_	_
DeepLab	_	_
V2	_	_
was	_	_
used	_	_
,	_	_
which	_	_
implemented	_	_
convolutional	_	_
neural	_	_
network	_	_
(	_	_
CNN	_	_
)	_	_
models	_	_
[	_	_
26	_	_
,	_	_
7	_	_
]	_	_
on	_	_
top	_	_
of	_	_
Caffe	_	_
[	_	_
22	_	_
]	_	_
.	_	_

#189
Specifically	_	_
,	_	_
the	_	_
VGG-16	_	_
network	_	_
was	_	_
used	_	_
with	_	_
a	_	_
modification	_	_
to	_	_
include	_	_
à	_	_
trous	_	_
spatial	_	_
pyramid	_	_
pooling	_	_
for	_	_
image	_	_
context	_	_
at	_	_
multiple	_	_
scales	_	_
by	_	_
convolutional	_	_
feature	_	_
layers	_	_
with	_	_
different	_	_
fields-of-view	_	_
[	_	_
8	_	_
,	_	_
18	_	_
]	_	_
.	_	_

#190
3.1.2	_	_
.	_	_

#191
Hardware	_	_
Experiments	_	_
were	_	_
run	_	_
on	_	_
the	_	_
same	_	_
hardware	_	_
as	_	_
used	_	_
in	_	_
Part	_	_
I	_	_
.	_	_

#192
As	_	_
a	_	_
dependency	_	_
for	_	_
the	_	_
DeepLab	_	_
V2	_	_
Caffe	_	_
version	_	_
,	_	_
the	_	_
archived	_	_
version	_	_
of	_	_
CUDA	_	_
7.5	_	_
was	_	_
installed	_	_
.	_	_

#193
3.2	_	_
.	_	_

#194
Methods	_	_
To	_	_
compare	_	_
performance	_	_
differences	_	_
,	_	_
7	_	_
experiments	_	_
were	_	_
performed	_	_
using	_	_
different	_	_
combinations	_	_
of	_	_
train	_	_
,	_	_
fine-tune	_	_
and	_	_
test	_	_
sets	_	_
.	_	_

#195
The	_	_
motivation	_	_
for	_	_
each	_	_
experiment	_	_
is	_	_
given	_	_
below	_	_
and	_	_
the	_	_
used	_	_
sets	_	_
and	_	_
image	_	_
ranges	_	_
are	_	_
shown	_	_
between	_	_
brackets	_	_
.	_	_

#196
A	_	_
Train	_	_
:	_	_
empirical	_	_
(	_	_
1-30	_	_
)	_	_
.	_	_

#197
Test	_	_
:	_	_
empirical	_	_
(	_	_
41-50	_	_
)	_	_
.	_	_

#198
An	_	_
experiment	_	_
to	_	_
see	_	_
if	_	_
the	_	_
model	_	_
can	_	_
learn	_	_
using	_	_
only	_	_
a	_	_
small	_	_
empirical	_	_
dataset	_	_
.	_	_

#199
This	_	_
provides	_	_
a	_	_
reference	_	_
for	_	_
comparison	_	_
of	_	_
performance	_	_
with	_	_
other	_	_
experiments	_	_
that	_	_
bootstrap	_	_
with	_	_
synthetic	_	_
or	_	_
translated	_	_
synthetic	_	_
images	_	_
and/or	_	_
fine-tune	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#200
Given	_	_
the	_	_
small	_	_
training	_	_
size	_	_
of	_	_
the	_	_
dataset	_	_
in	_	_
this	_	_
experiment	_	_
,	_	_
the	_	_
performance	_	_
was	_	_
expected	_	_
to	_	_
be	_	_
low	_	_
,	_	_
compared	_	_
to	_	_
all	_	_
other	_	_
experiments	_	_
that	_	_
tested	_	_
on	_	_
empirical	_	_
images	_	_
.	_	_

#201
B	_	_
Train	_	_
:	_	_
synthetic	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#202
Test	_	_
:	_	_
synthetic	_	_
(	_	_
8851-8900	_	_
)	_	_
.	_	_

#203
This	_	_
experiment	_	_
was	_	_
run	_	_
to	_	_
obtain	_	_
baseline	_	_
performance	_	_
of	_	_
the	_	_
model	_	_
when	_	_
having	_	_
access	_	_
to	_	_
a	_	_
large	_	_
and	_	_
detailed	_	_
annotated	_	_
synthetic	_	_
dataset	_	_
.	_	_

#204
Performance	_	_
is	_	_
expected	_	_
to	_	_
be	_	_
highest	_	_
of	_	_
all	_	_
experiments	_	_
because	_	_
of	_	_
the	_	_
perfect	_	_
labels	_	_
,	_	_
largest	_	_
dataset	_	_
size	_	_
and	_	_
relatively	_	_
low	_	_
image	_	_
feature	_	_
variance	_	_
compared	_	_
to	_	_
empirical	_	_
or	_	_
synthetic	_	_
translated	_	_
images	_	_
.	_	_

#205
C	_	_
Train	_	_
:	_	_
synthetic	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#206
Test	_	_
:	_	_
empirical	_	_
(	_	_
41-50	_	_
)	_	_
.	_	_

#207
A	_	_
reference	_	_
experiment	_	_
to	_	_
see	_	_
to	_	_
what	_	_
extent	_	_
a	_	_
network	_	_
trained	_	_
on	_	_
synthetic	_	_
images	_	_
can	_	_
generalise	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
without	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#208
Given	_	_
the	_	_
similarity	_	_
gap	_	_
between	_	_
synthetic	_	_
and	_	_
empirical	_	_
data	_	_
,	_	_
the	_	_
performance	_	_
should	_	_
be	_	_
relatively	_	_
low	_	_
compared	_	_
to	_	_
that	_	_
of	_	_
Experiment	_	_
A	_	_
or	_	_
when	_	_
compared	_	_
to	_	_
experiments	_	_
that	_	_
trained	_	_
on	_	_
a	_	_
more	_	_
realistic	_	_
dataset	_	_
,	_	_
e.g	_	_
translated	_	_
synthetic	_	_
as	_	_
in	_	_
Experiment	_	_
F	_	_
.	_	_

#209
D	_	_

#210
Train	_	_
:	_	_
synthetic	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#211
Fine-tune	_	_
:	_	_
empirical	_	_
(	_	_
1-30	_	_
)	_	_
.	_	_

#212
Test	_	_
:	_	_
empirical	_	_
(	_	_
41-50	_	_
)	_	_
.	_	_

#213
Similar	_	_
to	_	_
Experiment	_	_
C	_	_
,	_	_
but	_	_
with	_	_
an	_	_
extra	_	_
fine-tuning	_	_
step	_	_
using	_	_
empirical	_	_
images	_	_
.	_	_

#214
Performance	_	_
is	_	_
expected	_	_
to	_	_
be	_	_
higher	_	_
than	_	_
C	_	_
,	_	_
because	_	_
the	_	_
network	_	_
also	_	_
optimises	_	_
for	_	_
the	_	_
empirical	_	_
image	_	_
feature	_	_
distribution	_	_
.	_	_

#215
The	_	_
performance	_	_
of	_	_
this	_	_
experiment	_	_
is	_	_
expected	_	_
to	_	_
be	_	_
lower	_	_
than	_	_
that	_	_
of	_	_
Experiment	_	_
G	_	_
,	_	_
where	_	_
the	_	_
synthetic	_	_
images	_	_
were	_	_
replaced	_	_
by	_	_
translated	_	_
synthetic	_	_
images	_	_
,	_	_
because	_	_
the	_	_
synthetic	_	_
image	_	_
feature	_	_
distribution	_	_
is	_	_
more	_	_
dissimilar	_	_
with	_	_
the	_	_
empirical	_	_
distribution	_	_
than	_	_
the	_	_
translated	_	_
synthetic	_	_
distribution	_	_
with	_	_
the	_	_
empirical	_	_
distribution	_	_
is	_	_
.	_	_

#216
E	_	_
Train	_	_
:	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#217
Test	_	_
:	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
(	_	_
8851-8900	_	_
)	_	_
.	_	_

#218
This	_	_
experiment	_	_
was	_	_
run	_	_
to	_	_
obtain	_	_
baseline	_	_
performance	_	_
of	_	_
the	_	_
model	_	_
when	_	_
having	_	_
access	_	_
to	_	_
a	_	_
large	_	_
and	_	_
detailed	_	_
annotated	_	_
translated	_	_
synthetic	_	_
dataset	_	_
.	_	_

#219
The	_	_
performance	_	_
should	_	_
be	_	_
similar	_	_
of	_	_
that	_	_
of	_	_
Experiment	_	_
B	_	_
,	_	_
though	_	_
is	_	_
expected	_	_
to	_	_
a	_	_
bit	_	_
lower	_	_
due	_	_
to	_	_
the	_	_
extra	_	_
variance	_	_
that	_	_
the	_	_
empirical	_	_
feature	_	_
distribution	_	_
might	speculation	_
have	_	_
introduced	_	_
when	_	_
synthetic	_	_
images	_	_
were	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#220
F	_	_

#221
Train	_	_
:	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#222
Test	_	_
:	_	_
empirical	_	_
(	_	_
41-50	_	_
)	_	_
.	_	_

#223
With	_	_
this	_	_
experiment	_	_
,	_	_
we	_	_
could	capability-feasibility	_
check	_	_
to	_	_
what	_	_
extent	_	_
a	_	_
synthetic	_	_
trained	_	_
network	_	_
with	_	_
improved	_	_
realism	_	_
can	_	_
generalise	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
without	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#224
This	_	_
experiment	_	_
should	_	_
provide	_	_
the	_	_
main	_	_
result	_	_
for	_	_
our	_	_
third	_	_
hypothesis	_	_
that	_	_
states	_	_
that	_	_
without	_	_
any	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
,	_	_
improved	_	_
learning	_	_
for	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
achieved	_	_
using	_	_
only	_	_
translated	_	_
images	_	_
as	_	_
opposed	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
images	_	_
,	_	_
as	_	_
evaluated	_	_
in	_	_
Experiment	_	_
C	_	_
.	_	_

#225
G	_	_

#226
Train	_	_
:	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
(	_	_
1-8750	_	_
)	_	_
.	_	_

#227
Fine-tune	_	_
:	_	_
empirical	_	_
(	_	_
1-30	_	_
)	_	_
.	_	_

#228
Test	_	_
:	_	_
empirical	_	_
(	_	_
41-50	_	_
)	_	_
.	_	_

#229
This	_	_
experiment	_	_
should	_	_
provide	_	_
the	_	_
main	_	_
result	_	_
for	_	_
our	_	_
second	_	_
hypothesis	_	_
,	_	_
that	_	_
states	_	_
the	_	_
synthetic	_	_
images	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
can	_	_
be	_	_
used	_	_
for	_	_
improved	_	_
learning	_	_
of	_	_
empirical	_	_
images	_	_
,	_	_
as	_	_
compared	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
images	_	_
for	_	_
bootstrapping	_	_
(	_	_
Experiment	_	_
D	_	_
)	_	_
.	_	_

#230
Performance	_	_
of	_	_
this	_	_
experiment	_	_
was	_	_
expected	_	_
to	_	_
be	_	_
the	_	_
highest	_	_
amongst	_	_
all	_	_
our	_	_
experiments	_	_
that	_	_
tested	_	_
on	_	_
empirical	_	_
data	_	_
,	_	_
because	_	_
a	_	_
large	_	_
dataset	_	_
with	_	_
high	_	_
similarity	_	_
with	_	_
the	_	_
empirical	_	_
images	_	_
was	_	_
used	_	_
in	_	_
combination	_	_
with	_	_
fine-tuning	_	_
on	_	_
empirical	_	_
images	_	_
.	_	_

#231
3.2.1.	_	_
CNN	_	_
Training	_	_

#232
For	_	_
each	_	_
experiment	_	_
,	_	_
a	_	_
convolutional	_	_
neural	_	_
network	_	_
was	_	_
trained	_	_
and/or	_	_
fine-tuned	_	_
and	_	_
tested	_	_
according	_	_
to	_	_
the	_	_
dataset	_	_
scheme	_	_
as	_	_
described	_	_
in	_	_
Section	_	_
3.2	_	_
.	_	_

#233
The	_	_
hyperparameters	_	_
of	_	_
the	_	_
network	_	_
were	_	_
manually	_	_
optimised	_	_
using	_	_
separate	_	_
validation	_	_
datasets	_	_
for	_	_
combination	_	_
of	_	_
models	_	_
and	_	_
data	_	_
set	_	_
configurations	_	_
as	_	_
suggested	_	_
by	_	_
[	_	_
14	_	_
,	_	_
5	_	_
]	_	_
.	_	_

#234
This	_	_
resulted	_	_
in	_	_
using	_	_
Adaptive	_	_
Moment	_	_
Estimation	_	_
(	_	_
ADAM	_	_
)	_	_
[	_	_
23	_	_
]	_	_
with	_	_
β1	_	_
“	_	_
0.9	_	_
,	_	_
β2	_	_
“	_	_
0.999	_	_
,	_	_
ε	_	_
“	_	_
10´8	_	_
and	_	_
a	_	_
base	_	_
learning	_	_
rate	_	_
of	_	_
0.001	_	_
for	_	_
30,000	_	_
iterations	_	_
with	_	_
a	_	_
batch	_	_
size	_	_
of	_	_
4	_	_
.	_	_

#235
These	_	_
chosen	_	_
hyper-parameters	_	_
were	_	_
found	_	_
to	_	_
be	_	_
consistently	_	_
optimal	_	_
previously	_	_
[	_	_
4	_	_
]	_	_
and	_	_
therefore	_	_
we	_	_
fixed	_	_
them	_	_
across	_	_
conditions	_	_
.	_	_

#236
An	_	_
adjustment	_	_
was	_	_
made	_	_
in	_	_
the	_	_
layer	_	_
weight	_	_
initialisation	_	_
procedure	_	_
,	_	_
by	_	_
updating	_	_
the	_	_
model	_	_
to	_	_
using	_	_
MSRA	_	_
weight	_	_
fillers	_	_
[	_	_
20	_	_
,	_	_
25	_	_
]	_	_
.	_	_

#237
Furthermore	_	_
,	_	_
the	_	_
dropout	_	_
rate	_	_
[	_	_
28	_	_
]	_	_
was	_	_
adjusted	_	_
to	_	_
0.50	_	_
to	_	_
circumvent	_	_
early	_	_
over-fitting	_	_
and	_	_
facilitate	_	_
generalisation	_	_
.	_	_

#238
The	_	_
size	_	_
of	_	_
the	_	_
input	_	_
layer	_	_
was	_	_
cropped	_	_
to	_	_
929x929	_	_
pixels	_	_
,	_	_
which	_	_
was	_	_
the	_	_
maximum	_	_
that	_	_
our	_	_
GPU	_	_
memory	_	_
could	capability	_
handle	_	_
.	_	_

#239
3.2.2.	_	_
Performance	_	_
Evaluation	_	_

#240
To	_	_
calculate	_	_
the	_	_
performance	_	_
of	_	_
the	_	_
segmentation	_	_
,	_	_
we	_	_
used	_	_
the	_	_
Jaccard	_	_
Index	_	_
similarity	_	_
coefficient	_	_
as	_	_
an	_	_
evaluation	_	_
procedure	_	_
,	_	_
also	_	_
known	_	_
as	_	_
the	_	_
intersection-over-union	_	_
(	_	_
IOU	_	_
)	_	_
[	_	_
17	_	_
]	_	_
which	_	_
is	_	_
widely	_	_
used	_	_
for	_	_
semantic	_	_
segmentation	_	_
evaluation	_	_
[	_	_
12	_	_
,	_	_
11	_	_
]	_	_
.	_	_

#241
This	_	_
measure	_	_
is	_	_
defined	_	_
in	_	_
Equation	_	_
2	_	_
,	_	_
where	_	_
the	_	_
mean	_	_
IOU	_	_
over	_	_
all	_	_
object	_	_
part	_	_
classes	_	_
equals	_	_
the	_	_
intersection	_	_
of	_	_
the	_	_
segmentation	_	_
and	_	_
the	_	_
ground	_	_
truth	_	_
divided	_	_
by	_	_
their	_	_
union	_	_
.	_	_

#242
A	_	_
higher	_	_
IOU	_	_
implies	_	_
more	_	_
overlap	_	_
,	_	_
hence	_	_
better	_	_
performance	_	_
.	_	_

#243
To	_	_
derive	_	_
the	_	_
measure	_	_
,	_	_
a	_	_
pixel-level	_	_
confusion	_	_
matrix	_	_
C	_	_
was	_	_
calculated	_	_
first	_	_
for	_	_
each	_	_
image	_	_
I	_	_
in	_	_
data	_	_
set	_	_
D	_	_
:	_	_
Cij	_	_
“	_	_
ÿ	_	_
IPD	_	_
ˇ̌	_	_
p	_	_
P	_	_
I	_	_
|	_	_
SI	_	_
gtppq	_	_
“	_	_
i	_	_
^	_	_
SI	_	_
psppq	_	_
“	_	_
j	_	_
(	_	_
ˇ̌	_	_
,	_	_
(	_	_
1	_	_
)	_	_
where	_	_
SI	_	_
gtppq	_	_
is	_	_
the	_	_
ground	_	_
truth	_	_
label	_	_
of	_	_
pixel	_	_
p	_	_
in	_	_
image	_	_
I	_	_
and	_	_
SI	_	_
psppq	_	_
is	_	_
the	_	_
predicted	_	_
segmentation	_	_
label	_	_
.	_	_

#244
This	_	_
implies	_	_
that	_	_
Cij	_	_
equals	_	_
the	_	_
number	_	_
of	_	_
predicted	_	_
pixels	_	_
i	_	_
with	_	_
label	_	_
j	_	_
.	_	_

#245
The	_	_
average	_	_
IOU	_	_
over	_	_
all	_	_
classes	_	_
L	_	_
is	_	_
given	_	_
by	_	_
:	_	_
IOU	_	_
“	_	_
1	_	_
L	_	_
Lÿ	_	_
i“1	_	_
Cii	_	_
Gi	_	_
`	_	_
Pi	_	_
´	_	_
Cii	_	_
,	_	_
where	_	_
(	_	_
2	_	_
)	_	_
Gi	_	_
“	_	_
Lÿ	_	_
j“1	_	_
Cij	_	_
and	_	_
Pj	_	_
“	_	_
ÿ	_	_
i	_	_
Cij	_	_
(	_	_
3	_	_
)	_	_
HenceGi	_	_
denotes	_	_
the	_	_
total	_	_
number	_	_
of	_	_
pixels	_	_
labeled	_	_
with	_	_
class	_	_
i	_	_
in	_	_
the	_	_
ground	_	_
truth	_	_
and	_	_
Pj	_	_
the	_	_
total	_	_
number	_	_
of	_	_
pixels	_	_
with	_	_
prediction	_	_
j	_	_
in	_	_
the	_	_
image	_	_
.	_	_

#246
3.3	_	_
.	_	_

#247
Results	_	_
In	_	_
Figure	_	_
8	_	_
the	_	_
average	_	_
IOU	_	_
over	_	_
all	_	_
classes	_	_
for	_	_
Experiments	_	_
A	_	_
through	_	_
G	_	_
is	_	_
shown	_	_
,	_	_
as	_	_
well	_	_
as	_	_
previous	_	_
results	_	_
of	_	_
similar	_	_
experiments	_	_
A-D	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#248
In	_	_
Figure	_	_
9	_	_
the	_	_
performances	_	_
were	_	_
split	_	_
over	_	_
the	_	_
object	_	_
part	_	_
classes	_	_
.	_	_

#249
Qualitative	_	_
results	_	_
are	_	_
presented	_	_
in	_	_
Figure	_	_
10	_	_
.	_	_

#250
A	_	_
B	_	_
C	_	_
D	_	_
E	_	_
F	_	_
G	_	_
Experiment	_	_
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
IO	_	_
U	_	_
Figure	_	_
8	_	_
:	_	_
Average	_	_
IOU	_	_
over	_	_
all	_	_
object	_	_
part	_	_
classes	_	_
for	_	_
Experiment	_	_
A	_	_
with	_	_
empirical	_	_
training	_	_
(	_	_
)	_	_
,	_	_
Experiments	_	_
B	_	_
,	_	_
C	_	_
and	_	_
D	_	_
with	_	_
synthetic	_	_
image	_	_
bootstrapping	_	_
(	_	_
)	_	_
and	_	_
Experiments	_	_
E	_	_
,	_	_
F	_	_
and	_	_
G	_	_
with	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
image	_	_
bootstrapping	_	_
(	_	_
˚	_	_
)	_	_
.	_	_

#251
Previous	_	_
perfomance	_	_
[	_	_
4	_	_
]	_	_
for	_	_
similar	_	_
experiments	_	_
A-D	_	_
are	_	_
shown	_	_
for	_	_
reference	_	_
(	_	_
)	_	_
.	_	_

#252
0.1	_	_
0.2	_	_
0.3	_	_
0.4	_	_
0.5	_	_
0.6	_	_
0.7	_	_
0.8	_	_
0.9	_	_
Per	_	_
class	_	_
IOU	_	_
per	_	_
experiment	_	_
IO	_	_
U	_	_
A	_	_
B	_	_
C	_	_
D	_	_
E	_	_
F	_	_
G	_	_
Figure	_	_
9	_	_
:	_	_
For	_	_
Experiments	_	_
A	_	_
through	_	_
G	_	_
,	_	_
the	_	_
IOU	_	_
per	_	_
class	_	_
is	_	_
displayed	_	_
,	_	_
ordered	_	_
as	_	_
:	_	_
background	_	_
,	_	_
leafs	_	_
,	_	_
peppers	_	_
,	_	_
peduncles	_	_
,	_	_
stems	_	_
,	_	_
shoots	_	_
and	_	_
leaf	_	_
stems	_	_
,	_	_
wires	_	_
and	_	_
cuts	_	_
where	_	_
pepper	_	_
where	_	_
harvested	_	_
.	_	_

#253
3.4	_	_
.	_	_

#254
Discussion	_	_
and	_	_
conclusion	_	_
Compared	_	_
to	_	_
the	_	_
former	_	_
attempts	_	_
using	_	_
the	_	_
same	_	_
dataset	_	_
[	_	_
4	_	_
]	_	_
,	_	_
current	_	_
results	_	_
showed	_	_
an	_	_
overall	_	_
improved	_	_
performance	_	_
of	_	_
0.08	_	_
IOU	_	_
on	_	_
average	_	_
for	_	_
Experiments	_	_
A	_	_
through	_	_
D	_	_
,	_	_
as	_	_
can	_	_
be	_	_
seen	_	_
in	_	_
Figure	_	_
8	_	_
.	_	_

#255
The	_	_
two	_	_
differences	_	_
implemented	_	_
in	_	_
our	_	_
current	_	_
attempt	_	_
were	_	_
first	_	_
the	_	_
cropping	_	_
to	_	_
424x424	_	_
pixels	_	_
to	_	_
exclude	_	_
the	_	_
suction	_	_
cup	_	_
and	_	_
then	_	_
the	_	_
upscaling	_	_
to	_	_
1000x1000	_	_
pixels	_	_
.	_	_

#256
The	_	_
same	_	_
CNN	_	_
configuration	_	_
was	_	_
used	_	_
.	_	_

#257
Additional	_	_
experiments	_	_
showed	_	_
that	_	_
the	_	_
upscaling	_	_
was	_	_
the	_	_
main	_	_
cause	_	_
of	_	_
the	_	_
performance	_	_
increase	_	_
.	_	_

#258
This	_	_
might	speculation	_
be	_	_
explained	_	_
by	_	_
the	_	_
CNN’s	_	_
larger	_	_
field	_	_
of	_	_
view	_	_
,	_	_
allowing	_	_
for	_	_
detail	_	_
only	_	_
to	_	_
dissolve	_	_
by	_	_
convolutions	_	_
and	_	_
pooling	_	_
in	_	_
deeper	_	_
layers	_	_
of	_	_
the	_	_
network	_	_
.	_	_

#259
In	_	_
Experiment	_	_
A	_	_
,	_	_
the	_	_
aim	_	_
was	_	_
to	_	_
see	_	_
if	_	_
the	_	_
model	_	_
can	_	_
learn	_	_
to	_	_
segment	_	_
empirical	_	_
images	_	_
using	_	_
only	_	_
a	_	_
small	_	_
empirical	_	_
training	_	_
dataset	_	_
.	_	_

#260
The	_	_
CNN	_	_
reached	_	_
a	_	_
performance	_	_
of	_	_
0.41	_	_
average	_	_
IOU	_	_
(	_	_
see	_	_
Figure	_	_
8	_	_
)	_	_
.	_	_

#261
Relative	_	_
to	_	_
the	_	_
other	_	_
experiments	_	_
testing	_	_
on	_	_
empirical	_	_
images	_	_
,	_	_
the	_	_
performance	_	_
was	_	_
expected	_	_
to	_	_
be	_	_
low	_	_
due	_	_
to	_	_
the	_	_
small	_	_
training	_	_
dataset	_	_
size	_	_
.	_	_

#262
Indeed	_	_
compared	_	_
to	_	_
Experiment	_	_
D	_	_
and	_	_
G	_	_
the	_	_
performance	_	_
of	_	_
Experiment	_	_
A	_	_
was	_	_
lower	_	_
,	_	_
which	_	_
might	speculation	_
be	_	_
caused	_	_
by	_	_
the	_	_
D	_	_
and	_	_
G	_	_
models	_	_
being	_	_
bootstrapped	_	_
on	_	_
synthetic	_	_
or	_	_
translated	_	_
synthetic	_	_
images	_	_
.	_	_

#263
However	_	_
,	_	_
the	_	_
performance	_	_
of	_	_
Experiment	_	_
A	_	_
was	_	_
higher	_	_
compared	_	_
to	_	_
Experiments	_	_
C	_	_
and	_	_
F	_	_
,	_	_
which	_	_
can	_	_
be	_	_
explained	_	_
by	_	_
those	_	_
experiments	_	_
not	_	_
using	_	_
any	_	_
empirical	_	_
data	_	_
during	_	_
training	_	_
.	_	_

#264
Looking	_	_
at	_	_
the	_	_
per	_	_
class	_	_
performance	_	_
distribution	_	_
of	_	_
Experiment	_	_
A	_	_
in	_	_
Figure	_	_
9	_	_
,	_	_
we	_	_
note	_	_
that	_	_
the	_	_
cut	_	_
class	_	_
was	_	_
barely	_	_
recognised	_	_
having	_	_
an	_	_
IOU	_	_
of	_	_
0.04	_	_
.	_	_

#265
Recognising	_	_
all	_	_
classes	_	_
was	_	_
previously	_	_
considered	_	_
as	_	_
a	_	_
requirement	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#266
Therefore	_	_
we	_	_
concluded	_	_
that	_	_
training	_	_
with	_	_
empirical	_	_
images	_	_
alone	_	_
did	_	_
not	_	_
suffice	_	_
,	_	_
although	_	_
qualitative	_	_
results	_	_
(	_	_
see	_	_
Figure	_	_
10	_	_
)	_	_
looked	_	_
promising	_	_
and	_	_
useful	_	_
for	_	_
some	_	_
tasks	_	_
like	_	_
fruit	_	_
detection	_	_
.	_	_

#267
Experiment	_	_
B	_	_
was	_	_
run	_	_
to	_	_
obtain	_	_
baseline	_	_
performance	_	_
of	_	_
the	_	_
model	_	_
when	_	_
having	_	_
access	_	_
to	_	_
a	_	_
large	_	_
and	_	_
detailed	_	_
annotated	_	_
synthetic	_	_
dataset	_	_
.	_	_

#268
Performance	_	_
was	_	_
expected	_	_
to	_	_
be	_	_
highest	_	_
of	_	_
all	_	_
experiments	_	_
because	_	_
of	_	_
the	_	_
perfect	_	_
labels	_	_
,	_	_
largest	_	_
dataset	_	_
size	_	_
and	_	_
relatively	_	_
low	_	_
image	_	_
feature	_	_
variance	_	_
compared	_	_
to	_	_
empirical	_	_
or	_	_
synthetic	_	_
translated	_	_
images	_	_
.	_	_

#269
Indeed	_	_
B	_	_
achieved	_	_
the	_	_
best	_	_
performance	_	_
with	_	_
an	_	_
average	_	_
IOU	_	_
of	_	_
0.64	_	_
.	_	_

#270
This	_	_
performance	_	_
could	feasibility-options	_
be	_	_
used	_	_
as	_	_
a	_	_
baseline	_	_
to	_	_
indicate	_	_
the	_	_
maximum	_	_
obtainable	_	_
IOU	_	_
for	_	_
this	_	_
domain	_	_
and	_	_
currently	_	_
used	_	_
CNN	_	_
architecture	_	_
.	_	_

#271
The	_	_
performance	_	_
of	_	_
the	_	_
other	_	_
experiments	_	_
should	_	_
be	_	_
put	_	_
into	_	_
perspective	_	_
of	_	_
this	_	_
IOU	_	_
.	_	_

#272
Qualitative	_	_
results	_	_
still	_	_
showed	_	_
some	_	_
gaps	_	_
in	_	_
thin	_	_
and	_	_
elongated	_	_
classes	_	_
like	_	_
leaf	_	_
stems	_	_
and	_	_
shoots	_	_
,	_	_
although	_	_
results	_	_
were	_	_
much	_	_
improved	_	_
over	_	_
previous	_	_
segmentations	_	_
where	_	_
such	_	_
gaps	_	_
were	_	_
larger	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#273
Experiment	_	_
C	_	_
was	_	_
a	_	_
reference	_	_
experiment	_	_
to	_	_
see	_	_
to	_	_
what	_	_
extent	_	_
a	_	_
network	_	_
trained	_	_
on	_	_
synthetic	_	_
images	_	_
can	_	_
generalise	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
without	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#274
With	_	_
an	_	_
average	_	_
IOU	_	_
of	_	_
0.20	_	_
,	_	_
the	_	_
performance	_	_
approximately	_	_
doubled	_	_
over	_	_
previous	_	_
424x424	_	_
results	_	_
[	_	_
4	_	_
]	_	_
.	_	_

#275
Furthermore	_	_
,	_	_
the	_	_
performance	_	_
was	_	_
lower	_	_
than	_	_
that	_	_
of	_	_
Experiment	_	_
A	_	_
,	_	_
likely	_	_
because	_	_
the	_	_
used	_	_
synthetic	_	_
training	_	_
data	_	_
was	_	_
too	_	_
dissimilar	_	_
with	_	_
the	_	_
empirical	_	_
images	_	_
.	_	_

#276
Looking	_	_
at	_	_
the	_	_
per	_	_
class	_	_
distribution	_	_
,	_	_
classes	_	_
like	_	_
peduncle	_	_
and	_	_
cut	_	_
were	_	_
barely	_	_
recognised	_	_
(	_	_
IOUă0.1	_	_
)	_	_
and	_	_
the	_	_
wire	_	_
class	_	_
was	_	_
omitted	_	_
all	_	_
together	_	_
.	_	_

#277
Qualitatively	_	_
,	_	_
results	_	_
looked	_	_
far	_	_
from	_	_
similar	_	_
to	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#278
Looking	_	_
at	_	_
the	_	_
qualitative	_	_
results	_	_
,	_	_
we	_	_
can	_	_
conclude	_	_
that	_	_
training	_	_
only	_	_
with	_	_
synthetic	_	_
data	_	_
would	_	_
not	_	_
be	_	_
sufficient	_	_
for	_	_
many	_	_
tasks	_	_
,	_	_
given	_	_
the	_	_
current	_	_
learning	_	_
architecture	_	_
.	_	_

#279
In	_	_
Experiment	_	_
D	_	_
,	_	_
a	_	_
similar	_	_
training	_	_
scheme	_	_
of	_	_
Experiment	_	_
C	_	_
was	_	_
used	_	_
,	_	_
but	_	_
with	_	_
an	_	_
extra	_	_
fine-tuning	_	_
step	_	_
using	_	_
empirical	_	_
images	_	_
.	_	_

#280
The	_	_
IOU	_	_
performance	_	_
on	_	_
empirical	_	_
images	_	_
was	_	_
increased	_	_
to	_	_
0.48	_	_
.	_	_

#281
Hence	_	_
bootstrapping	_	_
with	_	_
synthetic	_	_
images	_	_
increased	_	_
performance	_	_
by	_	_
17	_	_
%	_	_
over	_	_
no	_	_
bootstrapping	_	_
in	_	_
Experiment	_	_
A	_	_
.	_	_

#282
We	_	_
conclude	_	_
that	_	_
bootstrapping	_	_
with	_	_
synthetic	_	_
images	_	_
and	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
used	_	_
to	_	_
close	_	_
the	_	_
gap	_	_
towards	_	_
the	_	_
optimal	_	_
estimated	_	_
possible	_	_
performance	_	_
of	_	_
Experiment	_	_
B	_	_
.	_	_

#283
Furthermore	_	_
,	_	_
we	_	_
note	_	_
that	_	_
all	_	_
classes	_	_
were	_	_
included	_	_
,	_	_
although	_	_
the	_	_
cut	_	_
class	_	_
was	_	_
again	_	_
barely	_	_
recognised	_	_
(	_	_
IOU=0.11	_	_
)	_	_
.	_	_

#284
Qualitatively	_	_
,	_	_
results	_	_
looked	_	_
close	_	_
to	_	_
the	_	_
ground	_	_
truth	_	_
.	_	_

#285
Experiment	_	_
E	_	_
trained	_	_
and	_	_
tested	_	_
on	_	_
a	_	_
large	_	_
dataset	_	_
,	_	_
similar	_	_
to	_	_
Experiment	_	_
B	_	_
,	_	_
but	_	_
instead	_	_
of	_	_
synthetic	_	_
images	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
were	_	_
used	_	_
.	_	_

#286
The	_	_
performance	_	_
of	_	_
0.56	_	_
IOU	_	_
was	_	_
lower	_	_
than	_	_
of	_	_
the	_	_
IOU=0.64	_	_
from	_	_
Experiment	_	_
B	_	_
,	_	_
as	_	_
expected	_	_
and	_	_
probably	_	_
due	_	_
to	_	_
the	_	_
extra	_	_
variance	_	_
that	_	_
the	_	_
empirical	_	_
feature	_	_
distribution	_	_
might	speculation	_
introduce	_	_
when	_	_
synthetic	_	_
images	_	_
were	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#287
Qualitatively	_	_
results	_	_
looked	_	_
comparable	_	_
.	_	_

#288
Experiment	_	_
F	_	_
evaluated	_	_
on	_	_
empirical	_	_
images	_	_
when	_	_
trained	_	_
on	_	_
synthetic	_	_
translated	_	_
to	_	_
empirical	_	_
images	_	_
,	_	_
without	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#289
With	_	_
this	_	_
experiment	_	_
,	_	_
we	_	_
could	feasibility	_
check	_	_
to	_	_
what	_	_
extent	_	_
a	_	_
synthetic	_	_
trained	_	_
network	_	_
with	_	_
improved	_	_
realism	_	_
can	_	_
generalise	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
,	_	_
without	_	_
yet	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#290
Compared	_	_
to	_	_
Experiment	_	_
C	_	_
(	_	_
using	_	_
synthetic	_	_
images	_	_
instead	_	_
of	_	_
translated	_	_
ones	_	_
)	_	_
,	_	_
the	_	_
performance	_	_
increased	_	_
with	_	_
55	_	_
%	_	_
to	_	_
an	_	_
average	_	_
IOU	_	_
of	_	_
0.31	_	_
.	_	_

#291
This	_	_
experiment	_	_
confirms	_	_
our	_	_
third	_	_
hypothesis	_	_
that	_	_
without	_	_
any	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
,	_	_
improved	_	_
learning	_	_
for	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
achieved	_	_
using	_	_
only	_	_
translated	_	_
images	_	_
as	_	_
opposed	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
images	_	_
.	_	_

#292
Although	_	_
qualitatively	_	_
,	_	_
also	_	_
improvements	_	_
could	feasibility	_
be	_	_
observed	_	_
over	_	_
Experiment	_	_
C	_	_
,	_	_
we	_	_
noted	_	_
from	_	_
the	_	_
class	_	_
performance	_	_
distribution	_	_
there	_	_
existed	_	_
still	_	_
a	_	_
relative	_	_
poor	_	_
performance	_	_
on	_	_
the	_	_
classes	_	_
wires	_	_
and	_	_
cuts	_	_
.	_	_

#293
In	_	_
Experiment	_	_
G	_	_
,	_	_
the	_	_
model	_	_
from	_	_
Experiment	_	_
F	_	_
was	_	_
fine-tuned	_	_
with	_	_
empirical	_	_
images	_	_
.	_	_

#294
This	_	_
experiment	_	_
should	_	_
provide	_	_
the	_	_
main	_	_
result	_	_
for	_	_
our	_	_
second	_	_
hypothesis	_	_
,	_	_
that	_	_
states	_	_
that	_	_
synthetic	_	_
images	_	_
translated	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
can	_	_
be	_	_
used	_	_
for	_	_
improved	_	_
learning	_	_
of	_	_
empirical	_	_
images	_	_
,	_	_
as	_	_
compared	_	_
to	_	_
using	_	_
only	_	_
synthetic	_	_
images	_	_
for	_	_
bootstrapping	_	_
,	_	_
as	_	_
evaluated	_	_
in	_	_
Experiment	_	_
D.	_	_
Our	_	_
hypothesis	_	_
is	_	_
confirmed	_	_
by	_	_
achieving	_	_
the	_	_
best	_	_
performance	_	_
on	_	_
empirical	_	_
data	_	_
of	_	_
an	_	_
IOU=0.52	_	_
.	_	_

#295
This	_	_
was	_	_
an	_	_
increase	_	_
of	_	_
27	_	_
%	_	_
over	_	_
Experiment	_	_
A	_	_
(	_	_
only	_	_
training	_	_
on	_	_
empirical	_	_
images	_	_
)	_	_
and	_	_
8	_	_
%	_	_
over	_	_
Experiment	_	_
D	_	_
.	_	_

#296
Qualitatively	_	_
,	_	_
results	_	_
in	_	_
Experiment	_	_
G	_	_
looked	_	_
close	_	_
to	_	_
the	_	_
ground	_	_
truth	_	_
and	_	_
comparable	_	_
to	_	_
results	_	_
of	_	_
Experiments	_	_
A	_	_
and	_	_
D.	_	_
Looking	_	_
at	_	_
the	_	_
class	_	_
distribution	_	_
,	_	_
all	_	_
classes	_	_
were	_	_
included	_	_
.	_	_

#297
Most	_	_
notably	_	_
the	_	_
cut	_	_
class	_	_
performance	_	_
increased	_	_
with	_	_
118	_	_
%	_	_
over	_	_
Experiment	_	_
D	_	_
and	_	_
with	_	_
600	_	_
%	_	_
over	_	_
Experiment	_	_
A	_	_
to	_	_
an	_	_
IOU	_	_
of	_	_
0.24	_	_
.	_	_

#298
To	_	_
summarise	_	_
,	_	_
we	_	_
have	_	_
seen	_	_
that	_	_
without	_	_
using	_	_
any	_	_
annotated	_	_
empirical	_	_
training	_	_
images	_	_
,	_	_
an	_	_
improved	_	_
performance	_	_
can	_	_
be	_	_
achieved	_	_
by	_	_
bootstrapping	_	_
with	_	_
translated	_	_
synthetic	_	_
images	_	_
instead	_	_
of	_	_
synthetic	_	_
images	_	_
.	_	_

#299
Furthermore	_	_
,	_	_
we	_	_
have	_	_
shown	_	_
that	_	_
by	_	_
also	_	_
fine-tuning	_	_
with	_	_
a	_	_
small	_	_
empirical	_	_
dataset	_	_
,	_	_
the	_	_
highest	_	_
performance	_	_
on	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
achieved	_	_
.	_	_

#300
4	_	_
.	_	_

#301
General	_	_
discussion	_	_
and	_	_
conclusion	_	_
In	_	_
Part	_	_
I	_	_
,	_	_
a	_	_
cycle	_	_
consistent	_	_
generative	_	_
adversarial	_	_
network	_	_
was	_	_
applied	_	_
to	_	_
synthetic	_	_
and	_	_
empirical	_	_
images	_	_
with	_	_
the	_	_
objective	_	_
to	_	_
generate	_	_
more	_	_
realistic	_	_
synthetic	_	_
images	_	_
by	_	_
translating	_	_
them	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#302
Our	_	_
analysis	_	_
showed	_	_
that	_	_
the	_	_
image	_	_
feature	_	_
distributions	_	_
of	_	_
these	_	_
translated	_	_
images	_	_
,	_	_
both	_	_
in	_	_
color	_	_
and	_	_
texture	_	_
,	_	_
were	_	_
improved	_	_
towards	_	_
the	_	_
empirical	_	_
images	_	_
.	_	_

#303
Regarding	_	_
our	_	_
first	_	_
hypothesis	_	_
,	_	_
it	_	_
was	_	_
confirmed	_	_
that	_	_
the	_	_
image	_	_
feature	_	_
difference	_	_
with	_	_
the	_	_
empirical	_	_
set	_	_
was	_	_
reduced	_	_
after	_	_
translation	_	_
of	_	_
the	_	_
synthetic	_	_
images	_	_
.	_	_

#304
Qualitatively	_	_
,	_	_
the	_	_
translated	_	_
synthetic	_	_
images	_	_
looked	_	_
highly	_	_
similar	_	_
to	_	_
the	_	_
real	_	_
world	_	_
images	_	_
.	_	_

#305
However	_	_
,	_	_
some	_	_
translation	_	_
artifacts	_	_
appeared	_	_
.	_	_

#306
Furthermore	_	_
the	_	_
Cycle-GAN	_	_
method	_	_
could	capability	_
not	_	_
improve	_	_
upon	_	_
geometric	_	_
dissimilarities	_	_
between	_	_
the	_	_
synthetic	_	_
and	_	_
the	_	_
empirical	_	_
domain	_	_
.	_	_

#307
The	_	_
latter	_	_
proved	_	_
an	_	_
advantage	_	_
however	_	_
,	_	_
as	_	_
the	_	_
synthetic	_	_
ground	_	_
truth	_	_
also	_	_
corresponded	_	_
to	_	_
the	_	_
translated	_	_
color	_	_
images	_	_
,	_	_
allowing	_	_
for	_	_
the	_	_
experiments	_	_
on	_	_
improved	_	_
learning	_	_
in	_	_
the	_	_
second	_	_
part	_	_
of	_	_
our	_	_
work	_	_
.	_	_

#308
In	_	_
Part	_	_
II	_	_
,	_	_
it	_	_
was	_	_
evaluated	_	_
to	_	_
what	_	_
extent	_	_
translated	_	_
synthetic	_	_
images	_	_
to	_	_
the	_	_
empirical	_	_
domain	_	_
could	capability	_
improve	_	_
on	_	_
CNN	_	_
learning	_	_
with	_	_
empirical	_	_
images	_	_
over	_	_
other	_	_
learning	_	_
strategies	_	_
.	_	_

#309
We	_	_
confirmed	_	_
our	_	_
second	_	_
hypotheses	_	_
that	_	_
by	_	_
using	_	_
translated	_	_
images	_	_
and	_	_
fine-tuning	_	_
with	_	_
empirical	_	_
images	_	_
,	_	_
the	_	_
highest	_	_
performance	_	_
for	_	_
empirical	_	_
images	_	_
can	_	_
be	_	_
achieved	_	_
(	_	_
IOU=0.52	_	_
)	_	_
compared	_	_
to	_	_
training	_	_
with	_	_
only	_	_
empirical	_	_
(	_	_
IOU=0.41	_	_
)	_	_
or	_	_
synthetic	_	_
data	_	_
(	_	_
IOU=0.48	_	_
)	_	_

#310
Besides	_	_
improving	_	_
segmentation	_	_
performance	_	_
on	_	_
empirical	_	_
images	_	_
using	_	_
translated	_	_
synthetic	_	_
images	_	_
instead	_	_
of	_	_
only	_	_
empirical	_	_
or	_	_
synthetic	_	_
images	_	_
during	_	_
training	_	_
,	_	_
another	_	_
key	_	_
contribution	_	_
of	_	_
our	_	_
work	_	_
is	_	_
the	_	_
further	_	_
minimisation	_	_
of	_	_
the	_	_
CNN’s	_	_
dependency	_	_
on	_	_
annotated	_	_
empirical	_	_
data	_	_
.	_	_

#311
We	_	_
confirmed	_	_
our	_	_
third	_	_
hypothesis	_	_
that	_	_
without	_	_
any	_	_
empirical	_	_
image	_	_
fine-tuning	_	_
,	_	_
learning	_	_
can	_	_
be	_	_
improved	_	_
with	_	_
translated	_	_
images	_	_
(	_	_
IOU=0.31	_	_
)	_	_
,	_	_
a	_	_
55	_	_
%	_	_
increase	_	_
over	_	_
just	_	_
using	_	_
synthetic	_	_
images	_	_
(	_	_
IOU=0.20	_	_
)	_	_
.	_	_

#312
The	_	_
work	_	_
presented	_	_
in	_	_
this	_	_
paper	_	_
can	_	_
be	_	_
seen	_	_
as	_	_
an	_	_
important	_	_
step	_	_
towards	_	_
improved	_	_
sensing	_	_
for	_	_
applied	_	_
computer	_	_
vision	_	_
domains	_	_
such	_	_
as	_	_
in	_	_
agricultural	_	_
robotics	_	_
,	_	_
medical	_	_
support	_	_
systems	_	_
or	_	_
autonomous	_	_
navigation	_	_
.	_	_

#313
It	_	_
facilitates	_	_
CNN	_	_
semantic	_	_
object	_	_
part	_	_
segmentation	_	_
learning	_	_
without	_	_
or	_	_
minimal	_	_
requirement	_	_
of	_	_
annotated	_	_
images	_	_
.	_	_

#314
Acknowledgement	_	_
This	_	_
research	_	_
was	_	_
partially	_	_
funded	_	_
by	_	_
the	_	_
European	_	_
Commission	_	_
in	_	_
the	_	_
Horizon2020	_	_
Programme	_	_
(	_	_
SWEEPER	_	_
GA	_	_
no	_	_
.	_	_

#315
644313	_	_
)	_	_
and	_	_
the	_	_
Dutch	_	_
Ministry	_	_
of	_	_
Economic	_	_
Affairs	_	_
(	_	_
EU140935	_	_
)	_	_
.	_	_

#316
5	_	_
.	_	_

#317
References	_	_
[	_	_
1	_	_
]	_	_
Bac	_	_
,	_	_
C.	_	_
,	_	_
Hemming	_	_
,	_	_
J.	_	_
,	_	_
van	_	_
Henten	_	_
,	_	_
E.	_	_
,	_	_
2013	_	_
.	_	_

#318
Robust	_	_
pixel-based	_	_
classification	_	_
of	_	_
obstacles	_	_
for	_	_
robotic	_	_
harvesting	_	_
of	_	_
sweetpepper	_	_
.	_	_

#319
Computers	_	_
and	_	_
Electronics	_	_
in	_	_
Agriculture	_	_
96	_	_
,	_	_
148	_	_
–	_	_
162	_	_
.	_	_

#320
URL	_	_
:	_	_
http	_	_
:	_	_
//www.sciencedirect.com/science/article/pii/	_	_
S0168169913001099	_	_
,	_	_
doi	_	_
:	_	_
http	_	_
:	_	_
//dx.doi.org/10.1016/j.compag.2013	_	_
.	_	_

#321
05.004	_	_
.	_	_

#322
[	_	_
2	_	_
]	_	_
Bac	_	_
,	_	_
C.W.	_	_
,	_	_
van	_	_
Henten	_	_
,	_	_
E.J.	_	_
,	_	_
Hemming	_	_
,	_	_
J.	_	_
,	_	_
Edan	_	_
,	_	_
Y.	_	_
,	_	_
2014	_	_
.	_	_

#323
Harvesting	_	_
robots	_	_
for	_	_
high-value	_	_
crops	_	_
:	_	_
State-of-the-art	_	_
review	_	_
and	_	_
challenges	_	_
ahead	_	_
.	_	_

#324
Journal	_	_
of	_	_
Field	_	_
Robotics	_	_
31	_	_
,	_	_
888–911	_	_
.	_	_

#325
doi:10.1002/rob.21525	_	_
.	_	_

#326
[	_	_
3	_	_
]	_	_
Barth	_	_
,	_	_
R.	_	_
,	_	_
IJsselmuiden	_	_
,	_	_
J.	_	_
,	_	_
Hemming	_	_
,	_	_
J.	_	_
,	_	_
van	_	_
Henten	_	_
,	_	_
E.J.	_	_
,	_	_
2017a	_	_
.	_	_

#327
Data	_	_
synthesis	_	_
methods	_	_
for	_	_
semantic	_	_
segmentation	_	_
in	_	_
agriculture	_	_
:	_	_
a	_	_
capsicum	_	_
annuum	_	_
dataset	_	_
.	_	_

#328
Computers	_	_
and	_	_
Electronics	_	_
in	_	_
Agriculture	_	_
doi:10.1016/	_	_
j.compag.2017.12.001	_	_
.	_	_

#329
[	_	_
4	_	_
]	_	_
Barth	_	_
,	_	_
R.	_	_
,	_	_
IJsselmuiden	_	_
,	_	_
J.	_	_
,	_	_
Hemming	_	_
,	_	_
J.	_	_
,	_	_
Henten	_	_
,	_	_
E.V.	_	_
,	_	_
2017b	_	_
.	_	_

#330
Synthetic	_	_
bootstrapping	_	_
of	_	_
convolutional	_	_
neural	_	_
networks	_	_
for	_	_
semantic	_	_
plant	_	_
part	_	_
segmentation	_	_
.	_	_

#331
Computers	_	_
and	_	_
Electronics	_	_
in	_	_
Agriculture	_	_
doi	_	_
:	_	_
https	_	_
:	_	_
//doi.org/10.1016/j.compag.2017.11.040	_	_
.	_	_

#332
[	_	_
5	_	_
]	_	_
Bengio	_	_
,	_	_
Y.	_	_
,	_	_
2012	_	_
.	_	_

#333
Practical	_	_
recommendations	_	_
for	_	_
gradient-based	_	_
training	_	_
of	_	_
deep	_	_
architectures	_	_
.	_	_

#334
CoRR	_	_
abs/1206.5533	_	_
.	_	_

#335
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/	_	_
1206.5533	_	_
.	_	_

#336
[	_	_
6	_	_
]	_	_
Chen	_	_
,	_	_
L.C.	_	_
,	_	_
Papandreou	_	_
,	_	_
G.	_	_
,	_	_
Kokkinos	_	_
,	_	_
I.	_	_
,	_	_
Murphy	_	_
,	_	_
K.	_	_
,	_	_
Yuille	_	_
,	_	_
A.L.	_	_
,	_	_
2015a	_	_
.	_	_

#337
Semantic	_	_
image	_	_
segmentation	_	_
with	_	_
deep	_	_
convolutional	_	_
nets	_	_
and	_	_
fully	_	_
connected	_	_
crfs	_	_
,	_	_
in	_	_
:	_	_
ICLR	_	_
.	_	_

#338
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1412.7062	_	_
.	_	_

#339
[	_	_
7	_	_
]	_	_
Chen	_	_
,	_	_
L.C.	_	_
,	_	_
Papandreou	_	_
,	_	_
G.	_	_
,	_	_
Kokkinos	_	_
,	_	_
I.	_	_
,	_	_
Murphy	_	_
,	_	_
K.	_	_
,	_	_
Yuille	_	_
,	_	_
A.L.	_	_
,	_	_
2015b	_	_
.	_	_

#340
Semantic	_	_
image	_	_
segmentation	_	_
with	_	_
deep	_	_
convolutional	_	_
nets	_	_
and	_	_
fully	_	_
connected	_	_
crfs	_	_
,	_	_
in	_	_
:	_	_
ICLR	_	_
.	_	_

#341
[	_	_
8	_	_
]	_	_
Chen	_	_
,	_	_
L.C.	_	_
,	_	_
Papandreou	_	_
,	_	_
G.	_	_
,	_	_
Kokkinos	_	_
,	_	_
I.	_	_
,	_	_
Murphy	_	_
,	_	_
K.	_	_
,	_	_
Yuille	_	_
,	_	_
A.L.	_	_
,	_	_
2016	_	_
.	_	_

#342
Deeplab	_	_
:	_	_
Semantic	_	_
image	_	_
segmentation	_	_
with	_	_
deep	_	_
convolutional	_	_
nets	_	_
,	_	_
atrous	_	_
convolution	_	_
,	_	_
and	_	_
fully	_	_
connected	_	_
crfs	_	_
.	_	_

#343
arXiv:1606.00915	_	_
.	_	_

#344
[	_	_
9	_	_
]	_	_
Chen	_	_
,	_	_
L.C.	_	_
,	_	_
Papandreou	_	_
,	_	_
G.	_	_
,	_	_
Schroff	_	_
,	_	_
F.	_	_
,	_	_
Adam	_	_
,	_	_
H.	_	_
,	_	_
2017	_	_
.	_	_

#345
Rethinking	_	_
Atrous	_	_
Convolution	_	_
for	_	_
Semantic	_	_
Image	_	_
Segmentation	_	_
.	_	_

#346
ArXiv	_	_
e-prints	_	_
arXiv:1706.05587	_	_
.	_	_

#347
[	_	_
10	_	_
]	_	_
Dittrich	_	_
,	_	_
F.	_	_
,	_	_
Woern	_	_
,	_	_
H.	_	_
,	_	_
Sharma	_	_
,	_	_
V.	_	_
,	_	_
Yayilgan	_	_
,	_	_
S.	_	_
,	_	_
2014	_	_
.	_	_

#348
Pixelwise	_	_
object	_	_
class	_	_
segmentation	_	_
based	_	_
on	_	_
synthetic	_	_
data	_	_
using	_	_
an	_	_
optimized	_	_
training	_	_
strategy	_	_
,	_	_
in	_	_
:	_	_
Networks	_	_
Soft	_	_
Computing	_	_
(	_	_
ICNSC	_	_
)	_	_
,	_	_
2014	_	_
First	_	_
International	_	_
Conference	_	_
on	_	_
,	_	_
pp	_	_
.	_	_

#349
388–394	_	_
.	_	_

#350
doi:10.1109/CNSC.2014.6906671	_	_
.	_	_

#351
[	_	_
11	_	_
]	_	_
Everingham	_	_
,	_	_
M.	_	_
,	_	_
Van	_	_
Gool	_	_
,	_	_
L.	_	_
,	_	_
Williams	_	_
,	_	_
C.K.I.	_	_
,	_	_
Winn	_	_
,	_	_
J.	_	_
,	_	_
Zisserman	_	_
,	_	_
A.	_	_
,	_	_
2010	_	_
.	_	_

#352
The	_	_
pascal	_	_
visual	_	_
object	_	_
classes	_	_
(	_	_
voc	_	_
)	_	_
challenge	_	_
.	_	_

#353
International	_	_
Journal	_	_
of	_	_
Computer	_	_
Vision	_	_
88	_	_
,	_	_
303–338	_	_
.	_	_

#354
[	_	_
12	_	_
]	_	_
Gabriela	_	_
Csurka	_	_
,	_	_
Diane	_	_
Larlus	_	_
,	_	_
F.P.	_	_
,	_	_
2013	_	_
.	_	_

#355
What	_	_
is	_	_
a	_	_
good	_	_
evaluation	_	_
measure	_	_
for	_	_
semantic	_	_
segmentation	_	_
?	_	_
,	_	_
in	_	_
:	_	_
Proceedings	_	_
of	_	_
the	_	_
British	_	_
Machine	_	_
Vision	_	_
Conference	_	_
,	_	_
BMVA	_	_
Press	_	_
.	_	_

#356
[	_	_
13	_	_
]	_	_
Gongal	_	_
,	_	_
A.	_	_
,	_	_
Amatya	_	_
,	_	_
S.	_	_
,	_	_
Karkee	_	_
,	_	_
M.	_	_
,	_	_
Zhang	_	_
,	_	_
Q.	_	_
,	_	_
Lewis	_	_
,	_	_
K.	_	_
,	_	_
2015	_	_
.	_	_

#357
Sensors	_	_
and	_	_
systems	_	_
for	_	_
fruit	_	_
detection	_	_
and	_	_
localization	_	_
:	_	_
A	_	_
review	_	_
.	_	_

#358
Computers	_	_
and	_	_
Electronics	_	_
in	_	_
Agriculture	_	_
116	_	_
,	_	_
8	_	_
–	_	_
19	_	_
.	_	_

#359
[	_	_
14	_	_
]	_	_
Goodfellow	_	_
,	_	_
I.	_	_
,	_	_
Bengio	_	_
,	_	_
Y.	_	_
,	_	_
Courville	_	_
,	_	_
A.	_	_
,	_	_
2016	_	_
.	_	_

#360
Deep	_	_
Learning	_	_
.	_	_

#361
MIT	_	_
Press	_	_
.	_	_

#362
http	_	_
:	_	_
//www.deeplearningbook.org	_	_
.	_	_

#363
[	_	_
15	_	_
]	_	_
Goodfellow	_	_
,	_	_
I.	_	_
,	_	_
Pouget-Abadie	_	_
,	_	_
J.	_	_
,	_	_
Mirza	_	_
,	_	_
M.	_	_
,	_	_
Xu	_	_
,	_	_
B.	_	_
,	_	_
Warde-Farley	_	_
,	_	_
D.	_	_
,	_	_
Ozair	_	_
,	_	_
S.	_	_
,	_	_
Courville	_	_
,	_	_
A.	_	_
,	_	_
Bengio	_	_
,	_	_
Y.	_	_
,	_	_
2014	_	_
.	_	_

#364
Generative	_	_
adversarial	_	_
nets	_	_
,	_	_
in	_	_
:	_	_
Ghahramani	_	_
,	_	_
Z.	_	_
,	_	_
Welling	_	_
,	_	_
M.	_	_
,	_	_
Cortes	_	_
,	_	_
C.	_	_
,	_	_
Lawrence	_	_
,	_	_
N.D.	_	_
,	_	_
Weinberger	_	_
,	_	_
K.Q	_	_
.	_	_

#365
(	_	_
Eds	_	_
.	_	_

#366
)	_	_
,	_	_
Advances	_	_
in	_	_
Neural	_	_
Information	_	_
Processing	_	_
Systems	_	_
27	_	_
.	_	_

#367
Curran	_	_
Associates	_	_
,	_	_
Inc.	_	_
,	_	_
pp	_	_
.	_	_

#368
2672–2680	_	_
.	_	_

#369
URL	_	_
:	_	_
http	_	_
:	_	_
//papers.nips	_	_
.	_	_

#370
cc/paper/5423-generative-adversarial-nets.pdf	_	_
.	_	_

#371
[	_	_
16	_	_
]	_	_
Haralick	_	_
,	_	_
R.M.	_	_
,	_	_
Shanmugam	_	_
,	_	_
K.	_	_
,	_	_
Dinstein	_	_
,	_	_
I.	_	_
,	_	_
1973	_	_
.	_	_

#372
Textural	_	_
features	_	_
for	_	_
image	_	_
classification	_	_
.	_	_

#373
IEEE	_	_
Transactions	_	_
on	_	_
Systems	_	_
,	_	_
Man	_	_
,	_	_
and	_	_
Cybernetics	_	_
SMC-3	_	_
,	_	_
610–621	_	_
.	_	_

#374
doi:10.1109/TSMC.1973.4309314	_	_
.	_	_

#375
[	_	_
17	_	_
]	_	_
He	_	_
,	_	_
H.	_	_
,	_	_
Garcia	_	_
,	_	_
E.A.	_	_
,	_	_
2009	_	_
.	_	_

#376
Learning	_	_
from	_	_
imbalanced	_	_
data	_	_
.	_	_

#377
IEEE	_	_
Transactions	_	_
on	_	_
Knowledge	_	_
and	_	_
Data	_	_
Engineering	_	_
21	_	_
,	_	_
1263–1284	_	_
.	_	_

#378
doi:10.1109/	_	_
TKDE.2008.239	_	_
.	_	_

#379
[	_	_
18	_	_
]	_	_
He	_	_
,	_	_
K.	_	_
,	_	_
Zhang	_	_
,	_	_
X.	_	_
,	_	_
Ren	_	_
,	_	_
S.	_	_
,	_	_
Sun	_	_
,	_	_
J.	_	_
,	_	_
2014	_	_
.	_	_

#380
Spatial	_	_
Pyramid	_	_
Pooling	_	_
in	_	_
Deep	_	_
Convolutional	_	_
Networks	_	_
for	_	_
Visual	_	_
Recognition	_	_
.	_	_

#381
Springer	_	_
International	_	_
Publishing	_	_
,	_	_
Cham	_	_
.	_	_

#382
pp	_	_
.	_	_

#383
346–361	_	_
.	_	_

#384
URL	_	_
:	_	_
http	_	_
:	_	_
//dx.doi.org/10	_	_
.	_	_

#385
1007/978-3-319-10578-9_23	_	_
,	_	_
doi:10.1007/978-3-319-10578-9_23	_	_
.	_	_

#386
[	_	_
19	_	_
]	_	_
He	_	_
,	_	_
K.	_	_
,	_	_
Zhang	_	_
,	_	_
X.	_	_
,	_	_
Ren	_	_
,	_	_
S.	_	_
,	_	_
Sun	_	_
,	_	_
J.	_	_
,	_	_
2015a	_	_
.	_	_

#387
Deep	_	_
residual	_	_
learning	_	_
for	_	_
image	_	_
recognition	_	_
.	_	_

#388
CoRR	_	_
abs/1512.03385	_	_
.	_	_

#389
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1512	_	_
.	_	_

#390
03385	_	_
.	_	_

#391
[	_	_
20	_	_
]	_	_
He	_	_
,	_	_
K.	_	_
,	_	_
Zhang	_	_
,	_	_
X.	_	_
,	_	_
Ren	_	_
,	_	_
S.	_	_
,	_	_
Sun	_	_
,	_	_
J.	_	_
,	_	_
2015b	_	_
.	_	_

#392
Delving	_	_
deep	_	_
into	_	_
rectifiers	_	_
:	_	_
Surpassing	_	_
human-level	_	_
performance	_	_
on	_	_
imagenet	_	_
classification	_	_
.	_	_

#393
CoRR	_	_
abs/1502.01852	_	_
.	_	_

#394
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1502.01852	_	_
.	_	_

#395
[	_	_
21	_	_
]	_	_
Isola	_	_
,	_	_
P.	_	_
,	_	_
Zhu	_	_
,	_	_
J.	_	_
,	_	_
Zhou	_	_
,	_	_
T.	_	_
,	_	_
Efros	_	_
,	_	_
A.A.	_	_
,	_	_
2016	_	_
.	_	_

#396
Image-to-image	_	_
translation	_	_
with	_	_
conditional	_	_
adversarial	_	_
networks	_	_
.	_	_

#397
CoRR	_	_
abs/1611.07004	_	_
.	_	_

#398
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1611.07004	_	_
.	_	_

#399
[	_	_
22	_	_
]	_	_
Jia	_	_
,	_	_
Y.	_	_
,	_	_
Shelhamer	_	_
,	_	_
E.	_	_
,	_	_
Donahue	_	_
,	_	_
J.	_	_
,	_	_
Karayev	_	_
,	_	_
S.	_	_
,	_	_
Long	_	_
,	_	_
J.	_	_
,	_	_
Girshick	_	_
,	_	_
R.	_	_
,	_	_
Guadarrama	_	_
,	_	_
S.	_	_
,	_	_
Darrell	_	_
,	_	_
T.	_	_
,	_	_
2014	_	_
.	_	_

#400
Caffe	_	_
:	_	_
Convolutional	_	_
architecture	_	_
for	_	_
fast	_	_
feature	_	_
embedding	_	_
.	_	_

#401
arXiv	_	_
preprint	_	_
arXiv:1408.5093	_	_
.	_	_

#402
[	_	_
23	_	_
]	_	_
Kingma	_	_
,	_	_
D.P.	_	_
,	_	_
Ba	_	_
,	_	_
J.	_	_
,	_	_
2014	_	_
.	_	_

#403
Adam	_	_
:	_	_
A	_	_
method	_	_
for	_	_
stochastic	_	_
optimization	_	_
.	_	_

#404
CoRR	_	_
abs/1412.6980	_	_
.	_	_

#405
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1412.6980	_	_
.	_	_

#406
[	_	_
24	_	_
]	_	_
Long	_	_
,	_	_
J.	_	_
,	_	_
Shelhamer	_	_
,	_	_
E.	_	_
,	_	_
Darrell	_	_
,	_	_
T.	_	_
,	_	_
2015	_	_
.	_	_

#407
Fully	_	_
convolutional	_	_
networks	_	_
for	_	_
semantic	_	_
segmentation	_	_
,	_	_
in	_	_
:	_	_
The	_	_
IEEE	_	_
Conference	_	_
on	_	_
Computer	_	_
Vision	_	_
and	_	_
Pattern	_	_
Recognition	_	_
(	_	_
CVPR	_	_
)	_	_
.	_	_

#408
[	_	_
25	_	_
]	_	_
Mishkin	_	_
,	_	_
D.	_	_
,	_	_
Matas	_	_
,	_	_
J.	_	_
,	_	_
2015	_	_
.	_	_

#409
All	_	_
you	_	_
need	_	_
is	_	_
a	_	_
good	_	_
init	_	_
.	_	_

#410
CoRR	_	_
abs/1511.06422	_	_
.	_	_

#411
URL	_	_
:	_	_
http	_	_
:	_	_
//arxiv.org/abs/1511.06422	_	_
.	_	_

#412
[	_	_
26	_	_
]	_	_
Papandreou	_	_
,	_	_
G.	_	_
,	_	_
Chen	_	_
,	_	_
L.C.	_	_
,	_	_
Murphy	_	_
,	_	_
K.	_	_
,	_	_
Yuille	_	_
,	_	_
A.L.	_	_
,	_	_
2015	_	_
.	_	_

#413
Weakly-	_	_
and	_	_
semi-supervised	_	_
learning	_	_
of	_	_
a	_	_
dcnn	_	_
for	_	_
semantic	_	_
image	_	_
segmentation	_	_
,	_	_
in	_	_
:	_	_
ICCV	_	_
.	_	_

#414
[	_	_
27	_	_
]	_	_
Ros	_	_
,	_	_
G.	_	_
,	_	_
Sellart	_	_
,	_	_
L.	_	_
,	_	_
Materzynska	_	_
,	_	_
J.	_	_
,	_	_
Vazquez	_	_
,	_	_
D.	_	_
,	_	_
Lopez	_	_
,	_	_
A.	_	_
,	_	_
2016	_	_
.	_	_

#415
The	_	_
SYNTHIA	_	_
Dataset	_	_
:	_	_
A	_	_
large	_	_
collection	_	_
of	_	_
synthetic	_	_
images	_	_
for	_	_
semantic	_	_
segmentation	_	_
of	_	_
urban	_	_
scenes	_	_
.	_	_

#416
[	_	_
28	_	_
]	_	_
Srivastava	_	_
,	_	_
N.	_	_
,	_	_
Hinton	_	_
,	_	_
G.	_	_
,	_	_
Krizhevsky	_	_
,	_	_
A.	_	_
,	_	_
Sutskever	_	_
,	_	_
I.	_	_
,	_	_
Salakhutdinov	_	_
,	_	_
R.	_	_
,	_	_
2014	_	_
.	_	_

#417
Dropout	_	_
:	_	_
A	_	_
simple	_	_
way	_	_
to	_	_
prevent	_	_
neural	_	_
networks	_	_
from	_	_
overfitting	_	_
.	_	_

#418
Journal	_	_
of	_	_
Machine	_	_
Learning	_	_
Research	_	_
15	_	_
,	_	_
1929–1958	_	_
.	_	_

#419
URL	_	_
:	_	_
http	_	_
:	_	_
//jmlr	_	_
.	_	_

#420
org/papers/v15/srivastava14a.html	_	_
.	_	_

#421
[	_	_
29	_	_
]	_	_
Zhu	_	_
,	_	_
J.Y.	_	_
,	_	_
Park	_	_
,	_	_
T.	_	_
,	_	_
Isola	_	_
,	_	_
P.	_	_
,	_	_
Efros	_	_
,	_	_
A.A.	_	_
,	_	_
2017	_	_
.	_	_

#422
Unpaired	_	_
image-to-image	_	_
translation	_	_
using	_	_
cycle-consistent	_	_
adversarial	_	_
networks	_	_
.	_	_

#423
arXiv	_	_
preprint	_	_
arXiv:1703.10593	_	_
.	_	_